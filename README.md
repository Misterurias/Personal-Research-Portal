**Personal Research Portal (PRP) --- AI Coding Assistant Impact Study**
=====================================================================

Author: Jorge Urias\
Course: AI Model Development\
Phases: 2 (Research-Grade RAG) + 3 (Research Portal Product)

* * * * *

ğŸ“Œ Research Domain
==================

Impact of AI coding assistants (e.g., GitHub Copilot, CodeWhisperer, ChatGPT) on:

-   Developer productivity
-   Code quality
-   Skill formation
-   Technical debt
-   Long-term maintainability

* * * * *

ğŸ¯ Main Research Question
=========================

> How do AI coding assistants affect productivity, code quality, and long-term software engineering outcomes?

* * * * *

ğŸ§  Phase Overview
=================

Phase 2 --- Research-Grade RAG
----------------------------

-   Structured citations
-   Traceable chunk-level evidence
-   Logged retrieval + generation
-   Reproducible evaluation pipeline
-   Trust safeguards (no fabricated citations)

Phase 3 --- Research Portal Product
---------------------------------

-   Streamlit-based research interface
-   Threaded research sessions
-   Exportable artifacts
-   Evaluation harness for product-level testing
-   Persistent logging and traceability

* * * * *

ğŸš€ Full Project Setup (Grader Instructions)
===========================================

1ï¸âƒ£ Clone Repository
--------------------
```
git clone https://github.com/Misterurias/Personal-Research-Portal.git
cd PersonalResearchProject
```
* * * * *

2ï¸âƒ£ Create Python Environment
-----------------------------
```
python -m venv .venv\
source .venv/bin/activate\
pip install -r requirements.txt
```
All dependencies are pinned for reproducibility.

* * * * *

3ï¸âƒ£ Install Local LLM (Ollama Required)
---------------------------------------

This project uses a fully local LLM via Ollama for reproducible generation.

Install Ollama:

<https://ollama.com>

Pull the required model:
```
ollama pull llama3
```
Verify it works:
```
ollama run llama3
```
The system assumes Ollama is running at:
```
http://localhost:11434
```
* * * * *

ğŸ–¥ï¸ Phase 3 --- Running the Research Portal (Streamlit App)
=========================================================

The main product interface is the Streamlit portal.

From the project root:
```
streamlit run Phase3/app/app.py
```
This launches the research portal in your browser.

* * * * *

What the Streamlit Portal Does
------------------------------

-   Accepts natural-language research questions
-   Retrieves top-k evidence chunks
-   Generates grounded answers with structured citations
-   Maintains threaded research sessions
-   Logs retrieval and generation artifacts
-   Supports exportable outputs

* * * * *

ğŸ“ Where Phase 3 Artifacts Are Stored
-------------------------------------

### ğŸ”¹ Threads (Research Conversations)
```
Phase3/threads/
```
Each JSON file represents a saved research session.

* * * * *

### ğŸ”¹ Snapshots (UI or Research State Snapshots)
```
Phase3/snapshots/
```
Used for capturing reproducible portal states.

* * * * *

### ğŸ”¹ Outputs (Exports / Generated Artifacts)
```
Phase3/outputs/
```
Contains:

-   Exported responses
-   Structured research outputs
-   Generated artifacts

* * * * *

### ğŸ”¹ Logs (Portal + Evaluation Logging)
```
Phase3/logs/
```

Includes:

-   Retrieval traces
-   Generation outputs
-   Prompt versions
-   Evaluation runs

* * * * *

ğŸ§ª Phase 3 Evaluation Harness
=============================

Phase 3 includes a product-level evaluation script.

To run the full Phase 3 evaluation:

```
python Phase3/phase3_eval.py
```
This script:

-   Runs predefined evaluation questions
-   Tests citation formatting
-   Checks grounding behavior
-   Logs outputs
-   Stores evaluation artifacts

* * * * *

ğŸ“Š Phase 3 Evaluation Logs Location
-----------------------------------

```
Phase3/logs/phase3_eval/
```

Each evaluation run is versioned by prompt version.

* * * * *

ğŸ§¾ Corpus & Data (Phase 2 Foundation)
=====================================

Corpus metadata:

```
Phase2/data/data_manifest.csv
```

Raw research artifacts:

```
Phase2/data/raw/
```

Processed chunks:

```
Phase2/data/processed/
```

FAISS index and embeddings built during ingestion.

* * * * *

ğŸ” RAG Architecture (Phases 2 & 3)
==================================

1.  PDF ingestion and cleaning
2.  Section-aware chunking
3.  Embedding with sentence-transformers
4.  FAISS vector index
5.  Top-k retrieval
6.  Local LLM generation (Ollama)
7.  Structured citations: (source_id, chunk_id)
8.  Logging of:
    -   Query
    -   Retrieved chunks
    -   Prompt version
    -   Model output

* * * * *

ğŸ›¡ï¸ Trust Safeguards
====================

-   No fabricated citations allowed
-   Strict abstention when insufficient evidence
-   Structured citation enforcement
-   Logged chunk IDs for traceability
-   Citation regex validation in evaluation
-   Versioned prompt control

* * * * *

ğŸ“ Full Repository Structure (Phases 2 + 3)
===========================================
```
â”œâ”€â”€ Jorge_Urias_Phase3_Report.pdf
â”œâ”€â”€ Phase1
â”‚   â”œâ”€â”€ AI_Usage
â”‚   â”‚   â”œâ”€â”€ Phase1AIUsageDiscussion.pdf
â”‚   â”‚   â”œâ”€â”€ Phase1AIUsageScoringDiscussion.pdf
â”‚   â”‚   â””â”€â”€ Phase1FullAIUsage.pdf
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ deliverables
â”‚   â”‚   â”œâ”€â”€ Phase1_Evaluation_Test_Cases.csv
â”‚   â”‚   â”œâ”€â”€ Phase1_Model_Evaluation.csv
â”‚   â”‚   â”œâ”€â”€ Phase1_Prompt_Template.pdf
â”‚   â”‚   â””â”€â”€ Phase1_Research_Portal.pdf
â”‚   â””â”€â”€ prompts
â”‚       â”œâ”€â”€ claim_evidence_extraction.md
â”‚       â””â”€â”€ paper_triage.md
â”œâ”€â”€ Phase2
â”‚   â”œâ”€â”€ AI_Usage
â”‚   â”‚   â””â”€â”€ Phase2AIUsageDiscussion.pdf
â”‚   â”œâ”€â”€ AI_Usage.md
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â””â”€â”€ run_query.cpython-313.pyc
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ data_manifest.csv
â”‚   â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â””â”€â”€ raw
â”‚   â”œâ”€â”€ docs
â”‚   â”‚   â””â”€â”€ JorgeUrias_Phase2_Report.pdf
â”‚   â”œâ”€â”€ extract_for_grading.py
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”œâ”€â”€ eval_runs
â”‚   â”‚   â”œâ”€â”€ evaluation_results
â”‚   â”‚   â””â”€â”€ runs
â”‚   â”œâ”€â”€ run_phase2.py
â”‚   â”œâ”€â”€ run_query.py
â”‚   â””â”€â”€ src
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __pycache__
â”‚       â”œâ”€â”€ eval
â”‚       â”œâ”€â”€ ingest
â”‚       â””â”€â”€ rag
â”œâ”€â”€ Phase3
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ app
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â””â”€â”€ phase3_eval
â”‚   â”œâ”€â”€ outputs
â”‚   â”‚   â”œâ”€â”€ artifacts
â”‚   â”‚   â””â”€â”€ exports
â”‚   â”œâ”€â”€ phase3_eval.py
â”‚   â”œâ”€â”€ snapshots
â”‚   â”‚   â”œâ”€â”€ Screenshot 2026-03-01 at 10.43.49â€¯PM.png
â”‚   â”‚   â”œâ”€â”€ Screenshot 2026-03-01 at 11.06.48â€¯PM.png
â”‚   â”‚   â”œâ”€â”€ Screenshot 2026-03-01 at 11.07.01â€¯PM.png
â”‚   â”‚   â”œâ”€â”€ Screenshot 2026-03-01 at 11.07.15â€¯PM.png
â”‚   â”‚   â”œâ”€â”€ Screenshot 2026-03-01 at 11.08.25â€¯PM.png
â”‚   â”‚   â””â”€â”€ Screenshot 2026-03-01 at 9.44.11â€¯PM.png
â”‚   â””â”€â”€ threads
â”‚       â”œâ”€â”€ 1305abff.json
â”‚       â”œâ”€â”€ 25304bcd.json
â”‚       â”œâ”€â”€ 36da15db.json
â”‚       â”œâ”€â”€ 4c41b85e.json
â”‚       â”œâ”€â”€ 5655720b.json
â”‚       â”œâ”€â”€ 60898815.json
â”‚       â”œâ”€â”€ 61f4f25f.json
â”‚       â”œâ”€â”€ 685ea13c.json
â”‚       â”œâ”€â”€ 6946031d.json
â”‚       â”œâ”€â”€ 741611ed.json
â”‚       â”œâ”€â”€ 7aef4a86.json
â”‚       â”œâ”€â”€ 80fd23c5.json
â”‚       â”œâ”€â”€ 81801b40.json
â”‚       â”œâ”€â”€ 8314b9c4.json
â”‚       â”œâ”€â”€ 8830f637.json
â”‚       â”œâ”€â”€ 8cc8495a.json
â”‚       â”œâ”€â”€ adfd9376.json
â”‚       â”œâ”€â”€ b2dc5454.json
â”‚       â”œâ”€â”€ bc558311.json
â”‚       â””â”€â”€ ff01cc8c.json
â”œâ”€â”€ README.md
â”œâ”€â”€ logs
â”‚   â””â”€â”€ runs
â”‚       â””â”€â”€ v2-trust-tightened-qrewrite
â””â”€â”€ requirements.txt
```

* * * * *

ğŸ§ª Reproducibility Notes
========================

-   All dependencies pinned
-   Local LLM specified
-   All evaluation runs logged
-   Artifacts persisted to disk
-   No external APIs required
-   Fully reproducible on local machine

* * * * *

âš ï¸ Common Issues
================

If Streamlit does not launch:

-   Ensure `.venv` is activated
-   Ensure `requirements.txt` is installed
-   Ensure Ollama is running

If model fails:

```
ollama list
```

Confirm `llama3` is installed.

* * * * *

ğŸ“„ Deliverables Included
========================

-   Research-grade RAG system (Phase 2)
-   Trust-enhanced RAG architecture
-   Streamlit research portal (Phase 3)
-   Evaluation harness (Phase 3)
-   Logs and traceable artifacts
-   Reports (Phase 2 + Phase 3)
-   AI usage disclosure

* * * * *

ğŸ“š AI Usage Disclosure
======================

See:

```
Phase2/AI_USAGE_LOG.md
```
* * * * *

ğŸ‘¨â€ğŸ’» Author
============

Jorge Urias\
AI Model Development --- Personal Research Portal\
Carnegie Mellon University