
AI Usage Disclosure: Phase 3 (Research Portal Product)
======================================================


Overview
--------

AI tools were used during Phase 3 primarily as development assistants and structural advisors while building the Research Portal product layer (Streamlit interface, evaluation harness, logging system, and export mechanisms).

All architectural decisions, evaluation logic, logging behavior, and trust safeguards were manually designed, implemented, and validated by the project author. The author retains full responsibility for correctness, reproducibility, and compliance with Phase 3 requirements.

Phase 3 extends Phase 2's research-grade RAG into a usable product. AI assistance was used to accelerate implementation --- not to automate design, evaluation, or grading decisions.

* * * * *

1\. Product Development Assistance
----------------------------------

**Tool Used:** ChatGPT (OpenAI)\
**Purpose:**

-   Debugging Streamlit application behavior

-   Refining prompt system rules for structured citation enforcement

-   Assisting with evaluation harness design (`phase3_eval.py`)

-   Improving JSON logging schema

-   Structuring thread persistence and export formatting

-   Troubleshooting Git issues during branch restoration

**Manual Modifications:**

-   All Streamlit app logic was manually integrated into the Phase 2 RAG system.

-   Thread storage, snapshot behavior, and artifact exports were manually implemented.

-   Prompt system versions were manually controlled and evaluated.

-   Evaluation metrics (`citations_valid`, `references_consistent`, `insufficient`) were manually defined and tested.

-   All logging paths and directory structures were manually verified.

No auto-generated UI or template frameworks were blindly adopted without review.

* * * * *

2\. Prompt Engineering & System Rule Iteration
----------------------------------------------

**Tool Used:** ChatGPT\
**Purpose:**

-   Brainstorming stricter system rule enforcement

-   Testing alternative citation formatting constraints

-   Refining abstention behavior wording

-   Exploring comparative prompt variations (Prompt 1, Prompt 2, Prompt 3)

**Manual Decisions:**

-   Definition of strict citation formatting `(source_id, chunk_id)`

-   Enforcement of exact `References:` block structure

-   Regex-based citation validation

-   Designing multi-version prompt comparison

-   Selecting evaluation query set for product testing

All prompt variations were tested through the evaluation harness and results were logged objectively. The final prompt choice was based on measurable metrics (citation validity, reference consistency, insufficiency rate).

* * * * *

3\. Evaluation Framework Expansion (Phase 3 Requirement)
--------------------------------------------------------

**Tool Used:** ChatGPT\
**Purpose:**

-   Suggesting structure for Phase 3 evaluation logging

-   Assisting in expanding evaluation outputs to include model responses and references

-   Reviewing evaluation summary formatting

**Manual Implementation:**

-   Evaluation JSON schema defined and coded manually

-   Citation validation logic implemented manually

-   Reference consistency checking implemented manually

-   All evaluation results generated by the system and not manually edited

Evaluation results reflect actual system outputs across prompt versions. No evaluation metrics were fabricated or altered.

* * * * *

4\. Documentation & Report Drafting
-----------------------------------

**Tool Used:** ChatGPT\
**Purpose:**

-   Structuring README updates

-   Drafting report sections in a formal academic tone

-   Formatting architecture descriptions

-   Drafting AI usage disclosures

**Manual Modifications:**

-   All claims in the final report reflect real implementation details.

-   Architectural diagrams and descriptions match actual repository structure.

-   Evaluation summaries are derived directly from logged JSON outputs.

-   All Phase 3 product features described in the report exist in the repository.

No technical claims were generated without verification.

* * * * *

5\. What AI Was NOT Used For
----------------------------

AI was **not** used to:

-   Generate corpus documents

-   Modify or fabricate research sources

-   Create citations outside the allowed evidence chunks

-   Alter retrieval outputs

-   Bypass reproducibility requirements

-   Inflate evaluation results

-   Write evaluation JSON logs manually

All answers produced by the portal are generated through the local RAG pipeline using:

-   Local embeddings

-   Local FAISS index

-   Local Ollama-hosted LLM

All citations resolve to real stored text in:

```
Phase2/data/raw/
Phase2/data/processed/

```

* * * * *

6\. Responsibility Statement
----------------------------

The author confirms that:

-   All research answers originate from the project's local corpus.

-   All evaluation metrics reflect actual model behavior.

-   All prompt variations were tested objectively.

-   No evaluation outputs were edited after generation.

-   AI-assisted suggestions were critically evaluated before integration.

-   The final Phase 3 product meets reproducibility and traceability requirements independent of AI tools.

* * * * *

7\. Summary of AI Role in Phase 3
---------------------------------

AI was used as:

-   A development assistant

-   A debugging tool

-   A structural reviewer

-   A writing assistant for documentation

AI was not used as:

-   An automated researcher

-   A citation generator

-   An evaluator

-   A grading tool

All intellectual responsibility, verification, and validation remain with the author.

* * * * *
