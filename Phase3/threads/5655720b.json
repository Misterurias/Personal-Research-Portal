{
  "thread_id": "5655720b",
  "timestamp": "2026-03-02T04:45:18+00:00",
  "query": "What impact do AI coding assistants have on code quality?",
  "answer": "What impact do AI coding assistants have on code quality?\n\nThe study \"Empirical Analysis of AI-Assisted Code Generation Tools: Impact on Code Quality, Security and Developer Productivity\" (2025) found that the use of AI-assisted code generation tools leads to a 31.4% increase in average developer productivity, while also introducing 23.7% more security vulnerabilities in generated codes. The study analyzed the quality metrics of code generated by three prominent AI-assisted code generation tools: GitHub Copilot, Amazon CodeWhisperer, and ChatGPT.\n\nThe analysis revealed that the latest versions of these tools generate correct code 65.2%, 46.3%, and 31.1% of the time, respectively. The study also found that the average technical debt, considering code smells, was 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes for Amazon CodeWhisperer.\n\nIn addition, another study \"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models\" (2022) found that AI-assisted code generation tools can improve code quality, but may also introduce new challenges and complexities for developers.\n\nReferences:\n(EnterpriseImpact2024, EnterpriseImpact2024_chunk_023)\n(EmpiricalToolAnalysis2025, EmpiricalToolAnalysis2025_chunk_001)\n\nNote: The references only include the citations used in the answer.",
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.7027324438095093,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023",
      "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1\u20137.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists\u2019 perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1\u201324.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402\u2013412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369\u2013391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172\u2013178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24\u201329, 2021, Proceedings, Part I 23. Springer, 171\u2013190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1\u201347.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585\u20131608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot\u2019s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62\u201371.\n[59] Ramaz"
    },
    {
      "rank": 2,
      "distance_l2": 0.7332595586776733,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website:"
    },
    {
      "rank": 3,
      "distance_l2": 0.7352527379989624,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_001",
      "text": "Examining the Use and Impact of an AI Code Assistant on Developer\nProductivity and Experience in the Enterprise\nJUSTIN D. WEISZ, IBM Research, USA\nSHRADDHA KUMAR\u2217, Cisco Systems, Inc., India\nMICHAEL MULLER, IBM Research, USA\nKAREN-ELLEN BROWNE, IBM Software, Ireland\nARIELLE GOLDBERG, IBM Infrastructure, USA\nELLICE HEINTZE, IBM Software, Germany\nSHAGUN BAJPAI, IBM Software, India\nAI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and\ntesting code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within\nIBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers\u2019 experiences\nwith WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their\nexpectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated\ncode. Our case study characterizes the impact of an LLM-powered assistant on developers\u2019 perceptions of productivity and it shows\nthat although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.\nCCS Concepts: \u2022 Human-centered computing \u2192Empirical studies in HCI; Field studies; \u2022 Software and its engineering \u2192\nCollaboration in software development; Automatic programming.\nAdditional Key Words and Phrases: Generative AI, LLM, software engineering, productivity, code assistant\nACM Reference Format:\nJustin D. Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. 2025.\nExamining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA \u201925), April 26-May 1, 2025, Yokohama, Japan. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3706599.3706670\n1\nIntroduction\nAI assistants powered by large language models (LLMs) are becoming increasingly prevalent in the workplace. A\nnumber of commercial and open-source coding assistants have been released for software engineers, developers, and\n\u2217Work conducted while an employee of IBM Software, Kochi, India.\nAuthors\u2019 Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen,"
    },
    {
      "rank": 4,
      "distance_l2": 0.7362462878227234,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yeti\u015ftiren \u00b7 I\u015f\u0131k \u00d6zsoy \u00b7 Miray\nAyerdem \u00b7 Eray T\u00fcz\u00fcn\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI\u2019s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yeti\u015ftiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nI\u015f\u0131k \u00d6zsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray T\u00fcz\u00fcn\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yeti\u015ftiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers\u2019 per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 5,
      "distance_l2": 0.7374346852302551,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_028",
      "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI \u201922 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1\u20137. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot\u2019s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE \u201922), November 07, 2022. ACM, Singapore, Singapore, 62\u201371. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yeti\u015ftiren, I\u015f\u0131k \u00d6zsoy, Miray Ayerdem, and Eray T\u00fcz\u00fcn. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n\u201923), August 06, 2023. ACM, Long Beach, CA, USA, 5673\u20135684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21\u201329. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
    }
  ],
  "citations": [
    {
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023"
    },
    {
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001"
    }
  ],
  "prompt_version": "v2.5-testing_system_rules_prompt2",
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding assistants on code quality.",
  "citations_valid": true,
  "references_consistent": true
}