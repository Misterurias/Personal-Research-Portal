[
  {
    "chunk_id": "AIProdDecrease2024_chunk_001",
    "source_id": "AIProdDecrease2024",
    "text": "AI-Assisted Programming Decreases the Productivity of\nExperienced Developers by Increasing the Technical Debt\nand Maintenance Burden\nFeiyang (Amber) Xu, Poonacha K. Medappa, Murat M. Tunc\nMartijn Vroegindeweij, Jan C. Fransoo\nTilburg University, the Netherlands\nf.xu_1@tilburguniversity.edu, p.k.medappa@tilburguniversity.edu, m.m.tunc@tilburguniversity.edu\nw.m.vroegindeweij@tilburguniversity.edu, jan.fransoo@tilburguniversity.edu\nGenAI solutions like GitHub Copilot have been shown to increase the productivity of software developers. Yet prior\nwork remains unclear on the quality of code produced and the challenges of maintaining it in software projects. If quality\ndeclines as volume grows, technical debt accumulates as experienced developers face increased workloads reviewing\nand reworking code from less-experienced contributors. We analyze developer activity in Open Source Software (OSS)\nprojects following the introduction of GitHub Copilot. We find that productivity indeed increases. However, the increase\nin productivity is primarily driven by less-experienced (peripheral) developers. We also find that code written after the\nadoption of AI requires more rework to satisfy repository standards, indicating a potential increase in technical debt.\nImportantly, the added rework burden falls on the more experienced (core) developers, who review 6.5% more code after\nCopilot\u2019s introduction, but show a 19% drop in their original code productivity. More broadly, this finding raises caution\nthat productivity gains of AI may mask the growing burden of maintenance on a shrinking pool of experts, together with\nincreased technical debt for the projects. The results highlight a fundamental tension in AI-assisted software development\nbetween short-term productivity gains and long-term system sustainability.\nKey words: GenAI, GitHub Copilot, Open Source Software, Software Maintenance, Technical Debt,\nDifference-in-Differences\n1.\nIntroduction\nHow will AI shape the future of knowledge-intensive industries? This question has taken on renewed\nsignificance with the recent rise of Genarative AI (GenAI) technologies, which are becoming an integral part\nof daily operations of software development, scientific research, healthcare and other expert-driven fields.\nA prominent example is GitHub Copilot, an AI-powered coding assistant designed to support developers by\ngenerating code suggestions and accelerating routine programming tasks (Peng et al. 2023). When GitHub\nlaunched Copilot, it was introduced as \u201cyour AI pair programmer,\" emphasizing not only its role as an\nautomation tool but also as a team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot\u2019s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_002",
    "source_id": "AIProdDecrease2024",
    "text": " team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot\u2019s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming tasks 55.8% faster (Peng\net al. 2023). Such productivity benefits lead to promises of faster time-to-market and increased revenue\nfor organizations developing software applications. Considering these shifts, major tech organizations have\nstarted to increasingly rely on AI in their projects - \u201cmore than a quarter of all new code at Google is gen-\nerated by AI, then reviewed and accepted by engineers,\" reported Google CEO Sundar Pichai in January,\n2025.1 Moreover, Microsoft CTO Kevin Scott expects that 95% of all code will be AI-generated by 2030.2\nWhile these productivity gains are promising, they also raise important questions about the quality and\nmaintainability of AI-generated code. Because AI tools can lower the skill barrier for writing code (Dakhel\net al. 2023), AI tools enable broader participation but may also encourage developers to rely on gener-\nated solutions without fully understanding the underlying design rationale and potential integration issues\n(Barrett et al. 2023). Such reliance increases the likelihood of quick fixes that favor short-term function-\nality over long-term maintainability (Barrett et al. 2023). Extant literature characterizes \u201cquick and dirty\u201d\nsoftware customizations made without a complete understanding of their future implications as technical\ndebt, as they undermine system reliability and impose long-term maintenance obligations (Kruchten et al.\n2012, Brown et al. 2010, Banker et al. 2021). As a result, project maintainers must devote additional effort\nto understanding, reviewing, and reworking AI-generated code before it can be safely integrated. In our\ncontext, we contend that the growing reliance on AI-assisted development may accelerate the accumulation\nof technical debt, as design shortcuts taken to expedite system deployment become embedded in software\nsystems (Ramasubbu and Kemerer 2016, 2021).\nThe technical debt and maintenance challenges that AI poses are expected to be especially pronounced\nin distributed software development teams, such as in Open Source Software (OSS) communities. In these\ncommunities, contributors from around the world collaborate, often voluntarily, to develop and maintain\nsoftware that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making\nit freely or cheaply available to the public (Eghbal 2020, Nagle 2019). Despite the voluntary nature of\nwork in these communities, OSS constitutes critical digital infrastructure for modern society, with estimates\nsuggesting that the total cost of reproducing this software would amount to $8.8 trillion (Hoffmann et al.\n2024).3 Given"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_003",
    "source_id": "AIProdDecrease2024",
    "text": " challenges that AI poses are expected to be especially pronounced\nin distributed software development teams, such as in Open Source Software (OSS) communities. In these\ncommunities, contributors from around the world collaborate, often voluntarily, to develop and maintain\nsoftware that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making\nit freely or cheaply available to the public (Eghbal 2020, Nagle 2019). Despite the voluntary nature of\nwork in these communities, OSS constitutes critical digital infrastructure for modern society, with estimates\nsuggesting that the total cost of reproducing this software would amount to $8.8 trillion (Hoffmann et al.\n2024).3 Given this critical role for both firms and society, the growing adoption of AI in OSS communities\nraises important questions about its broader impact. On the one hand, AI tools can lower the barrier for\nperipheral contributors (relatively newer contributors to the project who come from the community of users\nof software; Rullani and Haefliger 2013) to contribute to these software projects. On the other hand, this\nsurge in AI-assisted contributions may result in a build up of technical debt and increase the maintenance\n1 https://www.technologyreview.com/2025/01/20/1110180/the-second-wave-of-ai-coding-is-here/\n2 https://www.businessinsider.com/microsoft-cto-ai-generated-code-software-developer-job-change-2025-4\n3 These estimates suggests that firms would spend approximately 3.5 times more on software than they currently do if OSS did not\nexist (Hoffmann et al. 2024).\n3\nobligations of the software (Banker et al. 2021). If this mechanism holds, the performance consequences of\ntechnical debt are likely to intensify over time, as mounting maintenance burdens increasingly divert expert\nattention away from productive development activities, thereby amplifying the negative impact of technical\ndebt on project and firm performance.\nThis study seeks to address this tension. We can distinguish two important types of activities in OSS\nprojects - development and maintenance. The development activity in OSS projects involves producing\ncode and submitting pull requests (PRs) to the project. The open nature of OSS communities allows any-\none\u2014regardless of skill or experience - to contribute to this process (Rullani and Haefliger 2013). In\ncontrast, maintenance involves reactive tasks such as triaging issues, reviewing submitted PRs, suggesting\nmodifications or corrections, and ensuring that contributions align with project standards (Banker et al.\n2021, Ramasubbu and Kemerer 2021). These maintenance tasks are generally less intrinsically driven and\ntypically assigned to contributors who possess both technical expertise and trust of the community to ensure\nthe quality and reliability of the codebase (Medappa et al. 2023, Eghbal 2020). Thus, we seek to answer the\nquestion: How does AI-assisted programming impact the development and maintenance activities and the\ntechnical debt of the OSS projects?\nIn this study, we examine whether technical debt and maintenance efforts of OSS projects changed after\nthe introduction of GitHub Copilot through increased code review and rework effort on PRs. To empirically\ntest this, we exploit the release of GitHub Cop"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_004",
    "source_id": "AIProdDecrease2024",
    "text": "1, Ramasubbu and Kemerer 2021). These maintenance tasks are generally less intrinsically driven and\ntypically assigned to contributors who possess both technical expertise and trust of the community to ensure\nthe quality and reliability of the codebase (Medappa et al. 2023, Eghbal 2020). Thus, we seek to answer the\nquestion: How does AI-assisted programming impact the development and maintenance activities and the\ntechnical debt of the OSS projects?\nIn this study, we examine whether technical debt and maintenance efforts of OSS projects changed after\nthe introduction of GitHub Copilot through increased code review and rework effort on PRs. To empirically\ntest this, we exploit the release of GitHub Copilot as a technical preview in June 2021, which included lim-\nited programming language endorsement. We focus on OSS projects owned by Microsoft, as the company\nhad exclusive access to OpenAI\u2019s GPT-3, the model powering GitHub Copilot during its technical preview,\ndue to its investment in OpenAI and its prior acquisition of GitHub.4,5 The individual users in our dataset\nare contributors to Microsoft-owned OSS projects. We estimate the effect of Copilot at both the project\nand contributor levels using a Difference-in-Differences (DiD) design. Treatment and control groups were\ndefined based on the primary programming language: those using Copilot-endorsed languages formed the\ntreatment group, while non-endorsed language users served as the control (Yeverechyahu et al. 2024). For\nboth project and contributor levels, we collected data on programming activities and aggregated them at the\nmonthly level.\nWe examine the changes in code productivity after Copilot by three measures: lines of code added,\ncommits 6 and PRs submitted to the project. To capture the secondary effects of AI adoption on technical\ndebt and maintenance effortsof OSS communities, we focused on two complementary outcomes. First, we\n4 https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-\nmodel/\n5 https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-\nengineering-experience-using-generative-ai\n6 A commit is the fundamental unit of change on GitHub. Similar to saving a file that\u2019s been edited, a commit records changes to\none or more files on GitHub - https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-\ncommits/about-commits\n4\nmeasured technical debt at the project level using PR rework, which reflects the extent to which initially\nsubmitted contributions require modification before being integrated into the codebase (Ramasubbu and\nKemerer 2016, 2021). Higher levels of rework indicate greater reliance on expedient or insufficiently inte-\ngrated solutions. Second, we measured maintenance effort at the individual level using PR reviews, which\ncapture the time and effort required from contributors to evaluate, correct, and integrate submitted code\n(Medappa et al. 2023).\nBased on our analysis of a large-scale panel dataset from GitHub, we find that while AI adoption leads to\nproductivity gains, they also increase maintenance-related activities"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_005",
    "source_id": "AIProdDecrease2024",
    "text": "its\n4\nmeasured technical debt at the project level using PR rework, which reflects the extent to which initially\nsubmitted contributions require modification before being integrated into the codebase (Ramasubbu and\nKemerer 2016, 2021). Higher levels of rework indicate greater reliance on expedient or insufficiently inte-\ngrated solutions. Second, we measured maintenance effort at the individual level using PR reviews, which\ncapture the time and effort required from contributors to evaluate, correct, and integrate submitted code\n(Medappa et al. 2023).\nBased on our analysis of a large-scale panel dataset from GitHub, we find that while AI adoption leads to\nproductivity gains, they also increase maintenance-related activities due to a higher volume of review and\nrework needed per PR. Specifically, our analysis reveals a double-edged effect of GitHub Copilot on OSS\ndevelopment. At the project level, Copilot adoption is associated with a significant boost in productivity:\nprojects that supported Copilot saw increases in lines of code added, commits, and PRs. However, this surge\nin contributions came also with an increase in PR rework (2.4% more code revisions), indicating a possible\ndecline in the quality of code initially submitted resulting in an accumulation of technical debt. At the\ncontributor level, we observe an important redistribution of effort: the peripheral contributors (the less active\ncontributors to the projects) increased their development activity, taking advantage of Copilot\u2019s ability to\nlower coding barriers. Specifically, peripheral contributors, particularly those in the bottom percentiles in\nterms of their previous contributions, increased their commit activity by 43.5% and submitted 17.7% more\nPRs. In contrast, the core contributors reduced their commit activity by 19%, shifting their focus toward\nreviewing and maintaining code (a 6.5% increase), and shouldering a heavier quality assurance burden.\nTogether, these findings highlight how AI can enable broader participation in OSS, but also raise concerns\nabout the sustainability of these gains and the strain placed on a shrinking pool of experienced contributors\nwho maintain quality in OSS projects.\n2.\nRelated Literature and Conceptual Development\n2.1.\nAI Assisted Code Development in OSS Projects\nThe rapid advancement of GenAI technologies, particularly LLMs like ChatGPT and GitHub Copi-\nlot, is transforming how software is developed and how online knowledge communities operate. Current\nresearch on AI-assisted code development has shown the substantial impact of the technology on pro-\nductivity. A study to assess the productivity benefits of using Copilot revealed that developers who use\nCopilot completed their programming task 55.8% faster than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers\u2019 confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_006",
    "source_id": "AIProdDecrease2024",
    "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers\u2019 confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers\u2019 effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation\u2019s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that \u201cgeneral housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors\u2019 time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there\u2019s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and \u201cvibe coding\u201d practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed\u2013quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_007",
    "source_id": "AIProdDecrease2024",
    "text": " has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed\u2013quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid progress and is often perceived as highly efficient by its users, the resulting code is fre-\nquently described as fast but flawed. Moreover, quality assurance practices are commonly deprioritized in\nvibe coding workflows, with users skipping testing, accepting AI-generated outputs with little modification,\nor delegating verification back to the AI itself. These patterns raise concerns about latent defects and height-\nened maintenance demands, particularly in collaborative and production-oriented software environments\n(Schreiber and Tippe 2025).\n6\nThese maintenance-related concerns are amplified by evidence that less experienced programmers appear\nto benefit more from the use of Copilot. For novice developers, AI-driven code suggestions can help them\novercome steep learning curves and contribute even to mature OSS projects. However, both Dakhel et al.\n(2023) and Peng et al. (2023) caution that novice programmers may place undue trust in AI-generated\ncode without fully understanding its broader design or integration implications, increasing the likelihood of\ndownstream quality and integration challenges. Over time, these issues can accumulate as technical debt,\nincreasing the maintenance burden for OSS projects and shifting responsibility toward a small group of\nexperienced core contributors. Anecdotal evidence7 suggests that while AI-assisted coding can produce\nseemingly complete solutions with minimal effort, these outputs frequently exhibit substantive deficiencies\nwhen reviewed by experienced developers. Furthermore, Schreiber and Tippe (2025) show that even when\nAI-generated code passes functional tests, it often embeds latent security weaknesses and maintainability\nissues that shift effort from development to downstream debugging and remediation, effectively accumulat-\ning technical debt. Relatedly, the growing volume of low-quality, AI-generated security artifacts has begun\nto strain OSS security workflows. In January 2026, the cURL open-source project discontinued its bug\nbounty program after being overwhelmed by a flood of AI-generated vulnerability reports that consumed\nreviewer attention without yielding actionable findings 8. In other words, the ease of generating code masks\nunderlying structural and logical gaps that become apparent under expert scrutiny (Barrett et al. 2023). As\nwith other AI tools that create superficially polished but potentially shallow results, vibe coding may be\nappealing for rapid prototyping, yet the secondary effects that this may have on reliability or maintainability\nwhen applied to nontrivial software development tasks is an open question.\nOur study investigates this issue by analyzing how the adoption of GitHub Copilot reshapes contributor\neffort across production and maintenance activities, and whether it disproportionately shifts the burden of\nmaintenance onto a smaller group of core contributors.\n2.2.\nConceptualization of Technical Debt\nTechnical debt captures the inter"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_008",
    "source_id": "AIProdDecrease2024",
    "text": ". In other words, the ease of generating code masks\nunderlying structural and logical gaps that become apparent under expert scrutiny (Barrett et al. 2023). As\nwith other AI tools that create superficially polished but potentially shallow results, vibe coding may be\nappealing for rapid prototyping, yet the secondary effects that this may have on reliability or maintainability\nwhen applied to nontrivial software development tasks is an open question.\nOur study investigates this issue by analyzing how the adoption of GitHub Copilot reshapes contributor\neffort across production and maintenance activities, and whether it disproportionately shifts the burden of\nmaintenance onto a smaller group of core contributors.\n2.2.\nConceptualization of Technical Debt\nTechnical debt captures the intertemporal trade-off between short-term development speed and long-term\nsystem maintainability (Cunningham 1992). A growing body of research has established that technical debt\nis economically consequential, persistent, and shaped by organizational choices. Drawing on the informa-\ntion systems and computer science literature, we synthesize prior work in Table 1 and adopt a definition of\ntechnical debt in this study as accumulation of suboptimal code, design, or documentation decisions in PR\nprocess that increase PR Rework (technical debt) and PR Review (maintenance effort) - a definition that is\nparticularly prominent in the era of GenAI software development.\n7 https://stackoverflow.blog/2025/08/07/a-new-worst-coder-has-entered-the-chat-vibe-coding-without-code-knowledge/\n8 https://www.techradar.com/pro/security/curl-will-stop-bug-bounties-program-due-to-avalanche-of-ai-slop\n7\nMeasurement approaches for technical debt have evolved along two complementary axes. One strand\nemphasizes static code metrics (e.g., code complexity, duplication, test coverage) as proxies for debt accu-\nmulation, enabling automated detection and cross-project comparisons (Paramitha and Massacci 2023, Yoo\net al. 2025). Paramitha and Massacci (2023) show how dependency structures in open-source ecosystems\n(top 600 Python packages) amplify technical leverage, allowing localized weaknesses to propagate across\npackages. Yoo et al. (2025) similarly demonstrate that dependency network structures shape the diffusion\nof security vulnerabilities in software supply chains. Another strand operationalizes technical debt through\nprocess and outcome indicators\u2014such as rework rates, defect density, time-to-merge, and maintenance\neffort\u2014thereby connecting debt to developer behavior and organizational performance (Ramasubbu and\nKemerer 2016, Banker et al. 2021, Ramasubbu and Kemerer 2021). Both approaches have value: the former\nmetric-based techniques are scalable and actionable for tooling, while the later process indicators capture\nthe economic and human costs that managers and maintainers ultimately face.\nEmpirical studies across industry and OSS ecosystems consistently find that technical debt impairs long-\nrun productivity and increases maintenance burdens (Paramitha and Massacci 2023). Work in empirical\nsoftware engineering shows that higher measured debt correlates with increased bug rates, longer defect\nresolution times, and reduced velocity for feature delivery (Ramasubbu and Kemerer 2016). Rinta-Kahila\net al. (2023) further demonstrate that organizations can become \u201ctrapped\u201d"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_009",
    "source_id": "AIProdDecrease2024",
    "text": " Ramasubbu and Kemerer 2021). Both approaches have value: the former\nmetric-based techniques are scalable and actionable for tooling, while the later process indicators capture\nthe economic and human costs that managers and maintainers ultimately face.\nEmpirical studies across industry and OSS ecosystems consistently find that technical debt impairs long-\nrun productivity and increases maintenance burdens (Paramitha and Massacci 2023). Work in empirical\nsoftware engineering shows that higher measured debt correlates with increased bug rates, longer defect\nresolution times, and reduced velocity for feature delivery (Ramasubbu and Kemerer 2016). Rinta-Kahila\net al. (2023) further demonstrate that organizations can become \u201ctrapped\u201d in technical debt due to coor-\ndination failures, organizational inertia, and escalating switching costs. The introduction of GenAI tools\nsuch as GitHub Copilot represents a departure from these assumptions (Yeverechyahu et al. 2024). Unlike\nprior productivity-enhancing tools, GenAI dramatically lowers the marginal cost of producing code and\nreduces skill barriers to contribution (Peng et al. 2023). While prior studies document productivity gains\nfrom AI-assisted coding, they provide limited insight into how these gains translate into technical debt and\nmaintenance related challenges.\nFrom a technical debt perspective, GenAI may accelerate debt accumulation by increasing code volume\nwithout proportionate improvements in quality, the integration with the software, or architectural coher-\nence (Pimenova et al. 2025). By lowering the cost of producing code, AI-assisted programming encourages\nrapid iteration and experimentation, but can shift attention away from longer-term concerns such as main-\ntainability, readability, and alignment with existing system design (Barrett et al. 2023, Schreiber and Tippe\n2025). As a result, defects, inconsistencies, and design shortcuts may be introduced more quickly than\nthey can be identified and resolved. Over time, this imbalance can compound, transforming short-term pro-\nductivity gains into persistent maintenance obligations that must be absorbed by experienced developers\n(Eghbal 2020). While GenAI promises to enhance development speed and broaden participation, it may\nsimultaneously intensify the very technical debt that constrains system reliability, scalability, and long-run\nperformance (Ramasubbu and Kemerer 2021). If this logic is valid, we expect that GenAI may lead to a\ngreater accumulation of technical debt in OSS projects.\n8\nTable 1\nSelected Literature on Technical Debt and Software Maintenance\nStudy\nContext\nMethod\nMeasurement\nKey Findings\nBanker et al.\n(2021)\nCustomer\nrelationship\nmanagement\n(CRM)\nsystems in 26 firms\nEconometric analysis\nPercentage of customized codes in the\nCRM system that do not adhere to vendor-\nprescribed standard\nHigher technical debt is associated\nwith\nlower\nfirm\nperformance\nand\nreduced operational efficiency over\ntime.\nRamasubbu\n&\nKemerer\n(2021)\nOutsourced Commercial\nOff-The-Shelf\n(COT)\nenterprise systems\nEconometric analysis\nViolations of the design and programming\nstandards established by the vendor of the\nCOTS enterprise system\nActive remediation policies reduce\nlong-term\nmaintenance\ncosts,\nbut\nexcessive deferral leads to escalating\ntechnical debt.\nRamas"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_010",
    "source_id": "AIProdDecrease2024",
    "text": "Customer\nrelationship\nmanagement\n(CRM)\nsystems in 26 firms\nEconometric analysis\nPercentage of customized codes in the\nCRM system that do not adhere to vendor-\nprescribed standard\nHigher technical debt is associated\nwith\nlower\nfirm\nperformance\nand\nreduced operational efficiency over\ntime.\nRamasubbu\n&\nKemerer\n(2021)\nOutsourced Commercial\nOff-The-Shelf\n(COT)\nenterprise systems\nEconometric analysis\nViolations of the design and programming\nstandards established by the vendor of the\nCOTS enterprise system\nActive remediation policies reduce\nlong-term\nmaintenance\ncosts,\nbut\nexcessive deferral leads to escalating\ntechnical debt.\nRamasubbu\n&\nKemerer\n(2016)\nEnterprise software sys-\ntems\nEconometric analysis\nClient customizations violating vendor\nstandards (business logic / data schema),\nAPI violations\nAccumulated technical debt increases\nsystem failure hazards and shortens\nsystem lifespan.\nParamitha\n& Massacci\n(2023)\nTop 600 Python pack-\nages, Python open-source\necosystem\nBox plot, simulation\nTechnical leverage as the ratio between\ndependencies (other people\u2019s code) and\nown codes of a software package\nTechnical leverage amplifies mainte-\nnance risks through dependency net-\nworks, increasing downstream techni-\ncal debt.\nYoo\net\nal.\n(2025)\nSoftware supply chains\nEconometric analysis\nDependency centrality, vulnerability expo-\nsure\nComplex\ndependency\nstructures\nincrease\nvulnerability\npropagation,\nlinking technical debt to security risks.\nRinta-Kahila\net al. (2023)\nLegacy system replace-\nment\nQualitative case study,\nsystem-dynamics model\nSociotechnical debt indicators: architec-\ntural compromises, technical inertia & dig-\nital options foregone or constrained\nTechnical debt becomes entrenched\nthrough organizational routines, creat-\ning lock-in and delaying moderniza-\ntion.\nRolland\net\nal. (2018)\nDigital platforms in firms\nLongitudinal case analy-\nsis\nThe interactions between digital options\nand digital debt as a contribution to under-\nstanding the complex choices organization\nface in managing digital platforms\nStrategic flexibility often trades off\nwith accumulating digital and techni-\ncal debt.\nThis study\nOSS\nRepositories\non\nGitHub\nEconometric analysis\nAccumulation of suboptimal code, design,\nor documentation decisions in PR process\nthat increase PR Rework (technical debt)\nand PR Review (maintenance effort)\nCode written after the adoption of AI\nrequires more rework to satisfy reposi-\ntory standards\n9\n2.3.\nTechnical Debt in OSS projects\nPrior research on software quality and technical debt has largely focused on proprietary software developed\nwithin organizational boundaries, where development and maintenance are often treated as sequential and\ncentrally coordinated activities (Paramitha and Massacci 2023, Yoo et al. 2025, Ramasubbu and Kemerer\n2016, Banker et al. 2021, Ramasubbu and Kemerer 2021). In contrast, OSS is produced through a distributed\ndevelopment model in which participation is open and contributions can be submitted by anyone, blurring\nthe boundary between development and maintenance and creating distinct challenges for managing quality\n"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_011",
    "source_id": "AIProdDecrease2024",
    "text": " reposi-\ntory standards\n9\n2.3.\nTechnical Debt in OSS projects\nPrior research on software quality and technical debt has largely focused on proprietary software developed\nwithin organizational boundaries, where development and maintenance are often treated as sequential and\ncentrally coordinated activities (Paramitha and Massacci 2023, Yoo et al. 2025, Ramasubbu and Kemerer\n2016, Banker et al. 2021, Ramasubbu and Kemerer 2021). In contrast, OSS is produced through a distributed\ndevelopment model in which participation is open and contributions can be submitted by anyone, blurring\nthe boundary between development and maintenance and creating distinct challenges for managing quality\nand technical debt (Nagle 2019). Therefore, our study focuses on the PR review process, a core coordination\nmechanism in continuous integration and continuous deployment (CI/CD) of software.\nThe development workflow of an OSS project is illustrated in Figure 1. A feature of this development\nworkflow is that it allows multiple contributors to work independently on separate branches (or forks) and\nsubmit their changes for inclusion in the main project branch. This is done through a PR, which serves\nas a formal request to review and merge code contributions (such as new features or improvements) made\nby project contributors. Core or experienced contributors often handle key maintenance tasks: they review\nsubmitted PRs for quality and compliance, suggest improvements or corrections adhere to project standards\n(Banker et al. 2021), and integrate approved changes into the main project. This workflow enables dis-\ntributed development and community-driven innovation, allowing peripheral contributors who come from\nthe community of users of the project to contribute. At the same time, it ensures that the code is reviewed,\nrefined, and improved before merging into the main branch. The effectiveness of this review process has\nhelped OSS achieve remarkably high quality, surpassing proprietary software in metrics like bugs per 1,000\nlines of code.9 However, the success and effectiveness of this \"decentralized-development\" approach hinges\non the core contributors who actively participate in the review-rework process of the PRs submitted by\ndifferent contributors (Rullani and Haefliger 2013).\nFocusing on the PR review process is particularly well suited to OSS contexts, where code is continuously\ndeveloped, reviewed, and deployed into production, often with rapid and frequent version updates (Roberts\net al. 2006, Medappa et al. 2023). By analyzing PR reviews and rework as they occur, our study captures\nmaintenance activities in real time, providing a more accurate representation of how quality control and\ntechnical debt are managed within distributed development workflows. Importantly, the PR review process\nallows us to observe technical debt at the point of entry\u2014when new code is evaluated, revised, or rejected\nprior to integration. This technical debt measurement would be applied to other CI/CD software projects\nbeyond OSS community.\nPrior work conceptualizes technical debt as deferred maintenance or suboptimal design choices that\nincrease future rework costs, typically operationalized through downstream quality degradation or mainte-\nnance outcomes in traditional, organization-bound software development settings (Ramasubbu and Kemerer\n9 https://blogs.worldbank.org/en/opendata/quality-open-source-software-how-many-eyes-are-enough\n"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_012",
    "source_id": "AIProdDecrease2024",
    "text": " how quality control and\ntechnical debt are managed within distributed development workflows. Importantly, the PR review process\nallows us to observe technical debt at the point of entry\u2014when new code is evaluated, revised, or rejected\nprior to integration. This technical debt measurement would be applied to other CI/CD software projects\nbeyond OSS community.\nPrior work conceptualizes technical debt as deferred maintenance or suboptimal design choices that\nincrease future rework costs, typically operationalized through downstream quality degradation or mainte-\nnance outcomes in traditional, organization-bound software development settings (Ramasubbu and Kemerer\n9 https://blogs.worldbank.org/en/opendata/quality-open-source-software-how-many-eyes-are-enough\n10\nFigure 1\nThe workflow of OSS projects. It comprises of a primary branch of a GitHub (project), which typically\ncontains the main source code that serves as the foundation for new feature development, bug fixes, and updates.\nChanges to this branch are usually controlled through a structured review process conducted by maintainers to ensure\ncode quality and prevent issues.To contribute to a project, contributors submit a PR to merge their code changes,\nsuch as new features or improvements. It allows developers to submit modifications, request feedback, and merge\nupdates into the main branch. The review and revision process comprises of two activities that we measure in our\nstudy - PR review and PR rework. Eventually the reviewed and reworked changes will either be merged into the main\nbranch or undergo additional rounds of review and revision.\n2016, Banker et al. 2021, Ramasubbu and Kemerer 2021). In contrast, OSS development unfolds through\ncontinuous integration and deployment of new features, where design trade-offs and quality concerns are\noften raised and resolved at the PR level. In this context, PR rework provides a direct operationalization of\ntechnical debt, capturing realized remediation effort during code integration.\n3.\nData and Methods\n3.1.\nResearch Design\nData starts\nJuly 2020\nGitHub Copilot\nTechnical Preview\nJune 29, 2021\nData ends\nJuly 2022\nPre-Treatment Period\nPost-Treatment Period\nTreatment Group: Python, JavaScript, Ruby, TypeScript and Go\nControl Group: R, C, C#, C++, Java, PHP and Scala\nFigure 2\nThe timeline of our study period\nIn June 29, 2021, developed on OpenAI\u2019s GPT-3 model, GitHub launched a technical preview version of\nCopilot10, an AI-powered LLM designed to assist with coding. This early version of GitHub Copilot was\n10 In September 2020, Microsoft gained exclusive access to OpenAI\u2019s GPT3. This access allowed Microsoft to repurpose and\nmodify the model for code generation, leading to the development of GitHub Copilot. In June 2021, GitHub Copilot was launched\nas a technical preview. Anecdotal evidence and our interviews of Microsoft employees indicates that during the technical preview,\naccess to Copilot was restricted to selected GitHub users, specifically employees of Microsoft/GitHub and maintainers of popular\nprojects. This restricted access suggests that the primary users of Copilot during the technical preview were likely Microsoft and\nGitHub employees, along with selected Github uses who volunteered for the beta testing the software.\n11\nnot open to the public and endorsed five programming languages: Python,"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_013",
    "source_id": "AIProdDecrease2024",
    "text": ", Microsoft gained exclusive access to OpenAI\u2019s GPT3. This access allowed Microsoft to repurpose and\nmodify the model for code generation, leading to the development of GitHub Copilot. In June 2021, GitHub Copilot was launched\nas a technical preview. Anecdotal evidence and our interviews of Microsoft employees indicates that during the technical preview,\naccess to Copilot was restricted to selected GitHub users, specifically employees of Microsoft/GitHub and maintainers of popular\nprojects. This restricted access suggests that the primary users of Copilot during the technical preview were likely Microsoft and\nGitHub employees, along with selected Github uses who volunteered for the beta testing the software.\n11\nnot open to the public and endorsed five programming languages: Python, JavaScript, Ruby, TypeScript,\nand Go.11 Copilot was later launched to the public in June 2022, with contributors required to pay a monthly\nfee to subscribe to the AI pair programming service. With the public release in June 2022, Copilot gradually\nadded endorsement for more languages such as, C and Java. As shown in Figure 2, we define our observation\nperiod as 12 months before and 12 months after its introduction as technical preview. The measures were\naggregated to create a monthly panel dataset spanning from July 2020 to July 2022.\nOur study leverages the natural experiment created by the launch of GitHub Copilot as a technical\npreview. Specifically, we exploit Copilot\u2019s early-stage language endorsement, which included Python,\nJavaScript, Ruby, TypeScript, and Go, while excluding other comparable languages such as R, C, C#, C++,\nJava, PHP, and Scala. We select these non-Copilot-endorsed languages as the control group because they\noffer comparable functionality and are among the most frequently used languages in Microsoft-owned\nrepositories. Our identification strategy, which contrasts programming languages across treatment and con-\ntrol groups, is consistent with prior studies (Yeverechyahu et al. 2024).\nAnecdotal evidence12 and our interviews with Microsoft employees indicates that during the techni-\ncal preview, access to Copilot was restricted to selected GitHub contributors, specifically employees of\nMicrosoft/GitHub and maintainers of popular repositories who volunteered to participate in the restricted\npreview of the tool. This restricted access suggests that the primary users of Copilot during the techni-\ncal preview were likely Microsoft and GitHub (organization) employees, ensuring that the tool was tested\ninternally before its broader release.13\nSince Copilot usage cannot be identified at the individual level, we focus on Microsoft-owned repositories\nand contributors who actively contributed to these repositories during the observation period. By doing so,\nwe aim to capture changes in OSS contributor behavior driven by Copilot, as contributors to Microsoft-\nowned repositories are more likely to have access to the tool during the technical preview period.\n3.2.\nData Collection\nWe use GitHub\u2019s API service to collect all the data for this research, enabling efficient and precise querying\nof repository / individual activities. This approach enables us to gather detailed information on measure-\nments such as PRs, commits, reviews, authors, and repository / individual metadata, providing a comprehen-\nsive dataset for analyzing the impact of AI-assisted coding on OSS project development and maintenance\nactivities.\n11 https://github.blog/news-insights/product-news/introducing-github-Copilot-ai-pair-programmer/\n12 https"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_014",
    "source_id": "AIProdDecrease2024",
    "text": " aim to capture changes in OSS contributor behavior driven by Copilot, as contributors to Microsoft-\nowned repositories are more likely to have access to the tool during the technical preview period.\n3.2.\nData Collection\nWe use GitHub\u2019s API service to collect all the data for this research, enabling efficient and precise querying\nof repository / individual activities. This approach enables us to gather detailed information on measure-\nments such as PRs, commits, reviews, authors, and repository / individual metadata, providing a comprehen-\nsive dataset for analyzing the impact of AI-assisted coding on OSS project development and maintenance\nactivities.\n11 https://github.blog/news-insights/product-news/introducing-github-Copilot-ai-pair-programmer/\n12 https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-\nengineering-experience-using-generative-ai\n13 Dog Fooding; is Microsoft speak for internal use of their own software to ensure it is tested before public release\nhttps://devblogs.microsoft.com/oldnewthing/20110802-00/?p=10003\n12\nAt the project level, we define the treatment group as repositories whose primary programming language\nwas among those endorsed by GitHub Copilot during the technical preview, while repositories using non-\nendorsed languages (e.g., C, C#, C++, Java, PHP, R, Scala) serve as the control group. Our project-level\ndataset consists of 2,755 repositories, with 1,660 in the Copilot-endorsed treatment group and 1,095 in the\nnon-endorsed control group.\nOur individual level dataset consisting of 1,699 contributors from GitHub, with 1,186 in the Copilot-\nsupported treatment group and 513 in the non-Copilot-supported control group.14\n3.3.\nVariables\nTable 2 presents the main variables and provide descriptive statistics. Our dataset is aggregated to the\nrepository-month / contributor-month level, summary statistics are calculated based on repository-month /\ncontributor-month observations.\n3.3.1.\nDependent Variable: PR Rework\nIn this study, we operationalize technical debt through process and outcome indicators, using rework\nrates, and maintenance effort, thereby connecting debt to developer behavior and organizational perfor-\nmance (Banker et al. 2021, Ramasubbu and Kemerer 2021). Specifically, Banker et al. (2021), Ramasubbu\nand Kemerer (2021) measure technical debt as the proportion of customized code in customer relationship\nmanagement (CRM) systems that does not adhere to vendor-prescribed standards. Analogously, in the OSS\ncontext, we measure technical debt by the number of commit amendments (PR rework) required for a PR\nto satisfy repository standards, capturing the additional rework effort induced by nonconforming code.\nOn GitHub, a PR is a vehicle of contribution through which contributors participate in the development\nprocess. A PR typically contains one or more commits and often represents a \"patch\" or feature addition\nto the project. Any individual can submit a PR (a request to merge their contribution into the project),\nwhich is then (peer) reviewed by the core contributors, who have write access to the source code. The\ncore contributors reviewing the PR can decide to merge the PR into the"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_015",
    "source_id": "AIProdDecrease2024",
    "text": " adhere to vendor-prescribed standards. Analogously, in the OSS\ncontext, we measure technical debt by the number of commit amendments (PR rework) required for a PR\nto satisfy repository standards, capturing the additional rework effort induced by nonconforming code.\nOn GitHub, a PR is a vehicle of contribution through which contributors participate in the development\nprocess. A PR typically contains one or more commits and often represents a \"patch\" or feature addition\nto the project. Any individual can submit a PR (a request to merge their contribution into the project),\nwhich is then (peer) reviewed by the core contributors, who have write access to the source code. The\ncore contributors reviewing the PR can decide to merge the PR into the main project, request modifications\nor, reject it. If modifications are requested for a PR, the author of the PR can address the comments and\nmodification requests and re-submit the code in the form of follow-up commits for another round of review.\nThis revision process continues until all issues with the code are resolved and the code can be merged,\nor until the idea behind the code is no longer in alignment with the project goals and the PR is rejected.\nTaking this into account, we can measure the extent of rework done on a PR submitted by a contributor by\nidentifying the number of commits that are added to the PR after its initial submission. We use the PR rework\nmeasure as a proxy to determine the technical debt attributable to initial code contributions submitted to the\nproject and the maintenance-related efforts associated with the code contributions.\n14 There are 37,334 contributors who made at least one contribution to the repositories in our sample. The contributions include,\nfor example, posting a comment, submitting a commit, or conducting a PR review. Among them, we filtered out 5,308 contributors\nwho participated in more than three of the repositories we studied (we selected three repos to ensure sufficient variation for our PR\nreviewed repositories measure) . Then, we applied a programming language filter to construct a comparable treatment and control\ngroup for the individual-level analysis, and the data set was reduced to 1,699 contributors (who were users of the treated and control\nprogramming languages).\n13\nTable 2\nVariable Descriptions\nVariable\nDescription\nDependent Variables\nCode Added (log)\nLine of codes submitted to a repository in a given month.\nCommits (log)\nCommits submitted to a repository (by a contributor) in a given\nmonth. On GitHub, each commit typically represents a completed\npiece of work, such as fixing a bug, adding a feature, or improv-\ning existing code.\nPRs (log)\nPull requests submitted to a repository (by a contributor) in a\ngiven month. Each pull request represents a bundle of proposed\nchanges (commits) that must be reviewed and approved before\nbecoming part of the repository.\nTechinal Debt\nMeasured by PR rework (log), count of follow-up commits added\nto a PR after initial submission, capturing the extent of required\ncode revision.\nMaintenance Efforts\nMeasured by PR review (log), count of PR reviews conducted by\na contributor in a given month, analyzed at individual level.\nIndependent Variables\nTreatment\nIndicator variable equal to 1 if the repository\u2019s primary program-\nming language is one of the five Copilot-supported languages,\nand 0 otherwise.\nCopilot\nIndicator variable equal to 1 for months"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_016",
    "source_id": "AIProdDecrease2024",
    "text": "by a contributor) in a\ngiven month. Each pull request represents a bundle of proposed\nchanges (commits) that must be reviewed and approved before\nbecoming part of the repository.\nTechinal Debt\nMeasured by PR rework (log), count of follow-up commits added\nto a PR after initial submission, capturing the extent of required\ncode revision.\nMaintenance Efforts\nMeasured by PR review (log), count of PR reviews conducted by\na contributor in a given month, analyzed at individual level.\nIndependent Variables\nTreatment\nIndicator variable equal to 1 if the repository\u2019s primary program-\nming language is one of the five Copilot-supported languages,\nand 0 otherwise.\nCopilot\nIndicator variable equal to 1 for months after the introduction of\nGitHub Copilot (July 2021), and 0 otherwise.\nTreatment \u00d7 Copilot\nInteraction term capturing the DiD estimate of Copilot\u2019s effect on\nrepository-level development and maintenance outcomes.\nControl Variables\nPRs (log)\nIncluded as a control in PR rework regressions to account for\nheterogeneity in code submission volume across repositories.\nMonth-year fixed effects\nIndicator variables capturing temporal shocks common across all\nrepositories (e.g., seasonal patterns).\nRepository fixed effects\nRepository-level constants controlling for time-invariant hetero-\ngeneity (e.g., project age, governance structure).\nNote: All DVs are log-transformed. A value of one is added prior to transformation to address zeros.\n3.3.2.\nDependent Variable: PR Review and PR Reviewed Repos\nAs described in the previous section, we use PR rework as a proxy for the accumulation of technical debt.\nConsistent with prior literature (Banker et al. 2021, Ramasubbu and Kemerer 2021), we further conceptu-\nalize PR reviews and PR reviewed repositories as measures of maintenance effort aimed at preventing or\nmitigating technical debt.\nPR reviews capture the intensity of review activity undertaken by a contributor and are defined as the\nnumber of PR for which the contributor provides formal review during the observation period. This measure\nreflects the effort devoted to evaluating, debugging, and improving submitted code prior to integration.\nPR reviewed repositories capture the breadth of a contributor\u2019s maintenance responsibilities and are\ndefined as the number of distinct repositories in which the contributor conducts at least one PR review dur-\n14\ning the observation period. This variable reflects the extent to which maintenance effort is distributed across\nmultiple projects and repositories.\n3.3.3.\nModeration Variable: Core Contributors\nFor the conceptualization of core contributors we used contributors\u2019 level of activity during the pretreat-\nment period, measured by the number of commits, to classify contributors into core contributors (top 25%)\nand peripheral contributors (rest 75%) (Setia et al. 2012, Crowston et al. 2006). The classification of con-\ntributors allows us to examine whether the effect of Copilot varies depending on a core versus a periphery\ncontributor. From Figure 3, we observe that core contributors perform the majority of development activities\n(both in terms of commits and PR) in the projects.\nFigure 3\nHistogram of contributions for core and peripheral contributors: The core contributors decreased the\ndevelopment activities after the deployment of Copilot. The peripheral contributors displayed opposite behaviour.\n3.3.4.\nDescriptive Statistics\nWe report summary statistics for the key"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_017",
    "source_id": "AIProdDecrease2024",
    "text": " classify contributors into core contributors (top 25%)\nand peripheral contributors (rest 75%) (Setia et al. 2012, Crowston et al. 2006). The classification of con-\ntributors allows us to examine whether the effect of Copilot varies depending on a core versus a periphery\ncontributor. From Figure 3, we observe that core contributors perform the majority of development activities\n(both in terms of commits and PR) in the projects.\nFigure 3\nHistogram of contributions for core and peripheral contributors: The core contributors decreased the\ndevelopment activities after the deployment of Copilot. The peripheral contributors displayed opposite behaviour.\n3.3.4.\nDescriptive Statistics\nWe report summary statistics for the key variables used in our analyses. Table 3 presents the descriptive\nstatistics for the project-level analysis, including lines of code added, and the number of commits, PRs, and\nPR rework submitted; for the individual-level analysis, including the number of commits, PRs, PR reviews\nand PR reviewed repositories.\nTable 3\nDescriptive statistics for repository level analysis.\nVariable\nMean\nStd. Dev.\nMin\nMax\nCopilot (Dummy)\n0.301\n0.459\n0\n1\nCode Added (log)\n1.328\n2.883\n0\n17.034\nCommits (log)\n0.466\n1.052\n0\n8.075\nPRs (log)\n0.358\n0.834\n0\n6.789\nPR Rework (log)\n0.273\n0.894\n0\n7.536\n15\nTable 4\nDescriptive Statistics for individual level analysis.\nVariable\nMean\nStd. Dev.\nMin\nMax\nCopilot (Dummy)\n0.366\n0.482\n0\n1\nCommits (log)\n1.750\n1.694\n0\n10.116\nPRs (log)\n0.955\n1.155\n0\n6.201\nPR Reviews (log)\n0.789\n1.212\n0\n6.028\nPR Reviewed Repos (log)\n0.454\n0.664\n0\n5.215\n3.4.\nEstimation Models\n3.4.1.\nMain Statistical Analysis\nTo estimate the effect of GitHub Copilot on repository / individual-level development and maintenance\noutcomes, we employed a DiD regression framework. This approach exploits the staggered introduction\nof Copilot across programming languages, comparing changes in outcomes for repositories / individuals\nuse Copilot-supported languages (treatment group) with those use non-supported languages (control group)\nbefore and after Copilot\u2019s technical preview. By examining within-repository / individual changes over\ntime and contrasting them with contemporaneous changes in the control group, the DiD design isolates the\nimpact of Copilot adoption from time-invariant repository characteristics and common temporal shocks.\nThe project level DiD model (repo i month t) is provided below:\nRepository Level Effecti,t = \u03b20 +\u03b21Copiloti,t +\u03b3i +\u03b4t +\u03b5i,t\n(1)\nThe individual level DiD model (contributor i month t) is provided below:\nIndividual Level Effecti,t = \u03b20 +\u03b21Copiloti,t +\u03b22Core Contributori \u00d7Copiloti,t +\u03b3i +\u03b4t +\u03b5i,t\n(2"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_018",
    "source_id": "AIProdDecrease2024",
    "text": " preview. By examining within-repository / individual changes over\ntime and contrasting them with contemporaneous changes in the control group, the DiD design isolates the\nimpact of Copilot adoption from time-invariant repository characteristics and common temporal shocks.\nThe project level DiD model (repo i month t) is provided below:\nRepository Level Effecti,t = \u03b20 +\u03b21Copiloti,t +\u03b3i +\u03b4t +\u03b5i,t\n(1)\nThe individual level DiD model (contributor i month t) is provided below:\nIndividual Level Effecti,t = \u03b20 +\u03b21Copiloti,t +\u03b22Core Contributori \u00d7Copiloti,t +\u03b3i +\u03b4t +\u03b5i,t\n(2)\nwhere yi,t refers to the outcome measures (development and maintenance) for project / individual i in\nmonth t. Copiloti,t is the independent variable and a binary indicator that turns to 1 when the Copilot is\nreleased and functioned as our treatment. CoreContributori is the moderator and a binary indicator that\nturns to 1 when one individual is identified as core contributor by pretreatment code contribution behaviour.\nThe project-level / individual-level fixed effects are represented by \u03b3i, and \u03b4t represents the monthly fixed\neffects. \u03b5i,t is the robust standard error clustered at the project / individual level to account for the potential\nheteroskedasticity of the errors. The statistical results are presented in the appendix (Section 8.1).\n3.4.2.\nParallel Trends:\nWe followed prior research (e.g.,Autor 2003) to conduct an event study and test whether PR rework\nbefore the treatment are similar and parallel, an important assumption for the DiD framework. Specifically,\nthe equations to test the parallel trends are:\nPR reworkit = \u03b1 + \u2211\n\u03c4\u0338=\u22121\n\u03b2\u03c4 (Copiloti \u00d71{t \u2212Ti = \u03c4})+\u03b3i +\u03b4t +\u03b5i,t\n(3)\n16\nwhere PRreworkit denotes the log number of PR rework actions in repository i at time t; Copiloti indicates\nrepositories associated with Copilot-supported languages; and Ti denotes the Copilot introduction month.\nThe coefficients \u03b2\u03c4 capture the relative changes in rework activity \u03c4 months before or after treatment, using\n\u03c4 = \u22121 as the omitted baseline. Repository fixed effects (\u03b3i) absorb time-invariant heterogeneity, while\nmonth-year fixed effects (\u03b4i) account for aggregate shocks.\nThis event-study design enables us to visually and statistically evaluate the parallel-trends assumption\nand to characterize how Copilot\u2019s effects unfold over time, offering a richer interpretation than a single\naggregated DiD estimate.\n4.\nMain Results\n4.1.\nProject Level Analysis - Technical Debt\nWe analyzed the number of lines of code added, commits, and PRs as code development activities, and PR\nrework as a measurment of technical debt. Table 5 presents the results. After the introduction of GitHub\nCopilot, repositories whose primary programming language was endorsed by Copilot (treatment) experi-\nenced a significant increase in code development activities, such as the number of lines of code added\nincreased 17.7% (exp(0.163) \u22121 \u224817.7%), commits increased 4.1% (exp(0.04"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_019",
    "source_id": "AIProdDecrease2024",
    "text": " offering a richer interpretation than a single\naggregated DiD estimate.\n4.\nMain Results\n4.1.\nProject Level Analysis - Technical Debt\nWe analyzed the number of lines of code added, commits, and PRs as code development activities, and PR\nrework as a measurment of technical debt. Table 5 presents the results. After the introduction of GitHub\nCopilot, repositories whose primary programming language was endorsed by Copilot (treatment) experi-\nenced a significant increase in code development activities, such as the number of lines of code added\nincreased 17.7% (exp(0.163) \u22121 \u224817.7%), commits increased 4.1% (exp(0.04) \u22121 \u22484.1%), and PRs\nincreased 4.3% (exp(0.042)\u22121 \u22484.3%). This increase of productivity aligns with the analysis of industry\nexperts (Peng et al. 2023) and the findings of recent academic research (Yeverechyahu et al. 2024).\nTable 5\nThe impact of technical preview on development and maintenance activities.\nActivity:\nDevelopment\nTechnical Debt\nDV:\nLines of Added Code Commits Pull Requests\nPR Rework\nCopilot\n0.163***\n0.04*\n0.042***\n0.024**\n(0.06)\n(0.022)\n(0.015)\n(0.01)\nProject FE\n\u2713\n\u2713\n\u2713\n\u2713\nMonth FE\n\u2713\n\u2713\n\u2713\n\u2713\nPR Controls\n\u2713\nN\n66,120\n66,120\n66,168\n66,168\nAdj. R2\n0.516\n0.603\n0.691\n0.81\nNote: All DVs are log-transformed. Robust standard errors clustered at the project level\nare presented in parentheses. \u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\nMore importantly, we find that the submitted PRs requires greater amount of rework, indicating a likely\ndecrease in the quality of the submitted PRs. When analyzing the PRs submitted, the increased amount of\ncode resubmissions is significant, even when controlling for the number of PRs, indicating an increased\ndemand for code reviews per unit of code development. Specifically, we find that treatment repositories\nexperienced 2.4% (exp(0.024)\u22121 \u22482.4%) more code rework, keeping the number of PRs submitted con-\nstant. These AI paired contributions are less likely to adhere to repository standards and therefore generate\nadditional code rework effort, causing the accumulation of technical debt.\n17\nFigure 4 presents the event time analysis of the treatment effect on PR rework (leads-lags estimates). The\ncoefficients for the pre-treatment periods are statistically insignificant, indicating that the parallel trends\nassumption holds (Angrist and Pischke 2009). The leads-lags estimate also indicate a long-term effect of\nCopilot. With the increasing deployment and adoption of AI, we expect more contributor engagement and\nfrequent usage of Copilot. There is an upward trend in the post-treatment coefficients for PR rework that\n"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_020",
    "source_id": "AIProdDecrease2024",
    "text": "s submitted con-\nstant. These AI paired contributions are less likely to adhere to repository standards and therefore generate\nadditional code rework effort, causing the accumulation of technical debt.\n17\nFigure 4 presents the event time analysis of the treatment effect on PR rework (leads-lags estimates). The\ncoefficients for the pre-treatment periods are statistically insignificant, indicating that the parallel trends\nassumption holds (Angrist and Pischke 2009). The leads-lags estimate also indicate a long-term effect of\nCopilot. With the increasing deployment and adoption of AI, we expect more contributor engagement and\nfrequent usage of Copilot. There is an upward trend in the post-treatment coefficients for PR rework that\ngradually becomes more positive over time (regression results are provided in the Supplementary Material).\nThe observed pattern suggests a growing trend of increased code rework in repositories that supported AI\ncoding partners.\n-.2\n-.15\n-.1\n-.05\n0\n.05\n.1\n.15\n.2\nEstimate\n-12 -11 -10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nMonths Relative to GitHub Copilot\nLeads Lags Coefficients for PR Rework\nFigure 4\nParallel trends and dynamic effects of the Copilot treatment for PR rework\n4.2.\nIndividual Level Analysis - Maintenance Efforts\nWe next examined commits and PRs as code development activities, and PR reviews (PR controlled) and PR\nreviewed repositories (PR controlled) as maintenance related activities. Based on our DiD analysis (Table\n6), we find that core contributors engage in fewer development activities and in more maintenance activities\nafter the launch of Copilot. The core contributors performed significant less code commits submissions, a\ndecrease of 42.9% (exp(0.357) \u22121 \u224842.9%) compare to the rest of contributors. At the same time, the\ncore contributors reviewed more PRs and do so across a broader set of repositories. This shift suggests that\ncore contributors are not only stretching more of their limited time on maintenance tasks but also spreading\nthemselves thinner across more repositories. The resulting burden of maintenance may come at the cost of\nreduced productivity among more experienced contributors.\n5.\nRobustness Check\nWe performed a series of tests to ensure that our findings are robust to various model specifications.\nTogether, these tests increase confidence that our results are not driven by our model specification or unob-\nserved variables.\n18\nTable 6\nThe impact of technical preview on GitHub\u2019s core contributor behavior.\nActivity:\nDevelopment\nMaintenance\nDV:\nCommits\nPull Request PR Review PR Reviewed Repos\nCopilot\n0.142***\n0.091***\n0.04\n0.008\n(0.408)\n(0.03)\n(0.023)\n(0.012)\nCore Contributor -0.357***\n-0.027\n0.06*\n0.045**\n\u00d7 Copilot\n(0.052)\n(0.042)\n(0.035)\n(0.018)\nIndividual FE\n\u2713\n\u2713\n\u2713\n\u2713\nMonth FE\n\ufffd"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_021",
    "source_id": "AIProdDecrease2024",
    "text": " our model specification or unob-\nserved variables.\n18\nTable 6\nThe impact of technical preview on GitHub\u2019s core contributor behavior.\nActivity:\nDevelopment\nMaintenance\nDV:\nCommits\nPull Request PR Review PR Reviewed Repos\nCopilot\n0.142***\n0.091***\n0.04\n0.008\n(0.408)\n(0.03)\n(0.023)\n(0.012)\nCore Contributor -0.357***\n-0.027\n0.06*\n0.045**\n\u00d7 Copilot\n(0.052)\n(0.042)\n(0.035)\n(0.018)\nIndividual FE\n\u2713\n\u2713\n\u2713\n\u2713\nMonth FE\n\u2713\n\u2713\n\u2713\n\u2713\nPR Controlled\n\u2713\n\u2713\nN\n51,408\n51,408\n51,408\n51,408\nAdj. R2\n0.643\n0.607\n0.799\n0.757\nNote: All DVs are log-transformed. Robust standard errors clustered at the individual\nlevel are presented in parentheses. \u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\n5.1.\nMatching Results\nAs a robustness check, we employed Coarsened Exact Matching (CEM) to mitigate concerns about poten-\ntial selection bias and ensure a more balanced comparison between treatment and control groups. CEM\nallows us to pre-process the data by matching units on a set of covariates that may influence both treatment\nassignment and outcomes. We matched repositories based on pretreatment code development characteris-\ntics, including Code Added, Commits, and PRs. After matching, the repository level dataset consisting of n\n= 2,510 repositories, with 1,486 in the treatment group and 1,024 in the control group. The treatment and\ncontrol groups were more closely aligned in their baseline characteristics, reducing imbalance across key\nvariables. We then re-estimated the DiD models using the matched sample. The results are listed in the table\nbelow. The results remain consistent with our main findings, reinforcing the conclusion that the integration\nof GitHub Copilot is associated with increased rework and maintenance related activities.\nTable 7\nBalance Statistics: Unmatched and Matched Samples\nVariable\nUnmatched Sample\nMatched Sample\nTreated Control\nt-stat\np\nTreated\nControl\nt-stat\np\nCode Added (log)\n3.670\n4.143\n\u22122.648\n0.008\n3.298\n3.519 \u22121.253\n0.210\nCommits (log)\n1.470\n1.664\n\u22122.524\n0.012\n1.320\n1.402 \u22121.105\n0.269\nPull Request (log)\n1.066\n1.202\n\u22122.149\n0.032\n1.009\n0.974 \u22120.565\n0.572\n5.2.\nSensitivity Analysis\nTo assess the robustness of our estimated treatment effects on PR rework, we implement the coefficient\nstability approach proposed by Oster (2019). This method evaluates how sensitive the estimated DiD treat-\nment effect is to potential omitted-variable bias by comparing changes in the"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_022",
    "source_id": "AIProdDecrease2024",
    "text": ".519 \u22121.253\n0.210\nCommits (log)\n1.470\n1.664\n\u22122.524\n0.012\n1.320\n1.402 \u22121.105\n0.269\nPull Request (log)\n1.066\n1.202\n\u22122.149\n0.032\n1.009\n0.974 \u22120.565\n0.572\n5.2.\nSensitivity Analysis\nTo assess the robustness of our estimated treatment effects on PR rework, we implement the coefficient\nstability approach proposed by Oster (2019). This method evaluates how sensitive the estimated DiD treat-\nment effect is to potential omitted-variable bias by comparing changes in the coefficient of interest and the\nexplanatory power of the model when moving from a baseline specification (with only time and repository\n19\nTable 8\nThe impact of technical preview on project development and code quality measures after CEM Matching.\nConcept:\nDevelopment\nTechnical Debt\nDV:\nCode Added Commit Pull Request\nPR Rework\nCopilot\n0.202***\n0.051**\n0.038***\n0.019**\n(0.06)\n(0.021)\n(0.015)\n(0.01)\nProject FE\n\u2713\n\u2713\n\u2713\n\u2713\nMonth FE\n\u2713\n\u2713\n\u2713\n\u2713\nPR Controls\n\u2713\nN\n60,240\n60,240\n60,240\n60,240\nAdj. R2\n0.463\n0.517\n0.634\n0.779\nNote: All DVs are log-transformed. Robust standard errors clustered at the\nproject level are presented in parentheses. \u2217p<0.1; \u2217\u2217p<0.05; \u2217\u2217\u2217p<0.01\nTable 9\nSensitivity of Copilot Effect on PR Rework to Omitted Variable Bias (Oster \u03b4-analysis)\nPanel A. Regression Estimates\nUncontrolled\nControlled\nCopilot coefficient\n0.037\n0.023\nR-squared\n0.000\n0.818\nControls included\nNone\nPR, repo \u00d7 time FE\nPanel B. Oster Sensitivity Analysis\n\u03b4 (delta) estimate\n7.82\nRmax (assumed maximum R2)\n1.00\nImplied strength of unobservables\n7.82 \u00d7 observed selection\nfixed effects) to a fully controlled specification (including all covariates described in Section 2). Follow-\ning Oster (2019), we compute the bias-adjusted treatment effect under the assumption that the degree of\nselection on unobservables is proportional to the degree of selection on observables, using \u03b4 = 1 as the\nbenchmark and setting the maximum possible R2 (R* ) at 1.3\u00d7 the R2 of the fully controlled model.\nFor the PR rework outcome, the Oster-adjusted coefficient remains positive and statistically meaningful\nacross all delta values greater than 1, indicating that an unrealistically large amount of omitted-variable\nselection would be required to attenuate the estimated Copilot effect to zero. This stability suggests that the\nincrease in PR rework observed in our DiD estimates is unlikely to be driven by unobserved confounding\nand provides further confidence that our findings reflect a genuine shift in maintenance"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_023",
    "source_id": "AIProdDecrease2024",
    "text": " of\nselection on unobservables is proportional to the degree of selection on observables, using \u03b4 = 1 as the\nbenchmark and setting the maximum possible R2 (R* ) at 1.3\u00d7 the R2 of the fully controlled model.\nFor the PR rework outcome, the Oster-adjusted coefficient remains positive and statistically meaningful\nacross all delta values greater than 1, indicating that an unrealistically large amount of omitted-variable\nselection would be required to attenuate the estimated Copilot effect to zero. This stability suggests that the\nincrease in PR rework observed in our DiD estimates is unlikely to be driven by unobserved confounding\nand provides further confidence that our findings reflect a genuine shift in maintenance activities following\nthe introduction of AI-assisted coding tools.\n6.\nIndividual Level Heterogeneous Analyses\nTo examine heterogeneity among contributors, we conduct a series of subgroup analyses that exploit dif-\nferences in contributors\u2019 prior experience and roles within OSS projects. Following established practices\nin the OSS and information systems literature (Rullani and Haefliger 2013), we classify contributors based\non their level of past commit contributions during the pretreatment period, which serves as a proxy for\nexperience and project embeddedness. This approach allows us to distinguish between core contributors,\n20\nwho are typically responsible for code integration and maintenance, and peripheral contributors, who con-\ntribute less frequently and are more likely to focus on feature additions. We then estimate our DID models\nseparately for each subgroup, enabling us to assess whether the effects of AI-assisted programming differ\nsystematically across contributor types.\n6.1.\nSub Group Statistical Analysis\nThe individual level subgroup DiD model (contributor i month t) is provided below:\nSubgroup Individual Level Effecti,t = \u03b20 +\u03b21Copiloti,t \u00d7SubGroupi +\u03b4t +\u03b5i,t\n(4)\nwhere yi,t refers to the outcome measures (development and maintenance) for individual i in month t.\nCopiloti,t is the IV and a binary indicator that turns to 1 when the Copilot is released and functioned as our\ntreatment. SubGroupi is the moderator and a binary indicator that turns to 1 when one individual belongs\nto a subset by pretreatment code contribution behaviour. The individual-level fixed effects are represented\nby \u03b3i, and \u03b4t represents the monthly fixed effects. \u03b5i,t is the robust standard error clustered at the individual\nlevel to account for the potential heteroskedasticity of the errors.\n6.2.\nSub Group Analysis Results\nWe calculated the quantity of commits submitted during the pretreatment period, and split the contributors\naccordingly into four percentile groups: 0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The results\nare presented in Figure 5. The statistical results are presented in the appendix (Section 8.2).\nCompared to the control group, we observe a consistent decline in commit volume among core con-\ntributors, while peripheral contributors show the opposite trend. Core contributors shifted towards more\nmaintenance work, with a 19% decrease in commits and a 6.5% increase in PR reviews. In contrast, periph-\neral contributors, particularly those in the bottom percentiles, increased their commit activity by 43.5% and\nsubmitted 17"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_024",
    "source_id": "AIProdDecrease2024",
    "text": "ly into four percentile groups: 0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The results\nare presented in Figure 5. The statistical results are presented in the appendix (Section 8.2).\nCompared to the control group, we observe a consistent decline in commit volume among core con-\ntributors, while peripheral contributors show the opposite trend. Core contributors shifted towards more\nmaintenance work, with a 19% decrease in commits and a 6.5% increase in PR reviews. In contrast, periph-\neral contributors, particularly those in the bottom percentiles, increased their commit activity by 43.5% and\nsubmitted 17.7% more PRs. This pattern suggests that while Copilot lowered the barriers for peripheral\ncontributors to participate, it placed a greater maintenance burden on core contributors, redirecting their\nefforts from development-related to maintenance-related activities.\nOur analyses show that Copilot adoption increases PR rework, indicating a rise in technical debt within\nOSS projects. This pattern is consistent with two complementary reasoning. First, less-experienced contrib-\nutors may rely on vibe coding or shortcut-style AI-assisted development, generating larger volumes of code\nthat require substantial revision to meet project quality standards. Second, more-experienced contributors\n- who traditionally provide high-quality foundational code - appear to shift their time away from original\ndevelopment toward reviewing and correcting others\u2019 contributions. As a result, the supply of high-quality\ncode creation declines while the demand for expert maintenance increases, jointly producing the observed\nescalation in technical debt.\n21\n(a) Commits.\n(b) Pull Request.\n(c) PR Reviews.\n(d) PR Reviewed Repos.\nFigure 5\nContribution activities analysis by contributor subgroup. Panels show estimated coefficients (converted to\n%) from DiD regressions with 95% confidence intervals, capturing the relative change in activity post-Copilot exposure\ncompared to control repositories. (a) Commits: Conversely, commit activity declines progressively with contributor\u2019s\nexperience (measured by pretreatment commits quantity), with the top 25% experiencing a 19% reduction, suggesting\nreduced hands-on coding engagement. (b) Pull Requests: The most peripheral contributors (0\u201325%) significantly\nincrease their PR submissions (17.7%), highlighting increased participation from less experienced developers. (c) PR\nReviews: The top 25% of contributors (core) exhibit a significant increase in review activity, suggesting a shift of\nresponsibility towards the core contributors. (d) PR Reviewed Repositories: Similarly, only the core contributor group\nshows a meaningful rise in the number of distinct repositories reviewed, indicating a broader oversight role.\n7.\nDiscussion\n7.1.\nKey Findings and Discussion\nA growing literature documents that the adoption of GenAI technologies delivers substantial productivity\ngains but also introduces a range of unintended secondary effects. These include anchoring and overre-\nliance on AI-generated suggestions (Chen and Chan 2024), the reinforcement of societal and algorithmic\nbiases (Williams-Ceci et al. 2025, Nicoletti and Bass 2023), reduced diversity in outputs (Doshi and Hauser\n2024), widening worker inequalities (Humlum and Vestergaard 2025), AI\u2013AI bias (Laurito et al. 2025),\n"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_025",
    "source_id": "AIProdDecrease2024",
    "text": " of distinct repositories reviewed, indicating a broader oversight role.\n7.\nDiscussion\n7.1.\nKey Findings and Discussion\nA growing literature documents that the adoption of GenAI technologies delivers substantial productivity\ngains but also introduces a range of unintended secondary effects. These include anchoring and overre-\nliance on AI-generated suggestions (Chen and Chan 2024), the reinforcement of societal and algorithmic\nbiases (Williams-Ceci et al. 2025, Nicoletti and Bass 2023), reduced diversity in outputs (Doshi and Hauser\n2024), widening worker inequalities (Humlum and Vestergaard 2025), AI\u2013AI bias (Laurito et al. 2025),\nand dynamic feedback effects that amplify initial distortions over time (Glickman and Sharot 2025). Meta-\nanalytic evidence further suggests that while human\u2013AI teams excel in content generation tasks, they may\nunderperform in judgment and coordination-intensive settings relative to humans or AI alone (Vaccaro et al.\n22\n2024). Our study contributes to this literature by showing how these broader secondary (and unintended)\nconsequences of GenAI adoption manifest in software development through the lens of technical debt.\nIn OSS communities, the central concern is not merely whether AI increases output, but how it affects\ncode quality and long-term maintainability. By lowering the skill threshold for writing functional code\n(Dakhel et al. 2023), AI tools broaden participation but also increase the likelihood that contributors rely on\nAI-generated outputs without fully understanding their design implications or downstream consequences\n(Barrett et al. 2023). This dependency raises the risk that fragile, underspecified, or poorly integrated code\nenters production systems, thereby accelerating technical debt accumulation. Given the growing reliance of\nfirms and public institutions on OSS infrastructure (Nagle 2019), the economic and security consequences\nof such debt can be substantial.\nAs a case in point, in 2011 a major vulnerability (nicknamed Heartbleed) in an OpenSSL OSS project was\noverlooked and included in an update of the project. It went unnoticed for years, allowing any sophisticated\nhacker to capture secure information being passed to vulnerable web servers, including passwords, credit\ncard information, and other sensitive data. The widely held view on Heartbleed underscores the risks of\nunder-resourced maintenance related activities in OSS projects: :15\n\u201cThe mystery is not that a few overworked volunteers missed this bug; the mystery is why it hasn\u2019t\nhappened more often\u201d (Eghbal 2016, p. 13).\nEmpirically, we document three key findings. First, the introduction of GitHub Copilot leads to higher\ndevelopment activity at both the repository and individual levels, measured by commits and PR, consistent\nwith industry evidence on AI-driven productivity gains (Peng et al. 2023). Second, these gains are accompa-\nnied by a significant increase in maintenance-related activities, as AI-generated contributions require more\nrevisions before integration - an early indicator of technical debt accumulation. Third, the effects are highly\nheterogeneous: core contributors review more PRs, contribute fewer commits, and extend their mainte-\nnance responsibilities across a wider range of repositories, suggesting that AI-assisted contributions from\nperipheral developers increase coordination and review burdens.\nBy conceptualizing GenAI as an"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_026",
    "source_id": "AIProdDecrease2024",
    "text": ". First, the introduction of GitHub Copilot leads to higher\ndevelopment activity at both the repository and individual levels, measured by commits and PR, consistent\nwith industry evidence on AI-driven productivity gains (Peng et al. 2023). Second, these gains are accompa-\nnied by a significant increase in maintenance-related activities, as AI-generated contributions require more\nrevisions before integration - an early indicator of technical debt accumulation. Third, the effects are highly\nheterogeneous: core contributors review more PRs, contribute fewer commits, and extend their mainte-\nnance responsibilities across a wider range of repositories, suggesting that AI-assisted contributions from\nperipheral developers increase coordination and review burdens.\nBy conceptualizing GenAI as an endogenous shock to software production, this study advances the tech-\nnical debt literature in several important ways. First, consistent with prior work linking technical debt to firm\nperformance (Banker et al. 2021) and remediation costs (Ramasubbu and Kemerer 2016, 2021), we demon-\nstrate that AI-assisted code increases realized remediation effort, measured through PR rework. Second, we\nshow that technical debt is increasingly an outdated workload distribution phenomenon: maintenance costs\nare concentrated among a shrinking pool of core contributors, whose own productive output declines as\nmaintenance demands rise. Third, our findings complement research on organizational design and auton-\nomy (Paramitha and Massacci 2023, Yoo et al. 2025) by revealing how technological change can exacerbate\nasymmetries in effort and responsibility, even when the OSS repository workflow remains unchanged.\n15 https://mashable.com/archive/heartbleed-bug-websites-affected\n23\nTo illustrate the scale of Copilot\u2019s impact on OSS communities, Microsoft core contributors in our dataset\nconduct on average, 976 commits, 160 PRs, and 166 PR reviews annually before its introduction. The\nincreased volume of code associated with Copilot adoption results in an additional workload \u2013 each core\ncontributor is expected to review approximately 10 more PRs annually. This added maintenance burden\ncorresponds to a reduction of 164 commits and 9 PR contributions per year per core contributor. More\ncritically, GitHub\u2019s 2024 surveys reveal that more than one-third of contributors to the 10 most popular OSS\nprojects made their first contribution after signing up for GitHub Copilot, highlighting a significant influx\nof new and often less experienced developers16.\nWith annual contributions to OSS projects approaching 1 billion, this surge in participation significantly\nincreases the burden on core contributors, who take on the maintenance related tasks in the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_027",
    "source_id": "AIProdDecrease2024",
    "text": " the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced contrib-\nutors face higher review workloads and a concomitant decline in their own development activity. These\nresults highlight a redistribution of effort within OSS communities that has received limited attention in\nprior work.\nFrom a technical debt perspective, our findings suggest that AI-assisted programming alters the intertem-\nporal trade-off between short-term development speed and long-term maintainability. The shortcuts enabled\nby AI tools may accelerate the output while introducing code that is difficult to integrate, extend, or refactor.\nThe widespread use of AI-assisted pair programming - and, in extreme cases, \u201cvibe coding\u201d - can inject a\nlarger volume of difficult-to-maintain code (Pimenova et al. 2025, Fawzy et al. 2025) into software projects,\naccelerating the accumulation of technical debt. Such contributions create latent liabilities for projects, as\nmaintainers must later invest substantial effort to review, revise, or rewrite code to meet repository stan-\ndards. In this sense, AI does not merely increase the volume of contributions; it changes the composition of\nincoming code in ways that intensify technical debt accumulation.\nA key contribution of our study is to operationalize technical debt at its point of entry. We conceptual-\nize extensive PR rework as realized technical debt: the additional modification, coordination, and revision\n16 https://github.blog/news-insights/octoverse/octoverse-2024/\n24\neffort required to bring submitted code up to acceptable standards. This measure complements prior work\nthat captures technical debt through architectural metrics, defect accumulation, or long-run performance\noutcomes. By focusing on PR-level dynamics, we provide a micro-level view of how technical debt emerges\nin real time and how it is managed through ongoing maintenance effort.\nOur findings also raise concerns about the learning implications of AI-assisted development. With AI\nproviding rapid solutions, peripheral contributors may engage less deeply with underlying programming\nprinciples and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_028",
    "source_id": "AIProdDecrease2024",
    "text": "les and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation under AI-assisted\ndevelopment. More broadly, the dynamics documented here may extend beyond OSS to other knowledge-\nintensive domains where AI increases output without replacing expert judgment. As OSS components are\nincreasingly embedded in enterprise systems and public infrastructure (Nagle 2019), understanding how AI\nreshapes technical debt dynamics becomes critical not only for OSS sustainability but for the resilience of\nthe broader digital ecosystem.\nAs a concluding remark, the challenges observed in OSS, such as quality concerns and increased main-\ntenance burdens driven by productivity gains among newer and less-experienced contributors, should serve\nas an early warning for similar risks in other knowledge-intensive domains where AI is being promoted to\nboost productivity and innovation.\n25\nReferences\nAngrist JD, Pischke JS (2009) Mostly Harmless Econometrics: An Empiricist\u2019s Companion (Princeton University\nPress).\nBanker RD, Liang Y, Ramasubbu N (2021) Technical Debt and Firm Performance. Management Science 67(5):3174\u2013\n3194.\nBarrett C, Boyd B, Bursztein E, Carlini N, Chen B, Choi J, Chowdhury AR, Christodorescu M, Datta A, Feizi S, et al.\n(2023) Identifying and Mitigating the Security Risks of Generative AI. Foundations and Trends\u00ae in Privacy and\nSecurity 6(1):1\u201352.\nBrown N, Cai Y, Guo Y, Kazman R, Kim M, Kruchten P, Lim E, MacCormack A, Nord R, Ozkaya I, Sangwan R,\nSeaman C, Sullivan K, Zazworka N (2010) Managing Technical Debt in Software-reliant Systems. Proceedings\nof the FSE/SDP Workshop on Future of Software Engineering Research, 47\u201352, FoSER \u201910 (New York, NY,\nUSA: Association for Computing Machinery).\nBrynjolfsson E, Li D, Raymond LR (2025) Generative AI at Work. The Quarterly Journal of Economics 140(2):889\u2014\n-942.\nChen Z, Chan J (2024) Large Language Model in Creative Work: The Role of Collaboration Modality and User\nExpertise. Management Science 70(12):9101\u20139117.\nCrowston K, Wei K, Li Q, Howison J (2006) Core and Periphery in Free/libre and Open Source Software Team Com-\nmunications. Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS\u201906),\nvolume 6,"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_029",
    "source_id": "AIProdDecrease2024",
    "text": ", NY,\nUSA: Association for Computing Machinery).\nBrynjolfsson E, Li D, Raymond LR (2025) Generative AI at Work. The Quarterly Journal of Economics 140(2):889\u2014\n-942.\nChen Z, Chan J (2024) Large Language Model in Creative Work: The Role of Collaboration Modality and User\nExpertise. Management Science 70(12):9101\u20139117.\nCrowston K, Wei K, Li Q, Howison J (2006) Core and Periphery in Free/libre and Open Source Software Team Com-\nmunications. Proceedings of the 39th Annual Hawaii International Conference on System Sciences (HICSS\u201906),\nvolume 6, 118a\u2013118a (IEEE).\nCunningham W (1992) The WyCash Portfolio Management System. Addendum to the Proceedings of the 1992 Con-\nference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA \u201992), 29\u201330 (ACM\nSIGPLAN).\nDakhel AM, Majdinasab V, Nikanjam A, Khomh F, Desmarais MC, Jiang ZM (2023) GitHub Copilot AI Pair Pro-\ngrammer: Asset or Liability? Journal of Systems and Software 203:111734.\nDoshi AR, Hauser OP (2024) Generative AI Enhances Individual Creativity but Reduces the Collective Diversity of\nNovel Content. Science Advances 10(28):eadn5290.\nEghbal N (2016) Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure (Ford Foundation).\nEghbal N (2020) Working in Public: The Making and Maintenance of Open Source Software (Stripe Press).\nFawzy A, Tahir A, Blincoe K (2025) Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook \u2013 a Grey\nLiterature Review. arXiv preprint arXiv:2510.00328 .\nFriedman N (2021) Introducing GitHub Copilot: Your AI Pair Programmer. URL https://github.blog/\nnews-insights/product-news/introducing-github-copilot-ai-pair-programmer/.\nGlickman M, Sharot T (2025) How Human\u2013AI Feedback Loops Alter Human Perceptual, Emotional and Social judge-\nments. Nature Human Behaviour 9(2):345\u2013359.\n26\nHe H, Miller C, Agarwal S, K\u00e4stner C, Vasilescu B (2025) Does AI-Assisted Coding Deliver? A Difference-in-\nDifferences Study of Cursor\u2019s Impact on Software Projects. arXiv preprint arXiv:2511.04427 .\nHoffmann M, Boysel S, Nagle F, Peng S, Xu K (2025) Generative AI and the Nature of Work. Harvard Business\nSchool Strategy Unit Working Paper No. 25-021 .\nHoffmann M, Nagle F, Zhou Y (2024) The Value of Open Source Software. Harvard Business School Strategy Unit\nWorking Paper No. 24-038 .\nHumlum A, Vestergaard E (2025) The Unequal Adoption of ChatGPT Exacerbates Existing Inequalities Among Work-\ners. Proceedings of the National Academy of Sciences"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_030",
    "source_id": "AIProdDecrease2024",
    "text": "Differences Study of Cursor\u2019s Impact on Software Projects. arXiv preprint arXiv:2511.04427 .\nHoffmann M, Boysel S, Nagle F, Peng S, Xu K (2025) Generative AI and the Nature of Work. Harvard Business\nSchool Strategy Unit Working Paper No. 25-021 .\nHoffmann M, Nagle F, Zhou Y (2024) The Value of Open Source Software. Harvard Business School Strategy Unit\nWorking Paper No. 24-038 .\nHumlum A, Vestergaard E (2025) The Unequal Adoption of ChatGPT Exacerbates Existing Inequalities Among Work-\ners. Proceedings of the National Academy of Sciences 122(1):e2414972121.\nKruchten P, Nord RL, Ozkaya I (2012) Technical Debt: From Metaphor to Theory and Practice. IEEE Software\n29(6):18\u201321.\nLaurito W, Davis B, Grietzer P, Gaven\u02c7ciak T, B\u00f6hm A, Kulveit J (2025) AI\u2013AI Bias: Large Language Models Favor\nCommunications Generated by Large Language Models. Proceedings of the National Academy of Sciences\n122(31):e2415697122.\nLi X, Costello FJ, Kim K, Zhang XM (2025) Exploring Altered Open Source Software Development Patterns in a\nTime of Generative AI. SSRN Electronic Journal 5100609.\nMedappa PK, Tunc MM, Li X (2023) Sponsorship Funding in Open-Source Software: Effort Reallocation and\nSpillover Effects in Knowledge-Sharing Ecosystems. SSRN Electronic Journal 4484403.\nNagle F (2019) Open Source Software and Firm Productivity. Management Science 65(3):1191\u20131215.\nNagle F, Wheeler DA, Lifshitz-Assaf H, Ham H, Hoffman JL (2020) Report on the 2020 FOSS Contributor Survey.\nTechnical report, The Linux Foundation.\nNicoletti L, Bass D (2023) Humans Are Biased. Generative AI Is Even Worse. Bloomberg URL https://www.\nbloomberg.com/graphics/2023-generative-ai-bias/.\nOster EF (2019) Unobservable Selection and Coefficient Stability: Theory and Evidence. Journal of Business & Eco-\nnomic Statistics 37(2):187\u2013204.\nParamitha R, Massacci F (2023) Technical Leverage Analysis in the Python Ecosystem. Empirical Software Engineer-\ning 28(6):139.\nPeng S, Kalliamvakou E, Cihon P, Demirer M (2023) The Impact of AI on Developer Productivity: Evidence from\nGithub Copilot. arXiv preprint arXiv:2302.06590 .\nPimenova V, Fakhoury S, Bird C, Storey MA, Endres M (2025) Good Vibrations? A Qualitative Study of Co-Creation,\nCommunication, Flow, and Trust in Vibe Coding. arXiv preprint arXiv:2509.12491 .\nRamasubbu N, Kemerer CF (2016) Technical Debt and the Reliability of Enterprise Software Systems: A Competing\nRisks"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_031",
    "source_id": "AIProdDecrease2024",
    "text": "Peng S, Kalliamvakou E, Cihon P, Demirer M (2023) The Impact of AI on Developer Productivity: Evidence from\nGithub Copilot. arXiv preprint arXiv:2302.06590 .\nPimenova V, Fakhoury S, Bird C, Storey MA, Endres M (2025) Good Vibrations? A Qualitative Study of Co-Creation,\nCommunication, Flow, and Trust in Vibe Coding. arXiv preprint arXiv:2509.12491 .\nRamasubbu N, Kemerer CF (2016) Technical Debt and the Reliability of Enterprise Software Systems: A Competing\nRisks Analysis. Management Science 62(5):1487\u20131510.\nRamasubbu N, Kemerer CF (2021) Controlling Technical Debt Remediation in Outsourced Enterprise Systems Main-\ntenance: An Empirical Analysis. Journal of Management Information Systems 38(1):4\u201328.\n27\nRinta-Kahila T, Penttinen E, Lyytinen K (2023) Getting Trapped in Technical Debt: Sociotechnical Analysis of a\nLegacy System\u2019s Replacement. MIS Quarterly 47(1):1\u201332.\nRoberts JA, Hann IH, Slaughter SA (2006) Understanding the Motivations, Participation, and Performance of Open\nSource Software Developers: A Longitudinal Study of the Apache Projects. Management Science 52(7):984\u2013\n999.\nRodriguez M (2023) Research: Quantifying GitHub Copilot\u2019s Impact on Code Quality - The GitHub Blog. URL\nhttps://github.blog/2023-10-10-research-quantifying-github-copilots-impact-on-code-quality/.\nRullani F, Haefliger S (2013) The Periphery on Stage: The Intra-organizational Dynamics in Online Communities of\nCreation. Research Policy 42(4):941\u2013953.\nSchreiber M, Tippe P (2025) Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub\nRepositories, 153\u2013172 (Springer Nature Singapore).\nSetia P, Rajagopalan B, Sambamurthy V, Calantone R (2012) How Peripheral Developers Contribute to Open-Source\nSoftware Development. Information Systems Research 23(1):144\u2013163.\nSong F, Agarwal A, Wen W (2024) The Impact of Generative AI on Collaborative Open-Source Software Develop-\nment: Evidence from GitHub Copilot. arXiv preprint arXiv:2410.02091 .\nVaccaro M, Almaatouq A, Malone T (2024) When Combinations of Humans and AI are Useful: A Systematic Review\nand Meta-Analysis. Nature Human Behaviour 8(12):2293\u20132303.\nvon Hippel E, von Krogh G (2003) Open Source Software and the \u201cPrivate-Collective\u201d Innovation Model: Issues for\nOrganization Science. Organization Science 14(2):209\u2013223.\nWilliams-Ceci S, Jakesch M, Bhat A, Kadoma K, Zalmanson L, Naaman M (2025) Biased AI Writing Assistants Shift\nUsers\u2019 Att"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_032",
    "source_id": "AIProdDecrease2024",
    "text": "Xiv preprint arXiv:2410.02091 .\nVaccaro M, Almaatouq A, Malone T (2024) When Combinations of Humans and AI are Useful: A Systematic Review\nand Meta-Analysis. Nature Human Behaviour 8(12):2293\u20132303.\nvon Hippel E, von Krogh G (2003) Open Source Software and the \u201cPrivate-Collective\u201d Innovation Model: Issues for\nOrganization Science. Organization Science 14(2):209\u2013223.\nWilliams-Ceci S, Jakesch M, Bhat A, Kadoma K, Zalmanson L, Naaman M (2025) Biased AI Writing Assistants Shift\nUsers\u2019 Attitudes on Societal Issues. PsyArXiv Preprints doi:10.31234/osf.io/mhjn6_v2 .\nYeverechyahu D, Mayya R, Oestreicher-Singer G (2024) The Impact of Large Language Models on Open-source\nInnovation: Evidence from GitHub Copilot. arXiv preprint arXiv:2409.08379 .\nYoo E, Craighead CW, Samtani S (2025) Dependency Network Structure and Security Vulnerabilities in Software\nSupply Chains. Journal of Management Information Systems 42(4):1149\u20131176.\n28\nAppendix\nProject Level Lead Lag Analysis\nWe conducted a lead-lag analysis to examine the dynamic effects of GitHub Copilot adoption on reposi-\ntories\u2019 performance over time. Using a 24-month window centered around the technical preview release:\n12 months before and 12 months after, we estimated the monthly treatment effects relative to the month of\nlaunch. The coefficients of the analysis is listed below:\nTable 10\nRegression Results for PR rework\nVariable Coefficient Std. Err.\nt\nP > |t| 95% CI (Lower)\n95% CI (Upper)\nb12\n0.0058\n0.0216\n0.27\n0.789\n-0.0366\n0.0482\nb11\n0.0099\n0.0207\n0.48\n0.632\n-0.0306\n0.0504\nb10\n-0.0080\n0.0223\n-0.36\n0.720\n-0.0518\n0.0358\nb9\n-0.0004\n0.0222\n-0.02\n0.987\n-0.0438\n0.0431\nb8\n-0.0115\n0.0210\n-0.55\n0.586\n-0.0527\n0.0298\nb7\n-0.0313\n0.0228\n-1.37\n0.170\n-0.0759\n0.0134\nb6\n-0.0335\n0.0235\n-1.43\n0.154\n-0.0796\n0.0125\nb5\n-0.0289\n0.0218\n-1.33\n0.184\n-0.0715\n0.0138\nb4\n-0.0192\n0.0233\n-0.82\n0.411\n-0.0648\n0.026"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_033",
    "source_id": "AIProdDecrease2024",
    "text": "\n0.586\n-0.0527\n0.0298\nb7\n-0.0313\n0.0228\n-1.37\n0.170\n-0.0759\n0.0134\nb6\n-0.0335\n0.0235\n-1.43\n0.154\n-0.0796\n0.0125\nb5\n-0.0289\n0.0218\n-1.33\n0.184\n-0.0715\n0.0138\nb4\n-0.0192\n0.0233\n-0.82\n0.411\n-0.0648\n0.0265\nb3\n-0.0315\n0.0232\n-1.36\n0.174\n-0.0770\n0.0140\nb2\n0.0190\n0.0219\n0.87\n0.385\n-0.0239\n0.0619\nb1\nBaseline\na0\n0.0005\n0.0240\n0.02\n0.983\n-0.0466\n0.0477\na1\n0.0342\n0.0256\n1.34\n0.181\n-0.0159\n0.0843\na2\n0.0124\n0.0266\n0.47\n0.641\n-0.0398\n0.0647\na3\n-0.0094\n0.0268\n-0.35\n0.725\n-0.0620\n0.0431\na4\n0.0250\n0.0265\n0.94\n0.346\n-0.0269\n0.0769\na5\n0.0456\n0.0246\n1.85\n0.064\n-0.0027\n0.0940\na6\n0.0767\n0.0259\n2.96\n0.003\n0.0260\n0.1274\na7\n0.0543\n0.0269\n2.02\n0.044\n0.0015\n0.1072\na8\n0.0393\n0.0286\n1.37\n0.169\n-0.0167\n0.0953\na9\n0.0770\n0.0267\n2.88\n0.004\n0.0247\n0.1294\na10\n0.0957\n0.0278\n3.44\n0.001\n0.0412\n0.1503\na11\n0.0551\n0.0298\n1.85\n0.065\n-0.0034\n0.1136\n_cons\n0.2416\n0.0186\n12.98\n0.000\n0.2051\n0.2781\nIndividual Level Sub Group Analysis\nWe estimate the effect of Copilot on four subgroup of contributors based on the pretreatment contribution:\n0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The analysis are conducted for each of our outcome\nmeasures:"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_034",
    "source_id": "AIProdDecrease2024",
    "text": "0.0278\n3.44\n0.001\n0.0412\n0.1503\na11\n0.0551\n0.0298\n1.85\n0.065\n-0.0034\n0.1136\n_cons\n0.2416\n0.0186\n12.98\n0.000\n0.2051\n0.2781\nIndividual Level Sub Group Analysis\nWe estimate the effect of Copilot on four subgroup of contributors based on the pretreatment contribution:\n0 to 25%, 25% to 50%, 50% to 75% and 75% to 100%. The analysis are conducted for each of our outcome\nmeasures: PR Reviews, PR Reviewed Repos, commits and PRs. The PR Reviews, PR Reviewed Repos have\nPR controlled. The statistical results are presented in the following tables.\n29\nTable 11\nRegression Results for PR Reviews (PR controlled)\nCoefficient Std. Err. t-value P > |t| 95% CI (Lower) 95% CI (Upper)\nSubgroup 0%-25%\n0.0099\n0.0243\n0.41\n0.685\n-0.0379\n0.0577\nSubgroup 25%-50%\n0.0117\n0.0300\n0.39\n0.697\n-0.0472\n0.0706\nSubgroup 50%-75%\n-0.0073\n0.0352\n-0.21\n0.835\n-0.0764\n0.0617\nSubgroup 75%-100%\n0.0628\n0.0366\n1.72\n0.086\n-0.0089\n0.1345\nTable 12\nRegression Results for PR Reviewed Repos (PR controlled)\nCoefficient Std. Err. t-value P > |t| 95% CI (Lower) 95% CI (Upper)\nSubgroup 0%-25%\n0.0061\n0.0143\n0.43\n0.670\n-0.0220\n0.0342\nSubgroup 25%-50%\n0.0078\n0.0163\n0.48\n0.630\n-0.0241\n0.0397\nSubgroup 50%-75%\n0.0125\n0.0179\n0.70\n0.486\n-0.0227\n0.0477\nSubgroup 75%-100%\n0.0530\n0.0192\n2.76\n0.006\n0.0154\n0.0906\nTable 13\nRegression Results for Commits\nCoefficient Std. Err. t-value P > |t| 95% CI (Lower) 95% CI (Upper)\nSubgroup 0%-25%\n0.3614\n0.0447\n8.08\n0.000\n0.2737\n0.4492\nSubgroup 25%-50%\n0.1115\n0.0555\n2.01\n0.045\n0.0026\n0.2205\nSubgroup 50%-75%\n-0.0381\n0.0610\n-0.62\n0.532\n-0.1578\n0.0815\nSubgroup"
  },
  {
    "chunk_id": "AIProdDecrease2024_chunk_035",
    "source_id": "AIProdDecrease2024",
    "text": ".0906\nTable 13\nRegression Results for Commits\nCoefficient Std. Err. t-value P > |t| 95% CI (Lower) 95% CI (Upper)\nSubgroup 0%-25%\n0.3614\n0.0447\n8.08\n0.000\n0.2737\n0.4492\nSubgroup 25%-50%\n0.1115\n0.0555\n2.01\n0.045\n0.0026\n0.2205\nSubgroup 50%-75%\n-0.0381\n0.0610\n-0.62\n0.532\n-0.1578\n0.0815\nSubgroup 75%-100%\n-0.2149\n0.0551\n-3.90\n0.000\n-0.3230\n-0.1068\nTable 14\nRegression Results for PR\nCoefficient Std. Err. t-value P > |t| 95% CI (Lower) 95% CI (Upper)\nSubgroup 0%-25%\n0.1630\n0.0317\n5.15\n0.000\n0.1009\n0.2252\nSubgroup 25%-50%\n0.0756\n0.0399\n1.89\n0.059\n-0.0027\n0.1539\nSubgroup 50%-75%\n0.0373\n0.0465\n0.80\n0.423\n-0.0540\n0.1285\nSubgroup 75%-100%\n0.0644\n0.0444\n1.45\n0.147\n-0.0227\n0.1515\n"
  },
  {
    "chunk_id": "AIReview2025_chunk_001",
    "source_id": "AIReview2025",
    "text": "236 \nA Review of Research on AI-Assisted Code Generation and AI-\nDriven Code Review \nYuzhi Wang \nBeijing University of Technology, Beijing, China \nshenrenaisite@yeah.net \nAbstract. With the significant breakthroughs of deep learning technologies such as large language \nmodels (LLMs) in the field of code analysis, AI has evolved from an auxiliary tool to a key technology \nthat deeply participates in code optimization and resolving performance issues. As modern software \nsystem architectures become increasingly complex, the requirements for their performance have \nalso become more stringent. During the coding stage, developers find it difficult to effectively identify \nand resolve potential performance issues using traditional methods. This review focuses on the \napplication of artificial intelligence in two key areas: AI-assisted intelligent code generation and AI-\npovered code review. The review systematically analyzed the application of LLMs in software \ndevelopment, revealing a situation where efficiency gains coexist with quality challenges. In terms \nof code generation, models such as Code Llama and Copilot have significantly accelerated the \ndevelopment process. In the field of code review, AI can effectively handle code standards and low-\nseverity defects. However, in the future, this field still needs to address the issues of the reliability \nand security of the code generated by LLMs, as well as the insufficient explainability of the results of \nautomated performance analysis. The future research focus in this field lies in addressing issues \nsuch as the lack of interpretability and insufficient domain knowledge of LLMs. It is necessary to \nprioritize enhancing the reliability of AI recommendations and promoting the transformation of AI \nfrom an auxiliary tool to an intelligent Agent with self-repair capabilities, in order to achieve a truly \nefficient and secure human-machine collaboration paradigm. This article systematically reviews the \nrelevant progress, aiming to promote the transformation of software engineering from an artificial-\ndriven model to an AI-enhanced automated paradigm. It provides theoretical references for ensuring \nthe quality of backend code, improving product delivery speed, and enhancing system reliability. \nKeywords: AI; LLM; Code Generation; Code Review. \n1. Introduction \nRecently, the growing complexity of modern software applications has driven an increased \nemphasis on high-quality, maintainable source code, thereby heightening the difficulty for developers \nto write efficient and error-free code [1-3]. Therefore, there is an urgent need for a smarter approach \nto empover developers. The emergence of Artificial Intelligence has fundamentally transformed \nconventional approaches to code optimization and refactoring by introducing a new dimension of \nautomation [2]. A striking example is LLMs, which have exhibited remarkable potential in \nprogramming tasks [4], especially in code review [5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of"
  },
  {
    "chunk_id": "AIReview2025_chunk_002",
    "source_id": "AIReview2025",
    "text": "5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of the final software program [7], thereby \nreducing operational costs and driving the entire software engineering field towards a higher-level \nintelligent-assisted development paradigm. \n237 \n2. Literature Review \nIn recent years, the field of software engineering has developed rapidly. Software engineering \nencompasses the systematic and controlled design, development, maintenance, implementation and \nevolution of software systems [1]. As artificial intelligence has become a highly popular field at \npresent, it has also become a new component of software engineering [8]. To assist in software \ndevelopment, AI is now widely used in scenarios such as code generation and code review [9]. In \nparticular, large language models, as the latest breakthrough in the field of natural language \nprocessing in AI, have made particularly significant contributions to software development. LLM \nmainly employs the Transformer model as its core architecture. The LLM, which is built on large \ndatasets and advanced neural network architectures, demonstrates extremely high comprehension \ncapabilities, bringing about significant breakthroughs and possibilities to the field of software \ndevelopment [10]. For instance, models such as Codex [11], StarCode [12], Incoder [13], and Code \nLlama [14], Copilot[15]can all generate code with efficiency and accuracy comparable to that of \ndevelopers. These LLMs not only have extremely high efficiency but also excellent quality in code \nreview [10].  \n2.1 AI-Assisted Intelligent Code Generation \nCode generation is an automated process that converts structured or unstructured input information \n(such as natural language requirements descriptions, design documents, code snippets, etc.) into \nsource code. Its essence is to reflect the abstract intentions and task goals of the developers into \nspecific programming projects [16]. And based on LLM (Large Language Model) for code generation, \nby breaking down the tasks, having data storage with long-term and short-term memories, as well as \nthe invocation of external tools, these are currently important technical supports in the field of code \ngeneration [16]. This section will summarize the application effects and code generation quality of \nCodex and Copilot in the field of code generation. \n2.1.1 Code Llama \nThe Code Llama model series released by Meta AI focuses on code generation through long \nsequence contexts and instruction fine-tuning, and its mechanism is of great significance for the \ngeneration of complex logic at the back end [14]. \n2.1.1.1 Strong Context-Dependent and Long Sequence Processing Capabilities \nModern backend development typically involves complex distributed systems and microservice \narchitectures, and code generation must take into account global dependencies and architectural \nspecifications. Code L"
  },
  {
    "chunk_id": "AIReview2025_chunk_003",
    "source_id": "AIReview2025",
    "text": " these are currently important technical supports in the field of code \ngeneration [16]. This section will summarize the application effects and code generation quality of \nCodex and Copilot in the field of code generation. \n2.1.1 Code Llama \nThe Code Llama model series released by Meta AI focuses on code generation through long \nsequence contexts and instruction fine-tuning, and its mechanism is of great significance for the \ngeneration of complex logic at the back end [14]. \n2.1.1.1 Strong Context-Dependent and Long Sequence Processing Capabilities \nModern backend development typically involves complex distributed systems and microservice \narchitectures, and code generation must take into account global dependencies and architectural \nspecifications. Code Llama uses a sequence length of up to 16k tokens during training, and is capable \nof optimizing to support inputs of up to 100k tokens. This long-context support capability is crucial \nfor backend development, as it enables the model to understand global dependencies and adhere to \narchitectural conventions when generating new functions or modules [14].  \n2.1.1.2 Zero-Shot Instructions Follow Natural Language Programming \nThe Code Llama Instruct variant in the Code Llama series has been specifically optimized for \nhuman natural language programming instructions, featuring a powerful \"zero-shot instruction \nfollowing ability\" [14]. This means that developers can directly describe complex business logic or \nfunctional requirements using informal natural language (for example: \"Read the CSV file, only filter \nthe 7B model data, and draw a correlation heatmap of the Python and C++ fields\") [14]. The model \ncan accurately convert these instructions into complete, executable functions or scripts, significantly \nenhancing the efficiency from requirements to implementation. This capability is particularly \napplicable to common tasks in backend development such as API route definitions, parameter \nvalidation logic, or data processing functions.  \n238 \n2.1.1.3 Code Filling Capability \nCode Llama supports the code filling function based on the surrounding content, allowing \ndevelopers to set the boundaries of the code first, and then let the model automatically fill in the \nmissing business logic in the middle according to the context of the previous and following text [14]. \nThis provides core support for developers to carry out incremental development or iterative \noptimization in existing code \n2.1.2 Copilot \nAs the earliest and most widely used AI pair programming tool in the market, the core advantage \nof GitHub Copilot (based on the OpenAI Codex model) lies in its massive data training and real-time \ncode prediction mechanism [17]. \n2.1.2.1 Real-Time Context-Aware Prediction \nIts real-time context-aware prediction mechanism focuses on the immediacy based on the cursor \nposition, predicting the code snippet that best matches the current input. It is particularly good at \ngenerating repetitive template code, data structure initialization, and common I/O operation \nprocedures [15].  \n2.1.2.2 Translation of Comments into Code \nOne of the key features of Copilot is the ability to generate code based on natural language prompts \nor partial code input [17]. Developers usually provide natural language comments above functions or \ncode blocks to describe the required functionality. Copilot can then generate the corresponding \nfunction signatures, parameter definitions, and main logical framework based on this description. \nAlthough its instruction-following ability may not be as strong"
  },
  {
    "chunk_id": "AIReview2025_chunk_004",
    "source_id": "AIReview2025",
    "text": " real-time context-aware prediction mechanism focuses on the immediacy based on the cursor \nposition, predicting the code snippet that best matches the current input. It is particularly good at \ngenerating repetitive template code, data structure initialization, and common I/O operation \nprocedures [15].  \n2.1.2.2 Translation of Comments into Code \nOne of the key features of Copilot is the ability to generate code based on natural language prompts \nor partial code input [17]. Developers usually provide natural language comments above functions or \ncode blocks to describe the required functionality. Copilot can then generate the corresponding \nfunction signatures, parameter definitions, and main logical framework based on this description. \nAlthough its instruction-following ability may not be as strong as the specially fine-tuned Instruct \nmodel, it performs well in implementing the functions based on the comment prompts [17].  \n \nTable 1. Comparative Analysis of Code Llama and Copilot in Code Generation \nAspect \nCode Llama \nGitHub Copilot \nCore Positioning \nOpen Foundation Model, focused on model \nperformance and customization. \nCommercial Integration Tool, focused \non user experience and real-time \ndevelopment efficiency. \nContext Capability \nSupports inputs up to 100k tokens, \nhighlighting strong long-sequence context \nability \nRelies on a longer context window, \nanalyzing code and documentation in \nthe IDE in real-time. \nInstruction Following \nStrong zero-shot instruction following \nability, directly converting natural \nlanguage requests into complex code \nTriggered by comments or function \nsignatures, generating code completion \nor suggestions. \nQuality Focus \nEnhancing SOTA performance and \ngeneralizability, providing open-source \ncommunity support. \nImproving developer productivity and \nusability, addressing low-severity code \nissues \n \nBy examining the mechanisms of Code Llama and GitHub Copilot, it can be seen that the \napplication of LLM in the field of code generation has demonstrated characteristics of efficiency-\ndriven and specialized division of labor. \n2.2 AI-Powered Code Review \nCode review is a crucial stage in the software development life cycle, where the source code is \nexamined for errors and the practical rationality of the code is ensured [10]. The current code review \nprocess is problematic due to the large volume of code. It requires a significant amount of effort from \nthe reviewers, and the traditional code review tools lack depth, failing to provide more in-depth \nfeedback. Based on this background, there has emerged a trend of using LLMs for automated code \nreview. The core value of LLM in code review lies in its powerful semantic understanding and context \nreasoning capabilities, enabling it to surpass traditional rule-based static analysis tools [5]. This \n239 \nsection will demonstrate the specific implementation and application of LLM in code review by \ntaking Copilot as an example. \n2.2.1 Copilot \n2.2.1.1 The Application Focuses on the Operational Mechanism \nGitHub Copilot, as a pioneer in the field of AI-assisted programming, its code review function \ndemonstrates the potential of LLM in this area [18]. \nThe operation mechanism of GitHub Copilot in code review mainly involves analyzing the code \nblocks or modifications submitted by developers in the integrated development environment (IDE) \nin real time, and providing feedback in the form of embedded comments, suggested completions, or \ndifferentiated suggestions"
  },
  {
    "chunk_id": "AIReview2025_chunk_005",
    "source_id": "AIReview2025",
    "text": " static analysis tools [5]. This \n239 \nsection will demonstrate the specific implementation and application of LLM in code review by \ntaking Copilot as an example. \n2.2.1 Copilot \n2.2.1.1 The Application Focuses on the Operational Mechanism \nGitHub Copilot, as a pioneer in the field of AI-assisted programming, its code review function \ndemonstrates the potential of LLM in this area [18]. \nThe operation mechanism of GitHub Copilot in code review mainly involves analyzing the code \nblocks or modifications submitted by developers in the integrated development environment (IDE) \nin real time, and providing feedback in the form of embedded comments, suggested completions, or \ndifferentiated suggestions [18]. It can not only automatically suggest a uniform code style, variable \nnaming and comment format, but also identify and correct spelling errors, grammatical mistakes, as \nwell as simple structural adjustments that may affect the clarity of the code [18]. \n2.2.1.1.1 Case Study: Unsafe Function Usage Warning \nConsider a backend developer writing a function in C to handle user input, utilizing a traditional, \nunsafe string copying function (shown in Figure 1). \n \n \nFigure 1. Copilot Code Review Case \n \nImmediately after the developer types strcpy, Copilot might trigger a real-time warning, indicating \nthat this function poses a Buffer Overflow security risk because it does not check the size of the \ndestination buffer. \nThen, Copilot suggests using a safer alternative, such as strncpy or strcpy_s, emphasizing that the \nbuffer size must always be passed as a parameter. This embedded real-time suggestion helps \ndevelopers integrate memory safety awareness directly into the coding phase. \n2.2.1.2 The Challenges of Detecting Critical Security Flaws \nAlthough LLM has a strong ability for semantic understanding, it also has some limitations when \nit comes to detecting critical security vulnerabilities [5]. A study systematically evaluated the \neffectiveness of Copilot's code review function in detecting security flaws. The results showed that it \noften failed to detect critical vulnerabilities such as SQL Injection, Cross-Site Scripting (XSS), and \ninsecure deserialization [18]. \nThis incident highlights the limitations of current AI-assisted code review: The model excels in \nensuring code \"readability\" and \"conformity\", but still cannot replace dedicated security tools and \nmanual review in areas that require deep security knowledge and complex data flow analysis [5].  \n \n \n \n \n240 \n3. Discussion \nTable 2. The Comparison of AI-Assisted Code Generation and AI-Driven Code Review \nComparison Metric \nAI-Assisted Code Generation \nAI-Driven Code Review \nPrimary Objective \nEnhance Developer Productivity and accelerate \ndelivery speed. Automate code writing to reduce \nrepetitive work. \nEnsure Code Quality and proactive \ndefect detection. Reduce human effort \nand broaden the scope of issue \ndetection. \nAdvantages \n1. Strong Instruction Following: Can convert natural \nlanguage requirements into executable code (Code \nLlama [14]). \n2. Long Context Dependency: Supports understanding \ncomplex project-level contexts and dependencies. \n3. Real-Time Infilling: Embedded tools like Copilot \n[15] provide instant code completion, significantly \nboosting coding speed. \n1. Real-Time Embedded Feedback: \nInstantly identifies and corrects code \nstyle, idi"
  },
  {
    "chunk_id": "AIReview2025_chunk_006",
    "source_id": "AIReview2025",
    "text": "-Driven Code Review \nPrimary Objective \nEnhance Developer Productivity and accelerate \ndelivery speed. Automate code writing to reduce \nrepetitive work. \nEnsure Code Quality and proactive \ndefect detection. Reduce human effort \nand broaden the scope of issue \ndetection. \nAdvantages \n1. Strong Instruction Following: Can convert natural \nlanguage requirements into executable code (Code \nLlama [14]). \n2. Long Context Dependency: Supports understanding \ncomplex project-level contexts and dependencies. \n3. Real-Time Infilling: Embedded tools like Copilot \n[15] provide instant code completion, significantly \nboosting coding speed. \n1. Real-Time Embedded Feedback: \nInstantly identifies and corrects code \nstyle, idioms, and low-severity \ndefects within the IDE [18]. \n2. Semantic Understanding: Can \nidentify Code Smells and potential \nfuture risks that traditional tools often \nmiss [13]. \n3. Reduced Human Cost: Effectively \nhandles a large volume of low-level, \nrepetitive review tasks, allowing \nhuman focus on core logic. \nDisadvantages \n1. Reliability Risk: Generated code correctness rate is \nmoderate (Copilot approx. 46.3% [17]). \n2. Security Risk: Prone to generating code with \nsecurity vulnerabilities or unsafe API usage [18]. \n3. Verification Cost: Developers must spend extra time \nverifying and refactoring the generated code. \n1. Inadequate High-Risk \nVulnerability Detection: Struggles to \nfind complex, data-flow-dependent \nsecurity flaws like SQL Injection \n[18]. \n2. Lack of Explainability: Review \nsuggestions lack underlying \nreasoning, affecting developer trust \nand adoption. \nFuture Development Focus \n1. Agent-Based Closed-Loop Generation: Developing \nAI Agent models capable of self-testing and self-\ncorrection. \n2. Quality Improvement: Focusing on generating high-\nreliability, high-security code to reduce post-\nverification costs. \n3. Customization: Enhancing fine-tuning capabilities \nfor specific enterprise architectures and domain \nlanguages. \n1. Multimodal Analysis: Combining \ncode, test reports, and architectural \ndiagrams for more robust defect \nprediction. \n2. Explainable Security Review: \nImproving security vulnerability \ndetection and providing detailed Root \nCause Analysis. \n3. Human-AI Collaboration \nOptimization: Exploring seamless \nintegration of LLMs into the human \nreview process to boost overall \nefficiency. \n4. Conclusion \nIn the field of code generation, LLMs such as Code Llama and GitHub Copilot have become \nindispensable tools for developers in their daily work. By leveraging long sequence context and the \nability to follow natural language instructions, they achieve rapid conversion from requirement \ndescriptions to function/module skeletons, significantly accelerating the code delivery speed. On the \nother hand, in code review, LLMs demonstrate strong semantic understanding and context reasoning \ncapabilities, effectively identifying code anomalies, potential bugs, and providing best practice \nsuggestions. Tools like Copilot significantly reduce the burden on human reviewers by automating \nformatting and handling low-severity issues. \nAlthough AI has achieved remarkable results in enhancing efficiency and ensuring initial quality, \nit still faces multiple significant challenges in the process of moving towards a fully automated \nintelligent development paradigm. \nAlthough the"
  },
  {
    "chunk_id": "AIReview2025_chunk_007",
    "source_id": "AIReview2025",
    "text": "indispensable tools for developers in their daily work. By leveraging long sequence context and the \nability to follow natural language instructions, they achieve rapid conversion from requirement \ndescriptions to function/module skeletons, significantly accelerating the code delivery speed. On the \nother hand, in code review, LLMs demonstrate strong semantic understanding and context reasoning \ncapabilities, effectively identifying code anomalies, potential bugs, and providing best practice \nsuggestions. Tools like Copilot significantly reduce the burden on human reviewers by automating \nformatting and handling low-severity issues. \nAlthough AI has achieved remarkable results in enhancing efficiency and ensuring initial quality, \nit still faces multiple significant challenges in the process of moving towards a fully automated \nintelligent development paradigm. \nAlthough the code generated by LLM has high grammatical validity, its logical correctness still \nneeds to be improved. For instance, the evaluation results show that the proportion of the code \ngenerated by Copilot that is logically correct is only approximately 46.3%. This indicates that \ndevelopers still need to invest a lot of effort in verification and debugging. In terms of code review, \nLLM-assisted tools (such as Copilot) perform poorly in detecting high-severity security \n241 \nvulnerabilities (such as SQL injection, XSS, and insecure deserialization). This reveals the limitations \nof LLM in complex data flow analysis and deep security knowledge, and it cannot replace dedicated \nsecurity tools and manual audits. \nFuture research on AI-driven software development needs to combine the semantic understanding \nadvantages of LLM with the accuracy of traditional static analysis tools to build hybrid AI Agents. \nThese Agents will focus on security sensitivity and performance optimization, and be capable of \nperforming complex program analysis (such as data flow and control flow analysis), in order to \novercome the limitations of current LLM in detecting high-risk defects. It is also necessary to further \nfine-tune LLM for specific backend technology stacks and enterprise internal code repositories to \nenhance the model's ability to understand complex business logic and architectural specifications, and \nimprove the domain relevance and accuracy of the generated code. \nReferences \n[1] Nyaga, F. (2025). AI-Driven Software Engineering: A Systematic Review of Machine Learning\u2019s Impact \nand Future Directions. Preprints. https://doi.org/10.20944/preprints202504.0174.v1. \n[2] Konakanchi, S. (2025). Artificial Intelligence in Code Optimization and Refactoring. Journal of Data and \nDigital Innovation (JDDI), 2(1), 9-35. \n[3] Rao, B. S. M., Bandari, S. S. G., & Nc, R. (2025). Replacing AI Agents for Backend. \n[4] Fang, C., Miao, N., Srivastav, S., Liu, J., Zhang, R., Fang, R., ... & Homayoun, H. (2024). Large language \nmodels for code analysis: Do {LLMs} really do their job?. In 33rd USENIX Security Symposium \n(USENIX Security 24) (pp. 829-846). \n[5] Konda, R. AI-Powered Code Review Enhancing Software Quality with Intelligent Agents. IJLRP-\nInternational Journal of Leading Research Publication, 4(3). \n"
  },
  {
    "chunk_id": "AIReview2025_chunk_008",
    "source_id": "AIReview2025",
    "text": ". S. G., & Nc, R. (2025). Replacing AI Agents for Backend. \n[4] Fang, C., Miao, N., Srivastav, S., Liu, J., Zhang, R., Fang, R., ... & Homayoun, H. (2024). Large language \nmodels for code analysis: Do {LLMs} really do their job?. In 33rd USENIX Security Symposium \n(USENIX Security 24) (pp. 829-846). \n[5] Konda, R. AI-Powered Code Review Enhancing Software Quality with Intelligent Agents. IJLRP-\nInternational Journal of Leading Research Publication, 4(3). \n[6] Nam, D., Macvean, A., Hellendoorn, V., Vasilescu, B., & Myers, B. (2024, April). Using an llm to help \nwith code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software \nEngineering (pp. 1-13). \n[7] Liu, F., Liu, Y., Shi, L., Huang, H., Wang, R., Yang, Z., ... & Ma, Y. (2024). Exploring and evaluating \nhallucinations in llm-powered code generation. arXiv preprint arXiv:2404.00971. \n[8] Crawford, T., Duong, S., Fueston, R., Lawani, A., Owoade, S., Uzoka, A., ... & Yazdinejad, A. (2023). \nAI in software engineering: a survey on project management applications. arXiv preprint arXiv: 2307. \n15224. \n[9] Agha, A. S. (2025). Evaluating AI Efficiency in Backend Software Development-A Comparative \nAnalysis Across Frameworks. \n[10] Rasheed, Z., Sami, M. A., Waseem, M., Kemell, K. K., Wang, X., Nguyen, A., ... & Abrahamsson, P. \n(2024). Ai-powered code review with llms: Early results. arXiv preprint arXiv:2404.18496. \n[11] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., ... & Zaremba, W. (2021). \nEvaluating large language models trained on code. arXiv preprint arXiv:2107.03374. \n[12] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., ... & de Vries, H. (2023). Starcoder: \nmay the source be with you!. arXiv preprint arXiv:2305.06161. \n[13] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., ... & Lewis, M. (2022). Incoder: A \ngenerative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999."
  },
  {
    "chunk_id": "AIReview2025_chunk_009",
    "source_id": "AIReview2025",
    "text": "03374. \n[12] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., ... & de Vries, H. (2023). Starcoder: \nmay the source be with you!. arXiv preprint arXiv:2305.06161. \n[13] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., ... & Lewis, M. (2022). Incoder: A \ngenerative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999. p. \n[14] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., ... & Synnaeve, G. (2023). Code \nllama: Open foundation models for code. arXiv preprint arXiv:2308.12950. \n[15] Yetistiren, B., Ozsoy, I., & Tuzun, E. (2022, November). Assessing the quality of GitHub copilot\u2019s code \ngeneration. In Proceedings of the 18th international conference on predictive models and data analytics \nin software engineering (pp. 62-71). \n[16] Dong, Y., Jiang, X., Qian, J., Wang, T., Zhang, K., Jin, Z., & Li, G. (2025). A survey on code generation \nwith llm-based agents. arXiv preprint arXiv:2508.00083. \n[17] Yeti\u015ftiren, B., \u00d6zsoy, I., Ayerdem, M., & T\u00fcz\u00fcn, E. (2023). Evaluating the code quality of ai-assisted \ncode generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. arXiv \npreprint arXiv:2304.10778. \n[18] Amro, A., & Alalfi, M. H. (2025). GitHub's Copilot Code Review: Can AI Spot Security Flaws Before \nYou Commit?. arXiv preprint arXiv:2509.13650. \n"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_001",
    "source_id": "CopilotQuality2022",
    "text": ".\n.\nLatest updates: h\ue03cps://dl.acm.org/doi/10.1145/3558489.3559072\n.\n.\nRESEARCH-ARTICLE\nAssessing the quality of GitHub copilot\u2019s code generation\nBURAK YETISTIREN, Bilkent University, Ankara, Ankara, Turkey\n.\nISIK OZSOY, Bilkent University, Ankara, Ankara, Turkey\n.\nERAY T\u00dcZ\u00dcN, Bilkent University, Ankara, Ankara, Turkey\n.\n.\n.\nOpen Access Support provided by:\n.\nBilkent University\n.\nPDF Download\n3558489.3559072.pdf\n23 February 2026\nTotal Citations: 93\nTotal Downloads: 7054\n.\n.\nPublished: 07 November 2022\n.\n.\nCitation in BibTeX format\n.\n.\nPROMISE '22: 18th International\nConference on Predictive Models and\nData Analytics in So\ue039ware Engineering\nNovember 17, 2022\nSingapore, Singapore\n.\n.\nConference Sponsors:\nSIGSOFT\nPROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in So\ue039ware Engineering (November 2022)\nh\ue03cps://doi.org/10.1145/3558489.3559072\nISBN: 9781450398602\n.\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nBurak Yetistiren\nburakyetistiren@hotmail.com\nBilkent University\nAnkara, Turkey\nIsik Ozsoy\nozsoyisik@gmail.com\nBilkent University\nAnkara, Turkey\nEray Tuzun\neraytuzun@cs.bilkent.edu.tr\nBilkent University\nAnkara, Turkey\nABSTRACT\nThe introduction of GitHub\u2019s new code generation tool, GitHub\nCopilot, seems to be the first well-established instance of an AI\npair-programmer. GitHub Copilot has access to a large number of\nopen-source projects, enabling it to utilize more extensive code in\nvarious programming languages than other code generation tools.\nAlthough the initial and informal assessments are promising, a\nsystematic evaluation is needed to explore the limits and benefits\nof GitHub Copilot. The main objective of this study is to assess\nthe quality of generated code provided by GitHub Copilot. We\nalso aim to evaluate the impact of the quality and variety of input\nparameters fed to GitHub Copilot. To achieve this aim, we created\nan experimental setup for evaluating the generated code in terms\nof validity, correctness, and efficiency. Our results suggest that\nGitHub Copilot was able to generate valid code with a 91.5% success\nrate. In terms of code correctness, out of 164 problems, 47 (28.7%)\nwere correctly, while 84 (51.2%) were partially correctly, and 33\n(20.1%) were incorrectly generated. Our empirical analysis shows\nthat GitHub Copilot is a promising tool based on the results we\nobtained, however further and more comprehensive assessment is\nneeded in the future.\nCCS CONCEPTS\n\u2022 Software and its engineering \u2192Source code generation.\nKEYWORDS\nGitHub Copilot, code generation, code completion, AI pair program-\nmer"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_002",
    "source_id": "CopilotQuality2022",
    "text": " validity, correctness, and efficiency. Our results suggest that\nGitHub Copilot was able to generate valid code with a 91.5% success\nrate. In terms of code correctness, out of 164 problems, 47 (28.7%)\nwere correctly, while 84 (51.2%) were partially correctly, and 33\n(20.1%) were incorrectly generated. Our empirical analysis shows\nthat GitHub Copilot is a promising tool based on the results we\nobtained, however further and more comprehensive assessment is\nneeded in the future.\nCCS CONCEPTS\n\u2022 Software and its engineering \u2192Source code generation.\nKEYWORDS\nGitHub Copilot, code generation, code completion, AI pair program-\nmer, empirical study\nACM Reference Format:\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the Quality of\nGitHub Copilot\u2019s Code Generation. In Proceedings of the 18th International\nConference on Predictive Models and Data Analytics in Software Engineering\n(PROMISE \u201922), November 17, 2022, Singapore, Singapore. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3558489.3559072\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\n\u00a9 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9860-2/22/11...$15.00\nhttps://doi.org/10.1145/3558489.3559072\n1\nINTRODUCTION\nGitHub Copilot1 is a code generation tool that utilizes a variety of\ntechnologies, including a compatible IDE, and the OpenAI Codex\nModel2. GitHub Copilot can be installed and used as an extension to\nVisual Studio Code, Neovim, IDEs developed by JetBrains [6], and\nGitHub Codespaces3. The underlying service continuously takes\ncode samples from the users and sends the snippets to the under-\nlying OpenAI Codex Model. GitHub Copilot generates the code\nand presents the results of the OpenAI Codex Model by adjusting\nthe generated code to the current workspace of the programmer\n[4]. The Codex model relies on Generative Pre-trained Transformer\n(GPT) models that the company previously invented for text gen-\neration. The public code available on GitHub was used during the\nfine-tuning of the model to implement the code recognition and\ngeneration capabilities.\nThere are mixed reviews about the prospect of the GitHub Copi-\nlot. On the one hand, reducing development time, easing the devel-\nopment process by suggesting code for small utilities, and suggest-\ning better alternatives for code snippets are some of the positive\nfeedback developers provided [2, "
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_003",
    "source_id": "CopilotQuality2022",
    "text": ". GitHub Copilot generates the code\nand presents the results of the OpenAI Codex Model by adjusting\nthe generated code to the current workspace of the programmer\n[4]. The Codex model relies on Generative Pre-trained Transformer\n(GPT) models that the company previously invented for text gen-\neration. The public code available on GitHub was used during the\nfine-tuning of the model to implement the code recognition and\ngeneration capabilities.\nThere are mixed reviews about the prospect of the GitHub Copi-\nlot. On the one hand, reducing development time, easing the devel-\nopment process by suggesting code for small utilities, and suggest-\ning better alternatives for code snippets are some of the positive\nfeedback developers provided [2, 7, 13]. On the other hand, it is\nargued that the current state of technology is not promising enough\nto match human ingenuity. Considering the previous studies, the\nservice requires a vast amount of human interaction, making the\ncoding routine still heavily reliant on the programmer [1].\nThe reviews about GitHub Copilot we touched upon only in-\nclude brief and heuristic feedback in terms of the evaluation of the\nservice. We agree with the general consensus of the opinions about\nGitHub Copilot and find it worthwhile to evaluate the possible en-\nhancements a service like GitHub Copilot can offer. Clearly, GitHub\nCopilot is capable of generating code, but its value is undetermined.\nTo systematically evaluate GitHub Copilot, we propose to construct\nan experimental setup to assess the generated code in terms of\nvalidity, correctness, and efficiency. In this context, we defined the\nfollowing research questions:\nRQ1 What is the quality of the code generated by GitHub Copilot?\nRQ1.1 How valid are GitHub Copilot\u2019s code suggestions?\nRQ1.2 How correct are GitHub Copilot\u2019s code suggestions?\nRQ1.3 How efficient are GitHub Copilot\u2019s code suggestions?\nRQ2 What is the effect of using the docstrings on the generated\ncode quality?\nRQ3 What is the effect of using appropriate function names on the\ngenerated code quality?\nIn the following sections, we first elaborate on our experimental\nsetup in Section 2. In Section 3, we present the results we gathered\nfrom our setup. In Section 4, we share and evaluate our results. In\n1copilot.github.com\n2openai.com/blog/openai-codex/\n3github.com/features/codespaces\n62\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nSection 5, we discuss factors that might influence the validity of our\nresults. We provide an overview of other works that study GitHub\nCopilot in Section 6. Lastly, we conclude our study in Section 7.\n2\nMETHODOLOGY\nIn our experiment, we used HumanEval dataset [3], which is de-\nscribed in Section 2.1. To address the research questions, we created\nan experimental setup, which systematically evaluates the effective-\nness of GitHub Copilot that is described in Section 2.2. The details\nof our assessment are presented in Sections 2.3\u20132.5. In Sections\n2.6 and 2.7, we elaborate on the two additional experiments we\ncon"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_004",
    "source_id": "CopilotQuality2022",
    "text": "5, we discuss factors that might influence the validity of our\nresults. We provide an overview of other works that study GitHub\nCopilot in Section 6. Lastly, we conclude our study in Section 7.\n2\nMETHODOLOGY\nIn our experiment, we used HumanEval dataset [3], which is de-\nscribed in Section 2.1. To address the research questions, we created\nan experimental setup, which systematically evaluates the effective-\nness of GitHub Copilot that is described in Section 2.2. The details\nof our assessment are presented in Sections 2.3\u20132.5. In Sections\n2.6 and 2.7, we elaborate on the two additional experiments we\nconducted to test the effect of the function names and explanations\nof the generated code quality.\n2.1\nHumanEval Dataset\nFor our experiment, we use the HumanEval dataset [3]. This dataset\ncontains 164 problems. Each problem is accompanied by a task ID,\na prompt, the canonical solution, and unit tests. The structure of\na problem can be viewed in Figure 1. The task ID is the ID of that\nparticular problem which ranges from 0 to 163. The prompt part\ncontains the function prototype, the explanation of the problem,\nsome function calls and their output in a Python docstring, and\nlibrary imports, if applicable. A canonical solution is a solution\nto the problem provided by a \u201chuman\u201d programmer. The test part\ncontains unit tests as a Python function.\nFrom our literature survey4 on GitHub Copilot, we found that the\ncombination of code comments and function signatures yields more\nrobust results. We pass the function prototype and the docstring\nas input to GitHub Copilot. An example code generation done by\nGitHub Copilot, where the input problem is shown in Figure 1 can\nbe viewed in Listing 1.\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold:\nfloat) -> bool:\n\"\"\" Check if in given list of numbers , are any two\nnumbers closer to each other than given threshold.\n>>> has_close_elements ([1.0, 2.0, 3.0], 0.5)\nFalse\n>>> has_close_elements ([1.0, 2.8, 3.0, 4.0, 5.0,\n2.0], 0.3)\nTrue\n\"\"\"\nfor i in range(len(numbers)):\nfor j in range(i + 1, len(numbers)):\nif abs(numbers[i] - numbers[j]) < threshold:\nreturn True\nreturn False\nListing 1: Generated Code for the Example Problem (ID: 0)\n2.2\nExperimental Setup\nIn Figure 2, we provide a step-by-step illustration of the experi-\nment\u2019s workflow. Given the HumanEval problem dataset [3], we\nstart our experiment by extracting the problems. We achieve this\nby reading the dataset and representing each problem contained\nin the dataset with a separate JSON format file. After completing\n4https://github.com/burakyetistiren/-An-Empirical-Evaluation-of-GitHub-Copilot-s-\nCode-Generation-/blob/main/misc/article_names_and_links.pdf\nthe extraction procedure, we save the canonical solution, unit tests,\nand the prompt of a problem as separate Python files to the direc-\ntory corresponding"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_005",
    "source_id": "CopilotQuality2022",
    "text": " (ID: 0)\n2.2\nExperimental Setup\nIn Figure 2, we provide a step-by-step illustration of the experi-\nment\u2019s workflow. Given the HumanEval problem dataset [3], we\nstart our experiment by extracting the problems. We achieve this\nby reading the dataset and representing each problem contained\nin the dataset with a separate JSON format file. After completing\n4https://github.com/burakyetistiren/-An-Empirical-Evaluation-of-GitHub-Copilot-s-\nCode-Generation-/blob/main/misc/article_names_and_links.pdf\nthe extraction procedure, we save the canonical solution, unit tests,\nand the prompt of a problem as separate Python files to the direc-\ntory corresponding to the problem\u2019s ID. Subsequently, we generate\nsolutions by using an already prepared Python file containing the\nprompt. This prompt is the combination of the function signature\nand docstring contained in the function body. Given the dynamic\ncharacteristic of GitHub Copilot in terms of the interactions be-\ntween the programmer and the service, we implement the code\ngeneration step of our experiment manually.\nAfter the code generation step is completed, we start the assess-\nment phase by executing the tests on the generated solutions to\nassess code validity and code correctness. Afterward, we inspect the\ntime and space complexities of the generated code and the canoni-\ncal solution, by sending corresponding requests using OpenAI API5\nand compare the results. For each step of the assessment phase, we\nsave the results of the individual assessment related to the problem.\nThe extracted results can be seen in our reproduction package6.\nWe further test the validity of our findings in our literature survey\nabout providing GitHub Copilot with code comments and function\nsignatures, by implementing two additional experiments about\nthe significance of function names, parameters, and comments\nexplained in Sections 2.6 and 2.7.\ntask_id\nHumanEval/0\nprompt\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\ncanonical_solution\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\ntest\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\ndef check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_006",
    "source_id": "CopilotQuality2022",
    "text": " idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\ntest\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\ndef check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\nFigure 1: Example Problem (ID: 0) from HumanEval dataset\n5openai.com/api\n6https://github.com/burakyetistiren/-An-Empirical-Evaluation-of-GitHub-Copilot-s-\nCode-Generation-/blob/main/misc/Copilot_Results.pdf\n63\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nExtract Problems\nRead Problem\nFor all problems\nA\nA\nA\nM\nAutomated process\nManual process\nProcess implemented with Python\nProcess implemented with GitHub\nCopilot\nProcess implemented with OpenAI\nAPI\nAssess Code\nValidity\nA\nExtract Problems\nA\nRead Problem\nA\nExtract and Save\nPrompt\nA\nExtract and Save\nTests\nA\nExtract and Save\nCanonical Solution\nA\nGenerate Solution\nM\nAssess Code\nCorrectness\nA\nAssess Code Efficiency\n(Canonical Solution)\nA\nUnit Tests\nAssess Code Efficiency\n(Generated Solution)\nA\nFigure 2: Experiment Workflow\n2.3\nCode Validity (RQ1.1)\nCode validity is assessed in terms of how a given code segment\nis compliant with the rules and regulations (i.e., syntax rules) of\na given programming language and with any errors that could be\nraised during runtime. The dataset we use is constructed for the\nPython programming language; therefore, to check for code validity,\nwe make use of the Python 3.8 interpreter. After we initiate the\nprogram, we await any further errors that could be raised during\nruntime and record such cases.\n2.4\nCode Correctness (RQ1.2)\nFor code correctness, we want to assess the extent to which the\ngenerated code performs as intended"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_007",
    "source_id": "CopilotQuality2022",
    "text": "\n2.3\nCode Validity (RQ1.1)\nCode validity is assessed in terms of how a given code segment\nis compliant with the rules and regulations (i.e., syntax rules) of\na given programming language and with any errors that could be\nraised during runtime. The dataset we use is constructed for the\nPython programming language; therefore, to check for code validity,\nwe make use of the Python 3.8 interpreter. After we initiate the\nprogram, we await any further errors that could be raised during\nruntime and record such cases.\n2.4\nCode Correctness (RQ1.2)\nFor code correctness, we want to assess the extent to which the\ngenerated code performs as intended. As we previously stated, the\nproblems in the HumanEval dataset are accompanied by problem-\nspecific unit tests. On average, each problem comes with 7.7 unit\ntests [3]. We measured the code correctness as passed unit tests\ndivided by all unit tests for a specific problem.\n2.5\nCode Efficiency (RQ1.3)\nWhen analyzing the given generated solution, we employ time and\nspace complexity analysis and use the big-O: O(\ud835\udc53(\ud835\udc5b)) notation,\nwhere \ud835\udc53(\ud835\udc5b) is the solution generated by GitHub Copilot, and O sets\nan upper bound on time and space complexities.\nTo obtain the time and space complexities, we use the OpenAI\nAPI. Given the canonical solution and the generated code as input,\nwe make a query to the API, requesting the time and space com-\nplexities of both algorithms. We then evaluate GitHub Copilot\u2019s\nperformance for that particular problem from the dataset.\n2.6\nUsing only Function Names and Parameters\nWithout Prompt (RQ2)\nWe removed the docstrings from the problems to assess the effect\nof docstring on the generated solution. The docstring of a given\nproblem in the HumanEval dataset includes the explanation of the\nfunction as the intended purpose of what that problem should be\ndoing. This explanation is then accompanied by some sample test\ncases and their results (an example can be seen in the \u201cprompt\u201d\npart in Figure 1). We used GitHub Copilot to generate code by only\nusing the name and the parameters of the function as a reference.\nWe aimed to see how our results would change in comparison to\nour previous results.\n2.7\nUsing Dummy Function Names (RQ3)\nWe changed the function names of the problems with a dummy\nfunction name \u2018foo\u2019, to assess the effect of meaningful function\nnames on the generated solution. We used GitHub Copilot to gen-\nerate code using only the parameters and prompt of the function\nas a reference.\n3\nRESULTS\nAfter we executed our pipeline in Figure 2, we saved our results\nin a \u2018.csv\u2019 format file. To replicate our results, our setup could be\naccessed and downloaded 7 (Replication Package).\n3.1\nCode Validity (RQ1.1)\nAs we noted earlier, our metric for code validity is binary, such that\nif any errors were raised during the execution of a given Python\nscript, we denoted that script as invalid.\nOut of 164 generations to the problems, 14 were invalid and 150\nwere valid. This yielded a 91.5% success rate in terms of"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_008",
    "source_id": "CopilotQuality2022",
    "text": " code using only the parameters and prompt of the function\nas a reference.\n3\nRESULTS\nAfter we executed our pipeline in Figure 2, we saved our results\nin a \u2018.csv\u2019 format file. To replicate our results, our setup could be\naccessed and downloaded 7 (Replication Package).\n3.1\nCode Validity (RQ1.1)\nAs we noted earlier, our metric for code validity is binary, such that\nif any errors were raised during the execution of a given Python\nscript, we denoted that script as invalid.\nOut of 164 generations to the problems, 14 were invalid and 150\nwere valid. This yielded a 91.5% success rate in terms of generating\nvalid code.\n3.2\nCode Correctness (RQ1.2)\nWe used the number of passed unit tests divided by all unit tests\nto calculate the success percentage of the code. In Figure 3, we\nprovided the percentage distribution of code generations falling\nunder different categories (correct, partially correct, and incorrect).\nWe observed that for 28.7% of the problems, GitHub Copilot man-\naged to generate the correct code for the given problem, whereas it\n7https://github.com/burakyetistiren/-An-Empirical-Evaluation-of-GitHub-Copilot-s-\nCode-Generation-\n64\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\ncompletely failed to provide a correct solution for 20.1% of the prob-\nlems. Generated solutions for the remaining 51.2% of the problems\nwere partially correct. Partially correct generations are the ones\nthat pass at least one of the unit tests but not all of them. We believe\npartially correct generations are useful, with the assumption that if\nat least one unit test is passing, this is a potential indicator that with\nfurther improvements by the programmer, the code could become\ncorrect. To analyze the partially correct code generations, we cre-\nated a second pie chart in Figure 4, in which we eliminated correct\nand incorrect code generations, yielding 84 problems. We divided\n(0, 100) success space into four intervals. GitHub Copilot managed\na success rate of 13.1% for the interval of 100% > \ud835\udc41> 75%. Follow-\ningly, code was generated with a correctness score in the interval\nof 75% \u2265\ud835\udc41> 50%, 28.6% of the time. The next interval contained\nthe greatest fragment of the partially correct code generations with\na score of 35.7%, belonging to the interval of 50% \u2265\ud835\udc41> 25%. For\nthe last interval of 25% \u2265\ud835\udc41> 0%, the score was 22.6%.\n28.7%\nProportion of Correct\nGenerations\n51.2%\nProportion of Partially\nCorrect Generations\n20.1%\nProportion of Incorrect\nGenerations\nFigure 3: Distribution of Code Generations in terms of Cor-\nrectness\n13.1%\n100% > \ud835\udc41> 75%\n28.6%\n75% \u2265\ud835\udc41> 50%\n35.7%\n50"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_009",
    "source_id": "CopilotQuality2022",
    "text": "\nthe greatest fragment of the partially correct code generations with\na score of 35.7%, belonging to the interval of 50% \u2265\ud835\udc41> 25%. For\nthe last interval of 25% \u2265\ud835\udc41> 0%, the score was 22.6%.\n28.7%\nProportion of Correct\nGenerations\n51.2%\nProportion of Partially\nCorrect Generations\n20.1%\nProportion of Incorrect\nGenerations\nFigure 3: Distribution of Code Generations in terms of Cor-\nrectness\n13.1%\n100% > \ud835\udc41> 75%\n28.6%\n75% \u2265\ud835\udc41> 50%\n35.7%\n50% \u2265\ud835\udc41> 25%\n22.6%\n25% \u2265\ud835\udc41> 0%\nFigure 4: Distribution of Correctness Scores among Partially\nCorrect Generations\n3.3\nCode Efficiency (RQ1.3)\nTable 1 presents the results for time & space complexities. The\nrows represent functional correctness, and the columns show the\nefficiency comparison of the generated code and the canonical solu-\ntion. The efficiency results of valid incorrect and invalid incorrect\nsolutions are also included to eliminate any confusion that could\nbe caused by the absence of these problems.\nTime complexity: When we observed the efficiency results\nin terms of time complexities for the correctly generated code, for\n87.2% of the problems, GitHub Copilot was as efficient as a human\nprogrammer, as the canonical solutions are hand-written in the\nHumanEval dataset [3]. For the remaining 12.8% of the problems,\nGitHub Copilot managed to generate a more efficient solution. For\nthe partially correct code generations, the total number of problems\nwas 84. For 64 problems out of 84, which is 76.2% of the solutions\ngenerated by GitHub Copilot yielded the same level of efficiency\nas the canonical solution. Whereas for 3.6% of the partially correct\nsolutions, the code generated by GitHub Copilot was less efficient\nTable 1: Efficiency Results for Time Complexity (TC) and\nSpace Complexity (SC)\nMore Efficient\nSame in Efficiency\nLess Efficient\nTotal\nTC\nSC\nTC\nSC\nTC\nSC\nCorrect\n6\n2\n41\n42\n0\n3\n47\nPartially Correct\n17\n7\n64\n70\n3\n7\n84\nValid Incorrect\n3\n5\n14\n12\n2\n2\n19\nInvalid Incorrect\n2\n1\n11\n13\n1\n0\n14\nTotal\n28\n15\n130\n137\n6\n12\n164\nTable 2: Percentage Results for Code Correctness and Validity\nfor the Original Experiment and the Experiment Using only\nFunction Names and Parameters\nOriginal Experiment\nWith only Function Name\nValid\n91.5%\n79.3%\nCorrect\n28.7%\n19.5%\nPartially Correct\n51.2%\n26.9%\nValid Incorrect\n11.6%\n32.9%\nInvalid Incorrect\n8.5%\n20.7%\nthan the canonical solution, and for the remaining 20.2% of the\nproblems, the generated solution was more efficient.\nSpace complexity: For the correct code generations, we had\n47 problems, and 89"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_010",
    "source_id": "CopilotQuality2022",
    "text": "\n28\n15\n130\n137\n6\n12\n164\nTable 2: Percentage Results for Code Correctness and Validity\nfor the Original Experiment and the Experiment Using only\nFunction Names and Parameters\nOriginal Experiment\nWith only Function Name\nValid\n91.5%\n79.3%\nCorrect\n28.7%\n19.5%\nPartially Correct\n51.2%\n26.9%\nValid Incorrect\n11.6%\n32.9%\nInvalid Incorrect\n8.5%\n20.7%\nthan the canonical solution, and for the remaining 20.2% of the\nproblems, the generated solution was more efficient.\nSpace complexity: For the correct code generations, we had\n47 problems, and 89.4% of the problems had the same efficiency\nas the canonical solution. On the other hand, 4.2% of the results\nwere more, and 6.4% of the results were less efficient than the\ncorresponding canonical solution for the given problem. For the\npartially correct code, we had 84 problems, where 83.4% of the\nproblems had the same efficiency in terms of space complexities\nwith the corresponding canonical solution. Copilot managed to\ngenerate a more efficient solution 8.3% of the time, whereas it\ngenerated a less efficient solution again 8.3% of the time.\n3.4\nUsing only Function Names and Parameters\nWithout Prompt (RQ2)\nThe results we presented up until this point were the outputs of the\nexperiment where we provided the function name, parameters, and\nthe docstring as the inputs to get the generated code from GitHub\nCopilot. In this part, as we explained in Section 2.6, we removed\nthe docstring from each of our problems in the dataset. The results\nof this experiment are presented in Table 2 and Table 3.\nIn our original experiment where we used both the function\nname and the prompt, our code validity score was 91.5%. In our\nlatter experiment, where we only used the function names, our code\nvalidity score dropped to 79.3%. For code correctness, if we compare\nthese results to our results in this experiment, the rate of correctly\ngenerated code dropped to 19.5% (from 28.7%). The incorrectly\ngenerated code percentage increased to 53.7% (from 20.1%), and\nthe partially correctly generated code percentage dropped to 26.8%\n(from 51.2%).\nTo give more details about the partially correctly generated code,\nwe divided (0, 100) success space into four equal intervals, similar to\nthe assessment we explained in Section 3.2. The precise correctness\nresults for these intervals can be observed in Table 3.\n65\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nTable 3: Percentage Results for Code Correctness for Par-\ntially Correct Solutions for Given Intervals for the Original\nExperiment and the Experiment Using only Function Names\nand Parameters\nOriginal Experiment\nWith only Function Name\n100% > \ud835\udc41> 75%\n13.1%\n6.8%\n75% \u2265\ud835\udc41> 50%\n28.6%\n36.4%\n50% \u2265\ufffd"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_011",
    "source_id": "CopilotQuality2022",
    "text": " intervals, similar to\nthe assessment we explained in Section 3.2. The precise correctness\nresults for these intervals can be observed in Table 3.\n65\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nTable 3: Percentage Results for Code Correctness for Par-\ntially Correct Solutions for Given Intervals for the Original\nExperiment and the Experiment Using only Function Names\nand Parameters\nOriginal Experiment\nWith only Function Name\n100% > \ud835\udc41> 75%\n13.1%\n6.8%\n75% \u2265\ud835\udc41> 50%\n28.6%\n36.4%\n50% \u2265\ud835\udc41> 25%\n35.7%\n25.0%\n25% \u2265\ud835\udc41> 0%\n22.6%\n31.8%\nTable 4: Percentage Results for Code Correctness and Valid-\nity for the Original Experiment and the Experiment Using\nDummy Function Names\nOriginal Experiment\nWith Dummy Function Name\nValid\n91.5%\n84.2%\nCorrect\n28.7%\n26.8%\nPartially Correct\n51.2%\n40.2%\nValid Incorrect\n11.6%\n17.1%\nInvalid Incorrect\n8.5%\n15.9%\nTable 5: Percentage Results for Code Correctness for Partially\nCorrect Solutions for Given Intervals for the Original Exper-\niment and the Experiment Using Dummy Function Names\nWith Meaningful Function Name\nWith Dummy Function Name\n100% > \ud835\udc41> 75%\n13.1%\n4.6%\n75% \u2265\ud835\udc41> 50%\n28.6%\n40.9%\n50% \u2265\ud835\udc41> 25%\n35.7%\n31.8%\n25% \u2265\ud835\udc41> 0%\n22.6%\n22.7%\n3.5\nUsing Dummy Function Names (RQ3)\nIn this part, as explained in Section 2.7, we prompted GitHub Copilot\nto generate code for the same problems, this time with dummy\nfunction names instead of meaningful, and informative function\nnames. The results of the original and the new experiment are\npresented in Table 4 and Table 5. Out of 164 problems, 26 were\nfound to be invalid and the remaining 138 solutions were valid.\nThis yielded an 84.2% success rate in terms of generating valid\ncode. For code correctness, we have observed that for 26.8% of the\nproblems, GitHub Copilot generated the correct code for the given\nproblem, whereas 51.2% of the solutions were partially correct.\nGenerated solutions for 19.7% of the problems were incorrect.\nIn general, it was observed that using dummy function names\ninstead of meaningful ones reduces the performance of GitHub\nCopilot. The code validity score dropped from 91.5% to 84.2%. The\ncode correctness score among the valid solutions dropped from\n48.6% to 46.8%. The proportion of correct solutions decreased from\n28.7% to 26.8%, whereas the proportion of partially correct solutions\ndecreased from 51.2% to 40.2%. Moreover, we also observed an\nincrease in the proportion of incorrect solutions. We have observed\nthat the proportion of valid and incorrect solutions"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_012",
    "source_id": "CopilotQuality2022",
    "text": " were partially correct.\nGenerated solutions for 19.7% of the problems were incorrect.\nIn general, it was observed that using dummy function names\ninstead of meaningful ones reduces the performance of GitHub\nCopilot. The code validity score dropped from 91.5% to 84.2%. The\ncode correctness score among the valid solutions dropped from\n48.6% to 46.8%. The proportion of correct solutions decreased from\n28.7% to 26.8%, whereas the proportion of partially correct solutions\ndecreased from 51.2% to 40.2%. Moreover, we also observed an\nincrease in the proportion of incorrect solutions. We have observed\nthat the proportion of valid and incorrect solutions increased from\n11.6% to 17.1%, whereas the proportion of invalid and incorrect\nsolutions increased from 8.5% to 15.9%.\n4\nDISCUSSION\nIn this section, we discuss the results of our experiment explained\nin Section 3.\n4.1\nCode Validity (RQ1.1)\nAs we discussed, for our 164 problems, GitHub Copilot was able\nto generate valid code for 150 of them, yielding a success rate\nof 91.5%. The results showed that all errors were generated at\nruntime; furthermore, there were no errors caused by code with\nthe wrong syntax. In this regard, we can say that GitHub Copilot\nis competent in generating synthetically correct code. In terms of\nruntime errors, while they varied, the particular error we observed\nthe most was the incorrect use of the \u201cint()\u201d method in Python,\nwhere the function returns the integer equivalent of the expression\nprovided in the parentheses. The common characteristic of the\nproblem that caused this error was to sum the digits of the numbers\nin an array, but GitHub Copilot failed to vouch for the \u2018-\u2019 signs in\nthe negative numbers and also count them as characters. We also\nobserved additional errors in the indexing of the arrays, operations\non incompatible types, and non-terminating recursions.\nGitHub Copilot is capable of generating valid code 9 out of\n10 times.\n4.2\nCode Correctness (RQ1.2)\nIn contrast to a binary scenario for code correctness where the\ngenerated solution is either correct or incorrect, GitHub Copilot\nalso generates partially correct solutions. As it can be observed\nin Figure 3, for 51.2% of the problems, GitHub Copilot generated\npartially correct code. If more than 50% code correctness score\nis considered a success, then we can say that GitHub Copilot is\nsuccessful in generating code for 50% of the problems, instead of\n28.7%. Therefore, we argue that not only the fully correct solutions\nshould be considered a success, but the partially correct solutions\nshould also be taken into account. This is the case because usually\nin regular programming practices, it is seen that the first iteration\nmade on the written code is not correct, but over the next itera-\ntions, the code becomes correct. Therefore, we argue that GitHub\nCopilot is considerably successful in understanding user intent and\ngenerating the correct code. Our standpoint is further elaborated\non with an example in the following paragraph.\ndef digitSum(s):\n\"\"\" Task: Write a function that"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_013",
    "source_id": "CopilotQuality2022",
    "text": " Copilot is\nsuccessful in generating code for 50% of the problems, instead of\n28.7%. Therefore, we argue that not only the fully correct solutions\nshould be considered a success, but the partially correct solutions\nshould also be taken into account. This is the case because usually\nin regular programming practices, it is seen that the first iteration\nmade on the written code is not correct, but over the next itera-\ntions, the code becomes correct. Therefore, we argue that GitHub\nCopilot is considerably successful in understanding user intent and\ngenerating the correct code. Our standpoint is further elaborated\non with an example in the following paragraph.\ndef digitSum(s):\n\"\"\" Task: Write a function that takes a string as\ninput and returns the sum of the upper characters\nonly' ASCII codes.\nExamples:\ndigitSum (\"\") => 0\ndigitSum (\"abAB\") => 131\ndigitSum (\" abcCd \") => 67\ndigitSum (\" helloE \") => 69\ndigitSum (\" woArBld \") => 131\ndigitSum (\" aAaaaXa \") => 153\n\"\"\"\nsum = 0\nfor c in s:\nif c.isupper ():\nsum += ord(c) - ord('A') + 1\nreturn sum\nListing 2: Problem (ID: 66) with Low Success Rate\n66\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nWhen we analyzed why the generated code was incorrect, or\npartially correct with a poor success rate, we saw that with slight\npossible adjustments that could be made on top of the generated\ncode, the correctness of the solution could be easily improved. The\ncorrectness of the solution to the given problem shown in Listing\n2 is 13%, which falls into the lowest category shown in blue in\nFigure 4. But by only removing the subtraction of \u201cord(\u2018A\u2019) + 1\u201d\nfrom \u201cord(c)\u201d under the \u2018if\u2019 statement, in other words changing the\nline to be \u201csum += ord(c)\u201d, we can improve the correctness of this\nfunction by 87%, to be 100%. Hence, the code generations having\nlow correctness scores should not be considered complete failures,\nas we have seen that even with small adjustments the code could\nbe fixed.\ndef get_positive(l: list):\n\"\"\" Return only positive numbers in the list.\n>>> get_positive ([-1, 2, -4, 5, 6])\n[2, 5, 6]\n>>> get_positive ([5, 3, -5, 2, -3, 3, 9, 0, 123, 1,\n-10])\n[5, 3, 2, 3, 9, 123, 1]\n\"\"\"\nreturn [x for x in l if x > 0]\ndef odd_count(lst):\n\"\"\" Given a list of strings , where each string\nconsists of only digits , return a list. Each element\ni of the output should be \"the number of odd\nelements in the string i of the input.\" where all\nthe i's should be replaced by the number of odd\ndigits in the i'th string of the input.\n>>> odd_count"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_014",
    "source_id": "CopilotQuality2022",
    "text": ">>> get_positive ([5, 3, -5, 2, -3, 3, 9, 0, 123, 1,\n-10])\n[5, 3, 2, 3, 9, 123, 1]\n\"\"\"\nreturn [x for x in l if x > 0]\ndef odd_count(lst):\n\"\"\" Given a list of strings , where each string\nconsists of only digits , return a list. Each element\ni of the output should be \"the number of odd\nelements in the string i of the input.\" where all\nthe i's should be replaced by the number of odd\ndigits in the i'th string of the input.\n>>> odd_count ([ '1234567 '])\n[\"the number of odd elements 4n the str4ng 4 of the 4\nnput .\"]\n>>> odd_count ([ '3 ' ,\"11111111\"])\n[\"the number of odd elements 1n the str1ng 1 of the 1\nnput.\",\n\"the number of odd elements 8n the str8ng 8 of the 8\nnput .\"]\n\"\"\"\nreturn [str(sum(1 for c in s if c in '13579 ')) for s\nin lst]\nListing 3: Generated Solutions for a Simple (ID: 30) and More\nComplicated Problem (ID: 113)\nWe also observed that GitHub Copilot was able to solve some of\nthe problems (e.g., arithmetic operations, searching in an array, and\nother basic operations) more successfully, whereas it struggled to\ngenerate a correct solution for other problems. Two such samples\ncan be examined in Listing 3. In the first problem, it is asked to filter\nonly the positive numbers in a given array, which is a simple and\nprevalent problem. In the second one, we have a more complicated\nproblem to solve, where first, the number of odd numbers in a\ngiven string would be counted, then the \u2018i\u2019 characters in the string\nwould be replaced with the found number. In the samples where\nthe solution was not able to satisfy any of the test cases for the\nproblem, we observe that GitHub Copilot should be provided with\nadditional information. Moreover, as the problem consists of small\nsubproblems which are finding the number of odd numbers in the\nstring, and replacing \u2018i\u2019 characters with the numbers, adopting a\nstep-by-step approach would be more feasible for such a problem.\nFor example, in Listing 4, we had a task to return a tuple containing\nthe number of odd and even palindromes in a given range. This is\nsimilar to the complex problem we stated, in terms of containing\nsubproblems and their difficulty. Different from the previous one,\ntwo additional functions to solve the subproblems for checking if a\nnumber is an integer, and checking if a number is a palindrome are\ngenerated. This is a special case, as, in the main function, the gener-\nated code called two other functions, which were not generated yet.\nSubsequently, GitHub Copilot generated these two functions one\nafter another, as we put new lines for GitHub Copilot to proceed\nwith code generation. This special case allows us to exemplify the\nstep-by-step approach we discussed. We show that if the implicit\nsubproblems are solved"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_015",
    "source_id": "CopilotQuality2022",
    "text": " a given range. This is\nsimilar to the complex problem we stated, in terms of containing\nsubproblems and their difficulty. Different from the previous one,\ntwo additional functions to solve the subproblems for checking if a\nnumber is an integer, and checking if a number is a palindrome are\ngenerated. This is a special case, as, in the main function, the gener-\nated code called two other functions, which were not generated yet.\nSubsequently, GitHub Copilot generated these two functions one\nafter another, as we put new lines for GitHub Copilot to proceed\nwith code generation. This special case allows us to exemplify the\nstep-by-step approach we discussed. We show that if the implicit\nsubproblems are solved separately, and step-by-step, the success\nrate in code correctness could be improved, as the success rate of\nthis problem is 100%, whereas the solution to the previous prob-\nlem is incorrect. Therefore, we can say that for complex problems,\nmore information (i.e., code snippets, comments) should be given to\nGitHub Copilot. Then, the problem should be divided into simpler\nsubproblems, and the solutions to those subproblems should be\ngenerated one by one.\nOur results show that without the supervision of a program-\nmer, GitHub Copilot has a lower chance of generating correct\ncode; but with further input provided by the programmer,\nthe correctness of the generated code can be increased.\ndef even_odd_palindrome(n):\n\"\"\" Given a positive integer n, return a tuple that\nhas the number of even and odd integer palindromes\nthat fall within the range(1, n), inclusive.\nExample 1:\nInput: 3\nOutput: (1, 2)\nExplanation:\nInteger palindrome are 1, 2, 3. one of them is\neven , and two of them are odd.\nExample 2:\nInput: 12\nOutput: (4, 6)\nExplanation:\nInteger palindrome are 1, 2, 3, 4, 5, 6, 7, 8, 9,\n11. four of them are even , and 6 of them are odd.\nNote:\n1. 1 <= n <= 10^3\n2. returned tuple has the number of even and odd\ninteger palindromes respectively.\n\"\"\"\nreturn (sum(1 for i in range(1, n+1) if is_palindrome\n(i) and is_even(i)),\nsum(1 for i in range(1, n+1) if is_palindrome\n(i) and not is_even(i)))\ndef is_palindrome(n):\n\"\"\"\nGiven a positive integer n, return true if it is a\npalindrome , and false otherwise.\n\"\"\"\nreturn str(n) == str(n)[:: -1]\ndef is_even(n):\n\"\"\"\nGiven a positive integer n, return true if it is even\n, and false otherwise.\n\"\"\"\nreturn n % 2 == 0\nListing 4: A Special Case of Code Generation (ID: 107)\n67\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\n4.3\nCode Efficiency (RQ1.3)\nFor time and space complexities and different code correctness\ncategories (correct, partially correct, incorrect, and invalid), the\ngenerated code"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_016",
    "source_id": "CopilotQuality2022",
    "text": ", return true if it is a\npalindrome , and false otherwise.\n\"\"\"\nreturn str(n) == str(n)[:: -1]\ndef is_even(n):\n\"\"\"\nGiven a positive integer n, return true if it is even\n, and false otherwise.\n\"\"\"\nreturn n % 2 == 0\nListing 4: A Special Case of Code Generation (ID: 107)\n67\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\n4.3\nCode Efficiency (RQ1.3)\nFor time and space complexities and different code correctness\ncategories (correct, partially correct, incorrect, and invalid), the\ngenerated code was mostly as efficient as the canonical solution.\nThis could be considered a success of GitHub Copilot. Moreover,\nthere are problems where the generated solution was more efficient\nthan the canonical solution, which is interesting and worthy of\nconsideration. For instance, in Listing 5, the first solution in the\nupper box is the canonical solution, whereas the one in the lower\nbox is generated by GitHub Copilot. The problem asks to create a list\ncontaining all prime numbers until a given number. In the canonical\nsolution, the approach is to iterate up until the given number \u2018n\u2019, and\nat each number \u2018i\u2019, iterate until \u2018i\u2019 and check if any number less than\n\u2018i\u2019 divides \u2018i\u2019. In the generated solution, instead of checking every\nnumber less than \u2018i\u2019, the approach of checking up until the square\nroot of \u2018n\u2019 is used. GitHub Copilot also generated the function called\n\u2018is_prime\u2019 to be used in the solution. This way, a more efficient\nalgorithm for solving the same problem was implemented.\ndef count_up_to(n):\nprimes = []\nfor i in range(2, n):\nis_prime = True\nfor j in range(2, i):\nif i % j == 0:\nis_prime = False\nbreak\nif is_prime:\nprimes.append(i)\nreturn primes\n---------------------------------------------------------\ndef count_up_to(n):\n\"\"\" Implement a function that takes an non -negative\ninteger and returns an array of the first n integers\nthat are prime numbers and less than n.\nfor example:\ncount_up_to (5) => [2,3]\ncount_up_to (11) => [2,3,5,7]\ncount_up_to (0) => []\ncount_up_to (20) => [2,3,5,7,11,13,17,19]\ncount_up_to (1) => []\ncount_up_to (18) => [2,3,5,7,11,13,17]\n\"\"\"\nreturn [i for i in range(2, n) if is_prime(i)]\ndef is_prime(n):\n\"\"\" Write a function that returns true if the given\nnumber is a prime number and false otherwise.\nExample:\nis_prime (7) == True\nis_prime (8) == False\nis_prime (2) == True\nis_prime (1) == False\nis_prime (0) == False\n\"\"\"\nimport math\nfor i in range(2, int(math.sqrt(n))+1):\nif n % i == 0:\nreturn False\nreturn True\nListing 5: Comparison of the Canonical and the Generated\nSolutions for a Problem (ID: 96)"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_017",
    "source_id": "CopilotQuality2022",
    "text": "5,7,11,13,17]\n\"\"\"\nreturn [i for i in range(2, n) if is_prime(i)]\ndef is_prime(n):\n\"\"\" Write a function that returns true if the given\nnumber is a prime number and false otherwise.\nExample:\nis_prime (7) == True\nis_prime (8) == False\nis_prime (2) == True\nis_prime (1) == False\nis_prime (0) == False\n\"\"\"\nimport math\nfor i in range(2, int(math.sqrt(n))+1):\nif n % i == 0:\nreturn False\nreturn True\nListing 5: Comparison of the Canonical and the Generated\nSolutions for a Problem (ID: 96) in HumanEval Dataset\nEven though there are cases where the generated code was more\nefficient, like the example in Listing 5, we cannot state that GitHub\nCopilot is necessarily successful in generating more efficient code\nthan the canonical solutions. As we previously stated, according to\nour results, the bigger proportion of time and space complexities\nfor any category of code correctness was indicating that the effi-\nciencies of the generated and canonical solutions were the same.\nFurthermore, similar to the problems where the generated code is\nmore efficient than the canonical solution, the inverse case where\nthe canonical solution overpowers the generated code has a small\nfraction of the total problems, which can be seen in Table 1.\nOur results show that there is no significant difference in\nterms of code efficiency between the code generated by\nGitHub Copilot and a human programmer.\n4.4\nUsing only Function Names and Parameters\nWithout Prompt (RQ2)\nAccording to the results in Section 3.4, we observed a significant\ndrop in success for both the code correctness and code validity\nmetrics. For example, the code validity dropped by 12.2% after the\ndocstrings were removed from the problems, similarly, the code\ncorrectness score for correct generations dropped by 9.1%. These\nresults reflected a general performance drop affecting the validity\nand the correctness of the code. From this, we argue that the code\ngeneration performance of GitHub Copilot is correlated with the\nexplanation given as input for code generation.\nThere were some cases, where we did not see any decrease in\ncorrectness or validity scores. Such problems constituted 12.2%\nof the dataset for code correctness and 3.7% for code validity. We\nhave seen such cases mostly for problems that include substring\nsearch, value manipulations in an array, and character compari-\nson. Additionally, the names of such functions, accompanied by\nparameter names were self-explanatory, which means that GitHub\nCopilot could still make interpretations about the function without\nrequiring more details.\nFor the cases where the code correctness and validity scores\ndropped, we observed that these problems were more complicated.\nWhen we examined where the success rate of GitHub Copilot\ndropped, we observed cases where the function name and the pa-\nrameters alone failed to give details. This means that the name\nand parameters alone, are not sufficient to give details about such\nfunctions. For example, in one case we observed a function called\n\u201cwill_it_fly\u201d, which only has two parameters called \u2018q\u2019 and \u2018w\u2019. Copi-\nlot could not understand the purpose of the function and compared\nthe"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_018",
    "source_id": "CopilotQuality2022",
    "text": "-explanatory, which means that GitHub\nCopilot could still make interpretations about the function without\nrequiring more details.\nFor the cases where the code correctness and validity scores\ndropped, we observed that these problems were more complicated.\nWhen we examined where the success rate of GitHub Copilot\ndropped, we observed cases where the function name and the pa-\nrameters alone failed to give details. This means that the name\nand parameters alone, are not sufficient to give details about such\nfunctions. For example, in one case we observed a function called\n\u201cwill_it_fly\u201d, which only has two parameters called \u2018q\u2019 and \u2018w\u2019. Copi-\nlot could not understand the purpose of the function and compared\nthe two parameters if one of them was greater than the other. The\npurpose of the function was to check if \u2018q\u2019 was a palindromic list\nand if the sum of the elements in the list was less than \u2018w\u2019. In\nthis problem, per the syntax of Python, Copilot cannot know the\nvariable types of the parameters.\nWhile employing GitHub Copilot, the utilization of proper\nexplanations of the given problem is important in terms of\nacquiring correct and valid code. If possible the programmer\nshould provide an explanation of the problem accompanied\nby some sample unit tests as a docstring, comment, etc. while\ngenerating a solution.\n68\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\n4.5\nUsing Dummy Function Names (RQ3)\nThe aim of using dummy function names is to assess the effect of\nthe function name on the generated code. We observed different\nresults during our experiment. Five solutions were invalid in the\nexperiment with meaningful function names but valid in the ex-\nperiment with dummy function names. For example, problem #108\ndescribes a function that takes an array of integers and returns the\nnumber of elements whose sum of digits is positive. In this problem,\nthe function name can be considered misleading, therefore, using\na dummy function name may eliminate the ambiguity, so GitHub\nCopilot was able to generate a valid solution. Similarly, in problem\n#99, the function name was given as \u201cclosest_integer\u201d, which can\nbe misleading in terms of the parameter type. This is because, when\nwe used the function name, GitHub Copilot considered the input\ntype as float instead of a string. When we used a dummy name, it\ngenerated a solution that takes a string as explained in the function\nprompt.\nAnother observation is the change in code correctness. Three\nsolutions were correct in the experiment with meaningful function\nnames but incorrect in the experiment with dummy function names.\nFor example, in problem #79, the function name can be considered\ninformative, given as \u201cdecimal_to_binary\u201d. Therefore, when we\nchanged the function name to \u201cfoo\u201d, GitHub Copilot generated\na solution with less information as input and this resulted in an\nincorrect solution. We can make this statement for problems #14\nand #34 as well.\nIt can be concluded that changing the meaningful function names\nwith dummy function names reduced the performance of Copilot\nfor most of the problems. The exceptional problems were the ones\nin which either the function name is informative or misleading.\nConsidering"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_019",
    "source_id": "CopilotQuality2022",
    "text": " in code correctness. Three\nsolutions were correct in the experiment with meaningful function\nnames but incorrect in the experiment with dummy function names.\nFor example, in problem #79, the function name can be considered\ninformative, given as \u201cdecimal_to_binary\u201d. Therefore, when we\nchanged the function name to \u201cfoo\u201d, GitHub Copilot generated\na solution with less information as input and this resulted in an\nincorrect solution. We can make this statement for problems #14\nand #34 as well.\nIt can be concluded that changing the meaningful function names\nwith dummy function names reduced the performance of Copilot\nfor most of the problems. The exceptional problems were the ones\nin which either the function name is informative or misleading.\nConsidering the average results given in Tables 4 and 5, it can\nbe stated that generally changing meaningful function names to\ndummy function names affects the performance of GitHub Copilot\nnegatively.\nChoosing a meaningful name for a given function positively\naffects GitHub Copilot\u2019s performance in terms of generating\na correct and valid code.\nIn Figure 5, we demonstrate the code correctness results using a\nviolin chart. We show the mean value with a red and the median\nvalue with a green dashed line. For our three experiments which\nare the original experiment, experiment with only function names,\nand experiment with dummy function names, the median values\nare found to be 0.53, 0, and 0.44 respectively. The mean values are\nfound to be 0.54, 0.31, and 0.46 respectively. It can be observed\nthat we obtain better code correctness results when we use the\nfull prompt, more information, as in the original experiment. Fur-\nthermore, it could be observed that the utilization of the docstring\nfor explaining the intent of a given function is more essential than\ngiving it a meaningful name. Hence, in practical uses, programmers\nshould prioritize giving proper explanations to the functions to\ngather correct code. However, giving functions meaningful names\nis still essential per our results, therefore a combination of a proper\nexplanation and a meaningful function name is the ideal case.\nFigure 5: Code Correctness Score Distribution of the Prob-\nlems for Different Experiments\n5\nTHREATS TO VALIDITY\nIn this section, we discuss the possible factors of our experimen-\ntal setup and GitHub Copilot that may reduce the validity of our\nfindings.\n5.1\nConclusion Validity\nOpenAI Codex Model Version: While conducting our experiment,\nthe latest version of OpenAI Codex Model was code-davinci-001.\nFor any possible later evaluations of code efficiency with the same\nexperimental setup, the results might be different.\nTrivial Solutions: In some problems, GitHub Copilot generated\nsolutions to return simple statements like empty arrays or Boolean\nvalues. In this case, if there are test cases related to the problem,\nwhere such expressions are the desired output, those test cases pass\nby chance without any algorithm generated for the problem.\n5.2\nInternal Validity\nOne-shot code generation: While generating code with GitHub\nCopilot, we used the function names, parameters, and the corre-\nsponding docstring containing an explanation of the function and\na few instances of tests for that function. Furthermore, we did\nnot write any code to provide additional information to GitHub\n"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_020",
    "source_id": "CopilotQuality2022",
    "text": " efficiency with the same\nexperimental setup, the results might be different.\nTrivial Solutions: In some problems, GitHub Copilot generated\nsolutions to return simple statements like empty arrays or Boolean\nvalues. In this case, if there are test cases related to the problem,\nwhere such expressions are the desired output, those test cases pass\nby chance without any algorithm generated for the problem.\n5.2\nInternal Validity\nOne-shot code generation: While generating code with GitHub\nCopilot, we used the function names, parameters, and the corre-\nsponding docstring containing an explanation of the function and\na few instances of tests for that function. Furthermore, we did\nnot write any code to provide additional information to GitHub\nCopilot which would clarify more what our intent for that particu-\nlar problem is. Therefore, for most cases, the success rate could be\nincreased if we have given hints as code snippets to GitHub Copilot.\nReproduction of the Generations: While conducting our experi-\nments, we observed that GitHub Copilot had a nondeterministic\ncharacteristic, hence was generating different outputs for the same\ninput for different times we generated code in our trials prior to\nour experiment. We paid great attention not to including different\noutputs for the same input by generating code for our problems\nin one iteration, and saving the generated code, then conducting\nour evaluation on the saved code. Given that GitHub Copilot has\na dynamic characteristic that the underlying AI model of the tool\n69\nAssessing the Quality of GitHub Copilot\u2019s Code Generation\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nis being retrained as new repositories are added to GitHub, our\nresults might not be fully replicated given our experimental setup\nand input.\nCode Generation Methods: With GitHub Copilot, one can use\ntwo different approaches to generate code. The first one happens\nautomatically as a programmer proceeds to write code, GitHub\nCopilot suggests code snippets that might fit into that context. In\nthe other approach, whenever the programmer wants to generate\ncode, they press the \u2018ctrl + enter\u2019 key combination to see up to 10\ncode generations GitHub Copilot produces. In our experiment, we\nchose the first approach whenever possible, otherwise, we imple-\nmented the second approach and selected the suggestion at the top\nof the list. For 15 problems, GitHub Copilot failed to generate any\ncode after we entered the next line (after the user presses the \u2018enter\u2019\nkey and continues from the next line). Therefore, we had to apply\nthe \u2018ctrl + enter\u2019 key combination to see the solutions, and we were\nable to obtain code generations for all problems. As we had to use\ntwo different methods for code generation, we stated our practice\nas a possible factor to reduce the validity of our study.\nOn a further note, we also want to state the difference between\nthe solutions that are automatically generated, and the ones have\nshown when the \u2018ctrl + enter\u2019 key combination is applied. We\nobserved that for the same context, two methods yield different\nresults. Therefore, if in both methods, the code is generated, choos-\ning different methods for a set of problems may introduce possible\ninvalidity to a study. Hence, we tried to be as consistent as possible\nin our experiment by avoiding"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_021",
    "source_id": "CopilotQuality2022",
    "text": " solutions, and we were\nable to obtain code generations for all problems. As we had to use\ntwo different methods for code generation, we stated our practice\nas a possible factor to reduce the validity of our study.\nOn a further note, we also want to state the difference between\nthe solutions that are automatically generated, and the ones have\nshown when the \u2018ctrl + enter\u2019 key combination is applied. We\nobserved that for the same context, two methods yield different\nresults. Therefore, if in both methods, the code is generated, choos-\ning different methods for a set of problems may introduce possible\ninvalidity to a study. Hence, we tried to be as consistent as possible\nin our experiment by avoiding the latter method whenever possible.\nBlock and Line-by-Line Generation: We stated that for most\nof the cases GitHub Copilot managed to generate the solution of\na given function as bulk, but there were cases, where we had to\ngenerate the solution line-by-line. As we had no control over how\nGitHub Copilot would generate the code, we had to accept the\nmethod GitHub Copilot would choose for a particular problem. We\nstate these cases, as in line-by-line suggestion, the previously gen-\nerated lines might have an effect on the next line to be generated,\nwhereas in the first case code is generated at once as a bulk.\nGitHub Copilot Version: While conducting our experiment, the\nlatest version of GitHub Copilot was v1.7.4421. For any possible\nlater evaluations with the same experimental setup, the results\nmight be different.\n5.3\nConstruct Validity\nOpenAI API: We relied on the correctness of OpenAI to evalu-\nate time and space complexities. Therefore there may be instances\nwhere the code efficiency results are incorrect.\nNumber of test cases: The varying amount of test cases for the\ndataset may introduce a threat to our experiment. On average there\nare 7.7 test cases for each problem in the HumanEval dataset [3].\nHaving broader test cases, both for the amount and the scope can\nbe important. By extending the test cases, any potential corner case\nthat could be missed may be covered. This can be critical especially\nwhen some corner cases for a given problem are not involved. We\nplan to improve the test cases both in quantity and quality in our\nfuture work.\n5.4\nExternal Validity\nProblem Coverage: For our experiment, we evaluated the gen-\nerated solutions for 164 different problems, contained in the Hu-\nmanEval dataset. In the HumanEval dataset, the subjects of the\nproblems include algorithms, simple mathematics, reasoning, and\nlanguage comprehension [3]. For better and more insightful results,\nthe number of problems can be increased, and the comprehension\nof the problems could be broader. For instance, in the experimen-\ntal setup proposed by Xu et al. [15] for their code generation and\nretrieval tool, the scope of the problems consists of basic Python,\nfile, OS, web scraping, web server & client, data analysis & ML, and\ndata visualization. Such topics could be included in our dataset to\nboth broaden the comprehension and increase the number of our\nproblems. We consider this task as future work for our study.\n6\nRELATED WORK\nIn the last few years, code generation has attracted attention from\nresearchers such"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_022",
    "source_id": "CopilotQuality2022",
    "text": " and\nlanguage comprehension [3]. For better and more insightful results,\nthe number of problems can be increased, and the comprehension\nof the problems could be broader. For instance, in the experimen-\ntal setup proposed by Xu et al. [15] for their code generation and\nretrieval tool, the scope of the problems consists of basic Python,\nfile, OS, web scraping, web server & client, data analysis & ML, and\ndata visualization. Such topics could be included in our dataset to\nboth broaden the comprehension and increase the number of our\nproblems. We consider this task as future work for our study.\n6\nRELATED WORK\nIn the last few years, code generation has attracted attention from\nresearchers such as [5, 8, 12, 16]. In this study, we focus on GitHub\nCopilot which seems to be the first well-established instance of an\nAI pair-programmer.\nThe underlying model of GitHub Copilot, Codex, is externally\ndeveloped by OpenAI and employed by GitHub. Some of the ear-\nlier versions of the current Codex model used by GitHub Copilot\nwere evaluated by Chen et al. [3]. The Codex model relies on GPT\nmodels that OpenAI previously developed for natural language\ngeneration. The public code available on GitHub was used here\nwhile fine-tuning the model to implement the code recognition and\ngeneration capabilities. Furthermore, the model can recognize some\nother elements such as function signatures, code comments, etc.\nThe model can use such elements as inputs and generate related\noutputs. They found that a success rate of 70.2% could be reached\nin terms of code correctness, by generating 100 solutions for each\nproblem and choosing the most successful one among them. The\nsuccess rate was found to be only 28.8% for the case with one solu-\ntion per problem, which is consistent with our results. In this study,\nwe provided a detailed version of this assessment. We evaluated\nthe time and space complexities for generated solutions. Moreover,\nin order to extend the coverage of our study, we adjusted the Hu-\nmanEval dataset by changing meaningful function names with the\ndummy name \"foo\" and regenerated the solutions.\nThere are also experiment-based studies similar to ours, con-\nducted to evaluate GitHub Copilot. However, since GitHub Copilot\nis a considerably new tool, there are not many studies directly\nrelated to it. We list the available studies in the following.\nOne such study is conducted by Sobania et al. [11] in which the\ncode correctness of GitHub Copilot is evaluated, and the tool is\ncontrasted to the automatic program generators having the Genetic\nProgramming (GP) architecture. They found that there is not a\nsignificant difference between the two approaches on the bench-\nmark problems; however, the program synthesis approaches are not\nsufficient in supporting programmers compared to GitHub Copilot.\n70\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nAn evaluation of GitHub Copilot in terms of the security of the\ngenerated programs was implemented by Pearce et al. [10]. They\nevaluated the vulnerabilities in the generated code by Copilot. It\nwas determined that 40% of generated programs were vulnerable.\nAnother study discusses the"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_023",
    "source_id": "CopilotQuality2022",
    "text": " program generators having the Genetic\nProgramming (GP) architecture. They found that there is not a\nsignificant difference between the two approaches on the bench-\nmark problems; however, the program synthesis approaches are not\nsufficient in supporting programmers compared to GitHub Copilot.\n70\nPROMISE \u201922, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nAn evaluation of GitHub Copilot in terms of the security of the\ngenerated programs was implemented by Pearce et al. [10]. They\nevaluated the vulnerabilities in the generated code by Copilot. It\nwas determined that 40% of generated programs were vulnerable.\nAnother study discusses the effects of GitHub Copilot by con-\nducting a within-subjects user study [14]. It was found that GitHub\nCopilot did not cause a significant improvement in terms of speed\nand success rate. However, it was stated that most participants\npreferred to use Copilot in daily programming tasks since it saved\nthe effort for the basic tasks.\nNguyen et al. [9] evaluated GitHub Copilot using 33 different\nLeetCode questions and four different programming languages\n(Python, Java, JavaScript, and C). Their evaluation includes code\ncorrectness and code understandability for the generated code. They\nevaluated code correctness by measuring the ratio of passed tests\nfor each question, which is a similar approach to our study. Code\nunderstandability was measured by two different metrics, which are\ncognitive and cyclomatic complexity. In terms of code correctness,\nJava had the highest (57%) and JavaScript was the lowest (27%)\nscore. For code understandability, they determined that there was\nno statistical significance between the programming languages.\nIn general, most of the related works evaluated the quality of\ncode generated by GitHub Copilot or OpenAI\u2019s Codex model. The\nmajority of the studies focused on the evaluation of code correct-\nness, with the exception of the studies of Pearce et al. [10] where\nthe focus is code security, and Vaithilingam et al. [14] as their work\nmostly concentrates on the practical usage performance of GitHub\nCopilot. To the best of our knowledge, this is the first study that\nevaluates GitHub Copilot in terms of code correctness, code validity,\nand code efficiency. In this sense, we believe that our methodol-\nogy and results will contribute to the ongoing research about the\ncapabilities of GitHub Copilot and other code generation tools.\n7\nCONCLUSION\nIn our study, we performed an analysis on GitHub Copilot to as-\nsess the quality of code generation from correctness, validity, and\nefficiency perspectives. We found that GitHub Copilot was able to\ngenerate valid code for 91.5% of the problems in the HumanEval\nproblem dataset. 28.7% of solutions were correct, 51.2% were par-\ntially correct, and 20.1% of them were incorrect. In terms of effi-\nciency, we observed that Copilot could mostly match the efficiency\nof solutions written by humans.\nTo understand and evaluate the impact of input parameters\u2019\nquality on GitHub Copilot, we first assessed the effect of providing\nonly function names and parameters. Based on the results, 79.3%\nof the solutions were valid compared to"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_024",
    "source_id": "CopilotQuality2022",
    "text": " code generation from correctness, validity, and\nefficiency perspectives. We found that GitHub Copilot was able to\ngenerate valid code for 91.5% of the problems in the HumanEval\nproblem dataset. 28.7% of solutions were correct, 51.2% were par-\ntially correct, and 20.1% of them were incorrect. In terms of effi-\nciency, we observed that Copilot could mostly match the efficiency\nof solutions written by humans.\nTo understand and evaluate the impact of input parameters\u2019\nquality on GitHub Copilot, we first assessed the effect of providing\nonly function names and parameters. Based on the results, 79.3%\nof the solutions were valid compared to the 91.5% validity rate of\nthe original setup. For code correctness, we obtained a success rate\nof 19.5% for the correct, 26.8% for the partially correct, and 53.7%\nfor the incorrect code. We then assessed the impact of providing\ndummy function names and observed a drop to 84.2% in the validity\nrate. In terms of correctness, we found that 26.8% of the generations\nwere correct, whereas 51.2% of them were partially correct.\nOverall, the results suggest that GitHub Copilot is a promising\ntool. In the near future, AI pair programming tools like GitHub\nCopilot would potentially have a high impact on how we develop\nsoftware.\nAs future work, we aim to both increase the number of prob-\nlems and diversify the coverage and the difficulty level of problem\nscenarios. With minor modifications and customizations, the under-\nlying experimental framework that we proposed could be reused\nto evaluate other code generation algorithms in the future. To in-\ncrease the precision of the results, we plan to increase the number\nof unit tests for each problem. Finally, to extend our results and\ndiscussion, we plan to assess the code quality of GitHub Copilot\nusing maintainability and reliability metrics.\nREFERENCES\n[1] Matt Asay. 2021. GitHub copilot isn\u2019t changing the future.\nhttps://www.\ninfoworld.com/article/3625517/github-copilot-isnt-changing-the-future.html\n[2] Scott Carey. 2021. Developers react to github copilot. https://www.infoworld.\ncom/article/3624688/developers-react-to-github-copilot.html\n[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish\nSastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,\nElizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse,"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_025",
    "source_id": "CopilotQuality2022",
    "text": ", Greg Brockman,\nAlex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish\nSastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis,\nElizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,\nVedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam\nMcCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large\nLanguage Models Trained on Code. https://doi.org/10.48550/ARXIV.2107.03374\n[4] Neil A. Ernst and Gabriele Bavota. 2022. AI-Driven Development Is Here: Should\nYou Worry? IEEE Software 39, 2 (2022), 106\u2013110. https://doi.org/10.1109/MS.\n2021.3133805\n[5] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin,\nAnthony Tomasic, and Graham Neubig. 2018. Retrieval-Based Neural Code\nGeneration. https://doi.org/10.48550/ARXIV.1808.10025\n[6] JetBrains. 2022. GitHub copilot - intellij IDES plugin: Marketplace. Retrieved April\n25, 2022 from https://plugins.jetbrains.com/plugin/17718-github-copilot\n[7] Renato Losio. 2021. GitHub previews copilot, an openai-powered coding assistant.\nhttps://www.infoq.com/news/2021/07/github-copilot-pair-programmming/\n[8] Chen Lyu, Ruyun Wang, Hongyu Zhang, Hanwen Zhang, and Songlin Hu. 2021.\nEmbedding API dependency graph for neural code generation. Empirical Software\nEngineering 26, 4 (21 Apr 2021), 61. https://doi.org/10.1007/s10664-021-09968-2\n[9] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copilot\u2019s\nCode Suggestions. In 2022 IEEE/ACM 19th International Conference on Mining\nSoftware Repositories (MSR). 1\u20135. https://doi.org/10.1145/3524842.3528470\n[10] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and\nRamesh Karri. 2021. Asleep at the Keyboard? Assessing the Security of GitHub\nCopilot\u2019s Code Contributions. https://doi.org/10."
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_026",
    "source_id": "CopilotQuality2022",
    "text": ".org/10.1007/s10664-021-09968-2\n[9] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copilot\u2019s\nCode Suggestions. In 2022 IEEE/ACM 19th International Conference on Mining\nSoftware Repositories (MSR). 1\u20135. https://doi.org/10.1145/3524842.3528470\n[10] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and\nRamesh Karri. 2021. Asleep at the Keyboard? Assessing the Security of GitHub\nCopilot\u2019s Code Contributions. https://doi.org/10.48550/ARXIV.2108.09293\n[11] Dominik Sobania, Martin Briesch, and Franz Rothlauf. 2022.\nChoose Your\nProgramming Copilot: A Comparison of the Program Synthesis Performance\nof Github Copilot and Genetic Programming. In Proceedings of the Genetic\nand Evolutionary Computation Conference (Boston, Massachusetts) (GECCO\n\u201922). Association for Computing Machinery, New York, NY, USA, 1019\u20131027.\nhttps://doi.org/10.1145/3512290.3528700\n[12] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020.\nTreeGen: A Tree-Based Transformer Architecture for Code Generation. Proceed-\nings of the AAAI Conference on Artificial Intelligence 34, 05 (Apr. 2020), 8984\u20138991.\nhttps://doi.org/10.1609/aaai.v34i05.6430\n[13] Darryl K. Taft. 2021. GitHub copilot: A powerful, controversial autocomplete for\ndevelopers. https://thenewstack.io/github-copilot-a-powerful-controversial-\nautocomplete-for-developers/\n[14] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922).\nAssociation for Computing Machinery, New York, NY, USA, Article 332, 7 pages.\nhttps://doi.org/10.1145/3491101.3519665\n[15] Frank F. Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-IDE Code Genera-\ntion from Natural Language: Promise and Challenges. ACM Trans. Softw. Eng.\nMethodol. 31, 2, Article 29 (mar 2022), 47 pages. https://doi.org/10.1145/3487569\n[16] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and\nMingwen Wang. 2022. CodeGen-Test: An Automatic Code Generation Model\nIntegrating Program Test Information.\nhttps://doi.org/10.48550/ARXIV.2202.\n07612\n71"
  },
  {
    "chunk_id": "CopilotQuality2022_chunk_027",
    "source_id": "CopilotQuality2022",
    "text": "ilescu, and Graham Neubig. 2022. In-IDE Code Genera-\ntion from Natural Language: Promise and Challenges. ACM Trans. Softw. Eng.\nMethodol. 31, 2, Article 29 (mar 2022), 47 pages. https://doi.org/10.1145/3487569\n[16] Maosheng Zhong, Gen Liu, Hongwei Li, Jiangling Kuang, Jinshan Zeng, and\nMingwen Wang. 2022. CodeGen-Test: An Automatic Code Generation Model\nIntegrating Program Test Information.\nhttps://doi.org/10.48550/ARXIV.2202.\n07612\n71\n"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_001",
    "source_id": "DevExperienceGenAI2025",
    "text": "Developers\u2019 Experience with Generative AI - First Insights \nfrom an Empirical Mixed-Methods Field Study \nCharlotte \nBrandebusemeyer \n Digital Health \u2013 Connected \nHealthcare \n Hasso Plattner Institute, \nUniversity of Potsdam \n Potsdam, Germany \n char.brandebusemeyer@hpi.de \nTobias Schimmer \nSAP Labs \n SAP \n Newport Beach, USA \n tobias.schimmer@sap.com \n \nBert Arnrich \nDigital Health \u2013 Connected \nHealthcare \n Hasso Plattner Institute, \nUniversity of Potsdam \n Potsdam, Germany \nbert.arnrich@hpi.de \n \nABSTRACT \nWith the rise of AI-powered coding assistants, firms and \nprogrammers are exploring how to optimize their interaction \nwith them. Research has so far mainly focused on evaluating \noutput quality and productivity gains, leaving aside the \ndevelopers\u2019 experience during the interaction. In this study, \nwe take a multimodal, developer-centered approach to gain \ninsights into how professional developers experience the \ninteraction with Generative AI (GenAI) in their natural work \nenvironment in a firm. The aim of this paper is (1) to \ndemonstrate a feasible mixed-method study design with \ncontrolled and uncontrolled study periods within a firm \nsetting, (2) to give first insights from complementary \nbehavioral and subjective experience data on developers\u2019 \ninteraction with GitHub Copilot and (3) to compare the impact \nof interaction types (no Copilot use, in-code suggestions, chat \nprompts or both in-code suggestions and chat prompts) on \nefficiency, accuracy and perceived workload whilst working \non different task categories. Results of the controlled sessions \nin this study indicate that moderate use of either in-code \nsuggestions or chat prompts improves efficiency (task \nduration) and reduces perceived workload compared to not \nusing Copilot, while excessive or combined use lessens these \nbenefits. Accuracy (task completion) profits from chat \ninteraction. In general, subjective perception of workload \naligns with objective behavioral data in this study. During the \nuncontrolled period of the study, both higher cognitive load \nand productivity were perceived when interacting with AI \nduring everyday working tasks. This study motivates the use \nof comparable study designs, in e.g. workshop or hackathon \nsettings, to evaluate GenAI tools holistically and realistically \nwith a focus on the developers\u2019 experience. \n \nCCS CONCEPTS \n\u2022 Human-centered computing \uf0e0 Human computer interaction \n(HCI) \uf0e0 Empirical studies in HCI \nKEYWORDS \nGenerative AI, GitHub Copilot, developer experience, software \nengineering, empirical mixed-methods study, field study \nACM Reference format: \nCharlotte Brandebusemeyer, Tobias Schimmer, Bert Arnrich. 2026. \nDevelopers\u2019 Experience with Generative AI \u2013 First Insights from an \nEmpirical Mixed-Methods Study Field. In Proceedings of 2026 \nIEEE/ACM 48th International Conference on Software Engineering: \nSoftware Engineering in Practice (ICSE SEIP 2026). IEEE/ACM, Rio de \nJaneiro, Brazil, XX pages. Link/doi \n1 Introduction \n\u201cGenerative AI increases productivity\u201d \u2013 a phrase frequently \nencountered in industry and academic contexts. Since the \nrelease of AI-powered coding assistants such as GitHub \nCopilot in 2021,"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_002",
    "source_id": "DevExperienceGenAI2025",
    "text": " study, field study \nACM Reference format: \nCharlotte Brandebusemeyer, Tobias Schimmer, Bert Arnrich. 2026. \nDevelopers\u2019 Experience with Generative AI \u2013 First Insights from an \nEmpirical Mixed-Methods Study Field. In Proceedings of 2026 \nIEEE/ACM 48th International Conference on Software Engineering: \nSoftware Engineering in Practice (ICSE SEIP 2026). IEEE/ACM, Rio de \nJaneiro, Brazil, XX pages. Link/doi \n1 Introduction \n\u201cGenerative AI increases productivity\u201d \u2013 a phrase frequently \nencountered in industry and academic contexts. Since the \nrelease of AI-powered coding assistants such as GitHub \nCopilot in 2021, there has been a strong desire to integrate \nGenerative AI (GenAI) into workflows and products. The \nadoption is driven and encouraged by the promise of \nincreased productivity on the organization, team and \nindividual level. However, Gartner\u2019s 2025 Hype Cycle for \nArtificial Intelligence [23] found that GenAI has entered the \n\u201ctrough of disillusionment\u201d \u2013 a period characterized by the \nrecognition that technology does not meet the initial high \nexpectations. Research on developers\u2019 interaction with GenAI \ncan contribute to a systematic evaluation of potentials and \nshortcomings of this new technology. \nAI-powered coding assistants like GitHub Copilot were \ndeveloped to automate programming-related tasks. The aim is \nto support and relieve programmers and increase their \nproductivity. The focus of research has so far been on \nevaluating the capabilities and limitations of GenAI regarding \noutput quality [32, 34, 46, 47] and productivity gains [1, 8, 37, \nPermission to make digital or hard copies of part or all of this work for \npersonal or classroom use is granted without fee provided that copies are not \nmade or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page. Copyrights for third-party \ncomponents of this work must be honored. For all other uses, contact the \nowner/author(s). \nICSE-SEIP\u201926, Rio de Janeiro, Brazil \n\u00a9 2026 Copyright held by the owner/author(s).  \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \n49].  Although technology was developed for programmers, \nthe human aspect and programmers\u2019 behavior and experience \nduring the interaction with GenAI are often not the focus of \ncurrent research. A few recent studies have been approaching \nthis topic [1, 2, 8, 33, 37, 40, 42, 45, 49]. However, empirical \nstudies conducted in real-life organizational settings are \nlimited in academic research [1, 8, 33, 37, 42]. Studies also \noften rely solely on subjective data from, e.g., questionnaires, \ninterviews, or blog posts [8, 40, 42]. By combining multiple \nsubjective and objective data sources, one can compensate for \nindividual shortcomings of data collection methods and gain a \nmore nuanced understanding of the developer-GenAI \ninteraction. Additionally, there is systematic research missing \nwhich examines how different interaction types (in-code \nsuggestions vs. chat prompts), modes"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_003",
    "source_id": "DevExperienceGenAI2025",
    "text": ", 33, 37, 40, 42, 45, 49]. However, empirical \nstudies conducted in real-life organizational settings are \nlimited in academic research [1, 8, 33, 37, 42]. Studies also \noften rely solely on subjective data from, e.g., questionnaires, \ninterviews, or blog posts [8, 40, 42]. By combining multiple \nsubjective and objective data sources, one can compensate for \nindividual shortcomings of data collection methods and gain a \nmore nuanced understanding of the developer-GenAI \ninteraction. Additionally, there is systematic research missing \nwhich examines how different interaction types (in-code \nsuggestions vs. chat prompts), modes (ask, edit and agent), \nand task types (code generation, debugging, documentation, \ntesting) influence output quality, developers\u2019 interaction \nquality and developers\u2019 perceived workload in a natural work \nenvironment. Study designs including physiological data \ngathered via wristbands to analyze developers\u2019 GenAI \ninteraction experience more closely are also missing so far. \nTo address some of these research gaps, we conducted an \nempirical mixed-methods study with professional software \ndevelopers at SAP. Combining multimodal data \u2013 including \nsubjective experiences via questionnaires, behavioral data \nfrom \nscreen, \nmouse, \nand \nkeyboard \nrecordings, \nand \nphysiological data from a wristband \u2013 provides a multifaceted \nimpression \nof \nthe \ndeveloper-GenAI \ninteraction. \nThe \ncombination of controlled sessions and uncontrolled periods \nduring the study enables both experimental control in a real-\nworld firm setting and a realistic impression of professional \nsoftware developers\u2019 workdays. With this study setup, we \ninvestigate how GenAI interactions during (simulated) \nsoftware engineering tasks influence efficiency, accuracy, and \nperceived workload.  \nIn this paper, we present the entire study design to \ndemonstrate the feasibility and usefulness of conducting a \nmultimodal study in a firm context with professional software \ndevelopers. We hope to motivate future studies to employ a \ncomparable study design for an holistic evaluation of \ndevelopers\u2019 interaction and experience with new AI tools. \nWhilst the entire study setup is described in detail, a focus on \ncertain aspects of the extensive study design and multimodal \ndata gathered needed to be set in this paper: We would like to \ngive first insights into how behavioral and subjective data \ncombined contribute to a deeper understanding of GenAI\u2019s \nimpact on developers\u2019 experiences and productivity in the \nsoftware engineering context. Analyses focus on the \ncontrolled sessions of the study, whilst also providing first \ninsights into results from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators\u2026 \n(1) \u2026 efficiency \n(2) \u2026 accuracy \n(3) \u2026 the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_004",
    "source_id": "DevExperienceGenAI2025",
    "text": " from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators\u2026 \n(1) \u2026 efficiency \n(2) \u2026 accuracy \n(3) \u2026 the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is one of several GenAIs that can assist developers by \ngenerating, completing and modifying programming code \nbased on the context of the codebase and natural language \nprompts. \nIt \nis \nintegrated \nin \nsoftware \ndevelopment \nenvironments (IDEs) like VS Code and can function as an \u201cAI \npair programmer\u201d for developers.  \nIn recent years, multiple studies have been conducted to \nevaluate Copilot\u2019s output quality and developers\u2019 productivity \ngains through AI usage. Based on benchmark tasks, Dakhel et \nal. [32] found that Copilot can generate solutions for nearly all \ngiven tasks, but its outputs are less often correct than those of \nhumans, making it a potential asset for experienced \ndevelopers but a liability for novices who may not detect non-\noptimal suggestions. So far, there is no consensus on how to \nbest measure software developers\u2019 productivity, but time \nsavings, acceptance rate of Copilot suggestions, and successful \ncompletion of predefined tasks are frequently considered \nmetrics to evaluate developer productivity in the context of \nAI-assisted programming. Empirical studies using these \nmetrics report mixed results: Bakal et al. [1]  found that \ndevelopers using Copilot accepted 33% of suggestions and \nhad a 20% increase in lines of code; Peng et al. [37] observed \nthat programmers completed an HTTP server task 55.8% \nfaster with Copilot, with the largest productivity gains among \nless experienced programmers; while Vaithilingam et al. [45] \nfound no significant improvement in task time or success, \nthough participants still valued Copilot as a useful starting \npoint for daily programming tasks. In this study, we consider \ntask duration, the number of Copilot\u2019s suggestions, and \nsuccessful \ntask \ncompletion \nas \nproductivity \nmetrics. \nAdditionally, we examine the impact of Copilot interaction \ntypes, \ndevelopers\u2019 \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers\u2019 \nexperiences during work [6, 17, 36]. However, only a few \n"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_005",
    "source_id": "DevExperienceGenAI2025",
    "text": "\u2019 \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers\u2019 \nexperiences during work [6, 17, 36]. However, only a few \nstudies have used mixed-methods study designs and \nmultimodal data to analyze the developer-GenAI interaction. \nFor example, Tang et al. [44] combined IDE telemetry, eye-\ntracking, and subjective workload measures to study how \ndevelopers validate and repair LLM-generated code. They \nshowed that knowing whether the code was AI-generated \ninfluenced both the developers\u2019 debugging behavior and \ncognitive load, with cognitive load being higher when \ndevelopers were aware that the code was AI-generated. \nZiegler et al. [49] combined survey data, based on the SPACE \nframework [13], with telemetry data to assess the impact of \nCopilot on professional developers' productivity, finding that \nthe acceptance rate of in-code suggestions correlates with \nperceived productivity. With the study presented in this \npaper, we contribute to the growing mixed-methods and \nmultimodal research on developer-GenAI interaction. The aim \nis to gain a more developer-centered perspective on \ndevelopers\u2019 perception and productivity during their work \nwith AI. \n3 Study Procedure \nEach participant completed a four-day study, with controlled \nsessions on the first and last days and an uncontrolled period \nin between (Figure 1). Physiological data were continuously \nrecorded using the EmbracePlus wristband. Subjective \nexperiences were assessed through questionnaires, and \nbehavioral data were captured via screen, mouse, and \nkeyboard activity during the controlled sessions (Figure 1, \nData recorded). The study took place at two SAP locations in \nthe US. \n3.1 Controlled sessions \nEach participant met the experimenter individually in a \nmeeting room in the firm for the controlled sessions that \nframed the study (first controlled session on the first day, last \ncontrolled session on the last day). Here, they were briefed on \nthe study procedure, data collection, and data privacy, and \nthen gave their informed consent to participate. The \nparticipant was then equipped with the EmbracePlus \nwristband to measure the physiological activity, and the \nscreen, mouse and keyboard recording was started on the \nlaptop. The session began with a pre-questionnaire covering \njob characteristics, programming and GenAI experience, work \nsatisfaction, developer experience, and a personality test \n(Figure 1, Pre-Questionnaire). \nNext, participants completed an n-back task (1-back to 3-\nback) to assess cognitive performance.  Alphabetical letters \nneeded to be memorized n positions back. After each difficulty \nlevel, the cognitive load and the stress level were rated. A \nstartle cue (honking sound) was introduced after the 3"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_006",
    "source_id": "DevExperienceGenAI2025",
    "text": ". The \nparticipant was then equipped with the EmbracePlus \nwristband to measure the physiological activity, and the \nscreen, mouse and keyboard recording was started on the \nlaptop. The session began with a pre-questionnaire covering \njob characteristics, programming and GenAI experience, work \nsatisfaction, developer experience, and a personality test \n(Figure 1, Pre-Questionnaire). \nNext, participants completed an n-back task (1-back to 3-\nback) to assess cognitive performance.  Alphabetical letters \nneeded to be memorized n positions back. After each difficulty \nlevel, the cognitive load and the stress level were rated. A \nstartle cue (honking sound) was introduced after the 3-back \ntask to control for physiological responses during later data \nanalysis (Figure 1, Cognitive n-back tasks with a startle event). \nA five-minute breathing meditation video then served to relax \nthe participant and gave time for physiological activity to \nreturn to a resting state (Figure 1, 4-7-8 breathing \nmeditation). \nFigure 1: The study procedure and setup together with the A/B study design is depicted. In this paper, the focus lies on the \nblack surrounded sections. \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \nParticipants then completed three baseline coding tasks in \nJava\u2014one without Copilot and two with\u2014to familiarize \nthemselves with the Visual Studio Code IDE and the Copilot \nintegration. After each task, participants filled in a NASA Task \nLoad Index questionnaire (NASA-TLX), followed by a one-\nminute relaxation video. A two-minute video was played for \nfurther relaxation before the main tasks (Figure 1, 3 baseline \ncoding tasks).  \nThe main phase included six randomized tasks: coding, \ndebugging, code documentation, writing unit tests, summary \nand \nbrainstorming \ntasks. \nAs \nbefore, \nthe \nNASA-TLX \nquestionnaire was filled in, and a one-minute relaxation video \nwas shown between tasks (Figure 1, 6 main tasks). In the A/B \ndesign for the first controlled session, half of the participants \ndid not use Copilot during the main tasks; the others did \n(Figure 1, A/B study design). Participants completed a Copilot \nevaluation questionnaire either after the baseline tasks (if not \nusing Copilot) or at the end of the session (if using Copilot) \n(Figure 1, Copilot evaluation questionnaire). \nThe second session mirrored the first, starting with a \nmeditation video, followed by equivalent tasks, but this time \nperformed by all (both groups) with Copilot (Figure 1, A/B \nstudy design). After the tasks, participants filled in a post-\nquestionnaire with final evaluations of their GenAI interaction \nduring the study and an evaluation of the study procedure and \nsetup (Figure 1, Post-Questionnaire). Upon completion of the \nstudy, each participant received a 50$ Amazon voucher via \nemail as compensation for their time and effort in \nparticipating. \n3.2 Uncontrolled setting \nDuring the three days between the two controlled sessions, \nthe participants followed their normal workdays whilst \ndocumenting their working tasks. For each task, they tracked \nthe start and end times"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_007",
    "source_id": "DevExperienceGenAI2025",
    "text": " tasks, but this time \nperformed by all (both groups) with Copilot (Figure 1, A/B \nstudy design). After the tasks, participants filled in a post-\nquestionnaire with final evaluations of their GenAI interaction \nduring the study and an evaluation of the study procedure and \nsetup (Figure 1, Post-Questionnaire). Upon completion of the \nstudy, each participant received a 50$ Amazon voucher via \nemail as compensation for their time and effort in \nparticipating. \n3.2 Uncontrolled setting \nDuring the three days between the two controlled sessions, \nthe participants followed their normal workdays whilst \ndocumenting their working tasks. For each task, they tracked \nthe start and end times, the task, their feeling of cognitive load \nand productivity, and use of GenAI (if any). This workday \nquestionnaire was filled in on a sheet of paper. Participants \nwere encouraged to use a GenAI tool of their choice for at least \none continuous hour per day to ensure a consistent time \nwindow for physiological data analysis (Figure 1, Workday \nquestionnaire). At the end of the workday, the participants \nfilled in an end-of-workday questionnaire assessing their \ndeveloper experience and GenAI interaction for that day \n(Figure 1, End-of-workday questionnaire).  \n4 Study Design and Technical Details \n4.1 Study design \n4.1.1 A/B testing. The study used an A/B test design: Group A \ncompleted the first session without Copilot and the second \nsession with Copilot, while Group B used Copilot in both. This \nsetup allows for both within-group and between-group \ncomparisons of GenAI interaction over time (Figure 1, A/B \nstudy design). \n4.1.2 n-back task. The n-back task, commonly used in Cognitive \nPsychology and Neuroscience, assesses working memory, \ncognitive load, and attention. Participants viewed a sequence \nof letters, each shown for 0.5 seconds, followed by a cross for \ntwo seconds. The task was to identify whether the current \nletter matched the one n steps earlier. Difficulty increased \nwith n, since more letters need to be stored in working \nmemory. The task was performed three times with increasing \ndifficulty levels (n = 1, 2 and 3). 48 letters were shown per \ntask. The number of errors was displayed as feedback to the \nparticipants to maintain their motivation. After each difficulty \nlevel, participants rated their cognitive load and how insecure, \ndiscouraged, irritated, stressed and annoyed they were on a 7-\npoint Likert scale (1 = very low, 7 = very high) (Figure 1, \nCognitive n-back tasks with a startle event). \n \n4.1.3 Startle event. A startle event in the form of a honking \nsound was played after the final letter of the 3-back task as a \ncontrol cue to verify the reliability of the physiological data \nrecording (Figure 1, Cognitive n-back tasks with a startle \nevent). \n \n4.1.4 Relaxation videos. Relaxation videos were used between \ntasks to help participants recover physiologically after a \ncognitively demanding task before completing the next one \n(Figure 1, 4-7-8 breathing meditation,"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_008",
    "source_id": "DevExperienceGenAI2025",
    "text": " (1 = very low, 7 = very high) (Figure 1, \nCognitive n-back tasks with a startle event). \n \n4.1.3 Startle event. A startle event in the form of a honking \nsound was played after the final letter of the 3-back task as a \ncontrol cue to verify the reliability of the physiological data \nrecording (Figure 1, Cognitive n-back tasks with a startle \nevent). \n \n4.1.4 Relaxation videos. Relaxation videos were used between \ntasks to help participants recover physiologically after a \ncognitively demanding task before completing the next one \n(Figure 1, 4-7-8 breathing meditation, Relaxation video). \nFollowing the startle event, a five-minute guided meditation \nbreathing video [22] using the 4-7-8 breathing technique \npromoted relaxation. Between tasks, one-minute nature scene \nvideos were shown to the participants with the note to \nbreathe deeply and let the mind dive into the scene in the \nvideo. The aim was to minimize mind wandering whilst at the \nsame time using the positive effect of nature on mental well-\nbeing [4] to relax the participants and reduce their \nphysiological activity between the tasks. Video material for \nthe generation of the relaxation videos was copyright-free \n[14\u201316, 22, 39]. The open-source video editor Shotcut was \nused to create the video snippets [29]. \n \n4.1.5 Tasks during the controlled sessions. The programming-\nrelated tasks for the controlled sessions were selected from \nthe HumanEval-X benchmark dataset [48]. The coding \nlanguage was Java. To make the task difficulty comparable \nbetween different tasks and the two controlled sessions, code \nmetrics were used to select 15 tasks of comparable cognitive \ndemand. Specifically, the cognitive complexity [7], cyclomatic \ncomplexity [28], Halstead metrics [20], lines of code (LOC), \nnon-comment LOC (NCLOC), comment LOC (CLOC) and nested \nblock depth (NBD) were considered. The metrics were \ncalculated by the MetricsReloaded plugin [25] in IntelliJ IDEA \n2024.3.3.  \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nThree baseline tasks in the first session served to familiarize \nthe participants with the IDE and Copilot and were excluded \nfrom later analyses (Figure 1, 3 baseline coding tasks). The 12 \nmain tasks spanned six categories - coding, debugging, \ndocumentation, unit testing, summaries, and brainstorming - \nwith each category included in both sessions. The summary \ntasks consisted of summarizing an email and a video-recorded \nmeeting; brainstorming tasks asked for ideas on team-\nbuilding activities and how to make meetings more engaging, \ncollaborative and efficient (Figure 1, 6 main tasks during first \nand second controlled session). For every participant, the \norder of the 12 tasks was randomized, with each task category \noccurring once during each controlled session. This way, \norder, learning, fatigue, and other sequence-related biases are \ncontrolled for. \n \n4.1.6 Questionnaires. The participants\u2019 subjective experience \nwas evaluated via six questionnaires. Most questions are \nbased on previous research for"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_009",
    "source_id": "DevExperienceGenAI2025",
    "text": "ing - \nwith each category included in both sessions. The summary \ntasks consisted of summarizing an email and a video-recorded \nmeeting; brainstorming tasks asked for ideas on team-\nbuilding activities and how to make meetings more engaging, \ncollaborative and efficient (Figure 1, 6 main tasks during first \nand second controlled session). For every participant, the \norder of the 12 tasks was randomized, with each task category \noccurring once during each controlled session. This way, \norder, learning, fatigue, and other sequence-related biases are \ncontrolled for. \n \n4.1.6 Questionnaires. The participants\u2019 subjective experience \nwas evaluated via six questionnaires. Most questions are \nbased on previous research for comparability reasons. \n \n4.1.6.1 Pre-Questionnaire. The pre-questionnaire was filled in \nat the beginning of the first controlled session (Figure 1, Pre-\nQuestionnaire). Questions regarding the work environment \n[41], work experience [41], GenAI use and satisfaction [24, \n30], work satisfaction [38, 41], developer experience \naccording to the DevEx framework [11] - expanded with \nfurther questions on the flow state [31] and the DX Core 4 \nframework [43] - and the 50-item International Personality \nItem Pool (IPIP) based on Goldberg\u2019s markers for the Big-Five \nfactor structure [18, 19] were filled in. The aim was to gain \ngeneral yet extensive insights into the participant pool of this \nstudy. \n \n4.1.6.2 NASA Task Load Index. The NASA-TLX is a widely used \nquestionnaire to assess perceived workload [21]. It consists of \nsix 21-point Likert-scaled questions asking for mental \ndemand, physical demand, temporal demand, performance, \neffort and frustration during a task. Participants filled in the \nquestionnaire after each controlled session task to capture \nand later compare task impact (Figure 1, NASA-TLX \nquestionnaire). The raw NASA-TLX score \u2013 the average of the \nsix ratings \u2013 provided the measure for the perceived workload \nin this study. \n \n4.1.6.3 Copilot Evaluation. After using Copilot in the first \ncontrolled session (post-baseline for Group A, end of first \nsession for Group B), participants\u2019 interactions with in-code \nsuggestions and chat were evaluated separately (Figure 1, \nCopilot evaluation questionnaire). The questions are based on \nthe SPACE framework [13] and were adapted to evaluate \nspecifically GitHub Copilot use. \n \n4.1.6.4 Workday Questionnaire. During the normal workday, \nparticipants used a paper questionnaire to log tasks, start and \nend time of each task, cognitive load (7-point Likert scale), \nperceived productivity (6-point Likert scale), and whether \nGenAI was predominantly used to solve the task. They were \nencouraged to use some form of GenAI for at least one \nconsecutive hour during their workday (Figure 1, Workday \nquestionnaire). The aim was to evaluate for which tasks \ndevelopers currently use GenAI during their normal work \nroutines and how useful they find the interaction. \n \n4.1.6.5 End-of-workday Questionnaire. Similar questions to the \npre-questionnaire were asked at"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_010",
    "source_id": "DevExperienceGenAI2025",
    "text": ". During the normal workday, \nparticipants used a paper questionnaire to log tasks, start and \nend time of each task, cognitive load (7-point Likert scale), \nperceived productivity (6-point Likert scale), and whether \nGenAI was predominantly used to solve the task. They were \nencouraged to use some form of GenAI for at least one \nconsecutive hour during their workday (Figure 1, Workday \nquestionnaire). The aim was to evaluate for which tasks \ndevelopers currently use GenAI during their normal work \nroutines and how useful they find the interaction. \n \n4.1.6.5 End-of-workday Questionnaire. Similar questions to the \npre-questionnaire were asked at the end of each workday [11, \n38, 41]. This questionnaire was tailored to capture \nparticipants\u2019 experiences and extraordinary occurrences that \nmight affect data analysis during one specific day (Figure 1, \nEnd-of-workday questionnaire). \n \n4.1.6.6 Post-Questionnaire. The post-questionnaire [30, 41], \ncompleted after the second controlled session, evaluated \nparticipants\u2019 GenAI interaction, wristband use, and the study \nsetup to gather insights for future studies (Figure 1, Post-\nQuestionnaire). \n \n4.1.7 GenAI use. GitHub Copilot is integrated in the software \ndevelopment environment (IDE) VS Code. It gives developers \na broad range of interaction possibilities: Participants could \nfreely switch models, interaction types (in-code suggestions, \nchat, in-line chat), and modes (ask, edit, agent) in this study \ndesign. Copilot selects an appropriate prompt template based \non the chosen interaction type and mode. Differing templates \nexplain differing model outputs. The three main interaction \ntypes differ in the location where code suggestions are \ngenerated, the context scope of the codebase they consider, \nresponse latencies and intended use. In-code suggestions \nappear at the position of the cursor, using only a few lines of \ncode above and below the cursor position as a context for \ncode completions. This interaction type is optimized for short \nlatencies, which enables real-time suggestions similar to \nautocompletion, but generates more lines of code with ideally \nmore contextual relevance. In-line chat is used within the \ncodebase for specific questions and adaptations in the code. \nThe considered context is also limited to the surrounding lines \nof code. For chat, a separate window enables a dialogue-like \ninteraction for an ongoing conversation. Chat considers a \nwider context of the codebase within the range of an entire \nfile or multiple files. Responses are optimized for creativity \nand explanations. GPT-4o was set as the default model, and \nthe \u201cask\u201d mode was the default mode for the chat interaction \nat the beginning of the controlled sessions. During the \nuncontrolled period, they were free to use any GenAI tool. The \nstudy aims to examine the developers\u2019 interaction with GenAI \ntools, rather than assessing specific tools or model \nperformances.  \n4.2 Technical Setup \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \n4.2.1 Screen, mouse and keyboard recording. Behavioral data \nduring the controlled session were gathered via screen, \nkeyboard and mouse recordings. The open-source Open"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_011",
    "source_id": "DevExperienceGenAI2025",
    "text": " creativity \nand explanations. GPT-4o was set as the default model, and \nthe \u201cask\u201d mode was the default mode for the chat interaction \nat the beginning of the controlled sessions. During the \nuncontrolled period, they were free to use any GenAI tool. The \nstudy aims to examine the developers\u2019 interaction with GenAI \ntools, rather than assessing specific tools or model \nperformances.  \n4.2 Technical Setup \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \n4.2.1 Screen, mouse and keyboard recording. Behavioral data \nduring the controlled session were gathered via screen, \nkeyboard and mouse recordings. The open-source Open \nBroadcast Software (OBS) was used for screen recordings \n[35]. Keyboard use and mouse movement were recorded via \nthe JNativeHook library for global keyboard and mouse \nlistening for Java [3]. Recordings were limited to the study \nlaptop and the controlled sessions for personal and firm data \nprivacy reasons. \n \n4.2.2 Wearable device. Physiological data was recorded via the \nEmbracePlus wristband (FDA-cleared and CE-certified [9]). \nHeart activity (PPG), electrodermal activity (EDA), skin \ntemperature, and wrist movement were recorded during the \nwhole study period. The device provides raw, unprocessed \nphysiological data for transparent and explainable data \nanalysis. \n4.3 Participants \nTwenty-two SAP employees participated in the study - \nfourteen from one SAP location, eight from another, both \nwithin California, US. The participants consisted of 12 \nsoftware \ndevelopers/engineers, \n8 \nsenior \nsoftware \ndevelopers/engineers, one senior quality specialist and one \nprincipal software architect, averaging 5-10 years of \nIT/programming experience. All had Java experience (a study \nrequirement), with Java (n=22), Python (n=12), and JavaScript \n(n=11) as the most used programming languages among \nparticipants. Based on a 5-point scale, participants were on \naverage highly proficient in Java (M=4.45, min=3, max=5) and \nalso in GenAI use (M=3.77, min=2, max=5). The participants \nhad been using GenAI at work for on average 6-12 months and \nfor at least 1-6 months. The most commonly used GenAI tools \nat work were GitHub Copilot and ChatGPT. In their free time, \nthe participants mostly use ChatGPT and Google Gemini, with \ntwo participants stating that they do not use GenAI in their \nfree time. For one participant, only the pre-, post- and copilot \nevaluation \nquestionnaire \ndata \ncould \nbe \nused, \nsince \ninstructions were not followed. We refrained from collecting \nage and gender information due to privacy concerns. \nParticipation in this study was voluntary, and the design and \nprocedure of the study were reviewed and approved by the \nethics committee of the University of Potsdam. \n4.4 Data Preparation and Analysis \nIn this paper, the focus lies on analyzing the two controlled \nsessions of the study where developers work on simulated \nsoftware engineering tasks. Subjective experiences (from the \nNASA-TLX questionnaires) and behavioral interactions with"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_012",
    "source_id": "DevExperienceGenAI2025",
    "text": " not use GenAI in their \nfree time. For one participant, only the pre-, post- and copilot \nevaluation \nquestionnaire \ndata \ncould \nbe \nused, \nsince \ninstructions were not followed. We refrained from collecting \nage and gender information due to privacy concerns. \nParticipation in this study was voluntary, and the design and \nprocedure of the study were reviewed and approved by the \nethics committee of the University of Potsdam. \n4.4 Data Preparation and Analysis \nIn this paper, the focus lies on analyzing the two controlled \nsessions of the study where developers work on simulated \nsoftware engineering tasks. Subjective experiences (from the \nNASA-TLX questionnaires) and behavioral interactions with \nGenAI (from the screen recordings) contribute to the \nevaluation. Initial insights into participants' use of GenAI \nduring the uncontrolled part of the study (during their \nworkday) are also given. \n \n4.4.1 Data labeling. To analyze participants\u2019 interaction with \nGenAI during the controlled sessions, ~66 hours of screen \nrecordings were manually labeled. Labels included per \nparticipant and per task were: controlled session day (first or \nsecond day), task order (1-6 per day), task duration, task \ncompletion (completed, partly completed/not completed), \nCopilot use (with or without), the number of in-code \nsuggestions and the number of chat prompts. The in-line chat \neditor was categorized as chat interaction in this study. The \nlabeling of edge cases was reviewed by all authors of this \npaper. \n \n4.4.2 \nStatistical \nMethods. \nSignificant \nwithin-participant \ndifferences were assessed using the Wilcoxon signed-rank \ntest, and between-participant comparisons were conducted \nwith the Mann\u2013Whitney U test. \nTo predict task completion, duration, and perceived workload, \n(generalized) linear mixed-effects models were used, with \ntask categories and Copilot interaction types as fixed effects. \nFor task duration, the number of in-code and chat interactions \nwere additionally added as fixed effects to reflect interaction \nintensity. Participants and tasks were included as random \neffects to account for individual and task-level variability. \nDetails on constructed mixed-effects models, model fit \nevaluations, and the associated post hoc tests and p-value \nadjustments can be found in the supplemental material [5]. \n \n4.4.3 Pre-analyses for evaluating the controlled sessions. Pre-\nanalyses found no statistically significant difference in task \ndurations between the two controlled sessions (first and last \nday of the study) nor between the two participant groups \n(group A not using GenAI during the first controlled session \nvs. group B using GenAI during both controlled sessions) \n(Figure 1). Therefore, the tasks from both controlled sessions \nand the two participant groups are analyzed together in the \nfollowing. Please refer to the supplemental material for details \non the performed analyses [5]. \n5 Results \nThe following analysis examines how GenAI/Copilot use \nimpacts the productivity indicators task duration (efficiency), \ntask completion (accuracy), and raw NASA-TLX scores \n(perceived workload) during the predefined tasks of the \ncontrolled sessions. \n5.1 Task duration (efficiency) \nTo assess the impact of Copilot use on task duration, linear \nmixed"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_013",
    "source_id": "DevExperienceGenAI2025",
    "text": " \n(group A not using GenAI during the first controlled session \nvs. group B using GenAI during both controlled sessions) \n(Figure 1). Therefore, the tasks from both controlled sessions \nand the two participant groups are analyzed together in the \nfollowing. Please refer to the supplemental material for details \non the performed analyses [5]. \n5 Results \nThe following analysis examines how GenAI/Copilot use \nimpacts the productivity indicators task duration (efficiency), \ntask completion (accuracy), and raw NASA-TLX scores \n(perceived workload) during the predefined tasks of the \ncontrolled sessions. \n5.1 Task duration (efficiency) \nTo assess the impact of Copilot use on task duration, linear \nmixed-effects models predicting the log-transformed task \nduration were fit on the data and compared: \nlog(task duration) ~ interaction type + num_inCode + \nnum_chat + task categories + scaled raw NASA-TLX + \n(1|participants) + (1|tasks) \n \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nThe impact of interaction types (without GenAI use, only in-\ncode suggestions, only chat prompts or both in-code \nsuggestions and chat prompts), interaction intensity (number \nof in-code suggestions (num_inCode) and number of chat \nprompts (num_chat), task categories (coding, debugging, \ndocumentation, unit testing, summary brainstorming) and \nperceived workload (scaled raw NASA-TLX scores) on task \nduration is analyzed in the following paragraphs. As detailed \nin the supplemental material [5], both interaction type and \nintensity significantly improve model fit, individually and \ncombined. This suggests that how and how much developers \nused Copilot had an impact on the task durations. \n \n5.1.1 Impact of interaction type on task duration. To examine \nthe effect of interaction type on task duration, pairwise \ncomparisons using a Tukey post hoc test were conducted. \nParticipants \nworking \non \nthe \ntasks \nwith \nonly \nchat \n(estimate=0.44, \nSE=0.120, \np=0.0016) \nor \nonly \nin-code \nsuggestions \n(estimate=0.46, \nSE=0.159, p=0.0221) \nwere \nsignificantly faster than without Copilot. When both in-code \nsuggestions and chat were used during a task, task duration \ndid not differ significantly compared to not using Copilot \n(estimate=0.223, \nSE=0.170, \np=0.5559). \nNo \nsignificant \ndifferences could be observed between the three Copilot-\nassisted conditions (Figure 2). The results suggest that while \nindividual interaction with either in-code suggestions or chat \nimproves efficiency, combining interaction types does not \nyield additional time savings compared to not using Copilot to \nsolve a task. \n \nFigure 2: Raw task durations and model-predicted \ndurations grouped by Copilot interaction type are shown. \nAsterisks denote statistical significance (* p\u22640.05; ** \np\u22640.01). \n \n5.1.2 Impact of interaction intensity on task duration. To assess \nhow interaction frequency/intensity affects efficiency, average \ntask durations were compared across interaction"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_014",
    "source_id": "DevExperienceGenAI2025",
    "text": "). \nNo \nsignificant \ndifferences could be observed between the three Copilot-\nassisted conditions (Figure 2). The results suggest that while \nindividual interaction with either in-code suggestions or chat \nimproves efficiency, combining interaction types does not \nyield additional time savings compared to not using Copilot to \nsolve a task. \n \nFigure 2: Raw task durations and model-predicted \ndurations grouped by Copilot interaction type are shown. \nAsterisks denote statistical significance (* p\u22640.05; ** \np\u22640.01). \n \n5.1.2 Impact of interaction intensity on task duration. To assess \nhow interaction frequency/intensity affects efficiency, average \ntask durations were compared across interaction types and \ncounts against a no-Copilot baseline at 342.6s (Figure 3). Both \nin-code suggestions and chat prompts reduced task duration \nat lower interaction counts, with in-code showing consistent \nefficiency gains up to 13 interactions, and chat suggestions up \nto six interactions. Beyond these levels, task durations mostly \nexceeded the no-Copilot baseline. Combined use of both \ninteraction types showed slight efficiency gains at low \ninteractions but decreased performance at higher levels. \nLonger task durations at a higher number of chat prompts \nmay reflect time spent refining prompts. In contrast, the \nrelative efficiency of in-code suggestions may result from \nfaster execution on a trial-and-error basis. Combined use of \ninteraction types mostly indicated unsatisfying results from \none mode, leading to a switch. In sum, moderate GenAI \ninteractions improve efficiency, while excessive interactions \nmay introduce overhead that diminishes benefits. \n \nFigure 3: These three diagrams compare the efficiency of \nusing Copilot compared to no Copilot (blue dashed line) \nduring tasks. The three plots represent the three Copilot \ninteraction types. Dots show the mean task duration per \nnumber of interactions (green dots: more efficient than \nnot using Copilot; red dots: less efficient). \n \n5.1.3 Impact of task categories on task duration. Next, the \ninfluence of the task categories on task duration was analyzed. \nA full linear mixed-effects model including task categories as a \npredictor fit significantly better to the data than a reduced \nmodel \nexcluding \nthe \npredictor \n(\u03c7\u00b2(5)=20.04, \nAICfull_model=388.29, \nAICnull_model=398.32, \np=0.0012). \nThis \nindicates that task duration varied by task type.  \nIn the full model with the coding tasks as the reference, \ndebugging tasks were associated with significantly longer \ndurations (estimate=0.6606, p=0.0152), as were summary \ntasks (estimate=0.4887, p=0.0494). No significant differences \nwere \nfound \nfor \nthe \nother \ntasks. \nPost-hoc \npairwise \ncomparisons (Tukey-adjusted) indicated marginal differences \nbetween debugging vs. documentation (p=0.0526) and vs. \nbrainstorming tasks (p=0.0592), with debugging tasks taking \nlonger. No other task pairs differed significantly. The results \nsuggest that debugging tasks, and to a lesser extent summary \ntasks, required more time than other task categories. \nPer"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_015",
    "source_id": "DevExperienceGenAI2025",
    "text": " associated with significantly longer \ndurations (estimate=0.6606, p=0.0152), as were summary \ntasks (estimate=0.4887, p=0.0494). No significant differences \nwere \nfound \nfor \nthe \nother \ntasks. \nPost-hoc \npairwise \ncomparisons (Tukey-adjusted) indicated marginal differences \nbetween debugging vs. documentation (p=0.0526) and vs. \nbrainstorming tasks (p=0.0592), with debugging tasks taking \nlonger. No other task pairs differed significantly. The results \nsuggest that debugging tasks, and to a lesser extent summary \ntasks, required more time than other task categories. \nPerceived workload, measured via the scaled raw NASA-TLX \nscore was a significant positive predictor of task duration \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \n(estimate=0.32, p<0.001). An increase in perceived workload \nwas associated with increased task duration. \nIn sum, both the interaction type and intensity of Copilot use \nsignificantly influenced task duration. Moderate use of either \nchat or in-code suggestions improved efficiency compared to \nno Copilot use, while excessive and/or combined interactions \ndiminished the effect. Task type and perceived workload also \nhelp predict duration, with debugging tasks taking longer, and \na higher workload associated with increased task duration. \n5.2 Task completion (accuracy) \nA generalized linear mixed-effects model was used to assess \nhow GenAI interaction types and task categories influence \ntask completion:  \ntask completion ~ interaction type + task categories + \n(1|participants) + (1|tasks) \n \n5.2.1 Impact of interaction type on task completion. Interaction \ntype was a marginal predictor for task completion \n(\u03c7\u00b2(3)=7.4417, \nAICfull_model=223.91, \nAICnull_model=225.35, \np=padjusted=0.0591), with only chat prompts significantly \nincreasing task completion likelihood compared to no GenAI \n(estimate=1.19, p=0.0078). In-code suggestions or combining \nboth interaction types showed no significant effect. An \nexploratory Tukey post-hoc test confirmed that using only \nchat prompts led to significantly higher task completion than \nno Copilot use (estimate=1.19, SE=0.448, p=0.0389). Thus, \nusing chat alone appears to be most beneficial compared to \nnot using Copilot. The brainstorming and summary tasks \ncould only be completed with either no Copilot interaction or \nchat interaction, but not via in-code suggestions, which should \nbe considered when interpreting the results.  \n \n5.2.2 Impact of task category on task completion. Task \ncategories significantly influenced task completion prediction \n(\u03c7\u00b2(5)=21.69, AICfull_model=223.91, AICnull_model=235.6, p<0.001, \npadjusted=0.0012). Compared to coding, debugging (estimate=-\n2.92, \nSE=0.9539, \np=0.0022), \ntesting \n(estimate=-3.25, \nSE=0.9503, \np<0"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_016",
    "source_id": "DevExperienceGenAI2025",
    "text": " only be completed with either no Copilot interaction or \nchat interaction, but not via in-code suggestions, which should \nbe considered when interpreting the results.  \n \n5.2.2 Impact of task category on task completion. Task \ncategories significantly influenced task completion prediction \n(\u03c7\u00b2(5)=21.69, AICfull_model=223.91, AICnull_model=235.6, p<0.001, \npadjusted=0.0012). Compared to coding, debugging (estimate=-\n2.92, \nSE=0.9539, \np=0.0022), \ntesting \n(estimate=-3.25, \nSE=0.9503, \np<0.001), \ndocumentation \n(estimate=-2.32, \nSE=0.9506, \np=0.0144) \nand \nsummary \n(estimate=-2.96, \nSE=0.9713, p=0.0023) tasks had significantly lower task \ncompletion rates. The Tukey Post-hoc test confirmed that task \ncompletion rates for debugging (p=0.0293), testing (p=0.009) \nand summary (p=0.0300) were significantly lower than for \ncoding, while the difference for documentation was not \nsignificant after Tukey adjustment (p=0.1403). Since all \nbrainstorming tasks were completed successfully, there was a \ncomplete separation in the data, leading to model instabilities. \nNevertheless, we could observe that the task category \nsignificantly influenced task completion rates, with coding and \nbrainstorming tasks showing the highest task completion \nrates (Figure 4). \nIn sum, task completion was influenced by both the type of \nCopilot usage and the task category, with using only chat-\nbased prompts and working on coding or brainstorming tasks \neach being associated with higher success rates in this study. \n \nFigure 4: The percentage of tasks completed (or not) via a \nspecific Copilot interaction type is depicted (stacked \nbars). The percentage of task categories completed vs. \npartly completed/not completed by the participants is \npresented above and below the bars. Asterisks denote \nstatistical significance (* p\u22640.05; ** p\u22640.01). \n5.3 Perceived workload \nA linear mixed-effects model was fit to predict participants\u2019 \nperceived workload whilst working on the tasks: \nlog(raw NASA-TLX + 1) ~ interaction type + task categories + \n(1|participants) + (1|tasks) \n \n5.3.1 Impact of interaction type on perceived workload. \nInteraction types significantly impact the prediction of \nperceived \nworkload \n(\u03c7\u00b2(3)=32.02, \nAICfull_model=403.01, \nAICnull_model=429.03, p=padjusted<0.001). Compared to not using \nCopilot, using only chat prompts (estimate=\u20130.41, SE=0.1045, \np<0.001) and only in-code suggestions (estimate=\u20130.60, \nSE=0.1220, p<0.001) were both associated with significantly \nlower reported workload. The combined use showed no \nsignificant difference from not using Copilot (estimate=\u20130.13,"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_017",
    "source_id": "DevExperienceGenAI2025",
    "text": " perceived workload. \nInteraction types significantly impact the prediction of \nperceived \nworkload \n(\u03c7\u00b2(3)=32.02, \nAICfull_model=403.01, \nAICnull_model=429.03, p=padjusted<0.001). Compared to not using \nCopilot, using only chat prompts (estimate=\u20130.41, SE=0.1045, \np<0.001) and only in-code suggestions (estimate=\u20130.60, \nSE=0.1220, p<0.001) were both associated with significantly \nlower reported workload. The combined use showed no \nsignificant difference from not using Copilot (estimate=\u20130.13, \nSE=0.1376, p=0.3338). Post-hoc pairwise comparisons using \nTukey adjustment confirmed that both chat prompts \n(p<0.001) and in-code suggestions (p<0.001) conditions \nresulted in lower perceived workload ratings than without \nusing GenAI. Additionally, only using in-code suggestions \nresulted in significantly lower perceived workload compared \nto the combined GenAI conditions (p=0.0064) (Figure 5). No \nother pairwise comparisons were significant.  \nIn sum, both in-code suggestions and chat prompts lowered \nperceived workload compared to not using GenAI. However, \nusing both together led to a higher workload than using in-\ncode suggestions alone, suggesting that switching between \ninteraction types may increase workload compared to \nfocusing on a single interaction type. \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \n \nFigure \n5: The raw NASA-TLX scores and model \npredictions of perceived workload grouped by Copilot \ninteraction type are depicted.  Asterisks denote statistical \nsignificance (** p\u22640.01; *** p\u22640.001). \n \n5.3.2 Impact of task category on perceived workload. Task \ncategories significantly improved the prediction of perceived \nworkload (\u03c7\u00b2(5)=11.3, AICfull_model=403.01, AICnull_model=404.31, \np=padjusted=0.0458), though no individual categories differed \nsignificantly \nfrom \ncoding. \nComparability \nbetween \nthe \ncognitive load of the tasks was intended in the study design by \nchoosing tasks from the HumanEval-X dataset and selecting \ntasks based on cognitive load code metrics. These results \nvalidate the selection process of the tasks. \nOverall, using either in-code suggestions or chat prompts \nlowered perceived workload, whilst combining both stayed \ncomparable to not using GenAI or increased it compared to in-\ncode suggestions. Task categories influenced the overall \nperceived workload, but had no significant individual effects. \n5.3.3 Perceived cognitive load and productivity during the \nuncontrolled setting. Out of 445 documented working tasks, \n120 (27.42%) involved AI interaction. Of the 22 participants, \n12 found AI helpful every time they used it, one never found it \nhelpful, one did not use AI in the uncontrolled setting, and the  \nremaining eight participants found AI helpful only sometimes. \nComparing tasks worked on with vs. without AI interaction, \ncognitive load ratings differed significantly, with higher \nratings for tasks with"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_018",
    "source_id": "DevExperienceGenAI2025",
    "text": " stayed \ncomparable to not using GenAI or increased it compared to in-\ncode suggestions. Task categories influenced the overall \nperceived workload, but had no significant individual effects. \n5.3.3 Perceived cognitive load and productivity during the \nuncontrolled setting. Out of 445 documented working tasks, \n120 (27.42%) involved AI interaction. Of the 22 participants, \n12 found AI helpful every time they used it, one never found it \nhelpful, one did not use AI in the uncontrolled setting, and the  \nremaining eight participants found AI helpful only sometimes. \nComparing tasks worked on with vs. without AI interaction, \ncognitive load ratings differed significantly, with higher \nratings for tasks with AI interaction compared to without \n(W=0, p<0.0001, r=-0.8760). Productivity ratings also differed \nsignificantly and were higher when interacting with AI \ncompared to without (W=26, p=0.002, r=-0.6788). Within the \nsubgroup of eight participants who had mixed evaluations of \nAI helpfulness during their tasks, neither cognitive load nor \nproductivity ratings differed significantly between tasks rated \nas helpful vs. not helpful (cognitive load: W=7, p=0.2367, r=-\n0.8181; productivity: W=10, p=0.499, r=-0.7930). \n6 Discussion \nCopilot interaction vs. no interaction. This study found that \nusing one interaction type, i.e. either in-code suggestions or \nchat, can increase efficiency (shorter task duration) and \nreduce perceived workload (NASA-TLX score) compared to \nnot using Copilot. These findings align with Peng et al.\u2018s [37], \nwho also found that task completion was faster with Copilot \nthan without it. A notable result of this study is that combining \nboth interaction types does not yield benefits in task duration \nor perceived workload. Switching between interaction types \nmay indicate that the initially chosen interaction type was \nsuboptimal to solve a given problem. Such switches may \nresemble small interruptions that involve a shift in the \nproblem-solving approach and of attention, which can be \nassociated with increased workload [26, 27].  \nDuring the study days, when participants documented their \ntasks and AI use in an uncontrolled work setting, they \nreported a higher cognitive load during tasks involving AI \ninteraction than during non-AI tasks; yet, they also felt more \nproductive when using AI.  Among participants with mixed \nviews on AI helpfulness, experienced cognitive load and \nproductivity did not differ between tasks where AI was \nperceived as helpful vs not helpful. Comments from the \nworkday documentation and the results from the controlled \nsessions suggest that the elevated cognitive load during AI-\nrelated tasks stems from switching between coding-related \ntasks (e.g., while working on code in an everyday setting, such \nas coding, debugging, and testing which alternate naturally) \nand different interaction types with AI. Tang et al. [44] \nsimilarly found in their study that when developers knew that \ncode was generated with AI, cognitive load was perceived \nhigher. They observed frequent switching between source \ncode and comments during validation and repair of LLM-\ngenerated code. Their results strengthen our interpretation \nthat higher cognitive load during AI-related tasks"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_019",
    "source_id": "DevExperienceGenAI2025",
    "text": " between tasks where AI was \nperceived as helpful vs not helpful. Comments from the \nworkday documentation and the results from the controlled \nsessions suggest that the elevated cognitive load during AI-\nrelated tasks stems from switching between coding-related \ntasks (e.g., while working on code in an everyday setting, such \nas coding, debugging, and testing which alternate naturally) \nand different interaction types with AI. Tang et al. [44] \nsimilarly found in their study that when developers knew that \ncode was generated with AI, cognitive load was perceived \nhigher. They observed frequent switching between source \ncode and comments during validation and repair of LLM-\ngenerated code. Their results strengthen our interpretation \nthat higher cognitive load during AI-related tasks may be \nhigher due to increased context switching. Further detailed \nanalyses on the relation between working tasks and AI use are \ngoing to be addressed in future studies. \nIn-code suggestions vs. chat. The degree to which GenAI use \nhas beneficial efficiency effects depends on the interaction \nintensity, i.e. the amount of GenAI use. High use of in-code \nsuggestions can still lead to efficiency gains, whereas only a \nlow number of interactions with chat or a combination of both \ninteraction types proves more efficient than not using Copilot. \nAs could be observed in the screen recordings, in-code \nsuggestions are used in a trial-and-error fashion, leading to a \nhigher volume of generated and discarded code. In contrast, \nthe interaction with chat requires more high-level thought to \nformulate the prompt, explaining the lower number of chat \ninteractions for efficiency gains. If a prompt is formulated in a \ncomprehensive way for the model to generate the desired \noutput, fewer interactions are needed to receive a result. Both \ninteraction types individually increase efficiency up to a \ncertain number of interactions compared to not using Copilot \nModerate use of either in-code or chat suggestions \nimproves efficiency and reduces workload, while \nexcessive or combined use lessens these benefits. Only \nusing chat boosts task completion in this study. Task type \naffects efficiency and task completion, with coding and \nbrainstorming tasks having higher success in this study. \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \nwhilst the combination of both interaction types during a task \nlessens the benefits. The study results also indicate that \nperceived workload is lowest for in-code suggestions, which \ncan be explained by the code suggestions appearing directly at \nthe cursor position. The benefit of chat for task completion \n(accuracy) may be explained by the interaction type \nconsidering more code context when giving an output.  \nTask categories impact perceived workload in general \nand differ in their impact on efficiency and accuracy. For \nthe controlled sessions, code-related tasks were selected from \nthe HumanEval-X dataset based on code metrics to keep the \ntasks across task categories comparable. Task categories did \nnot differ in perceived workload, which validates the selection \nprocess of the tasks based on code metrics associated with \ncognitive load. Nevertheless, they have an impact on \nperceived workload. Results further indicated that more time \nwas required to complete debugging tasks, and coding and \nbrainstorming tasks had higher successful completion rates. \nTask categories, therefore, in general impact perceived \nworkload and differ in their impact on efficiency and"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_020",
    "source_id": "DevExperienceGenAI2025",
    "text": " more code context when giving an output.  \nTask categories impact perceived workload in general \nand differ in their impact on efficiency and accuracy. For \nthe controlled sessions, code-related tasks were selected from \nthe HumanEval-X dataset based on code metrics to keep the \ntasks across task categories comparable. Task categories did \nnot differ in perceived workload, which validates the selection \nprocess of the tasks based on code metrics associated with \ncognitive load. Nevertheless, they have an impact on \nperceived workload. Results further indicated that more time \nwas required to complete debugging tasks, and coding and \nbrainstorming tasks had higher successful completion rates. \nTask categories, therefore, in general impact perceived \nworkload and differ in their impact on efficiency and accuracy. \n7 Considerations, Limitations and Outlook \nThe question may arise how transferable these study results \nand their interpretation are for GenAI interaction in general \nwith other tools. Whilst interaction behavior and experience \nare tool-specific, GenAI coding assistants either have or are \nmoving towards including both in-code suggestions and a \nnatural language or chat interface. This study has shown that \nthe way of interacting with coding assistants, as in the \ninteraction types used, can impact developers\u2019 experience and \nproductivity. We therefore expect transferability of our \nresults, but more studies in this area are needed for \nverification. \nThe aim of the study was to capture developers\u2019 interaction \nwith GenAI as realistically as possible. While a natural work \nenvironment increases realism, one cannot control for all \nconfounding factors. By including both controlled and \nuncontrolled periods, along with questions regarding mood, \nwork environment and unusual occurrences, we accounted for \nsome of these factors to contextualize the results. Adding to \nthe realistic setting, participants were proficient and \nprofessional Java developers with varying levels of GenAI \nexperience. Future studies could verify these study results \nwith a different developer cohort regarding programming \nlanguage, programming experience, and GenAI experience. \nThe controlled sessions involved simpler tasks than \ndevelopers\u2019 daily software engineering tasks. Simplifications \nwere made to manage time constraints of the study, ensure \nconsistent task length and workload across tasks, whilst also \nconsidering a variety of task types. During the tasks, the \nparticipants used a laptop with a German (QWERTZ) \nkeyboard, unfamiliar to the US-based developers. Participants \nwere, however, informed about the layout differences and \nwere given an introduction to the altered key placements. All \ntasks and each interaction type included typing; therefore, any \nadditional time, cognitive effort or frustration would affect all \nconditions similarly. No task or interaction type bias is \nexpected. Task randomization and aggregated analyses across \nparticipants further minimize keyboard-related effects. \nThe NASA-TLX performance subscale is inversely scaled \ncompared to the others, which some participants only noticed \nafter completing several tasks. They reported the issue and \nthe scores were manually adapted after the study. To maintain \nconsistency with the standard NASA-TLX format, the \nquestionnaire structure was not modified in advance. \nFuture work is going to analyze developer\u2013GenAI interaction \nduring the uncontrolled period in greater detail and examine \ndevelopers\u2019 cognitive load using physiological, keyboard, and \nmouse data. A similar study setup could be used to study \ndevelopers\u2019 interaction with different and/or multiple code \ngeneration tools and large language models to understand the \nopport"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_021",
    "source_id": "DevExperienceGenAI2025",
    "text": " across \nparticipants further minimize keyboard-related effects. \nThe NASA-TLX performance subscale is inversely scaled \ncompared to the others, which some participants only noticed \nafter completing several tasks. They reported the issue and \nthe scores were manually adapted after the study. To maintain \nconsistency with the standard NASA-TLX format, the \nquestionnaire structure was not modified in advance. \nFuture work is going to analyze developer\u2013GenAI interaction \nduring the uncontrolled period in greater detail and examine \ndevelopers\u2019 cognitive load using physiological, keyboard, and \nmouse data. A similar study setup could be used to study \ndevelopers\u2019 interaction with different and/or multiple code \ngeneration tools and large language models to understand the \nopportunities and risks of GenAI for cognitive load in software \ndevelopment. Risks include additional interruptions and \ngenerated code that is difficult to review and maintain [12],  \nimpacting developer-AI interaction.  \n8 Conclusion \nIn this study, we examined professional developers\u2019 \nexperience during their interaction with GenAI in their \neveryday work setting. The focus of this paper was to analyze \nthe controlled sessions of the study and consider the \nefficiency, accuracy and perceived workload, as well as the \nimpact of different interaction types with Copilot. We found \nthat both interaction types used individually positively affect \nefficiency and perceived workload compared to not using \nCopilot. Chat use also improved task accuracy, compared to no \nCopilot use. However, combining both interaction types did \nnot show additional benefits, which is likely due to the \nincreased workload resulting from interaction and context \nswitching. The findings highlight the need for a more nuanced \nunderstanding of how developers interact with GenAI. Mixed-\nmethods study setups like the one described in this paper \nenable a holistic analysis of developers\u2019 interaction experience \nwith GenAI tools within an everyday work environment. In \nfuture, similar study setups that combine subjective and \nobjective data sources could be deployed within firms for \ndetailed evaluation of GenAI tools and the experienced \ninteraction of developers with them.  \nSUPPLEMENTAL MATERIAL \nDetails on methods and analyses are made available [5].  \nACKNOWLEDGEMENTS \nThe work of Charlotte Brandebusemeyer is funded by the \nSAP-HPI Research Program. \nDevelopers\u2019 Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nREFERENCES \n[1] Gal Bakal, Ali Dasdan, Yaniv Katz, Michael Kaufman, and Guy Levin. 2025. \nExperience with GitHub Copilot for Developer Productivity at Zoominfo. \nArXiv (January 2025). https://doi.org/10.48550/arXiv.2501.13282 \n[2] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded \nCopilot: How Programmers Interact with Code-Generating Models. \nProceedings of the ACM on Programming Languages 7, OOPSLA1 (April \n2023), 85\u2013111. https://doi.org/10.1145/3586030 \n[3] Alexander Barker. 2022. JNativeHook: Global Keyboard and Mouse \nListener \nfor \nJava. \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://github.com/kwhat/jnativehook"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_022",
    "source_id": "DevExperienceGenAI2025",
    "text": "doi.org/10.48550/arXiv.2501.13282 \n[2] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded \nCopilot: How Programmers Interact with Code-Generating Models. \nProceedings of the ACM on Programming Languages 7, OOPSLA1 (April \n2023), 85\u2013111. https://doi.org/10.1145/3586030 \n[3] Alexander Barker. 2022. JNativeHook: Global Keyboard and Mouse \nListener \nfor \nJava. \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://github.com/kwhat/jnativehook \n[4] Marc G. Berman, John Jonides, and Stephen Kaplan. 2008. The Cognitive \nBenefits of Interacting With Nature. Psychol Sci 19, 12 (December 2008), \n1207\u20131212. https://doi.org/10.1111/j.1467-9280.2008.02225.x \n[5] Charlotte Brandebusemeyer, Tobias Schimmer, and Bert Arnrich. 2025. \nSupplemental Material for Developers\u2019 Experience with Generative AI - \nFirst Insights from an Empirical Mixed-Methods Field Study. Zenodo. \nhttps://doi.org/10.5281/zenodo.17818081 \n[6] Charlotte Brandebusemeyer, Tobias Schimmer, and Bert Arnrich. 2025. \nWearables to Measure Developer Experience at Work. In 2025 IEEE/ACM \n47th \nInternational \nConference \non \nSoftware \nEngineering: \nSoftware \nEngineering in Practice (ICSE-SEIP), April 27, 2025. IEEE, 23\u201333. \nhttps://doi.org/10.1109/ICSE-SEIP66354.2025.00008 \n[7] G. Ann Campbell. 2023. {Cognitive Complexity} a new way of measuring \nunderstandability. SonarSource S.A. \n[8] Mariana Coutinho, Lorena Marques, Anderson Santos, Marcio Dahia, Cesar \nFran\u00e7a, and Ronnie de Souza Santos. 2024. The Role of Generative AI in \nSoftware Development Productivity: A Pilot Case Study. In Proceedings of \nthe 1st ACM International Conference on AI-Powered Software (AIware \n2024), \nJuly \n10, \n2024. \nACM, \nNew \nYork, \nNY, \nUSA, \n131\u2013138. \nhttps://doi.org/10.1145/3664646.3664773 \n[9] Empatica Inc. 2025. EmbracePlus. Retrieved July 29, 2025 from \nhttps://www.empatica.com/en-eu/embraceplus/ \n[10] Stefan Feuerriegel, Jochen Hartmann, Christian Janiesch, and Patrick \nZschech. 2024. Generative AI. Business & Information Systems Engineering \n66, 1 (February 2024), 111\u2013126. https://doi.org/10.1007/s12599-023-\n00834-7 \n[11] Nicole Forsgren, Eirini Kalliamvakou, Abi"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_023",
    "source_id": "DevExperienceGenAI2025",
    "text": "://doi.org/10.1145/3664646.3664773 \n[9] Empatica Inc. 2025. EmbracePlus. Retrieved July 29, 2025 from \nhttps://www.empatica.com/en-eu/embraceplus/ \n[10] Stefan Feuerriegel, Jochen Hartmann, Christian Janiesch, and Patrick \nZschech. 2024. Generative AI. Business & Information Systems Engineering \n66, 1 (February 2024), 111\u2013126. https://doi.org/10.1007/s12599-023-\n00834-7 \n[11] Nicole Forsgren, Eirini Kalliamvakou, Abi Noda, Michaela Greiler, Brian \nHouck, and Margaret Anne Storey. 2023. DevEx in Action: A study of its \ntangible \nimpacts. \nQueue \n21, \n6 \n(December \n2023), \n47\u201377. \nhttps://doi.org/10.1145/3639443 \n[12] Nicole Forsgren and Abi Noda. 2025. Frictionless: 7 Steps to Remove \nBarriers, Unlock Value, and Outpace Your Competition in the AI Era. Shift \nKey Press. \n[13] Nicole Forsgren, Margaret-Anne Storey, Chandra Maddila, Thomas \nZimmermann, Brian Houck, and Jenna Butler. 2021. The SPACE of \nDeveloper Productivity: There\u2019s more to it than you think. Queue 19, 1 \n(February 2021), 20\u201348. https://doi.org/10.1145/3454122.3454124 \n[14] Free HD videos - no copyright. 2019. [4K] Autumn Leaves | Drone Aerial \nView | Free Stock Footage | Free HD Videos - No Copyright. Retrieved July \n29, \n2025 \nfrom \nhttps://www.youtube.com/watch?v=o-\n7fsuJtEhk&list=PL4Gr5tOAPttKUXrXjulSCYa-L4xIwDyTi&index=13 \n[15] Free HD videos - no copyright. 2019. [4K] Beauty Of Nature | Drone Aerial \nView | Free stock footage | Free HD Videos - No Copyright. Retrieved July \n29, \n2025 \nfrom \nhttps://www.youtube.com/watch?v=RK1RRVR9A2g&t=934s \n[16] Free HD videos - no copyright. 2020. Sunset Stock Footage | Royalty Free | \nFree HD Videos - no copyright. Retrieved July 29, 2025 from \nhttps://www.youtube.com/watch?v=TGLV2Fw-k5I&t=527s \n[17] Thomas Fritz, Andrew Begel, Sebastian C. M\u00fcller, Serap Yigit-Elliott, and \nManuela Z\u00fcger. 2014. Using psycho-physiological measures to assess task \ndifficulty in software development. In Proceedings - International \nConference on Software Engineering, May 31, 2014. IEEE Computer Society, \n402\u2013413. https://doi.org/10.1145/2568225.2568266 \n[18] Lewis R Goldberg. 1992. The development"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_024",
    "source_id": "DevExperienceGenAI2025",
    "text": "alty Free | \nFree HD Videos - no copyright. Retrieved July 29, 2025 from \nhttps://www.youtube.com/watch?v=TGLV2Fw-k5I&t=527s \n[17] Thomas Fritz, Andrew Begel, Sebastian C. M\u00fcller, Serap Yigit-Elliott, and \nManuela Z\u00fcger. 2014. Using psycho-physiological measures to assess task \ndifficulty in software development. In Proceedings - International \nConference on Software Engineering, May 31, 2014. IEEE Computer Society, \n402\u2013413. https://doi.org/10.1145/2568225.2568266 \n[18] Lewis R Goldberg. 1992. The development of markers for the Big-Five \nfactor structure. Psychol Assess 4, 1 (1992), 26\u201342. \n[19] Lewis R Goldberg. 2024. International Personality Item Pool: A Scientific \nCollaboratory for the Development of Advanced Measures of Personality \nTraits and Other Individual Differences. Retrieved July 29, 2025 from \nhttps://ipip.ori.org/new_ipip-50-item-scale.htm \n[20] Maurice H. Halstead. 1977. Elements of Software Science. Elsevier. \n[21] Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX); 20 Years Later. \nProceedings of the Human Factors and Ergonomics Society Annual Meeting \n50, \n9 \n(October \n2006), \n904\u2013908. \nhttps://doi.org/10.1177/154193120605000909 \n[22] Headfulness - Luke Horton. 2024. Calming 4-7-8 Breathing (5 Minutes). \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://www.youtube.com/watch?v=DAp3aiC57ZQ&t=17s \n[23] Haritha Khandabattu. 2025. The 2025 Hype Cycle for Artificial Intelligence \nGoes Beyond GenAI. Gartner. Retrieved September 13, 2025 from \nhttps://www.gartner.com/en/articles/hype-cycle-for-artificial-\nintelligence \n[24] Mansi Khemka and Brian Houck. 2024. Toward Effective AI Support for \nDevelopers. \nCommun \nACM \n67, \n11 \n(November \n2024), \n42\u201349. \nhttps://doi.org/10.1145/3690928 \n[25] Bas Leijdekkers. 2021. MetricsReloaded. Retrieved July 29, 2025 from \nhttps://github.com/BasLeijdekkers/MetricsReloaded \n[26] Gloria Mark, Daniela Gudith, and Ulrich Klocke. 2008. The cost of \ninterrupted work: more speed and stress. In Proceedings of the SIGCHI \nConference on Human Factors in Computing Systems, April 06, 2008. ACM, \nNew York, NY, USA, 107\u2013110. https://doi.org/10.1145/1357054.1357072 \n[27] Gloria Mark, Shamsi Iqbal, Mary Czerwinski, and Paul Johns. 2015. Focused,"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_025",
    "source_id": "DevExperienceGenAI2025",
    "text": ". Retrieved July 29, 2025 from \nhttps://github.com/BasLeijdekkers/MetricsReloaded \n[26] Gloria Mark, Daniela Gudith, and Ulrich Klocke. 2008. The cost of \ninterrupted work: more speed and stress. In Proceedings of the SIGCHI \nConference on Human Factors in Computing Systems, April 06, 2008. ACM, \nNew York, NY, USA, 107\u2013110. https://doi.org/10.1145/1357054.1357072 \n[27] Gloria Mark, Shamsi Iqbal, Mary Czerwinski, and Paul Johns. 2015. Focused, \nAroused, but so Distractible: Temporal Perspectives on Multitasking and \nCommunications. In Proceedings of the 18th ACM Conference on Computer \nSupported Cooperative Work & Social Computing, February 28, 2015. ACM, \nNew York, NY, USA, 903\u2013916. https://doi.org/10.1145/2675133.2675221 \n[28] T.J. McCabe. 1976. A Complexity Measure. IEEE Transactions on Software \nEngineering \nSE-2, \n4 \n(December \n1976), \n308\u2013320. \nhttps://doi.org/10.1109/TSE.1976.233837 \n[29] LLC Meltytech. 2025. Shotcut Video Editor. Retrieved July 29, 2025 from \nhttps://www.shotcut.org/ \n[30] Microsoft and LinkedIn. 2024. 2024 Work Trend Index Annual Report - AI at \nWork is Here. Now Comes the Hard Part. Retrieved July 29, 2025 from \nhttps://www.microsoft.com/en-us/worklab/work-trend-index/ai-at-\nwork-is-here-now-comes-the-hard-part \n[31] Giovanni B. Moneta. 2021. On the conceptualization and measurement of \nflow. In Advances in Flow Research. Springer Science, New York, 31\u201369. \nhttps://doi.org/10.1007/978-3-030-53468-4_2 \n[32] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse \nKhomh, Michel C. Desmarais, and Zhen Ming (Jack) Jiang. 2023. GitHub \nCopilot AI pair programmer: Asset or Liability? Journal of Systems and \nSoftware \n203, \n(September \n2023). \nhttps://doi.org/10.1016/j.jss.2023.111734 \n[33] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2024. \nReading Between the Lines: Modeling User Behavior and Costs in AI-\nAssisted Programming. In Proceedings of the CHI Conference on Human \nFactors in Computing Systems (CHI \u201924), May 11, 2024. ACM, Honolulu, HI, \nUSA, 1\u201316. https://doi.org/10.1145/3613904.3641936 \n[34] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub \ncopilot\u2019s code suggestions. In"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_026",
    "source_id": "DevExperienceGenAI2025",
    "text": "doi.org/10.1016/j.jss.2023.111734 \n[33] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2024. \nReading Between the Lines: Modeling User Behavior and Costs in AI-\nAssisted Programming. In Proceedings of the CHI Conference on Human \nFactors in Computing Systems (CHI \u201924), May 11, 2024. ACM, Honolulu, HI, \nUSA, 1\u201316. https://doi.org/10.1145/3613904.3641936 \n[34] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub \ncopilot\u2019s code suggestions. In Proceedings of the 19th International \nConference on Mining Software Repositories (MSR \u201922), May 23, 2022. ACM, \nPittsburgh, PA, USA, 1\u20135. https://doi.org/10.1145/3524842.3528470 \n[35] OBS Project. 2025. OBS - Open Broadcaster Software. Retrieved July 29, \n2025 from https://obsproject.com/ \n[36] Norman Peitek, Annabelle Bergum, Maurice Rekrut, Jonas Mucke, Matthias \nNadig, Chris Parnin, Janet Siegmund, and Sven Apel. 2022. Correlates of \nprogrammer efficacy and their link to experience: a combined EEG and \neye-tracking study. In Proceedings of the 30th ACM Joint European Software \nEngineering Conference and Symposium on the Foundations of Software \nEngineering, November 07, 2022. ACM, New York, NY, USA, 120\u2013131. \nhttps://doi.org/10.1145/3540250.3549084 \n[37] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The \nImpact of AI on Developer Productivity: Evidence from GitHub Copilot. \nArXiv (February 2023). https://doi.org/10.48550/arXiv.2302.06590 \n[38] Paige S Rutner, Bill C Hardgrave, and D Harrison Mcknight. 2008. \nEmotional Dissonance and the Information Technology Professional. MIS \nQuarterly \n32, \n3 \n(September \n2008), \n635\u2013652. \nhttps://doi.org/10.2307/25148859 \n[39] Safari Realms. 2024. Waterfalls | Nature | Relaxation | Free HD Videos - No \nCopyright \nFootages. \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://www.youtube.com/watch?v=t3LA4Zx7p_I \n[40] Advait Sarkar, Andrew D. Gordon, Carina Negreanu, Christian Poelitz, Sruti \nSrinivasa Ragavan, and Ben Zorn. 2022. What is it like to program with \nartificial \nintelligence? \nArXiv \n(2022), \n1\u201327. \nhttps://doi.org/10.48550/arXiv.2208.06213 \n[41] Margaret-Anne Storey, T Zimmermann, C Bird, J"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_027",
    "source_id": "DevExperienceGenAI2025",
    "text": " Free HD Videos - No \nCopyright \nFootages. \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://www.youtube.com/watch?v=t3LA4Zx7p_I \n[40] Advait Sarkar, Andrew D. Gordon, Carina Negreanu, Christian Poelitz, Sruti \nSrinivasa Ragavan, and Ben Zorn. 2022. What is it like to program with \nartificial \nintelligence? \nArXiv \n(2022), \n1\u201327. \nhttps://doi.org/10.48550/arXiv.2208.06213 \n[41] Margaret-Anne Storey, T Zimmermann, C Bird, J Czerwonka, B Murphy, and \nE Kalliamvakou. 2019. Supplemental material for towards a theory of \nsoftware developer job satisfaction and perceived productivity. Zenodo. \nRetrieved from https://zenodo.org/records/3451354#.XYUr-OdKjOQ \n[42] Viktoria Stray, Nils Brede Moe, Nivethika Ganeshan, and Simon Kobbenes. \n2025. Generative AI and Developer Workflows: How GitHub Copilot and \nChatGPT Influence Solo and Pair Programming. In Proceedings of the 58th \nICSE SEIP 2026, Rio de Janeiro, Brazil \nBrandebusemeyer et al. \n \n \n \nHawaii International Conference on System Sciences (HICSS \u201925), January 07, \n2025. 7381\u20137390. https://doi.org/10.24251/HICSS.2025.883 \n[43] Laura Tacho. 2024. Introducing Core 4: The best way to measure and \nimprove \nyour \nproduct \nvelocity. \nRetrieved \nJuly \n29, \n2025 \nfrom \nhttps://www.lennysnewsletter.com/p/introducing-core-4-the-best-way-\nto \n[44] Ningzhi Tang, Meng Chen, Zheng Ning, Aakash Bansal, Yu Huang, Collin \nMcMillan, and Toby Jia-Jun Li. 2024. Developer Behaviors in Validating and \nRepairing LLM-Generated Code Using IDE and Eye Tracking. In 2024 IEEE \nSymposium on Visual Languages and Human-Centric Computing (VL/HCC), \nSeptember \n02, \n2024. \nIEEE, \n40\u201346. \nhttps://doi.org/10.1109/VL/HCC60511.2024.00015 \n[45] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI \u201922 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1\u20137. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot\u2019s code generation."
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_028",
    "source_id": "DevExperienceGenAI2025",
    "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI \u201922 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1\u20137. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot\u2019s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE \u201922), November 07, 2022. ACM, Singapore, Singapore, 62\u201371. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yeti\u015ftiren, I\u015f\u0131k \u00d6zsoy, Miray Ayerdem, and Eray T\u00fcz\u00fcn. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n\u201923), August 06, 2023. ACM, Long Beach, CA, USA, 5673\u20135684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21\u201329. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
  },
  {
    "chunk_id": "DevExperienceGenAI2025_chunk_029",
    "source_id": "DevExperienceGenAI2025",
    "text": "PLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21\u201329. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website:"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_002",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n2\n \ncode completion capabilities [3].The acceptance of AI-assisted coding tools is getting faster, and it is \nrevealed by the latest industry surveys, which show that more than 65% of professional developers use AI \nsupport in some form as part of their daily routine [4]. These tools have been incorporated into the \ndevelopment processes of large tech companies that account for 30-50% productivity gains and have also \nreported significant time-to-market reductions for software products [5]. Nevertheless, the fast adoption \nof these tools has been so extensive that even empirical studies have not been able to catch up with their \nimplications on critical software engineering outcomes, like code quality, security, and long-term \nmaintainability, through rigorous research [6]. \nThe research gap is even more pronounced when the risk factors of AI-generated code are taken into \naccount. Initial experiments have pointed out issues such as security flaws, licensing confusion, and the \noccurrence of hidden bugs that will not be discovered during code reviewing process [7, 8]. Moreover, the \nimpact on the development of programmers' skills, particularly that of junior developers, who would \notherwise depend on AI-generated suggestions, is still unclear [9]. There being no empirical studies taking \na comprehensive approach to the assessment of these issues, the bottleneck of knowledge in software \nengineering research is in fact the multifaceted impacts of these issues. \nThis research paper fills this void by performing a large-scale controlled experiment whose main objective \nis to evaluate in a systematic way the impact of AI-assisted code generator tools on the three main \ndimensions: code quality, security vulnerability introduction, and developer productivity. Previous studies \nhave limited themselves to specific scenarios or synthetic benchmarks. On the contrary, our study will \ninvolve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_003",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "olve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious programming languages and projects. Finally, the research gives the software organizations that \nwant to use AI tools good insights and practices for risk reduction. Thus, the implications of our results \nare very important for the education of software engineers, the industry's practices and the direction of \nfuture research in the area of AI and software development. \n \n2. Literature Review \nThe areas where artificial intelligence and software engineering meet have become the center of a huge \nnumber of research studies, with code generation and program synthesis being the two main areas. The \nfirst automated code generation attempts relied on template-based methods and rule-based systems \nproducing the so-called boilerplate codes from high-level specifications [10]. The deep learning era totally \nchanged the picture, with the application of sequence-to-sequence models and recurrent neural networks \nto the code synthesis tasks [11]. The introduction of the transformer architectures was the turning point, \nwith models such as CodeBERT and GraphCodeBERT being able to perform on a par with the best \nmethods in the problem categories of code understanding and generation [12, 13]. \nRecently, the development of large language models has further changed the scenery in code generation. \nFor instance, GPT-3 showed outstanding learning ability through a few examples for programming tasks \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n3\n \n[14], while Codex, the engine behind GitHub Copilot, scored highly in competitive programming \nproblems [15]. The following studies have looked into the models' capabilities in various programming \nlanguages and difficult algorithms [16, 17]. \nThe findings of empirical studies of AI-assisted coding tools presented a somewhat mixed picture. Ziegler \net al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_004",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming languages used [21]. Nguyen and colleagues put forward a new set of metrics for \nmeasuring AI-generated code and reported that, although AI-powered software is very good at producing \nsyntactically correct code, it often strays from the best design patterns [22]. \nAmong the critical issues raised security implications have been the foremost. Static analysis studies \nshowed that AI systems might learn from their training data and thus recreate the same weaknesses as in \nthe case of existing vulnerabilities [23]. Pearce and co-workers pointed out the presence of a vulnerable \npattern that was quite common in GitHub Copilot's code which included SQL injection, cross-site \nscripting, and use of insecure cryptographic methods [24]. \nNevertheless, there are still some limitations that are inherent to such a growing research area. The \nmajority of research works have been limited to the evaluation of just a few tools and or small domains \nmaking the possibility of drawing general conclusions quite difficult. Only a couple of research works \nhave dealt with long-term effects of refactoring, skilled programmers, and maintenance costs. Moreover, \nthe interaction between the experience level of the developer and the effectiveness of the AI tool is still to \nbe fully explored [25]. Through a thorough and multi-language study involving developers of different \nexperience levels working on varied software engineering tasks, we fill these gaps. \n \n3. Methodology \n3.1 Research Design \nThis research utilized a between-subjects experimental design to assess the influences of AI-assisted code \ngeneration tools on software development outcomes. The independent variable was the presence of AI-\nassisted coding tools (experimental group vs. control group), while the dependent variables were the \nquality metrics, security vulnerability counts, and productivity measures of the coded produced. The \nexperiment was set up in such a way that it reduced the possibility of confounding variables while keeping \nthe ecological validity through the use of realistic programming tasks that are typical of professional \nsoftware development scenarios. Figure 1 shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_005",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2) data structure implementations (e.g., \ncustom hash tables, tree structures), (3) API integration tasks (e.g., REST API client implementations), \nand (4) complex algorithmic challenges (e.g., graph algorithms, optimization problems). Each task \nincluded detailed specifications, input/output examples, and acceptance criteria. Tasks were designed to \nbe completable within 45-60 minutes and were piloted with 10 developers not included in the main study \nto validate difficulty and time requirements. Table 1 displays Task characteristics by various frequently \nused programming languages. \n \nTable 1: Task Characteristics by Programming Language \nLanguage Simple \nData Structure API Integration Complex Algorithm \nPython \nTask P1 \nTask P2 \nTask P3 \nTask P4 \nJava \nTask J1 \nTask J2 \nTask J3 \nTask J4 \nJavaScript Task JS1 Task JS2 \nTask JS3 \nTask JS4 \nC++ \nTask C1 \nTask C2 \nTask C3 \nTask C4 \n \n3.3 Tools and Environment \nThe experimental group used GitHub Copilot built into Visual Studio Code as the AI-assisted coding tool, \nrepresenting the most widely used commercial product at that time. The control group utilized Visual \nStudio Code with the same extensions and settings as the experimental group, without any AI-assisted \ncapabilities. Both groups had access to standard, commonly used development resources: documentation, \nStack Overflow, and search engines, in order to replicate a realistic development environment. All coding \ntasks took place in a controlled laboratory, with controlled hardware (16GB RAM with Intel i7 processors) \nto minimize the impact of environmental variability. \n \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n5\n \n3.4 Data Collection \nThroughout the experiment, we collected multiple data streams to comprehensively assess the code \nquality, security, productivity and behavioral patterns. We utilized language-specific static analysis tools \nto analyze all submitted code: SonarQube analyzed the code for Python and JavaScript, PMD analyzed \nthe code for Java, and Cppcheck analyzed C++ code. We analyzed cyclomatic complexity (average per \nfunction, maximum per function), maintainability index (0-100 scale), density of code smells (per 100 \nlines of code), % of code duplication, documentation coverage. We identified security vulnerabilities with \nSnyk Static Application Security Testing (SAST) in all"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_006",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "6, November-December 2025 \n5\n \n3.4 Data Collection \nThroughout the experiment, we collected multiple data streams to comprehensively assess the code \nquality, security, productivity and behavioral patterns. We utilized language-specific static analysis tools \nto analyze all submitted code: SonarQube analyzed the code for Python and JavaScript, PMD analyzed \nthe code for Java, and Cppcheck analyzed C++ code. We analyzed cyclomatic complexity (average per \nfunction, maximum per function), maintainability index (0-100 scale), density of code smells (per 100 \nlines of code), % of code duplication, documentation coverage. We identified security vulnerabilities with \nSnyk Static Application Security Testing (SAST) in all languages, and confirmed and categorized \nvulnerabilities by severity (critical, high, medium, low) and type (injection flaws, authentication \nvulnerabilities, and cryptographic flaws). \nTo evaluate productivity, the total time to completion of the automated tasks was recorded, along with \nNASA Task Load Index (NASA-TLX) surveys administered after each task. NASA-TLX quantifier \ncognative load across six areas: mental demand, physical demand, temporal demand, performance, effort, \nand frustration, Additionally, we conducted semi-structured interviews for qualitative data with 30 \nrandomly selected participants (15 per group) after the experiment to learn about tool usage and coding \nbehaviors, and the benefits and challenges, and any adaptations that occurred during coding. \n3.5 Data Analysis \nMixed-model ANOVA analyzes quantitative data with group (experimental or control) as the between-\nsubjects factor, and programming language as the within-subjects factor. Cohen's d is used, when \nappropriate, to calculate effect sizes in pairwise comparisons. Non-parametric tests (Mann-Whitney U, \nKruskal-Wallis) are used, where needed, to address non-normally distributed data. Security vulnerability \ncounts are analyzed with Poisson regression to account for typical characteristics of count data. The \nqualitative interview data is analyzed through thematic analysis, with two independent coders attaining \nhigh interrater reliability (Cohen\u2019s \u03ba = 0.84). \n \n4. Results \n4.1 Productivity Analysis \nThe experimental group was found to have a significantly more productive work rate than the control \ngroup across all programming languages. On average, the time to complete tasks in the experimental group \nwas 38.4 minutes, versus 56.1 minutes in the control group, which is a statistically significant reduction \nin time of 31.4% (t(118)=8.92, p<0.001, d=1.63). The improved time to complete tasks was exhibited by \nall levels of experience; however, senior developers had a slightly smaller (27.3%) improvement than \njunior (34.1%) and mid-level (32.8%) developers. \nProductivity gain was evaluated depending on programming language. The largest productivity gain was \nseen in Python (36.7%), followed by JavaScript (32.4%), Java (29.1%), and C++ (27.6%). The smaller \nproductivity gain in C++ was attributed to spending more time debugging AI-generated code, specifically, \npertaining to memory management and pointer operations. \nAn assessment of cognitive load using NASA-TLX produced the least consistent"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_007",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "001, d=1.63). The improved time to complete tasks was exhibited by \nall levels of experience; however, senior developers had a slightly smaller (27.3%) improvement than \njunior (34.1%) and mid-level (32.8%) developers. \nProductivity gain was evaluated depending on programming language. The largest productivity gain was \nseen in Python (36.7%), followed by JavaScript (32.4%), Java (29.1%), and C++ (27.6%). The smaller \nproductivity gain in C++ was attributed to spending more time debugging AI-generated code, specifically, \npertaining to memory management and pointer operations. \nAn assessment of cognitive load using NASA-TLX produced the least consistent results. Participants in \nthe experimental group reported lower mental demand compared to the control group (mean=42.3 vs 58.7, \np<0.001) and lower temporal pressure (mean=39.1 vs 61.4, p<0.001), but reported a higher level of \nfrustration (mean=48.2 vs 38.6, p=0.003) in response to inaccurate or misleading AI suggestions. \nImportantly, 67% of participants in the experimental group reported that they occasionally over-relied on \nthe suggestions generated by the AI and did not critically think about alternative solutions. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n6\n \n4.2 Code Quality Metrics \nThe static analyses found subtle effects on code quality. Cyclomatic complexity had a statistically \nsignificant mean reduction in the experimental group compared to the control group. The mean cyclomatic \ncomplexity of a function in the experimental group was 4.3 compared to 5.0 in the control group (p=0.008), \nindicating a lower level of complexity in the design of function. This change in cyclomatic complexity \nwas not shared equally among the four programming languages (Python: -21.3% , JavaScript: -18.7%, \nJava: -9.2%, C++: -8.4%). \nThe Maintainability Index (MI) was significantly different in the experimental group (MI mean=71.4 vs \ncontrol MI mean=60.2, p<0.001) and this difference was primarily driven by better overall organization \nwith the code and code being more uniformly formatted. The AI tools produced code that was organized, \nwith well-defined variable naming conventions, and proper separation of concerns. There was no \nsignificant difference in code smell density between the two groups (experimental: 2.8 code smells per \n100 LOC vs control: 2.6 code smells per 100 LOC, p=0.421), indicating that while the AI tools improve \nthe structural quality, they did not eliminate common antipatterns. Statistics for the same code quality \nindicators can be found in table 2. \n \nTable 2: Code Quality Metrics Comparison \nMetric \nExperimental Group \nControl Group \np-\nvalue \nEffect \nSize \n(d) \nCyclomatic Complexity \n4.3 (plus, minus"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_008",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " The AI tools produced code that was organized, \nwith well-defined variable naming conventions, and proper separation of concerns. There was no \nsignificant difference in code smell density between the two groups (experimental: 2.8 code smells per \n100 LOC vs control: 2.6 code smells per 100 LOC, p=0.421), indicating that while the AI tools improve \nthe structural quality, they did not eliminate common antipatterns. Statistics for the same code quality \nindicators can be found in table 2. \n \nTable 2: Code Quality Metrics Comparison \nMetric \nExperimental Group \nControl Group \np-\nvalue \nEffect \nSize \n(d) \nCyclomatic Complexity \n4.3 (plus, minus 0.8) \n5.0 (plus, minus 1.1) \n0.008 \n0.73 \nMaintainability Index \n71.4 (plus, minus 8.2) \n60.2 (plus, minus 9.7) \n<0.001 \n1.24 \nCode Smell Density \n2.8 (plus, minus 1.2) \n2.6 (plus, minus 1.1) \n0.421 \n0.13 \nDocumentation \nCoverage \n68.3% (plus, minus 12.4) 41.7% \n(plus, \nminus \n15.8) \n<0.001 \n1.89 \nCode Duplication \n8.7% (plus, minus 3.2) \n4.2% (plus, minus 2.1) \n0.002 \n1.64 \n \nThe experimental group achieved significantly more documentation coverage, compared to the control \ngroup (68.3% vs. 41.7%, p<0.001), because AI tools typically output inline comments and function do \nstrings automatically and in great detail. The qualitative analysis revealed that 34% of the total AI \ngenerated documentation was inaccurate or simply generic descriptions with no reflection in the actual \nfunction behaviour. This finding illustrates the need for some level of human review. \nCode duplication levels in the experimental group significantly increased (8.7% vs. 4.2%, p=0.002) \nbecause AI models generate similar code patterns across different functions, as well as reproducing \npatterns frequently encountered in their training data rather than creating reusable abstractions. \n4.3 Security Vulnerability Analysis \nSecurity analysis revealed disconcerting trends with AI-generated code.  The experimental group averaged \n23.7% more vulnerabilities than the control group per 1,000 lines of code (mean = 3.8 vs. 3.1, p = 0.018).  \nWhen vulnerability severity was examined, things were even more concerning - experimental group had \n12% fewer low severity vulnerabilities at the same time as 47% more high severity and 89% more critical \nvulnerabilities. \nAnalysis of security vulnerabilities by programming language displayed a notable amount of difference in \nthe results. The C++ code from the experimental group showed approximately 34.8% more vulnerabilities \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7,"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_009",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "1, p = 0.018).  \nWhen vulnerability severity was examined, things were even more concerning - experimental group had \n12% fewer low severity vulnerabilities at the same time as 47% more high severity and 89% more critical \nvulnerabilities. \nAnalysis of security vulnerabilities by programming language displayed a notable amount of difference in \nthe results. The C++ code from the experimental group showed approximately 34.8% more vulnerabilities \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n7\n \nthan the control group, and these vulnerabilities were related to memory safety, including buffer \noverflows, use-after-free, and memory leaks. The JavaScript code showed approximately 28.3% more \nvulnerabilities than the control group, and the vast majority of the vulnerabilities were related to \ninfrastructure vulnerabilities, such as input validation and cross-site-scripting vulnerabilities. The Python \ncode showed approximately 16.4% more vulnerabilities than the control group, the vast majority of which \narose as a result of problems with authentication and authorization. Finally, the Java code showed the \nsmallest increase of approximately 12.7%, and the majority of vulnerabilities were associated with \nexceptions and resource management. Security Vulnerabilities Analysis by Language is displayed in Table \n3. \n \nTable 3: Security Vulnerability Analysis by Language \nLanguage Exp. Group (per \n1000 LOC) \nControl \nGroup \n% Increase Common Vulnerability Types \nPython \n3.6 \n3.1 \n+16.4% \nAuth /Authorization issues \nJava \n \n \n+12.7% \nException handling, Resource mgmt. \nJavaScript 4.3 \n3.4 \n+28.3% \nXSS, Input validation \nC++ \n5.1 \n3.8 \n+34.8% \nMemory safety, Buffer overflows \nOverall \n3.8 \n3.1 \n+23.7% \n- \n \nThe most prevalent vulnerability patterns in AI-generated code were (1) hard-coded credentials and API \nkeys (42 cases), (2) SQL injection vulnerabilities from concatenated strings in SQL queries (38 cases), (3) \ninsecure cryptographic sourcing insecure algorithms (29 cases), (4) poor input sanitization and validation \n(67 cases), and (5) insecure deserialization (23 cases). Of that, 76% of the security vulnerabilities in the \nAI generated code went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_011",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \nnature of the system training data and (possible) simpler syntax; meanwhile, the poorly rated secure \ncomputing of C++ largely reflects the complexity of memory management and constraints on how the \nprobability model encodes security. \nGiven the 89% increase in critical security vulnerabilities, it is an urgent concern, and it is caused by three \nfactors: AI models reproducing vulnerability patterns from the training data, developers not applying \nsufficient amount of security scrutiny to AI-generated code, and current AI models do not understand \nsecurity context and threat models. In fact, the finding that 76% of vulnerabilities in AI-generated code \nwere missed was a dangerous mixing of increased creation of vulnerabilities while decreasing \nvulnerability detection. The cognitive load results presented a clear paradox of decreased mental demand \nand increased frustration, as this reflected a qualitatively different cognitive demand. AI tools do decrease \ncognitive load for syntactic recall and boilerplate generation but introduce new cognitive demands related \nto prompt engineering, evaluating the suggestions, and correcting errors; and the frustration likely captures \ndeveloper's struggle with incorrect or false suggestions and the cognitive dissonance of evaluating code \nthat was not conceptualized by themselves. \n5.2 Implications for Practice \nSoftware organizations leveraging AI-assisted coding tools must have in place a variety of mitigation \nmechanisms including security review processes that are mandatory for AI-generated code concentrating \non the security issues that have been detected, security training that is improved by stressing the limitations \nof AI code, the use of SAST instruments that are integrated with AI-specific rule sets, and the creation of \norganizational regulations that define the appropriate use of AI assistance and manual implementation. \nThe deployment of AI tools should be done in different ways depending on the seniority level of \ndevelopers. Junior developers should be provided with additional support and guidance so that they do not \nbecome overly dependent on the tool and their skills are fostered, whereas senior developers should be \ngiven the freedom to use the tool for validation purposes which is linked to the best results. Companies \nshould pay for extensive training courses instead of expecting instant productivity increases because, \naccording to our findings, developers need experience equivalent to 3-4 tasks to reach their maximum \nperformance. Training should focus on the skills necessary to critically evaluate AI suggestions, prompt \nengineering, and being security-aware in order to get the most benefits and the least risks. \n \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7,"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_012",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "given the freedom to use the tool for validation purposes which is linked to the best results. Companies \nshould pay for extensive training courses instead of expecting instant productivity increases because, \naccording to our findings, developers need experience equivalent to 3-4 tasks to reach their maximum \nperformance. Training should focus on the skills necessary to critically evaluate AI suggestions, prompt \nengineering, and being security-aware in order to get the most benefits and the least risks. \n \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n9\n \n5.3 Theoretical Contributions \nThis research adds something new to software engineering by showing real-world evidence for the dual-\nprocess model in AI-assisted coding. We found that when developers use AI tools, they switch between \ntwo ways of thinking. There\u2019s the fast, almost automatic mode where people quickly accept AI suggestions \nfor routine code and then there\u2019s the slower, more careful mode, where they stop and think harder, \nespecially when the code is new or deals with security. \nHowever, security is one area where developers actually rely too much on that fast, automatic thinking. \nThey trust the AI\u2019s code without going deeper and can create exploitable vulnerabilities. That\u2019s a concern, \nand therefore, we need to figure out ways to get developers to slow down and examine what the AI is \nproviding, particularly in sensitive locations. \nThe results of our study push at the edges of our understanding about learning to program with AI. AI \ntools change the way people learn to code. They rely on the ease of programming knowledge \u2014 \nremembering syntax or patterns \u2014 until you get to the more difficult stuff, when learning might stall. \nWhile on one hand, the benefits of AI are apparent in how quickly it is possible to learn the basics, at the \nsame time, it may hinder the development of deeper problem-solving skills that comes with experience. \nTo truly understand the process of something like this is to be involved in longitudinal work that examines \nre-escalation and de-escalation of skills as a developer continues to rely on AI over time. \n5.4 Limitations \nSeveral limitations impact the generalizability of our findings. While the experimental setting is \ncontrolled, which helps with internal validity, it is not representative of real-world development context \nwith large and complex codebases, team dynamics, and organizational constraints. Our results can only \nrefer to GitHub Copilot, and we cannot conclude validity of other tools leveraged AI in different forms or \narchitectures. The 60-minute tasks did not allow us to explore the long-term ramifications of maintenance. \nThe sample of practitioners was drawn from Western technology companies and there are implications for \ngeneralizability among practice globally based on our sample. Additionally, the static analysis tools used \nas part of the scanning for a security vulnerability had important limitations, as discussed in several works, \nof being too discriminatory (false positives) and failing to positively identify classes of vulnerabilities that \ncan only be found using a dynamic analysis. Therefore, even with some of the limitations addressed by \nsome manual validation, we evaluated only the immediate introduction of a vulnerability and not the \nprocesses"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_013",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " we cannot conclude validity of other tools leveraged AI in different forms or \narchitectures. The 60-minute tasks did not allow us to explore the long-term ramifications of maintenance. \nThe sample of practitioners was drawn from Western technology companies and there are implications for \ngeneralizability among practice globally based on our sample. Additionally, the static analysis tools used \nas part of the scanning for a security vulnerability had important limitations, as discussed in several works, \nof being too discriminatory (false positives) and failing to positively identify classes of vulnerabilities that \ncan only be found using a dynamic analysis. Therefore, even with some of the limitations addressed by \nsome manual validation, we evaluated only the immediate introduction of a vulnerability and not the \nprocesses by which a vulnerability is subsequently identified and remediated once a code sample is placed \ninto production software, when investigating a security vulnerability. \n5.5 Future Research Directions \nNumerous important questions springing from our observations will require investigation. Longitudinal \nstudies depicting the development of AI-assisted codebases over extended periods of time will provide \nimportant insights into maintenance costs and the buildup of technical debt. Cross-comparative studies of \ndifferent AI tool implementations will also help clarify whether observed effects are associated with a \nspecific tool, or are more generalizable. Equally pressing, interventions also merit investigation, which \ncould include making AI models security-aware, humans-in-the-loop generation that performs automated \nsecurity-checks, enhanced developer training, or techniques of prompt engineering that pro-actively \nreduce the actual introduction of vulnerabilities. Interactions between AI tooling and developers' own \nskills should also be in steady inquiry, especially whether junior developers using AI tools are developing \nproblem-solving skills equivalent to developers going through training in the traditional sense. The \nanswers to such questions would be noteworthy for educational practice and hiring. Finally, we could \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n10\n \nconduct research that examines adaptive AI assistance that provides support at each stage based on \ndevelopers' competencies in order to move optimal learning trajectories forward, and that also assesses \nproductivity gains in relationship to skills development. \n \n6. Conclusion \nIn this study, we provide an in-depth data-driven investigation of AI-assisted code generation technologies \nthat demonstrate considerable productivity increases (i.e., an average of 31.4%) and, concerning, a total \nincrease in vulnerabilities of 23.7% and a total increase in critical severity of 89%. We also demonstrate \nthat some dimensions of code quality are improved with AI-assisted code generation tools (i.e., \nmaintainability, cyclomatic complexity) but caution is warranted with operational risks, to code itself (i.e. \nextra code duplication) and security vulnerabilities. We also examined differences across programming \nlanguages, and in particular, we found that while using AI-assisted code generation technologies is \nconstructive in Python, it warrants heightened caution around codex in C++ (to name only one). Finally, \nwhile we examined experience differences, we found that junior developers require support to prevent \nexcessive dependency on AI and senior developers could receive the maximum"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_014",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " 23.7% and a total increase in critical severity of 89%. We also demonstrate \nthat some dimensions of code quality are improved with AI-assisted code generation tools (i.e., \nmaintainability, cyclomatic complexity) but caution is warranted with operational risks, to code itself (i.e. \nextra code duplication) and security vulnerabilities. We also examined differences across programming \nlanguages, and in particular, we found that while using AI-assisted code generation technologies is \nconstructive in Python, it warrants heightened caution around codex in C++ (to name only one). Finally, \nwhile we examined experience differences, we found that junior developers require support to prevent \nexcessive dependency on AI and senior developers could receive the maximum benefit from an AI tool \nwhen used independently. The security findings are quite concerning, which suggests that the injection of \nvulnerabilities is potentially higher and the chances of being detected is lowered, thus introducing grave \nrisks for organizations. \nSoftware organizations need to introduce processes for security review, advanced training, and the use of \nautomated security analysis in order to recognize possible patterns related to AI-generated code. It is no \nlonger possible to assume AI-generated code is unsafe or that the software development processes you \nhave in place will contain the risks associated with using AI-generated code based on the evidence we \npresent. AI-generated code is a fundamentally transformative and irreversibly changing practice in the \nsoftware engineering of the discipline and the question is how we can achieve the benefit and reduce risk \nthrough additional empirical research and the introduction of specific practices for AI security while \ncontinuing to have experts who are humans. Our research offers organizations an evidence-based basis for \nthe implementation of the AI tool and provides a benchmark for future longitudinal research design of \nlong-term outcomes of the analysis of the argued effort of transformational technology for deployment. \n \n7. References \n1. Chen, M., Tworek, J., Jun, H., et al. (2024). Evaluating large language models trained on code. ACM \nTransactions on Software Engineering and Methodology, 33(4), 1-42. \n2. Barke, S., James, M. B., & Polikarpova, N. (2023). Grounded Copilot: How programmers interact \nwith code-generating models. Proceedings of the ACM on Programming Languages, 7(OOPSLA1), \n85-111. \n3. Vaithilingam, P., Zhang, T., & Glassman, E. L. (2024). Expectation vs. experience: Evaluating the \nusability of code generation tools powered by large language models. CHI Conference on Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \n"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_015",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n11\n \n6. Sarkar, A., Ross, N. A., Anantharaman, V., et al. (2024). What is it like to program with artificial \nintelligence? Proceedings of the ACM Conference on Computer-Supported Cooperative Work, 156-\n189. \n7. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI \nassistants? ACM Conference on Computer and Communications Security, 2785-2799. \n8. Asare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub Copilot a substitute for human pair-\nprogramming? An empirical study. ACM Transactions on Software Engineering, 49(2), 1-34. \n9. Dakhel, A. M., Majdinasab, V., Nikanjam, A., et al. (2023). GitHub Copilot AI pair programmer: \nAsset or liability? Journal of Systems and Software, 203, 111734. \n10. Allamanis, M., Brockschmidt, M., & Khademi, M. (2018). Learning to represent programs with \ngraphs. International Conference on Learning Representations, 1-16. \n11. Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and \nnatural languages. Findings of the Association for Computational Linguistics: EMNLP 2020, 1536-\n1547. \n12. Guo, D., Ren, S., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems,"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_016",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": "., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems, 33, 1877-1901. \n15. Chen, M., Tworek, J., Jun, H., et al. (2021). Evaluating large language models trained on code. arXiv \npreprint arXiv:2107.03374. \n16. Nijkamp, E., Pang, B., Hayashi, H., et al. (2023). CodeGen: An open large language model for code \nwith multi-turn program synthesis. International Conference on Learning Representations, 1-25. \n17. Li, Y., Choi, D., Chung, J., et al. (2022). Competition-level code generation with AlphaCode. Science, \n378(6624), 1092-1097. \n18. Ziegler, A., Kalliamvakou, E., Li, X., et al. (2022). Productivity assessment of neural code completion. \nACM Joint European Software Engineering Conference and Symposium on the Foundations of \nSoftware Engineering, 21-29. \n19. Sandoval, G., Pearce, H., Nys, T., et al. (2023). Security implications of large language model code \nassistants: A user study. IEEE Symposium on Security and Privacy Workshops, 100-109. \n20. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2022). Do users write more insecure code with AI \nassistants? arXiv preprint arXiv:2211.03622. \n21. Nguyen, N., & Nadi, S. (2022). An empirical evaluation of GitHub Copilot's code suggestions. \nProceedings of the 19th International Conference on Mining Software Repositories, 1-5. \n22. Liang, J. T., Yang, C., & Myers, B. A. (2024). A large-scale survey on the usability of AI programming \nassistants: Successes and challenges. ACM Transactions on Computer-Human Interaction, 31(2), 1-\n45. \n23. Pearce, H., Ahmad, B., Tan, B., et al. (2022). Asleep at the keyboard? Assessing the security of GitHub \nCopilot's code contributions. IEEE Symposium on Security and Privacy, 754-768. \n24. Pearce, H., Tan, B., Ahmad, B., et al. (2023). Examining zero-shot vulnerability repair with large \nlanguage models. IEEE Symposium on Security and Privacy, 2339-2356. \nInternational Journal for Multidisciplinary Research (IJF"
  },
  {
    "chunk_id": "EmpiricalToolAnalysis2025_chunk_017",
    "source_id": "EmpiricalToolAnalysis2025",
    "text": " A large-scale survey on the usability of AI programming \nassistants: Successes and challenges. ACM Transactions on Computer-Human Interaction, 31(2), 1-\n45. \n23. Pearce, H., Ahmad, B., Tan, B., et al. (2022). Asleep at the keyboard? Assessing the security of GitHub \nCopilot's code contributions. IEEE Symposium on Security and Privacy, 754-768. \n24. Pearce, H., Tan, B., Ahmad, B., et al. (2023). Examining zero-shot vulnerability repair with large \nlanguage models. IEEE Symposium on Security and Privacy, 2339-2356. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   \u25cf   Website: www.ijfmr.com       \u25cf   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n12\n \n25. Prather, J., Pettit, R., McMurry, K., et al. (2023). The robots are here: Navigating the generative AI \nrevolution in computing education. ACM Conference on Innovation and Technology in Computer \nScience Education, 108-121 \n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_001",
    "source_id": "CodeQualityComparison2023",
    "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yeti\u015ftiren \u00b7 I\u015f\u0131k \u00d6zsoy \u00b7 Miray\nAyerdem \u00b7 Eray T\u00fcz\u00fcn\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI\u2019s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yeti\u015ftiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nI\u015f\u0131k \u00d6zsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray T\u00fcz\u00fcn\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yeti\u015ftiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers\u2019 per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_002",
    "source_id": "CodeQualityComparison2023",
    "text": ", our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers\u2019 per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant keystrokes, and explore new APIs without\nhaving to go through the mental effort of switching to an external documentation tool\nor API browser. Some of the well-known code completion tools include IntelliSense in\nVisual Studio Code1 and the built-in code completion in the JetBrains IDEs2. Although\nthese tools can output code snippets, they differ fundamentally from code generators.\nThe advent of advanced language processing technologies has led to the emergence\nof Large Language Models (LLMs). While LLMs have numerous use cases, we focus on\ntheir code generation capabilities. Unlike code completion tools, code generators actively\nutilize LLMs by providing the programmer\u2019s input to the specified LLM and returning\nthe output to the programmer\u2019s workspace. Currently, code generators\u2019 outputs cannot\nbe produced locally, unlike code completion tools. Additionally, code generators can\ngenerate longer outputs in the form of lines or blocks of code, which can build function\nbodies or other constructs. Moreover, code generators can convert natural language\ninputs into source code, a key distinction from code completion tools.\nOur motivation for conducting this study stems from the growing interest in AI-\nassisted code generators and the spread of unverified information about them. We\nrecognize the popularity and potential of state-of-the-art AI-assisted code generators, as\nwell as the heuristic feedback from various communities. In line with our previous study\n(Yetistiren et al., 2022), we believe it is worthwhile to evaluate the potential benefits\nof these generators. Although these tools can generate code, their value remains unde-\ntermined. To systematically assess these code generators, we propose an experimental\nsetup evaluating the generated code based on Code Validity, Code Correctness, Code\nSecurity, Code Reliability, and Code Maintainability.\nWe have chosen GitHub Copilot, Amazon CodeWhisperer, and ChatGPT for our\nstudy, leading us to formulate the following research questions:\nRQ1 What is the quality of the code generated by the code generation tools?\n1 code.visualstudio.com/docs/editor/intellisense\n2 jetbrains.com\nTitle Suppressed Due to Excessive Length\n3\nRQ1.1 How valid are the code generation tools\u2019 code suggestions?\nRQ1.2 How correct are code generation tools\u2019 code suggestions?\nRQ1.3 How secure are code generation tools\u2019 code suggestions?\nRQ1.4 How reliable are code generation tools\u2019 code suggestions?\nRQ1.5 How maintainable are code generation tools\u2019 code suggestions?\nRQ2 What is the impact of using the docstrings on the generated code quality?\nRQ3 What is the impact of using meaningful function names on the generated"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_003",
    "source_id": "CodeQualityComparison2023",
    "text": " the quality of the code generated by the code generation tools?\n1 code.visualstudio.com/docs/editor/intellisense\n2 jetbrains.com\nTitle Suppressed Due to Excessive Length\n3\nRQ1.1 How valid are the code generation tools\u2019 code suggestions?\nRQ1.2 How correct are code generation tools\u2019 code suggestions?\nRQ1.3 How secure are code generation tools\u2019 code suggestions?\nRQ1.4 How reliable are code generation tools\u2019 code suggestions?\nRQ1.5 How maintainable are code generation tools\u2019 code suggestions?\nRQ2 What is the impact of using the docstrings on the generated code quality?\nRQ3 What is the impact of using meaningful function names on the generated code\nquality?\nRQ4 How did the code generation tools evolve over time?\nWe believe our study will enable users to more effectively leverage AI-assisted code\ngenerators for generating accurate, valid, reliable, maintainable, and secure results.\nIn addition, tool developers can benefit from our findings to identify and enhance\nthe strengths and address the weaknesses of their tools in real-world situations. The\ncomparative aspect of our study provides valuable insights into the performance of each\ncode generation tool relative to its competitors.\nThe structure of our study is as follows: In Section 2, we provide some background\ninformation about the code generation tools we evaluate. In Section 3, we provide a\ndetailed explanation of the research questions we have determined by elaborating on our\nexperimental setup. Our results are presented in Section 4, and they are discussed in\nSection 5. The threats that influence the validity of our study are addressed in Section\n6. In Section 7, we discuss related work. Finally, in Section 8, we conclude our study.\n2 Background\n2.1 GitHub Copilot\nGitHub Copilot3 is a code generation tool that utilizes a variety of technologies, including\na compatible IDE, and the OpenAI Codex Model4. GitHub announced GitHub Copilot\nfor technical preview in the Visual Studio Code development environment on June 29,\n2021 (Friedman, 2021). GitHub declared on June 21, 2022, that Copilot was out of\nthe technical preview phase and is now accessible as a subscription-based service for\nindividual developers (Dohmke, 2022). It currently has subscription plans for individuals\nand businesses. GitHub Copilot can be installed and used as an extension to Visual\nStudio Code, Neovim, IDEs developed by JetBrains5, and GitHub Codespaces6. The\nunderlying service continuously takes user code samples and sends the snippets to the\nunderlying OpenAI Codex Model. GitHub Copilot generates the code and presents the\nresults of the OpenAI Codex Model by adjusting the generated code to the current\nworkspace of the programmer (Ernst and Bavota, 2022).\nThe Codex model relies on Generative Pre-trained Transformer (GPT) models the\ncompany previously invented for text generation. The public code available on GitHub\nwas used during the fine-tuning of the model to implement the code recognition and\ngeneration capabilities.\n3 copilot.github.com\n4 openai.com/blog/openai-codex\n5 plugins.jetbrains.com/plugin/17718-github-copilot\n6 github.com/features/codespaces\n4\nBurak Yeti\u015ftiren et al.\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_004",
    "source_id": "CodeQualityComparison2023",
    "text": " OpenAI Codex Model. GitHub Copilot generates the code and presents the\nresults of the OpenAI Codex Model by adjusting the generated code to the current\nworkspace of the programmer (Ernst and Bavota, 2022).\nThe Codex model relies on Generative Pre-trained Transformer (GPT) models the\ncompany previously invented for text generation. The public code available on GitHub\nwas used during the fine-tuning of the model to implement the code recognition and\ngeneration capabilities.\n3 copilot.github.com\n4 openai.com/blog/openai-codex\n5 plugins.jetbrains.com/plugin/17718-github-copilot\n6 github.com/features/codespaces\n4\nBurak Yeti\u015ftiren et al.\nFeatures\nChatGPT\nAmazon CodeWhisperer\nGitHub Copilot\nIDE Support\nNo IDE Support\nJetBrains, Visual Studio\nCode, AWS Cloud9, or the\nAWS Lambda console\nIntelliJ IDEA, Android Stu-\ndio, AppCode, CLion, Code\nWith Me Guest, DataGrip,\nDataSpell,\nGoLand,\nJet-\nBrains Client, MPS, Ph-\npStorm, PyCharm, Rider,\nRubyMine, WebStorm\nFirst Release Time\nNov-30-2022\nJune-23-2022\nOct-29-2021\nDeveloper\nOpenAI\nAWS\nOpenAI-Microsoft\nProviding\nReferences\nto\nSuggestions\nNO\nYES\nNO\nExplanation of Suggestions\nYES\nNO\nNO\nProviding Multiple Sugges-\ntions\nNO (Theoretically user can\nmanually ask for another\nsuggestion.)\nYES (Up to 5)\nYES (Up to 10)\nTraining Data Source\nGitHub Repositories,\nOpenAI\nCodex\nDataset,\nother code repositories such\nas GitLab, Bitbucket, and\nSourceForge\n\u201cVast amounts of publicly\navailable code\"\n\u201c...trained on all languages\nthat appear in public repos-\nitories\" (Fine-tuned)\nProgramming\nLanguages\nwork best with (according\nto the vendor)\nN/A\nC#,\nJava,\nJavaScript,\nPython, and TypeScript\nC, C++, C#, Go, Java,\nJavaScript, PHP, Python,\nRuby,\nScala,\nand\nType-\nScript\nMultipurpose (other than\nprogramming)\nYES\nNO\nNO\nSubscription\nChatGPT Free\nChatGPT Plus ($20 per\nmonth)\nFree Preview\nCopilot for Students (Free)\nCopilot for Individuals ($10\nper month)\nCopilot for Business ($19\nper user, per month)\nCan be Used Offline?\nNO\nNO\nNO\nCan it Access Local Files?\nNO\nYES\nYES\nTable 1 Comparing Relevant Code Generation Tools\n2.2 Amazon CodeWhisperer\nAmazon CodeWhisperer7 improves developer productivity by generating code recom-\nmendations based on both developers\u2019 comments in English and prior code in the IDE.\nAWS announced Amazon CodeWhisperer Preview on June 23, 2022, (Bays, 2022). The\ncode recommendations provided by CodeWhisperer are based on ML models trained\non various data sources, such as Amazon\u2019s sources and other open-source codes. When\ndevelopers write a comment in their IDE\u2019s code editor, CodeWhisperer will automati-\ncally examine the comment and determine the best-suited cloud services"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_005",
    "source_id": "CodeQualityComparison2023",
    "text": "?\nNO\nYES\nYES\nTable 1 Comparing Relevant Code Generation Tools\n2.2 Amazon CodeWhisperer\nAmazon CodeWhisperer7 improves developer productivity by generating code recom-\nmendations based on both developers\u2019 comments in English and prior code in the IDE.\nAWS announced Amazon CodeWhisperer Preview on June 23, 2022, (Bays, 2022). The\ncode recommendations provided by CodeWhisperer are based on ML models trained\non various data sources, such as Amazon\u2019s sources and other open-source codes. When\ndevelopers write a comment in their IDE\u2019s code editor, CodeWhisperer will automati-\ncally examine the comment and determine the best-suited cloud services and public\nlibraries. Then, it will provide a code snippet directly within the code editor. Moreover,\nCodeWhisperer simplifies the use of AWS services for developers by offering suggestions\nfor AWS API code across top services such as Amazon Elastic Compute Cloud (EC2),\nAWS Lambda, and Amazon Simple Storage Service (S3).\nCodeWhisperer supports multiple IDEs including JetBrains, Visual Studio Code,\nAWS Cloud9, or the AWS Lambda console as part of the AWS IDE toolkit. Moreover,\nit currently supports Java, JavaScript, Python, C#, and Typescript. As an additional\nfeature, CodeWhisperer has a reference tracker that detects the code recommendations\nsimilar to particular CodeWhisperer training data and provides those references to\ndevelopers. CodeWhisperer can also scan the code and define the security issues.\n7 aws.amazon.com/codewhisperer\nTitle Suppressed Due to Excessive Length\n5\n2.3 ChatGPT\nChatGPT8 is a language model announced by OpenAI on November 30, 2022. The\nsubscription plan, ChatGPT Plus, is available since February 1, 2023 (OpenAI, 2023).\nChatGPT uses advanced machine learning algorithms to generate human-like text\nresponses. It is trained on vast amounts of text data from the internet. It is capable\nof answering a wide range of questions, admitting its mistakes, challenging incorrect\npremises, and rejecting inappropriate requests. While the primary purpose of a chatbot\nis to imitate human conversation, ChatGPT is highly versatile and can perform a wide\nrange of tasks such as coding and debugging software, providing responses to exam\nquestions, composing poetic works and musical lyrics, and more (Tung, 2023).\nChatGPT has been adjusted specifically from a model within the GPT-3.5 series9,\nwhich completed its training process early in 2022 using supervised learning as well\nas reinforcement learning. Moreover, OpenAI continues to gather information from\nChatGPT users to improve and refine its performance.\nIt is also notable that ChatGPT has become the fastest-growing app in history\naccording to the study of the Union Bank of Switzerland (UBS). In January 2023,\nChatGPT attracted 13 million unique visitors daily, over twice the number it received\nin December according to the study. The report also states that despite being only two\nmonths old, ChatGPT has already reached a monthly user base of 100 million. (Cerullo,\n2023).\n2.4 Comparison of ChatGPT, GitHub Copilot, and Amazon CodeWhisperer\nWhen we performed the experiment"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_006",
    "source_id": "CodeQualityComparison2023",
    "text": " well\nas reinforcement learning. Moreover, OpenAI continues to gather information from\nChatGPT users to improve and refine its performance.\nIt is also notable that ChatGPT has become the fastest-growing app in history\naccording to the study of the Union Bank of Switzerland (UBS). In January 2023,\nChatGPT attracted 13 million unique visitors daily, over twice the number it received\nin December according to the study. The report also states that despite being only two\nmonths old, ChatGPT has already reached a monthly user base of 100 million. (Cerullo,\n2023).\n2.4 Comparison of ChatGPT, GitHub Copilot, and Amazon CodeWhisperer\nWhen we performed the experiment on GitHub Copilot, Amazon CodeWhisperer, and\nChatGPT for our study, we could observe some advantages and limitations of the tools.\nAccording to our observations, and the background knowledge we stated above, we\ncreated Table 1. In the table, it can be seen that while GitHub Copilot and Amazon\nCodeWhisperer have IDE support, ChatGPT does not have this yet, apart from its API\nsupport. Moreover, as we mentioned in earlier parts of Section 2, ChatGPT and Amazon\nCodeWhisperer were introduced in 2022. However, GitHub Copilot was announced in\n2021. Hence, it can be said that GitHub Copilot is one of the pioneers of this field.\nAdditionally, it is notable that OpenAI developed both ChatGPT and GitHub Copilot\nwhile AWS developed Amazon CodeWhisperer. ChatGPT does not have any specific\ninformation about supported programming languages but GitHub Copilot and Amazon\nCodeWhisperer specify the supported programming languages and we can see that\nGitHub Copilot supports more programming languages than Amazon CodeWhisperer in\nTable 1. Although Amazon CodeWhisperer is still free as it is in the technical preview\nstage, and GitHub Copilot has different subscription plans. ChatGPT is also available\nin the technical preview but it also has a subscription plan.\nFurthermore, we added our observations about the tools to Table 1. Firstly, we\nobserved that Amazon CodeWhisperer and GitHub Copilot could provide more than\none recommendation and it would be easy to choose the most relevant one from the\noptions for users. By contrast, ChatGPT mainly provided one suggestion at a time\nunless we did not ask for additional suggestions. On the other hand, ChatGPT was the\n8 openai.com/blog/chatgpt\n9 platform.openai.com/docs/model-index-for-researchers\n6\nBurak Yeti\u015ftiren et al.\nonly tool that explained its recommendations in detail and was used for purposes other\nthan programming. We also observed Amazon CodeWhisperer was the only tool that\npresented the recommendations\u2019 source. It is notable to mention that while Amazon\nCodeWhisperer and GitHub Copilot could access users\u2019 local files, ChatGPT could not\naccess them. Lastly, we observed that none of these tools could be used offline.\n3 Methodology\nUnder the subsections below, we elaborate on our methodology. Section 3.1 gives detail\nabout the data we use. To address the research questions, we created an experimental\nsetup, which systematically evaluates the effectiveness of code generation tools, that is\ndescribed in Section "
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_007",
    "source_id": "CodeQualityComparison2023",
    "text": " et al.\nonly tool that explained its recommendations in detail and was used for purposes other\nthan programming. We also observed Amazon CodeWhisperer was the only tool that\npresented the recommendations\u2019 source. It is notable to mention that while Amazon\nCodeWhisperer and GitHub Copilot could access users\u2019 local files, ChatGPT could not\naccess them. Lastly, we observed that none of these tools could be used offline.\n3 Methodology\nUnder the subsections below, we elaborate on our methodology. Section 3.1 gives detail\nabout the data we use. To address the research questions, we created an experimental\nsetup, which systematically evaluates the effectiveness of code generation tools, that is\ndescribed in Section 3.2. The details of our assessment are presented in Section 3.3. In\nSections 3.4 and 3.5, we elaborate on the two additional experiments we conducted to\ntest the effect of the function names and explanations of the generated code quality.\nMoreover, we use different versions of GitHub Copilot and Amazon CodeWhisperer to\nassess the performance of those code generation tools over time, which is described in\nSection 3.6.\ntask_id\nHumanEval/0\nprompt\nfrom typing import List\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \"\"\"\ncanonical_solution\n    for idx, elem in enumerate(numbers):\n        for idx2, elem2 in enumerate(numbers):\n            if idx != idx2:\n                distance = abs(elem - elem2)\n                if distance < threshold:\n                    return True\n    return False\ntest\nMETADATA = {\n    'author': 'jt',\n    'dataset': 'test'\n}\ndef check(candidate):\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert candidate([1.1, 2.2, 3"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_008",
    "source_id": "CodeQualityComparison2023",
    "text": "5.9, 4.0, 5.0], 0.95) == True\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\nFig. 1 Example Problem (ID: 0) from HumanEval dataset\nTitle Suppressed Due to Excessive Length\n7\nfrom\ntyping\nimport\nList\ndef\nhas_close_elements (numbers: List[float], threshold: float) ->\nbool:\n\"\"\"\nCheck if in given\nlist of numbers , are any two\nnumbers\ncloser\nto each\nother\nthan\ngiven\nthreshold.\n>>> has_close_elements ([1.0 , 2.0, 3.0] , 0.5)\nFalse\n>>> has_close_elements ([1.0 , 2.8, 3.0, 4.0, 5.0, 2.0] , 0.3)\nTrue\n\"\"\"\nfor i in range(len(numbers)):\nfor j in range(i + 1, len(numbers)):\nif abs(numbers[i] - numbers[j]) < threshold:\nreturn\nTrue\nreturn\nFalse\nListing 1 Generated Code for the Example Problem by GitHub Copilot v1.7.4421 (ID: 0)\n3.1 HumanEval Dataset\nFor our experiment, we use the HumanEval dataset proposed by Chen et al. (2021). This\ndataset contains 164 problems. Each problem is accompanied by a task ID, a prompt,\nthe canonical solution, and unit tests. The structure of a problem can be viewed in\nFigure 1. The task ID is the ID of that particular problem which ranges from 0 to 163.\nThe prompt part contains the function prototype, the explanation of the problem, some\nfunction calls and their output in a Python docstring, and library imports, if applicable.\nA canonical solution is considered as a correct solution which is coded by a \u201chuman\u201d\nprogrammer. The test part contains unit tests as a Python function.\nWe pass the function prototype and the docstring as input to code generation tools.\nAn example code generation done by GitHub Copilot, where the input problem is shown\nin Figure 1 can be viewed in Listing 1.\n3.2 Experimental Setup\nIn Figure 2, we focus on what artifacts were employed for which tools, and which metric\nthese combinations correspond to. Whereas in Figure 3, we provide a step-by-step\nillustration of the experiment\u2019s workflow. In this figure, given the HumanEval problem\ndataset (Chen et al., 2021), we start our experiment by extracting the problems. We\nachieve this by reading the dataset and representing each problem with a separate\nJSON format file. After completing the extraction procedure, we save the"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_009",
    "source_id": "CodeQualityComparison2023",
    "text": " docstring as input to code generation tools.\nAn example code generation done by GitHub Copilot, where the input problem is shown\nin Figure 1 can be viewed in Listing 1.\n3.2 Experimental Setup\nIn Figure 2, we focus on what artifacts were employed for which tools, and which metric\nthese combinations correspond to. Whereas in Figure 3, we provide a step-by-step\nillustration of the experiment\u2019s workflow. In this figure, given the HumanEval problem\ndataset (Chen et al., 2021), we start our experiment by extracting the problems. We\nachieve this by reading the dataset and representing each problem with a separate\nJSON format file. After completing the extraction procedure, we save the unit tests\nand the prompt of a problem as separate Python files to the directory corresponding\nto the problem\u2019s ID. Subsequently, we generate solutions using an already prepared\nPython file containing the prompt. This prompt combines the function signature and\ndocstring contained in the function body. Given the dynamic characteristic of code\ngeneration tools, GitHub Copilot, Amazon CodeWhisperer, and ChatGPT, in terms\nof the interactions between the programmer and the service, we implement the code\ngeneration step of our experiment manually by employing the Visual Studio Code IDE\nfor GitHub Copilot and Amazon CodeWhisperer and using the interface provided by\nOpenAI in a given browser for ChatGPT. After the code generation step is completed,\n8\nBurak Yeti\u015ftiren et al.\nHumanEval Problem Dataset\nProblem\nFunction Signature\nFunction Comment\nGenerated Code\nCode Correctness\nUnit Tests\nCode Validity\nPython Interpreter\nSonarQube Code Inspector\nCode Security\nCode Maintainability\nCode Reliability\nCode\nGeneration\nTools\nFig. 2 Experimental Setup\nExtract Problems\nRead Problem\nFor all problems\nA\nA\nA\nM\nAutomated process\nManual process\nProcess implemented with Python\nCode Generation Tools\nAssess Code\nValidity\nA\nExtract Problems\nA\nRead Problem\nA\nExtract and Save\nPrompt\nA\nExtract and Save\nTests\nA\nGenerate Solution\nM\nAssess Code\nCorrectness\nA\nUnit Tests\nAssess Code\nSecurity\nA\nAssess Code\nMaintainability\nA\nSonarQube\nAssess Code\nReliability\nA\nFig. 3 Experiment Workflow\nwe start the assessment phase by executing the tests on the generated solutions to assess\ncode validity and code correctness. After this, we utilize SonarQube to find the security\nrating, the number of bugs, and the number of code smells for each problem, these\ncorrespond to the Code Security, Code Reliability, and Code Maintainability metrics.\nWe detect the bugs and code smells separately, since the code smells are mostly different\nthan bugs, they do not necessarily cause the code to be incorrect, but introduce some\nuneasiness in the code which can cause more problems in the future. For each step of\nTitle Suppressed Due to Excessive Length\n9\nthe assessment phase, we save the individual assessment results related to the problem.\nThe extracted results can be seen in our reproduction package10.\nWe further test the validity of our findings in our literature survey about providing\ncode generation tools with code comments and function signatures, by implementing\ntwo additional experiments about the significance of function names, parameters"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_010",
    "source_id": "CodeQualityComparison2023",
    "text": "\ncorrespond to the Code Security, Code Reliability, and Code Maintainability metrics.\nWe detect the bugs and code smells separately, since the code smells are mostly different\nthan bugs, they do not necessarily cause the code to be incorrect, but introduce some\nuneasiness in the code which can cause more problems in the future. For each step of\nTitle Suppressed Due to Excessive Length\n9\nthe assessment phase, we save the individual assessment results related to the problem.\nThe extracted results can be seen in our reproduction package10.\nWe further test the validity of our findings in our literature survey about providing\ncode generation tools with code comments and function signatures, by implementing\ntwo additional experiments about the significance of function names, parameters, and\ncomments explained in Sections 3.4 and 3.5.\n3.3 Code Metrics\nWe have evaluated our results in terms of code validity, code correctness, code security,\ncode reliability, and code maintainability. Our metric for code validity is binary, in\nwhich we have two possible values, \u20180\u2019 and \u20181\u2019 indicating if the solution is valid or not.\nThis is assessed in terms of how a given code segment is compliant with the rules and\nregulations (i.e., syntax rules) of a given programming language and with any errors\nthat could be raised during runtime. The dataset we use is constructed for the Python\nprogramming language; therefore, to check for code validity, we use the Python 3.10.10\ninterpreter.\nFor code correctness, we want to assess the extent to which the generated code\nperforms as intended. As we previously stated, the problems in the HumanEval dataset\nare accompanied by problem-specific unit tests. On average, each problem comes with\n7.7 unit tests (Chen et al., 2021). We measured the code correctness as passed unit\ntests divided by all unit tests for a specific problem. Considering the abundance of unit\ntests, we believe that the most convenient way to assess code correctness is to make use\nof the provided tests.\nWe have also evaluated the average code correctness which is measured as the sum\nof all code correctness scores divided by the problem count. While calculating average\ncode correctness, we consider the code correctness score of invalid code generations as 0.\nWe show our calculation methods for Code Correctness and Average Code Correct-\nness below. CCS stands for Code Correctness Score, and CCSi is the Code Correctness\nScore for the ith problem. The range of i is 0 to 163.\nCode Correctness =\nP163\ni=0 CCSi [CCSi = 1]\n164\nAverage Code Correctness =\nP163\ni=0 CCSi\n164\nFinally, we used SonarQube11 to assess code security, code reliability, and code\nmaintainability metrics. For code security, we define the term vulnerability, which\ncompromises the security of the code; therefore, introducing security risks, considering\nthe possible deployment of the code in question as software or part of the software. We\nuse SonarQube\u2019s Security Module to assess code security. This module calculates the\nnumber of vulnerabilities in a given code. To assess code maintainability, SonarQube\n10 https://github.com/mirayayerdem/Github-Copilot-Amazon-Whisperer-\nChatGPT/blob/main/misc/All_Experiment_"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_011",
    "source_id": "CodeQualityComparison2023",
    "text": "i=0 CCSi\n164\nFinally, we used SonarQube11 to assess code security, code reliability, and code\nmaintainability metrics. For code security, we define the term vulnerability, which\ncompromises the security of the code; therefore, introducing security risks, considering\nthe possible deployment of the code in question as software or part of the software. We\nuse SonarQube\u2019s Security Module to assess code security. This module calculates the\nnumber of vulnerabilities in a given code. To assess code maintainability, SonarQube\n10 https://github.com/mirayayerdem/Github-Copilot-Amazon-Whisperer-\nChatGPT/blob/main/misc/All_Experiment_Results.xlsx\n11 sonarqube.org\n10\nBurak Yeti\u015ftiren et al.\nruns its evaluation on the given code in terms of the count of code smells present in the\ncode. For code reliability, we count the number of bugs in the code using SonarQube.\n3.4 Using only Function Signatures (RQ2)\nIn this experiment, we removed the docstrings from the problems to assess the effect of\ndocstrings on the generated solution. The docstring of a given problem in the HumanEval\ndataset includes the explanation of the function as the intended purpose of what that\nproblem should be doing. This explanation is then accompanied by some sample test\ncases and their results (an example can be seen in the \u201cprompt\u201d part in Figure 1). We\nused GitHub Copilot, Amazon CodeWhisperer, and ChatGPT to generate code by only\nusing the name and the parameters of the function as a reference. We aimed to see how\nour results would change in comparison to our previous results.\n3.5 Using Dummy Function Names (RQ3)\nWe changed the function names of the problems with the dummy function name \u2018foo\u2019,\nto assess the effect of meaningful function names on the generated solution. The tools\nare then employed to generate code with such inputs. We assess the generated code\nusing the Code Validity and Correctness metrics.\n3.6 Evaluation of Code Generation Tools Over Time (RQ4)\nSince the initial release of GitHub Copilot, there were multiple official updates that\nthe tool received, apart from the continuous training of the underlying LLM of GitHub\nCopilot. Considering that we have ready-to-use results for GitHub Copilot from our\nold study (Yetistiren et al., 2022), as a part of this study we are also evaluating how a\ngiven code generation tool has evolved over time. In that regard, we will be comparing\nGitHub Copilot v1.7.4421 and v1.70.8099 versions. Since AWS does not specify the\nversion of CodeWhisperer, we are unable to specify the versions we have used to run\nthe prior and later experiments; we can only provide the dates of our two experiments,\nwhich are November \u201822 for prior and January \u201823 for the latter experiment. For this\nresearch question, we use the Code Correctness and Code Validity metrics for the three\nexperiment types we explained in Sections 3.2, 3.4, and 3.5.\n4 Results\n4.1 Code Validity (RQ1.1)\nThe use of interpreters in Python made it easier for us to evaluate code"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_012",
    "source_id": "CodeQualityComparison2023",
    "text": "1.7.4421 and v1.70.8099 versions. Since AWS does not specify the\nversion of CodeWhisperer, we are unable to specify the versions we have used to run\nthe prior and later experiments; we can only provide the dates of our two experiments,\nwhich are November \u201822 for prior and January \u201823 for the latter experiment. For this\nresearch question, we use the Code Correctness and Code Validity metrics for the three\nexperiment types we explained in Sections 3.2, 3.4, and 3.5.\n4 Results\n4.1 Code Validity (RQ1.1)\nThe use of interpreters in Python made it easier for us to evaluate code validity by\nsimply trying to execute the code and catch errors at runtime. This should not be\nconfused with runtime errors; in Python, like runtime errors, the syntax errors can\nalso be detected by executing the script, unlike the compiler approach used in other\nhigh-level programming languages like Java and C++.\nTitle Suppressed Due to Excessive Length\n11\nFig. 4 Distribution of Validly Generated Samples among the Code Generation Tools\nAs we noted earlier, our metric for code validity is binary, such that if any errors were\nraised during the execution of a given Python script, we denoted that script as invalid.\nMoreover, for such scripts, we did not calculate the correctness score, as consideration\nof such scores could impose possible threats to the validity of our evaluation.\nThe full Code Validity results of our experiments can be visualized in Figure 4. Out\nof 164 generations of GitHub Copilot to the problems, 150 were valid. This yielded a\n91.5% success rate in terms of generating valid code. Amazon CodeWhisperer generated\na valid code for 148 of the problems, which yields a 90.2% success rate. ChatGPT was\nable to generate a valid code for 153 problems, which yields a 93.3% success rate.\n4.2 Code Correctness (RQ1.2)\nWe used the number of passed unit tests divided by all unit tests to calculate the\nsuccess percentage of the code for each problem, which we have defined in Section 3.3.\nIn Figures 6 - 11, we provided the percentage distribution of code generations falling\nunder different categories (correct, partially correct, and incorrect). Moreover, we also\nmeasured the average code correctness score by dividing the summation of all code\ncorrectness scores by the number of all problems, which we have again defined in Section\n3.3. In Figure 5, we provided the comparisons of code correctness scores and average\ncode correctness scores among code generation tools.\n12\nBurak Yeti\u015ftiren et al.\nFig. 5 Code Correctness and Average Code Correctness Scores of the Code Generation Tools\n46.3%\nProportion of Correct\nGenerations\n23.2%\nProportion of Partially\nCorrect Generations\n30.5%\nProportion of Incorrect\nGenerations\nFig. 6 Distribution of Code Generations in terms of Correctness for GitHub Copilot\n26.3%\n100% > CCSi > 75%\n34.2%\n75% \u2265CCSi > 50%\n23.7%\n50% \u2265CCSi > 25%\n15.8%\n25% \u2265"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_013",
    "source_id": "CodeQualityComparison2023",
    "text": " the comparisons of code correctness scores and average\ncode correctness scores among code generation tools.\n12\nBurak Yeti\u015ftiren et al.\nFig. 5 Code Correctness and Average Code Correctness Scores of the Code Generation Tools\n46.3%\nProportion of Correct\nGenerations\n23.2%\nProportion of Partially\nCorrect Generations\n30.5%\nProportion of Incorrect\nGenerations\nFig. 6 Distribution of Code Generations in terms of Correctness for GitHub Copilot\n26.3%\n100% > CCSi > 75%\n34.2%\n75% \u2265CCSi > 50%\n23.7%\n50% \u2265CCSi > 25%\n15.8%\n25% \u2265CCSi > 0%\nFig. 7 Distribution of Correctness Scores among Partially Correct Generations for GitHub\nCopilot\nWe observed that for 46.3% of the problems, GitHub Copilot managed to generate\nthe correct code for the given problem, whereas it completely failed to provide a\ncorrect solution for 30.5% of the problems. Generated solutions for the remaining\n23.2% of the problems were partially correct, as shown in Figure 6. Partially correct\ngenerations are the ones that pass at least one of the unit tests but not all of them.\nWe believe partially correct generations are useful, with the assumption that if at least\nTitle Suppressed Due to Excessive Length\n13\none unit test is passing, this is a potential indicator that with further improvements\nby the programmer, the code could become correct. To analyze the partially correct\ncode generations, we created a second pie chart in Figure 7, in which we eliminated\ncorrect and incorrect code generations, yielding 38 problems. We divided (0, 100)\nsuccess space into four intervals. GitHub Copilot managed a success rate of 26.3%\nfor the interval of 100% > CCSi > 75%, where CCSi refers to the code correctness\nscore of the problem. Followingly, code was generated with a correctness score in the\ninterval of 75% \u2265CCSi > 50%, 34.2% of the time. The next interval contained the\npartially correct code generations with a score of 23.7%, belonging to the interval of\n50% \u2265CCSi > 25%. For the last interval of 25% \u2265CCSi > 0%, the score was 15.8%.\nWe also found the average code correctness score of GitHub Copilot as 59.85%, shown\nin Figure 5.\n31.1%\nProportion of Correct\nGenerations\n40.2%\nProportion of Partially\nCorrect Generations\n28.7%\nProportion of Incorrect\nGenerations\nFig. 8 Distribution of Code Generations in terms of Correctness for Amazon Code Whisperer\n15.2%\n100% > CCSi > 75%\n37.9%\n75% \u2265CCSi > 50%\n21.2%\n50% \u2265CCSi > 25%\n25.7%\n25% \u2265CCSi > 0%\nFig. 9 Distribution of Correctness Scores among Partially Correct Generations for Amazon\nCode Whisperer\nAmazon CodeWhisperer was able to generate the correct code for 31.1% of the\nproblems, whereas it completely failed to provide a correct solution for"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_014",
    "source_id": "CodeQualityComparison2023",
    "text": "%\nProportion of Partially\nCorrect Generations\n28.7%\nProportion of Incorrect\nGenerations\nFig. 8 Distribution of Code Generations in terms of Correctness for Amazon Code Whisperer\n15.2%\n100% > CCSi > 75%\n37.9%\n75% \u2265CCSi > 50%\n21.2%\n50% \u2265CCSi > 25%\n25.7%\n25% \u2265CCSi > 0%\nFig. 9 Distribution of Correctness Scores among Partially Correct Generations for Amazon\nCode Whisperer\nAmazon CodeWhisperer was able to generate the correct code for 31.1% of the\nproblems, whereas it completely failed to provide a correct solution for 28.7% of the\nproblems. Generated solutions for the remaining 40.2% of the problems were partially\ncorrect, which is demonstrated in Figure 8. As given in Figure 9 Amazon CodeWhisperer\nmanaged a success rate of 15.2% for the interval of 100% > CCSi > 75%. Followingly,\ncode was generated with a correctness score in the interval of 75% \u2265CCSi > 50%,\n37.9% of the time. The next interval contained the partially correct code generations\nwith a score of 21.2%, belonging to the interval of 50% \u2265CCSi > 25%. For the last\ninterval of 25% \u2265CCSi > 0%, the score was 25.7%. Additionally, we found the average\ncode correctness score of Amazon CodeWhisperer as 51.95%, shown in Figure 5.\n65.2%\nProportion of Correct\nGenerations\n22.6%\nProportion of Partially\nCorrect Generations\n12.2%\nProportion of Incorrect\nGenerations\nFig. 10 Distribution of Code Generations in terms of Correctness for ChatGPT\n14\nBurak Yeti\u015ftiren et al.\nTable 2 Code Security, Code Reliability, and Code Maintainability Results\nCode Security\nCode Reliability\nCode Maintainability\nSecurity Rating\nNumber of Bugs\nNumber of Smells\n< 1\n1\n2\n3\n3 <\n1\n2\n3\n3 <\nCopilot v1.70.8099 (New)\n0\n3\n0\n0\n0\n14\n3\n2\n0\nCodeWhisperer Jan \u201923 (New)\n0\n1\n0\n0\n0\n22\n2\n0\n0\nChatGPT 9 Jan \u201923 Version\n0\n2\n0\n0\n0\n13\n1\n1\n0\n16.2%\n100% > CCSi > 75%\n46%\n75% \u2265CCSi > 50%\n24.3%\n50% \u2265CCSi > 25%\n13.5%\n25% \u2265CCSi > 0%\nFig. 11 Distribution of Correctness Scores among Partially Correct Generations for ChatGPT\nAs it can be seen in Figure 10, ChatGPT generated the correct code for 65.2% of\nthe problems, whereas it could not generate a correct solution for 12.2% of the problems.\nFor the remaining 22.6% of the problems, it was able to generate partially correct code.\nConsidering the partially correct solutions, shown in Figure"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_015",
    "source_id": "CodeQualityComparison2023",
    "text": "1\n0\n16.2%\n100% > CCSi > 75%\n46%\n75% \u2265CCSi > 50%\n24.3%\n50% \u2265CCSi > 25%\n13.5%\n25% \u2265CCSi > 0%\nFig. 11 Distribution of Correctness Scores among Partially Correct Generations for ChatGPT\nAs it can be seen in Figure 10, ChatGPT generated the correct code for 65.2% of\nthe problems, whereas it could not generate a correct solution for 12.2% of the problems.\nFor the remaining 22.6% of the problems, it was able to generate partially correct code.\nConsidering the partially correct solutions, shown in Figure 11, ChatGPT managed a\nsuccess rate of 16.2% for the interval of 100% > CCSi > 75%. Followingly, code was\ngenerated with a correctness score in the interval of 75% \u2265CCSi > 50%, 46.0% of the\ntime. The next interval contained the partially correct code generations with a score\nof 24.3%, belonging to the interval of 50% \u2265CCSi > 25%. For the last interval of\n25% \u2265CCSi > 0%, the score was 13.5%. We also found the average code correctness\nscore of ChatGPT as 78.1%, shown in Figure 5.\n4.3 Code Security & Code Reliability & Code Maintainability (RQ1.3 & RQ1.4 &\nRQ1.5)\nFor Code Security, Maintainability, and Reliability, we employed SonarQube to find the\nsecurity rating, the number of bugs, and the number of smells for each problem. The\nresults of our evaluation can be visualized in Table 2.\nFor none of the problems for all of the generators, we did not see any security rating\nthat was below 1, which is the maximum possible rating. Due to the constraints of the\ndataset we used for our study, the security results we obtained were limited. The usage\nof an alternative dataset with a different problem scope, and problems that yield longer\nsolutions may reflect better Code Security results.\nRegarding Code Reliability, there were three problems containing a single bug for\nGitHub Copilot, one problem containing a single bug for Amazon CodeWhisperer, and\ntwo problems containing a single bug for ChatGPT. All of the bugs observed in the\ngenerations provided by Copilot (Problem IDs: #33, #37, #100) were categorized\nas major bugs by SonarQube, and the time required to solve the given bug was 15\nminutes each. All of these problems were bug-free when their solutions were generated\nby Amazon CodeWhisperer or ChatGPT. The single bug (Problem ID: #102) we\nobserved with the solutions of Amazon CodeWhisperer was again a major bug, and the\nTitle Suppressed Due to Excessive Length\n15\nTable 3 List of the Code Smells of the Generated Code\nCopilot\nCodeWhisperer\nChatGPT\nCode Smell\nNumber of Problems\nTechnical Debt\nSeverity\n- Rename this variable; it shadows a built-in.\n6\n2\n1\n5 mins\nMajor\n- Remove the unused function parameter.\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_016",
    "source_id": "CodeQualityComparison2023",
    "text": " the time required to solve the given bug was 15\nminutes each. All of these problems were bug-free when their solutions were generated\nby Amazon CodeWhisperer or ChatGPT. The single bug (Problem ID: #102) we\nobserved with the solutions of Amazon CodeWhisperer was again a major bug, and the\nTitle Suppressed Due to Excessive Length\n15\nTable 3 List of the Code Smells of the Generated Code\nCopilot\nCodeWhisperer\nChatGPT\nCode Smell\nNumber of Problems\nTechnical Debt\nSeverity\n- Rename this variable; it shadows a built-in.\n6\n2\n1\n5 mins\nMajor\n- Remove the unused function parameter.\n1\n5\n0\n5 mins\nMajor\n- Refactor this function to reduce its Cognitive\nComplexity.\n6\n2\n2\n-\nCritical\n- Merge this if statement with the enclosing\none.\n2\n0\n1\n5 mins\nMajor\n- Rename this parameter x to match the regular\nexpression \u02c6[_a-z][a-z0-9_]*$.\n2\n2\n2\n2 mins\nMinor\n- Remove the unused local variable x.\n2\n0\n4\n5 mins\nMinor\n- Rename function x to match the regular ex-\npression \u02c6[a-z_][a-z0-9_]*$.\n5\n5\n5\n10 mins\nMajor\n- Specify an exception class to catch or reraise\nthe exception.\n1\n0\n0\n5 mins\nCritical\n- Extract this nested conditional expression into\nan independent statement.\n1\n0\n0\n5 mins\nMajor\n- Complete the task associated to this \u201cTODO\"\ncomment.\n0\n7\n0\n0 mins\nInfo\n- Remove commented out code.\n0\n3\n0\n5 mins\nMajor\n- Use concise character class syntax \u2018\\d\u2019 instead\nof \u2018[0-9]\u2019.\n0\n0\n2\n5 mins\nMinor\n- Replace this x call by a y function call.\n0\n0\n1\n2 mins\nCritical\nNote: Versions considered for this table: GitHub Copilot - 1.70.8099, Amazon CodeWhisperer - Jan \u201923, ChatGPT - 9 Jan 2023 Version\ncorresponding solutions generated by the other generators were bug-free. The estimated\ntime to solve this bug was five minutes. For ChatGPT, we had a blocker (Problem ID:\n#8), and a major bug (Problem ID: #141). The estimated time to solve the bug in\nproblem #8 was 15 minutes, and this was 10 minutes for problem #141. Again, the\nbugs were unique to ChatGPT among all the generators.\nRegarding Code Maintainability, we created a list of the code smells encountered\nin the generated code, shown in Table 3. Here we list each smell, accompanied by the\nfrequencies we have encountered for each code generator, the technical debt of the\nparticular smell (estimated time to resolve the issue), and the severity of the smell. As\nseen in Table 2, 14 problems contained a single, three problems contained two, and 2\nproblems contained three code smells. The average technical debt for the problems that\ncontained at least one smell was 9.1"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_017",
    "source_id": "CodeQualityComparison2023",
    "text": " 15 minutes, and this was 10 minutes for problem #141. Again, the\nbugs were unique to ChatGPT among all the generators.\nRegarding Code Maintainability, we created a list of the code smells encountered\nin the generated code, shown in Table 3. Here we list each smell, accompanied by the\nfrequencies we have encountered for each code generator, the technical debt of the\nparticular smell (estimated time to resolve the issue), and the severity of the smell. As\nseen in Table 2, 14 problems contained a single, three problems contained two, and 2\nproblems contained three code smells. The average technical debt for the problems that\ncontained at least one smell was 9.1 minutes and the total estimated time to solve every\nsmell was 172 minutes. For Amazon CodeWhisperer, we have encountered 22 problems\nwhere there were single, and 2 problems with two code smells. There were not any\ninstances, which contained more than two code smells. The average technical debt for\nthe problems containing at least one smell was 5.6 minutes, and the total estimated time\nto solve the smells was 117 minutes. For ChatGPT, there were 13 problems containing a\nsingle, and 1 problem containing two code smells. On average, the technical debt of the\nproblems accompanied by at least a single smell was 8.9 minutes. The total estimated\ntime to solve all the smells was 134 minutes.\n16\nBurak Yeti\u015ftiren et al.\nTable 4 Percentage Results of all code generation tools for Original Experiment (ORG), Only\nFunction Name (OFN) and Dummy Function Name (DFN)\nCopilot v1.70.8099 (New)\nCodeWhisperer Jan \u201923 (New)\nChatGPT 9 Jan \u201923 Version\nORG\nOFN\nDFN\nORG\nOFN\nDFN\nORG\nOFN\nDFN\nValid\n91.5%\n78.0%\n93.9%\n90.2%\n78.0%\n89.6%\n93.3%\n76.8%\n92.7%\nCorrect\n46.3%\n20.1%\n42.1%\n31.1%\n14.6%\n27.4%\n65.2%\n22.0%\n61.6%\nPartially Correct\n23.2%\n26.8%\n26.8%\n40.2%\n29.9%\n36.6%\n22.6%\n27.4%\n25.6%\nIncorrect\n30.5%\n53.1%\n31.1%\n28.7%\n55.5%\n36.0%\n12.2%\n50.6%\n12.8%\n4.4 Using only Function Names and Parameters Without Prompt (RQ2)\nThe results we presented up until this point were the outputs of the experiment where\nwe provided the function name, parameters, and the docstring as the inputs to get the\ngenerated code. In this part, as we explained in Section 3.2, we removed the docstring\nfrom each of our problems in the dataset. The results of this experiment are presented\nin Table 4.\nIn our original experiment where we used both the function name and the prompt,\nour code validity score was 91.5% for GitHub Copilot, 90.2% for Amazon CodeWhisperer,\nand"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_018",
    "source_id": "CodeQualityComparison2023",
    "text": ".2%\n50.6%\n12.8%\n4.4 Using only Function Names and Parameters Without Prompt (RQ2)\nThe results we presented up until this point were the outputs of the experiment where\nwe provided the function name, parameters, and the docstring as the inputs to get the\ngenerated code. In this part, as we explained in Section 3.2, we removed the docstring\nfrom each of our problems in the dataset. The results of this experiment are presented\nin Table 4.\nIn our original experiment where we used both the function name and the prompt,\nour code validity score was 91.5% for GitHub Copilot, 90.2% for Amazon CodeWhisperer,\nand 93.3% for ChatGPT. In our latter experiment, where we only used the function\nnames, our code validity score dropped to 78.0% for GitHub Copilot, 78.0% for Amazon\nCodeWhisperer, and 76.8% for ChatGPT.\nFor code correctness, if we compare the results of the two experiments for GitHub\nCopilot, the rate of correctly generated code dropped from 46.3% to 20.1%. The\nincorrectly generated code percentage increased from 30.5% to 53.1%, and the partially\ncorrectly generated code percentage increased from 23.2% to 26.8%. For Amazon\nCodeWhisperer, the rate of correctly generated code dropped from 31.1% to 14.6%. The\nincorrectly generated code percentage increased from 28.7% to 55.5%, and the partially\ncorrectly generated code percentage decreased from 40.2% to 29.9%. For ChatGPT,\nthe rate of correctly generated code dropped from 65.2% to 22.0%. The incorrectly\ngenerated code percentage increased from 12.2% to 50.6%, and the partially correctly\ngenerated code percentage increased from 22.6% to 27.4%.\n4.5 Using Dummy Function Names (RQ3)\nIn this part, as explained in Section 3.2, we prompted GitHub Copilot, Amazon\nCodeWhisperer, and ChatGPT to generate code for the same problems, this time with\ndummy function names instead of meaningful, and informative function names. We\nreplaced the function names with \u2018foo\u2019. The original and new experiment results are\npresented in Table 4.\nOur code validity score increased to 93.9% for GitHub Copilot and decreased to\n89.6% for Amazon CodeWhisperer and 92.7% for ChatGPT.\nFor code correctness, if we compare the results of the two experiments for GitHub\nCopilot, the rate of correctly generated code dropped from 46.3% to 42.1%. The\nincorrectly generated code percentage increased from 30.5% to 31.1%, and the partially\ncorrectly generated code percentage increased from 23.2% to 26.8%. For Amazon\nCodeWhisperer, the rate of correctly generated code dropped from 31.1% to 27.4%. The\nincorrectly generated code percentage increased from 28.7% to 36.0%, and the partially\nTitle Suppressed Due to Excessive Length\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_019",
    "source_id": "CodeQualityComparison2023",
    "text": " 92.7% for ChatGPT.\nFor code correctness, if we compare the results of the two experiments for GitHub\nCopilot, the rate of correctly generated code dropped from 46.3% to 42.1%. The\nincorrectly generated code percentage increased from 30.5% to 31.1%, and the partially\ncorrectly generated code percentage increased from 23.2% to 26.8%. For Amazon\nCodeWhisperer, the rate of correctly generated code dropped from 31.1% to 27.4%. The\nincorrectly generated code percentage increased from 28.7% to 36.0%, and the partially\nTitle Suppressed Due to Excessive Length\n17\ncorrectly generated code percentage decreased from 40.2% to 36.6%. For ChatGPT,\nthe rate of correctly generated code dropped from 65.2% to 61.6%. The incorrectly\ngenerated code percentage increased from 12.2% to 12.8%, and the partially correctly\ngenerated code percentage increased from 22.6% to 25.6%.\n4.6 Evaluation of Code Generation Tools Over Time (RQ4)\nIn this part, as explained in Section 3.6, we have evaluated GitHub Copilot and Amazon\nCodeWhisperer using the newer versions.\nAs shown in Figure 12, compared to the experiment results where we used older\nversions of GitHub Copilot and Amazon CodeWhisperer, our code validity score, 91.5%,\ndid not change for GitHub Copilot and the validity score of Amazon CodeWhisperer,\nthe validity score dropped to 90.2% (from 95.1%).\nFor code correctness, if we compare the results of the two experiments for GitHub\nCopilot, the rate of correctly generated code increased to 46.3% (from 28.7%). The\nincorrectly generated code percentage increased to 30.5% (from 20.1%), and the partially\ncorrectly generated code percentage decreased to 23.2% (from 51.2%). For Amazon\nCodeWhisperer, the rate of correctly generated code increased to 31.1% (from 24.4%).\nThe incorrectly generated code percentage decreased to 28.7% (from 45.1%), and the\npartially correctly generated code percentage increased to 40.2% (from 30.5%).\n5 Discussion\n5.1 Code Validity (RQ1.1)\nAs we discussed, for our 164 problems, GitHub Copilot was able to generate valid code\nfor 150 of them, yielding a success rate of 91.5%. Amazon CodeWhisperer was able to\ngenerate valid code for 148 problems, yielding a success rate of 90.2% and ChatGPT\nwas able to generate valid code for 153 of them, yielding a success rate of 93.3%.\nThe causes of the invalid code generated by GitHub Copilot were operations with\nincompatible types (Listing 2), syntax errors, and usage of the functions of unimported\nlibraries.\nAmazon CodeWhisperer had the following causes preventing a particular code from\nbeing valid: usage of the functions of unimported libraries (Listing 3), improper list\nindexing, operations with incompatible types,"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_020",
    "source_id": "CodeQualityComparison2023",
    "text": "150 of them, yielding a success rate of 91.5%. Amazon CodeWhisperer was able to\ngenerate valid code for 148 problems, yielding a success rate of 90.2% and ChatGPT\nwas able to generate valid code for 153 of them, yielding a success rate of 93.3%.\nThe causes of the invalid code generated by GitHub Copilot were operations with\nincompatible types (Listing 2), syntax errors, and usage of the functions of unimported\nlibraries.\nAmazon CodeWhisperer had the following causes preventing a particular code from\nbeing valid: usage of the functions of unimported libraries (Listing 3), improper list\nindexing, operations with incompatible types, searching for values that are not in a\nparticular list (Listing 4), incorrect usage of the assert statements, syntax errors, and\nstack overflow errors.\nLastly, ChatGPT had the following causes for invalid code: improper list and string\nindexing, syntax errors (Listing 5), operations with incompatible types (Listing 6), and\nthe usage of the functions of unimported libraries.\nFrom these results, we can see that there were many common issues among the code\ngeneration tools that were the causes of invalid code. While the frequencies of these\nissues were not unique among the tools, the small number of invalid codes should refrain\nus to make a generalization of any issue to correspond to a particular tool more than\nsome other one. We argue that the occurrence of similar issues among the tools, also\nthe similar rates of success of the code generation tools suggest that they are practically\nsimilar to each other in terms of being able to successfully generate valid code. The\n18\nBurak Yeti\u015ftiren et al.\nFig. 12 Code Validity Scores of the Code Generation Tools\napproximation appears to be that the code generation tools are able to generate valid\ncode 9 out of 10 times. In the generated code, some issues like syntax errors are more\nvisible to the programmer, than for example operations with incompatible types. When\nthe latter occurs in a given code, it is less unlikely that the programmer notices this issue\nsince in some instances the code can run without any errors for a given input; however,\nfail for another one. Therefore we want to highlight this particular vulnerability of the\ngenerated code by the tools.\nAll code generation tools are capable of generating valid code 9 out of 10\ntimes with mostly similar types of issues. The practitioners should expect\nthat for 10% of the time the generated code by the code generation tools\nwould be invalid. Moreover, they should test their code thoroughly to catch\nall possible cases that may cause the generated code to be invalid.\n5.2 Code Correctness (RQ1.2)\nIn our previous study, before we obtained our results, we hypothesized that code\ngeneration tools could either understand the user intent or not (Yetistiren et al., 2022).\nThis characteristic would result in a correct or incorrect code for a given problem.\nHowever, like our previous study, here as well; in contrast to a binary scenario\nfor code correctness where the generated solution is either correct or incorrect, code\ngeneration tools also generated partially correct solutions. As shown in Figure 6, for\n23.2% of the problems, GitHub Copilot generated partially correct code and in Figure\nTitle"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_021",
    "source_id": "CodeQualityComparison2023",
    "text": "\nall possible cases that may cause the generated code to be invalid.\n5.2 Code Correctness (RQ1.2)\nIn our previous study, before we obtained our results, we hypothesized that code\ngeneration tools could either understand the user intent or not (Yetistiren et al., 2022).\nThis characteristic would result in a correct or incorrect code for a given problem.\nHowever, like our previous study, here as well; in contrast to a binary scenario\nfor code correctness where the generated solution is either correct or incorrect, code\ngeneration tools also generated partially correct solutions. As shown in Figure 6, for\n23.2% of the problems, GitHub Copilot generated partially correct code and in Figure\nTitle Suppressed Due to Excessive Length\n19\ndef\ncompare_one(a, b):\n\"\"\"\nCreate a function\nthat\ntakes\nintegers , floats , or\nstrings\nrepresenting\nreal\nnumbers , and\nreturns\nthe\nlarger\nvariable\nin\nits\ngiven\nvariable\ntype. Return\nNone if the\nvalues\nare\nequal.\nNote: If a real\nnumber is\nrepresented\nas a string , the\nfloating\npoint\nmight be . or ,\ncompare_one (1, 2.5)\n--> 2.5\ncompare_one (1, \"2 ,3\")\n--> \"2 ,3\"\ncompare_one (\"5,1\", \"6\")\n--> \"6\"\ncompare_one (\"1\" , 1) --> None\n\"\"\"\nif a == b:\nreturn\nNone\nreturn\nmax(a, b)\nListing 2 Invalid Code Generation by GitHub Copilot (ID: 137)\ndef\nstring_to_md5 (text):\n\"\"\"\nGiven a string \u2019text \u2019, return\nits md5\nhash\nequivalent\nstring.\nIf \u2019text \u2019 is an empty\nstring , return\nNone.\n>>> string_to_md5 (\u2019Hello\nworld \u2019) ==\n\u20193\ne25960a79dbc69b674cd4ec67a72c62 \u2019\n\"\"\"\nif text == \u2019\u2019:\nreturn\nNone\nelse:\nreturn\nhashlib.md5(text).hexdigest ()\nListing 3 Invalid Code Generation by Amazon CodeWhisperer (ID: 162)\n7, it can be seen that 60.4% of the partially correct generations have correctness score\nabove 50.0%.\nAs it can be observed in Figure 8, for 40.2% of the problems, Amazon CodeWhisperer\ngenerated partially correct code and in Figure 9, it can be seen that 53.1% of the\npartially correct generations have correctness score above 50.0%.\nAs it can be observed in Figure 10, for 22.6% of the problems, ChatGPT generated\npartially correct code and in Figure 11, it can be seen that 63.2% of the partially correct\ngenerations have correctness score above 50.0%.\nTherefore, we argue that not only the entirely correct solutions should be considered\na success, but the partially correct solutions should also be taken into account. This\nis the case because usually in regular programming practices, it is seen that the first\niteration made on the written code is not correct. Still, over the next iterations, the code\nbecomes correct. Therefore, we argue that GitHub Copilot, Amazon Code"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_022",
    "source_id": "CodeQualityComparison2023",
    "text": ".0%.\nAs it can be observed in Figure 10, for 22.6% of the problems, ChatGPT generated\npartially correct code and in Figure 11, it can be seen that 63.2% of the partially correct\ngenerations have correctness score above 50.0%.\nTherefore, we argue that not only the entirely correct solutions should be considered\na success, but the partially correct solutions should also be taken into account. This\nis the case because usually in regular programming practices, it is seen that the first\niteration made on the written code is not correct. Still, over the next iterations, the code\nbecomes correct. Therefore, we argue that GitHub Copilot, Amazon CodeWhisperer,\nand ChatGPT are considerably successful in understanding user intent and generating\nthe correct code. Our standpoint is further elaborated on with an example in the\nfollowing paragraphs.\nFor example, in Listing 7, we give an example of partially correctly generated code\nby ChatGPT. The generated code by ChatGPT defines the bounds and the increment\n20\nBurak Yeti\u015ftiren et al.\ndef\nstrange_sort_list (lst):\n\u2019\u2019\u2019\nGiven\nlist of integers , return\nlist in\nstrange\norder. Strange\nsorting , is when\nyou\nstart\nwith\nthe\nminimum\nvalue , then\nmaximum\nof the\nremaining\nintegers , then\nminimum\nand so on.\nExamples:\nstrange_sort_list ([1, 2, 3, 4]) == [1, 4, 2, 3]\nstrange_sort_list ([5, 5, 5, 5]) == [5, 5, 5, 5]\nstrange_sort_list ([]) == []\n\u2019\u2019\u2019\nif not lst:\nreturn\nlst\nmin_val = min(lst)\nmax_val = max(lst)\nlst.remove(min_val)\nlst.remove(max_val)\nreturn [min_val] + [max_val] + strange_sort_list (lst)\nListing 4 Invalid Code Generation by Amazon CodeWhisperer (ID: 70)\nfrom\ntyping\nimport List , Tuple\ndef\nsum_product(numbers: List[int ]) -> Tuple[int , int ]:\n\"\"\" For a given\nlist of integers , return a tuple\nconsisting\nof a\nsum and a product\nof all the\nintegers\nin a list. Empty\nsum\nshould\nbe equal to 0 and\nempty\nproduct\nshould be equal to 1.\n\"\"\"\nif not\nnumbers:\nreturn (0, 1)\nelse:\nreturn (sum(numbers), 1)\nfor i in\nnumbers:\nresult\n*= i\nreturn (result , sum_result)\nListing 5 Invalid Code Generation by ChatGPT (ID: 8)\ndef\ncheck_dict_case (dict):\nif not\ndict:\nreturn\nFalse\nlower = all(key.islower () for key in dict)\nupper = all(key.isupper () for key in dict)\nreturn\nlower or upper\nListing 6 Invalid Code Generation by ChatGPT (ID: 95)\nof the for loop with (1). The code correctness score for this approach is 20%. However,\nif we change the bounds and the increment of the same for loop as shown in (2), the\ncode correctness score increases to 100%. In Listing 8,"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_023",
    "source_id": "CodeQualityComparison2023",
    "text": " in\nnumbers:\nresult\n*= i\nreturn (result , sum_result)\nListing 5 Invalid Code Generation by ChatGPT (ID: 8)\ndef\ncheck_dict_case (dict):\nif not\ndict:\nreturn\nFalse\nlower = all(key.islower () for key in dict)\nupper = all(key.isupper () for key in dict)\nreturn\nlower or upper\nListing 6 Invalid Code Generation by ChatGPT (ID: 95)\nof the for loop with (1). The code correctness score for this approach is 20%. However,\nif we change the bounds and the increment of the same for loop as shown in (2), the\ncode correctness score increases to 100%. In Listing 8, we present the code generated for\nproblem #54. The return statement marked with (1) is the Amazon CodeWhisperer\u2019s\ngeneration. This solution to the problem has a 57% code correctness score. The return\nTitle Suppressed Due to Excessive Length\n21\nFig. 13 Distribution of Correctly Generated Samples among the Code Generation Tools\nstatement marked with (2) is the correct approach for the solution to the problem. Such\nexamples show that, even with minor additional input from the programmer, the code\ncorrectness scores can be improved. We have only provided two examples for Amazon\nCodeWhisperer and ChatGPT; however, the examples for each code generation tool\ncan be diversified including GitHub Copilot.\ndef\nis_palindrome (string: str) -> bool:\n\"\"\"\nTest if given\nstring is a palindrome\n\"\"\"\nreturn\nstring == string [:: -1]\ndef\nmake_palindrome (string: str) -> str:\n\"\"\"\nFind\nthe\nshortest\npalindrome\nthat\nbegins\nwith a supplied\nstring.\nAlgorithm\nidea is simple:\n- Find\nthe\nlongest\npostfix\nof\nsupplied\nstring\nthat is a\npalindrome.\n- Append to the end of the\nstring\nreverse\nof a string\nprefix\nthat\ncomes\nbefore\nthe\npalindromic\nsuffix.\n\"\"\"\nif not\nstring:\nreturn\n\u2019\u2019\n(1) for i in range(len(string), 0,\n-1):\n(2) for i in range (0, len(string), 1):\nif\nis_palindrome (string[i:]):\nreturn\nstring + string [:i][:: -1]\nListing 7 Partially Correct Code Generation by ChatGPT (ID: 10)\nIn Listing 9, we exemplify our other finding about the code generation tools we have\nevaluated. The solutions to two functions (\u201ccommon\" and \u201cminPath\") are generated\n22\nBurak Yeti\u015ftiren et al.\ndef\nsame_chars(s0: str , s1: str):\n\"\"\"\nCheck if two\nwords\nhave\nthe\nsame\ncharacters .\n>>> same_chars(\u2019eabcdzzzz \u2019, \u2019dddzzzzzzzddeddabc \u2019)\nTrue\n>>> same_chars(\u2019abcd \u2019, \u2019dddddddabc \u2019)\nTrue\n>>> same_chars(\u2019dddddddabc \u2019, \u2019abcd \u2019)\nTrue\n>>> same_chars(\u2019eabcd \u2019, \u2019dddddddabc \u2019)\nFalse\n>>> same_chars(\u2019abcd \u2019, \u2019dddddddabce \u2019)\nFalse\n>>> same_chars(\u2019eabcdzzzz \u2019, \u2019dddzz"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_024",
    "source_id": "CodeQualityComparison2023",
    "text": "minPath\") are generated\n22\nBurak Yeti\u015ftiren et al.\ndef\nsame_chars(s0: str , s1: str):\n\"\"\"\nCheck if two\nwords\nhave\nthe\nsame\ncharacters .\n>>> same_chars(\u2019eabcdzzzz \u2019, \u2019dddzzzzzzzddeddabc \u2019)\nTrue\n>>> same_chars(\u2019abcd \u2019, \u2019dddddddabc \u2019)\nTrue\n>>> same_chars(\u2019dddddddabc \u2019, \u2019abcd \u2019)\nTrue\n>>> same_chars(\u2019eabcd \u2019, \u2019dddddddabc \u2019)\nFalse\n>>> same_chars(\u2019abcd \u2019, \u2019dddddddabce \u2019)\nFalse\n>>> same_chars(\u2019eabcdzzzz \u2019, \u2019dddzzzzzzzddddabc \u2019)\nFalse\n\"\"\"\n(1)\nreturn\nsorted(s0) == sorted(s1)\n(2)\nreturn\nset(s0) == set(s1)\nListing 8 Partially Correct Code Generation by Amazon CodeWhisperer (ID: 54)\nby GitHub Copilot. As it can be seen the prompt for the \u201ccommon\" function is more\nstraightforward, whereas the prompt for the \u201cminPath\" function is more complicated.\nSuch characteristics of these prompts are reflected in the code generation performance\nof GitHub Copilot; for the first function, a correct solution is generated, and for the\nlatter one GitHub Copilot failed to generate any algorithm. Similar to our previous\nfinding, this tendency can also be generalized to all of the code generation tools that\nwe evaluated.\nTo discuss the comparative code generation success of GitHub Copilot, Amazon\nCodeWhisperer, and ChatGPT, we created Figure 13. The Venn diagram in this figure\nshows us the correct code generation capabilities of the code generation tools more in-\ndepth, in comparison to the mere percentage values. With 36 unique problems, ChatGPT\nmanaged to generate the correct solution for more problems than GitHub Copilot and\nAmazon CodeWhisperer. This is followed by GitHub Copilot with 15 problems, and\ntwo problems with Amazon CodeWhisperer. Moreover, GitHub Copilot and ChatGPT\ngenerated correct solutions to 29 problems where Amazon CodeWhisperer failed; this\nnumber was 19 problems for the union of ChatGPT and Amazon CodeWhisperer.\nOur findings from this evaluation can also be supported by the percentage values.\nThese were 46.3% Code Correctness and 59.85% Average Code Correctness for GitHub\nCopilot; 31.1% Code Correctness and 51.95% Average Code Correctness for Amazon\nCodeWhisperer; and 65.2% Code Correctness and 78.1% Average Code Correctness for\nChatGPT.\nTitle Suppressed Due to Excessive Length\n23\ndef\ncommon(l1: list , l2: list):\n\"\"\" Return\nsorted\nunique\ncommon\nelements\nfor two\nlists.\n>>> common ([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121])\n[1, 5, 653]\n>>> common ([5, 3, 2, 8], [3, 2])\n[2, 3]\n\"\"\"\nresult = []\nfor i in l1:\nif"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_025",
    "source_id": "CodeQualityComparison2023",
    "text": "% Code Correctness and 78.1% Average Code Correctness for\nChatGPT.\nTitle Suppressed Due to Excessive Length\n23\ndef\ncommon(l1: list , l2: list):\n\"\"\" Return\nsorted\nunique\ncommon\nelements\nfor two\nlists.\n>>> common ([1, 4, 3, 34, 653, 2, 5], [5, 7, 1, 5, 9, 653, 121])\n[1, 5, 653]\n>>> common ([5, 3, 2, 8], [3, 2])\n[2, 3]\n\"\"\"\nresult = []\nfor i in l1:\nif i in l2 and i not in result:\nresult.append(i)\nreturn\nsorted(result)\n# #####################################################\ndef\nminPath(grid , k):\n\"\"\"\nGiven a grid\nwith N rows\nand N columns (N >= 2) and a positive\ninteger k, each\ncell of the\ngrid\ncontains a value. Every\ninteger\nin the\nrange [1, N * N] inclusive\nappears\nexactly\nonce on the\ncells of the\ngrid.\nYou\nhave to find\nthe\nminimum\npath of length k in the\ngrid. You\ncan\nstart\nfrom\nany cell , and in each\nstep\nyou can\nmove to any of\nthe\nneighbor\ncells , in other\nwords , you can go to cells\nwhich\nshare an edge\nwith\nyou\ncurrent\ncell. Please\nnote\nthat a path of\nlength k means\nvisiting\nexactly k cells (not\nnecessarily\ndistinct). You\nCANNOT go off the\ngrid. A path A (of length k) is\nconsidered\nless\nthan a path B (of length k) if after\nmaking\nthe\nordered\nlists of the\nvalues on the\ncells\nthat A and B go\nthrough (let \u2019s call\nthem\nlst_A\nand\nlst_B), lst_A is\nlexicographically\nless\nthan lst_B , in other\nwords , there\nexist\nan\ninteger\nindex i (1\n<= i <= k) such\nthat\nlst_A[i] < lst_B[i]\nand for any j (1\n<= j < i) we have\nlst_A[j] = lst_B[j]. It is\nguaranteed\nthat\nthe\nanswer is unique. Return an\nordered\nlist of\nthe\nvalues on the\ncells\nthat\nthe\nminimum\npath go\nthrough.\nExamples:\nInput: grid = [ [1,2,3], [4,5,6], [7,8,9]], k = 3\nOutput: [1, 2, 1]\nInput: grid = [ [5,9,3], [4,1,6], [7,8,2]], k = 1\nOutput: [1]\n\"\"\"\npass\nListing 9 Code Generation by GitHub Copilot for a Problem with Easier, and More\nComplicated Prompt\nFor better Code Correctness scores, continuous input from the practitioners\nis needed for all of the code generation tools we evaluated. Additionally, we\nhave found that the generated solutions for longer and more complex prompts\nfor the functions yielded lower Code Correct"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_026",
    "source_id": "CodeQualityComparison2023",
    "text": " = [ [1,2,3], [4,5,6], [7,8,9]], k = 3\nOutput: [1, 2, 1]\nInput: grid = [ [5,9,3], [4,1,6], [7,8,2]], k = 1\nOutput: [1]\n\"\"\"\npass\nListing 9 Code Generation by GitHub Copilot for a Problem with Easier, and More\nComplicated Prompt\nFor better Code Correctness scores, continuous input from the practitioners\nis needed for all of the code generation tools we evaluated. Additionally, we\nhave found that the generated solutions for longer and more complex prompts\nfor the functions yielded lower Code Correctness scores, in contrast to the\nfunctions that had simpler instructions contained in the prompt. For indi-\nvidual Code Correctness performances of the code generation tools, we have\nfound that ChatGPT was the most successful and Amazon CodeWhisperer\nwas the least successful tool. Practitioners that will potentially employ these\ntools should await similar results for the correctness of their code generated\nby these code generation tools.\n24\nBurak Yeti\u015ftiren et al.\n5.3 Code Security & Code Reliability & Code Maintainability (RQ1.3 & RQ1.4 &\nRQ1.5)\nCode Security: As explained in Section 4.3, we have seen no difference between\nthe generators in terms of Code Security. Moreover, the security rating of all of the\nproblems for each generator had the maximum rating, therefore per our results and our\nbenchmark dataset, we can say that the generators are equally successful in terms of\ngenerating secure code.\nCode Reliability: Our Code Reliability results were represented by the number of\nbugs observed in each sample, the severity of these particular bugs, and the estimated\ntime to solve them. In Figures 14 one of the bugs can be observed. The cause of this\nbug is the inconsistent usage of the \u2018if\u2019 statement, that the same expression is written\nunder different conditions. The other bugs (Problem #37 and #100) we have found\nfor GitHub Copilot have the same cause as the previous problem. In Figure 15, the\nonly bug we found by SonarQube for Amazon CodeWhisperer is shown. The cause of\nthis bug is the single iteration of a \u2018while\u2019 loop. The bugs for ChatGPT can be seen in\nFigures 16 and 17. One of these bugs was caused by the wrong indentation usage for\nthe \u2018return\u2019 statement and the other one was caused by the regular expression, that the\n\u02c6character has higher precedence and the expression would be anchored upon.\nThe vulnerabilities therefore should be taken into account by the potential users of\nthe code generation tools, and they might reflect some of the weaknesses of the code\ngenerators and be showing one of the possible directions of improvement. As we have\nargued, the generators have unique bugs, and the bugs do not correspond to the gener-\nated solution for the same problem by other generators. Therefore, we cannot prove\nthe superiority of a given generator to another regarding its reliable code generation\ncapabilities. From another perspective, regarding the estimated time to eliminate the\nbugs, we have seen that the most successful candidate was Amazon CodeWhisperer\nwith 5 minutes on"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_027",
    "source_id": "CodeQualityComparison2023",
    "text": " that the\n\u02c6character has higher precedence and the expression would be anchored upon.\nThe vulnerabilities therefore should be taken into account by the potential users of\nthe code generation tools, and they might reflect some of the weaknesses of the code\ngenerators and be showing one of the possible directions of improvement. As we have\nargued, the generators have unique bugs, and the bugs do not correspond to the gener-\nated solution for the same problem by other generators. Therefore, we cannot prove\nthe superiority of a given generator to another regarding its reliable code generation\ncapabilities. From another perspective, regarding the estimated time to eliminate the\nbugs, we have seen that the most successful candidate was Amazon CodeWhisperer\nwith 5 minutes on average, and the least successful candidate was GitHub Copilot, with\n15 minutes. The average estimated time to eliminate the bugs contained in the code\ngenerated by ChatGPT was 12.5 minutes. While from this perspective, there appears\nto be a ranking among the code generation tools, we believe that considering our\nprevious point too, a conclusion regarding the bug-free code generation capabilities of\nthese tools should not be made solely relying on the estimated time to eliminate the bugs.\nCode Maintainability: During our evaluation, we have seen some code smells among\nall the generators, varying in severity and resulting in a considerable amount of Techni-\ncal Debt. The exact results were shown in Table 2 and explained in Section 4.3. The\nthree most common issues were, improper naming of the function or variable, and high\ncognitive complexities.\nIn Figure 18, the instances of the code with high cognitive complexity and improper\nnaming of variables can be seen. This particular example was generated by GitHub\nCopilot; however, the smells in this code were also seen in some of the code generated\nby Amazon CodeWhisperer and ChatGPT. Figure 19 shows the other most common\ntype of smell, which is the improper naming of the function. There is also an additional\nsmell, which is the improper naming of a variable, which is named after a reserved word\nin Python. The naming of the function is found to be improper by SonarQube since it\nTitle Suppressed Due to Excessive Length\n25\nFig. 14 GitHub Copilot v1.70.8099 Bug in Problem #33\nFig. 15 Amazon CodeWhisperer Jan \u201923 Bug in Problem #102\nadopted the camel case approach, which should have been the Snake Case approach for\nPython.\nIn general, we have seen that our findings for Code Maintainability of the generated code\nby GitHub Copilot, Amazon CodeWhisperer, and ChatGPT, where we have seen some\nshortcomings were the most noteworthy ones. For Code Security and Code Reliability\nmetrics, we did not find significant results, which we could generalize to all the code\ngenerators, or show a given generator was performing better than the others. However,\nwe could generalize the Code Maintainability results; they allowed us to list all of the\ncode smells we observed using our benchmark dataset. Most importantly, apart from\nsome smells, we could see them in the code generated by all of the generators.\nCode that contains some bugs should be excepted by the practitioners of\nGitHub Copilot, Amazon CodeWhisperer, and ChatGPT. However, per our"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_028",
    "source_id": "CodeQualityComparison2023",
    "text": " CodeWhisperer, and ChatGPT, where we have seen some\nshortcomings were the most noteworthy ones. For Code Security and Code Reliability\nmetrics, we did not find significant results, which we could generalize to all the code\ngenerators, or show a given generator was performing better than the others. However,\nwe could generalize the Code Maintainability results; they allowed us to list all of the\ncode smells we observed using our benchmark dataset. Most importantly, apart from\nsome smells, we could see them in the code generated by all of the generators.\nCode that contains some bugs should be excepted by the practitioners of\nGitHub Copilot, Amazon CodeWhisperer, and ChatGPT. However, per our\nresults, they are not as common as the code smells, which we have observed\nfor all code generators. Practicioners should note that if their code contains\nsome smells, the average time to solve them is 9.1 minutes for GitHub Copilot,\n5.6 minutes for Amazon CodeWhisperer, and 8.9 minutes for ChatGPT. In\nterms of Code Security, our results showed that the practitioners should await\nto get secure code from the generators.\n26\nBurak Yeti\u015ftiren et al.\nFig. 16 ChatGPT 9 Jan \u201923 version Bug in Problem #8\nFig. 17 ChatGPT 9 Jan \u201923 version Bug in Problem #141\nFig. 18 GitHub Copilot v1.70.8099 Smells (3) in Problem #106\nTitle Suppressed Due to Excessive Length\n27\nFig. 19 ChatGPT 9 Jan \u201923 version Smells (2) in Problem #66\nFig. 20 Code Correctness Score Distribution of the Problems for Different Experiments -\nGitHub Copilot v1.70.8099\n5.4 Using only Function Names and Parameters Without Prompt (RQ2)\nAccording to the results presented in Section 4.4, we observed a significant drop in\nboth the code validity and code correctness metrics. For example, the code validity\ndropped by 13.5% for GitHub Copilot, 12.2% for Amazon CodeWhisperer, and 16.5%\nfor ChatGPT after the docstrings were removed from the problems. Similarly, the\ncode correctness score dropped by 26.2%, 16.5%, and 43.2% respectively. These results\nreflected a general performance drop affecting the validity and the correctness of the\ncode. From this, we argue that the code generation performance of code generation\ntools correlates with the input explanation. In Figures 20, 21, and 22 the distribution\n28\nBurak Yeti\u015ftiren et al.\nFig. 21 Code Correctness Score Distribution of the Problems for Different Experiments -\nAmazon CodeWhisperer Jan \u201923\nFig. 22 Code Correctness Score Distribution of the Problems for Different Experiments -\nChatGPT 9 Jan \u201923 Version\nTitle Suppressed Due to Excessive Length\n29\nfor the code correctness scores of the code generation tools are visualized. Through\nthese distributions, it is trivial to discern that the correct code generation capabilities\nof the tools tend to be affected negatively. This is demonstrated by the cumulation of\nproblems on the lower half part of the distributions, and the dropped"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_029",
    "source_id": "CodeQualityComparison2023",
    "text": " Figures 20, 21, and 22 the distribution\n28\nBurak Yeti\u015ftiren et al.\nFig. 21 Code Correctness Score Distribution of the Problems for Different Experiments -\nAmazon CodeWhisperer Jan \u201923\nFig. 22 Code Correctness Score Distribution of the Problems for Different Experiments -\nChatGPT 9 Jan \u201923 Version\nTitle Suppressed Due to Excessive Length\n29\nfor the code correctness scores of the code generation tools are visualized. Through\nthese distributions, it is trivial to discern that the correct code generation capabilities\nof the tools tend to be affected negatively. This is demonstrated by the cumulation of\nproblems on the lower half part of the distributions, and the dropped mean and median\nvalues. From this, we argue that the lack of a proper explanation of the problems yields\nlower validity and correctness scores. Therefore, in practical usage, practitioners should\npay attention to providing instructions for the code they tend to write to the tools.\nThere were some cases, where we did not see any decrease in correctness or validity\nscores. For GitHub Copilot, such problems constituted 82.3% of the dataset for code\nvalidity and 45.1% for code correctness. For Amazon CodeWhisperer, in 84.1% of the\nproblems for code validity and 63.4% for code correctness, we did not observe a decrease.\nFor ChatGPT, the scores did not decrease for 78.7% of the dataset for code validity\nand 51.2% for code correctness. We have seen such cases mostly for problems that\ninclude substring search, value manipulations in an array, and character comparison.\nAdditionally, the names of such functions, accompanied by parameter names were\nself-explanatory, which means that GitHub Copilot could still make interpretations\nabout the function without requiring more details.\nFor the cases where the code correctness and validity scores dropped, we observed\nthat these problems were more complicated. When we examined where the success\nrate of code generation tools dropped, we observed cases where the function name and\nthe parameters alone failed to give details. This means that the name and parameters\nalone are not informative enough to give details about such functions. For example,\nin one case, a function called \u201cwill_it_fly\u201d only has two parameters called \u2018q\u2019 and\n\u2018w\u2019. Amazon CodeWhisperer and ChatGPT generated the correct code in our original\nexperiment where we used the function name and prompt but after removing the\nfunction explanation from the input, they were not able to generate valid code. To be\nmore precise, ChatGPT could not generate any code at all. The purpose of the function\nwas to check if \u2018q\u2019 was a palindromic list and if the sum of the elements in the list was\nless than \u2018w\u2019. The generations of Amazon CodeWhisperer with and without prompt\ncan be seen in Listing 10.\nWhen using code generation tools, it is crucial to provide clear and accurate\nproblem descriptions to obtain valid and correct code. Whenever possible,\nprogrammers should include a comprehensive explanation of the problem,\nalong with sample unit tests in the form of docstrings, comments, or other\nforms of documentation during the solution generation process.\n5.5 Using Dummy Function Names (RQ3)\nAccording to the results in Section 4.5, we did not"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_030",
    "source_id": "CodeQualityComparison2023",
    "text": " all. The purpose of the function\nwas to check if \u2018q\u2019 was a palindromic list and if the sum of the elements in the list was\nless than \u2018w\u2019. The generations of Amazon CodeWhisperer with and without prompt\ncan be seen in Listing 10.\nWhen using code generation tools, it is crucial to provide clear and accurate\nproblem descriptions to obtain valid and correct code. Whenever possible,\nprogrammers should include a comprehensive explanation of the problem,\nalong with sample unit tests in the form of docstrings, comments, or other\nforms of documentation during the solution generation process.\n5.5 Using Dummy Function Names (RQ3)\nAccording to the results in Section 4.5, we did not observe a dramatic change in the\ncode validity scores, while there was a drop in the code correctness scores. For example,\nthe code validity increased by 2.4% for GitHub Copilot and dropped by 0.6% for\nAmazon CodeWhisperer and ChatGPT after the function names became \u2018foo\u2019. The code\ncorrectness score dropped by 4.2%, 3.7%, and 3.6%, respectively. These results reflected\na general performance affecting the validity did not change much but it dropped for the\ncorrectness of the code. From this, we argue that the code generation performance of\ncode generation tools correlates with the input explanation. Figures 20, 21, and 22 show\nour point visually as well. However, as one may notice, the change in the distributions,\n30\nBurak Yeti\u015ftiren et al.\n1 # Code\ngeneration\nof\nAmazon\nCodeWhisperer\nwith\nprompt\n2 def\nwill_it_fly(q,w):\n3\n\u2019\u2019\u2019\n4\nWrite a function\nthat\nreturns\nTrue if the\nobject q will fly , and\nFalse\notherwise. The\nobject q will\nfly if it\u2019s balanced (it is\na palindromic\nlist) and the sum of its\nelements\nis less\nthan or\nequal\nthe\nmaximum\npossible\nweight w.\n5\n6\nExample:\n7\nwill_it_fly ([1, 2], 5)\n--> False\n8\n# 1+2 is less\nthan\nthe\nmaximum\npossible\nweight , but it\u2019s\nunbalanced.\n9\n10\nwill_it_fly ([3, 2, 3], 1)\n--> False\n11\n# it\u2019s balanced , but\n3+2+3 is more\nthan\nthe\nmaximum\npossible\nweight.\n12\n13\nwill_it_fly ([3, 2, 3], 9)\n--> True\n14\n# 3+2+3 is less\nthan\nthe\nmaximum\npossible\nweight , and it\u2019s\nbalanced.\n15\n16\nwill_it_fly ([3] , 5) --> True\n17\n# 3 is less\nthan\nthe\nmaximum\npossible\nweight , and it\u2019s balanced.\n18\n\u2019\u2019\u2019\n19\nreturn\nsum(q) <= w and q == q[:: -1]\n20\n21\n22 # Code\ngeneration\nof\nAmazon\nCodeWhisperer\nwithout\nprompt\n23 def\nwill_it_fly(q, w):\n24\nif q == 0:\n25\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_031",
    "source_id": "CodeQualityComparison2023",
    "text": ", 2, 3], 9)\n--> True\n14\n# 3+2+3 is less\nthan\nthe\nmaximum\npossible\nweight , and it\u2019s\nbalanced.\n15\n16\nwill_it_fly ([3] , 5) --> True\n17\n# 3 is less\nthan\nthe\nmaximum\npossible\nweight , and it\u2019s balanced.\n18\n\u2019\u2019\u2019\n19\nreturn\nsum(q) <= w and q == q[:: -1]\n20\n21\n22 # Code\ngeneration\nof\nAmazon\nCodeWhisperer\nwithout\nprompt\n23 def\nwill_it_fly(q, w):\n24\nif q == 0:\n25\nreturn \"NO\"\n26\nelif q == 1:\n27\nreturn \"YES\"\n28\nelse:\n29\nreturn\nwill_it_fly (q-w, w)\nListing 10 Generated Code of Amazon CodeWhisperer for the Example Problem (ID: 72)\nin other words, the cumulation of the problems on the lower half, and the drop of the\nmean and median values were not as radical as we observed them for the experiments\nwith only function names. Hence, we argue that while providing the proper function\nnames is crucial itself, the effect of not providing a proper explanation is more drastic.\nAccording to the majority of our results, the absence of clear and descriptive function\nnames is one of the primary causes of the decline in code correctness and average code\ncorrectness. To illustrate, as shown in Listing 11, in problem #79 it is asked to convert\nthe given decimal number to binary and add \u2018db\u2019 to both the start and end of the string\nand return it. In the original experiment, its name was \u2018decimal_to_binary\u2019, which\nis a highly descriptive name considering the purpose of the function. Hence, Amazon\nCodeWhisperer resulted in 100% correctness in the original setup. When we eliminated\nthe function name, we observed that the solution to the problem converted to an invalid\none. On the other hand, GitHub Copilot and ChatGPT successfully solved this problem\nin both setups. When we considered problem #1, it asked to separate the parenthesis\ngroups given in one string. The method should find the nested groups and separate\nthem. The function\u2019s name was \u2018separate_paren_groups\u2019 in the original setup and it\nTitle Suppressed Due to Excessive Length\n31\n1 def\ndecimal_to_binary (decimal):\n2\n\"\"\" You\nwill be given a number in\ndecimal\nform\nand\nyour\ntask is\nto\nconvert\nit to binary\nformat. The\nfunction\nshould\nreturn a\nstring , with\neach\ncharacter\nrepresenting a binary\nnumber. Each\ncharacter\nin the\nstring\nwill be\n\u20190\u2019 or\n\u20191\u2019. There\nwill be an\nextra\ncouple\nof\ncharacters\n\u2019db\u2019 at the\nbeginning\nand at the end\nof the\nstring. The\nextra\ncharacters\nare\nthere to help\nwith\nthe\nformat.\n3\nExamples:\n4\ndecimal_to_binary (15)\n# returns \"db1111db\"\n5\ndecimal_to_binary (32)\n# returns \"db100000db\"\n6\n\"\"\"\n7\n8\n#Result of Amazon\nCodeWhisperer"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_032",
    "source_id": "CodeQualityComparison2023",
    "text": "format. The\nfunction\nshould\nreturn a\nstring , with\neach\ncharacter\nrepresenting a binary\nnumber. Each\ncharacter\nin the\nstring\nwill be\n\u20190\u2019 or\n\u20191\u2019. There\nwill be an\nextra\ncouple\nof\ncharacters\n\u2019db\u2019 at the\nbeginning\nand at the end\nof the\nstring. The\nextra\ncharacters\nare\nthere to help\nwith\nthe\nformat.\n3\nExamples:\n4\ndecimal_to_binary (15)\n# returns \"db1111db\"\n5\ndecimal_to_binary (32)\n# returns \"db100000db\"\n6\n\"\"\"\n7\n8\n#Result of Amazon\nCodeWhisperer\nin the\noriginal\nexperiment\n9\nreturn\n\u2019db\u2019 + bin(decimal)[2:] + \u2019db\u2019\n10\n11\n#Result of Amazon\nCodeWhisperer\nin the\ndummy\nexperiment\n12\nreturn\nsum ([1 for i in num if i in \"2357 BCDE\"])\nListing 11 Generated Codes for the Example Problem (ID: 79)\ncan be seen in Listing 12. While ChatGPT had 100% correctness in the original setup,\nit resulted in 0% correctness in the dummy function name setup, which suggests the\nimportance of the proper function name for the code generation tools.\nConsidering the results given in Table 4, even though the code validity scores are\nclose, the code correctness scores decreased. It can be concluded that changing the\nmeaningful function names with dummy function names reduced the performance of\nCopilot for most of the problems. It can be stated that generally changing meaningful\nfunction names to dummy function names affects the performance of GitHub Copilot\nnegatively compared to the original experiment where we use both meaningful function\nnames and prompts.\nSelecting a meaningful name for a function can significantly improve the\nperformance of code generation tools in generating accurate code. It is\nimportant for practitioners to assign clear and descriptive names to functions.\nHowever, our findings suggest that providing thorough explanations for\nfunctions is even more critical than giving them meaningful names. Ideally,\nboth practices should be employed to produce the most accurate and valid\ncode possible.\n5.6 Evaluation of Code Generation Tools Over Time (RQ4)\nThe results mentioned in Section 4.6 demonstrate that both GitHub Copilot and Amazon\nCodeWhisperer increased the number of correct code suggestions on their new versions.\nAs it can be observed from Figure 23 and 25, GitHub Copilot v1.70.8099 had passed\nsuggestions for 46.3% unit tests and average correctness for 59.9% of all problems,\nwhile GitHub Copilot v1.7.4421 had passed suggestions for 28.7% the unit tests and\naverage correctness for 53.6% of all problems. This data indicates that there is a notable\nimprovement of 62% in GitHub Copilot\u2019s performance in terms of passed suggestions\nfor unit tests and there is an improvement of 12% in the average correctness results.\n32\nBurak Yeti\u015ftiren et al.\n1 from\ntyping\nimport\nList\n2 def\nseparate_paren_groups ( paren_string : str) -> List[str ]:\n3\n\"\"\"\nInput to this\nfunction\nis a string\ncontaining\nmultiple\ngroups\nof\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_033",
    "source_id": "CodeQualityComparison2023",
    "text": "9% of all problems,\nwhile GitHub Copilot v1.7.4421 had passed suggestions for 28.7% the unit tests and\naverage correctness for 53.6% of all problems. This data indicates that there is a notable\nimprovement of 62% in GitHub Copilot\u2019s performance in terms of passed suggestions\nfor unit tests and there is an improvement of 12% in the average correctness results.\n32\nBurak Yeti\u015ftiren et al.\n1 from\ntyping\nimport\nList\n2 def\nseparate_paren_groups ( paren_string : str) -> List[str ]:\n3\n\"\"\"\nInput to this\nfunction\nis a string\ncontaining\nmultiple\ngroups\nof\nnested\nparentheses. Your\ngoal is to\nseparate\nthose\ngroup\ninto\nseparate\nstrings\nand\nreturn\nthe\nlist of those.\nSeparate\ngroups\nare\nbalanced (each\nopen\nbrace is\nproperly\nclosed\n) and not\nnested\nwithin\neach\nother\nIgnore\nany\nspaces in the\ninput\nstring.\n4\n\"\"\"\n5\n6\n7\n#Result of\nChatGPT\nin the\noriginal\nexperiment\n8\ngroups = []\n9\ncurrent_group = []\n10\nbalance = 0\n11\nfor ch in\nparen_string :\n12\nif ch == \u2019 \u2019:\n13\ncontinue\n14\nelif ch == \u2019(\u2019:\n15\nbalance\n+= 1\n16\ncurrent_group .append(ch)\n17\nelif ch == \u2019)\u2019:\n18\nbalance\n-= 1\n19\ncurrent_group .append(ch)\n20\nif\nbalance\n== 0:\n21\ngroups.append(\u2019\u2019.join( current_group ))\n22\ncurrent_group = []\n23\nreturn\ngroups\n24\n25\n#Result of\nChatGPT\nin the\ndummy\nexperiment\n26\nparen_string = paren_string .replace(\" \", \"\")\n27\ngroups = []\n28\nstack = []\n29\nstart = 0\n30\nfor i, char in\nenumerate( paren_string ):\n31\nif char == \"(\":\n32\nstack.append(i)\n33\nelif\nchar == \")\":\n34\nif len(stack) > 0:\n35\nstart = stack.pop ()\n36\ngroups.append( paren_string [start:i+1])\n37\nreturn\ngroups\nListing 12 Generated Codes for the Example Problem (ID: 01)\nFigure 24 - 26 provides that Amazon CodeWhisperer had passed solutions for 24.4%\nof all unit tests and average correctness for 36.5% of all problems in November 2022,\nwhile it provided passed solutions for 31.1% of all problems and average correctness for\n52% of all problems in January 2023. These results demonstrate an improvement of\n28% in terms of passed suggestions of unit tests and there is an improvement of 42% in\nthe average correctness results of Amazon CodeWhisperer.\nThe new version of GitHub Copilot enhanced the correctness of 65% of the partially\ncorrect recommendations from the previous version, according to our analysis of the\nfindings. We also noticed that 9 out of the 33 incorrect answers from the previous\nversion of GitHub Copilot were better in the new version. GitHub Copilot generally\nsuggested code that was close to the"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_034",
    "source_id": "CodeQualityComparison2023",
    "text": " 2022,\nwhile it provided passed solutions for 31.1% of all problems and average correctness for\n52% of all problems in January 2023. These results demonstrate an improvement of\n28% in terms of passed suggestions of unit tests and there is an improvement of 42% in\nthe average correctness results of Amazon CodeWhisperer.\nThe new version of GitHub Copilot enhanced the correctness of 65% of the partially\ncorrect recommendations from the previous version, according to our analysis of the\nfindings. We also noticed that 9 out of the 33 incorrect answers from the previous\nversion of GitHub Copilot were better in the new version. GitHub Copilot generally\nsuggested code that was close to the prior version. Minor but substantial changes were\nTitle Suppressed Due to Excessive Length\n33\n1 def\nsolve(s):\n2\n\"\"\" You are\ngiven a string s. if s[i] is a letter , reverse\nits\ncase\nfrom\nlower to upper or vise versa , otherwise\nkeep it as it\nis. If the\nstring\ncontains\nno letters , reverse\nthe\nstring. The\nfunction\nshould\nreturn\nthe\nresulted\nstring.\n3\nExamples\n4\nsolve (\"1234\") = \"4321\"\n5\nsolve (\"ab\") = \"AB\"\n6\nsolve (\"# a@C \") = \"# A@c\"\n7\n\"\"\"\n8\n9\n10 #Result of the old\nversion\nof Github\nCopilot\n11\nreturn\n\u2019\u2019.join(c.lower () if c.isupper () else c.upper () for c in\ns) or s[:: -1]\n12\n13 #Result of the new\nversion\nof Github\nCopilot\n14\nif not any(c.isalpha () for c in s):\n15\nreturn s[:: -1]\n16\nreturn\n\u2019\u2019.join(c.swapcase () if c.isalpha () else c for c in s)\nListing 13 Generated Code for the Example Problem (ID: 161)\nFig. 23 GitHub Copilot Improvement in Code Correctness\nmade, though, to enhance the recommendation\u2019s accuracy. To illustrate, The Listing\n13 shows the code recommendations of two versions of GitHub Copilot for the same\nproblem. The problem #161 in the experiment where the input for the function is a\nstring, expected to swap the case of the letters in the string if there are any. If there are\nno letters in the string, the function should only return the reversed string. The previous\n34\nBurak Yeti\u015ftiren et al.\nFig. 24 Amazon CodeWhisperer Improvement in Code Correctness\nFig. 25 GitHub Copilot Improvement in Average Code Correctness\nTitle Suppressed Due to Excessive Length\n35\nFig. 26 Amazon CodeWhisperer Improvement in Average Code Correctness\nversion of GitHub Copilot offered an incorrect recommendation for this problem since\nthe expression related to reversing the string was not located appropriately within the\nfunction. In the new version of GitHub Copilot, we observed that although its suggestion\nwas generally very similar to the previous one, it placed the swapping expression in the\ncorrect location in the function and this modification caused an increase in correctness\nfrom 75% to 100%.\nAdditionally, we performed a comparison of the outcomes between the two different\nAmazon CodeWhisperer versions"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_035",
    "source_id": "CodeQualityComparison2023",
    "text": " Correctness\nFig. 25 GitHub Copilot Improvement in Average Code Correctness\nTitle Suppressed Due to Excessive Length\n35\nFig. 26 Amazon CodeWhisperer Improvement in Average Code Correctness\nversion of GitHub Copilot offered an incorrect recommendation for this problem since\nthe expression related to reversing the string was not located appropriately within the\nfunction. In the new version of GitHub Copilot, we observed that although its suggestion\nwas generally very similar to the previous one, it placed the swapping expression in the\ncorrect location in the function and this modification caused an increase in correctness\nfrom 75% to 100%.\nAdditionally, we performed a comparison of the outcomes between the two different\nAmazon CodeWhisperer versions. According to our research, 50% of the partly correct\nsuggestions made by the prior version of Amazon CodeWhisperer were made more\ncorrectly by the new version. In addition, we discovered that 46 out of the 74 incorrect\nsuggestions made by the prior version of Amazon CodeWhisperer were made correctly\nby the new version. In most cases where the new version of Amazon CodeWhisperer\nimproved upon the previous version, we observed that the previous version frequently\nreturned default values such as empty list, zero, false, or empty string. Although these\nrecommendations sometimes produced correct results with some unit tests, they resulted\nin a low rate of partial correctness. With the updated Amazon CodeWhisperer, this\nproblem was largely resolved, and the updated version offered more logical and precise\nanswers.\nAdditionally, we noticed that 10 out of 14 incorrect answers from the previous\nversion of GitHub Copilot were removed in the new version. These incorrect answers\nwere typically the result of syntactical errors, excess of maximum recursion depth, or\nthe usage of modules that are not imported. For example, the Listing 14 shows that\nthe old version of GitHub Copilot recommended an unsuccessful recursion function\nwithout a base case for the problem #123, which leads to a compilation error. As a\n36\nBurak Yeti\u015ftiren et al.\nresult of introducing a base case to the code, the new version of GitHub Copilot was\nable to fix this error completely. As another example, while the old version of GitHub\nCopilot did not add import the \u2018reduce\u2019 module, the new version of GitHub Copilot\nrecommended a different solution without using the \u2018reduce\u2019 module, which resulted\nin 100% correctness. However, we found that almost none of the invalid suggestions\nmade by the earlier version of Amazon CodeWhisperer changed their status in the new\nversion when we analyzed the validity rate for both versions.\n1 def\nget_odd_collatz (n):\n2\n\"\"\"\n3\nGiven a positive\ninteger n, return a sorted\nlist\nthat\nhas the\nodd\nnumbers\nin\ncollatz\nsequence. The\nCollatz\nconjecture\nis a\nconjecture\nin\nmathematics\nthat\nconcerns a sequence\ndefined\nas\nfollows: start\nwith\nany\npositive\ninteger n. Then\neach\nterm is\nobtained\nfrom\nthe\nprevious\nterm as\nfollows: if the\nprevious\nterm\nis even , the\nnext\nterm is one\nhalf of the\nprevious\nterm. If the\nprevious\n"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_036",
    "source_id": "CodeQualityComparison2023",
    "text": " rate for both versions.\n1 def\nget_odd_collatz (n):\n2\n\"\"\"\n3\nGiven a positive\ninteger n, return a sorted\nlist\nthat\nhas the\nodd\nnumbers\nin\ncollatz\nsequence. The\nCollatz\nconjecture\nis a\nconjecture\nin\nmathematics\nthat\nconcerns a sequence\ndefined\nas\nfollows: start\nwith\nany\npositive\ninteger n. Then\neach\nterm is\nobtained\nfrom\nthe\nprevious\nterm as\nfollows: if the\nprevious\nterm\nis even , the\nnext\nterm is one\nhalf of the\nprevious\nterm. If the\nprevious\nterm is odd , the\nnext\nterm is 3 times\nthe\nprevious\nterm\nplus 1. The\nconjecture\nis that no matter\nwhat\nvalue of n,\nthe\nsequence\nwill\nalways\nreach 1.\n4\nNote:\n5\n1.\nCollatz (1) is [1]. 2.\nreturned\nlist\nsorted in\nincreasing\norder.\n6\nFor\nexample:\n7\nget_odd_collatz (5)\nreturns [1, 5] # The\ncollatz\nsequence\nfor 5\nis [5, 16, 8, 4, 2, 1], so the odd\nnumbers\nare\nonly 1, and 5.\n8\n\"\"\"\n9\n10\n11\n#Result of old\nversion\nof GitHub\nCopilot\nin the\noriginal\nexperiment\n12\nreturn [n] + get_odd_collatz (3 * n + 1) if n % 2 == 1 else\nget_odd_collatz (n // 2)\n13\n14\n#Result of new\nversion\nof GitHub\nCopilot\nin the\noriginal\nexperiment\n15\nreturn\nsorted ([i for i in\ncollatz(n) if i%2])\n16 def\ncollatz(n):\n17\nif n == 1:\n18\nreturn\n[1]\n19\nif n%2:\n20\nreturn [n] + collatz (3*n+1)\n21\nreturn [n] + collatz(n//2)\nListing 14 Generated Code for the Example Problem (ID: 123)\nOn the other hand, our research showed that 17 out of the 47 correct answers\nprovided by the previous version of GitHub Copilot had incorrect or only partly correct\nanswers in the new version. The outcomes also showed that of the 84 partial correct\nanswers in the old GitHub Copilot version, 19 of them had reduced percentages of\ncorrectness in the new GitHub Copilot version. When we examined the answers from\nthe two versions of GitHub Copilot, we found that the new version of GitHub Copilot\nfrequently recommended \u00b4pass\u2019, which reduced the correctness of the answer. We\nobserved that GitHub Copilot sometimes answered the problem nearly correctly, and\nsometimes it only answered as \u2018pass\u2019 to the same problem. These issues may be related\nTitle Suppressed Due to Excessive Length\n37\nto the undetermined nature of GitHub Copilot. We also noticed that some problems\nwere resolved in the previous version of GitHub Copilot with the correct solutions. Still,\nin the new version of GitHub Copilot,"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_037",
    "source_id": "CodeQualityComparison2023",
    "text": " 19 of them had reduced percentages of\ncorrectness in the new GitHub Copilot version. When we examined the answers from\nthe two versions of GitHub Copilot, we found that the new version of GitHub Copilot\nfrequently recommended \u00b4pass\u2019, which reduced the correctness of the answer. We\nobserved that GitHub Copilot sometimes answered the problem nearly correctly, and\nsometimes it only answered as \u2018pass\u2019 to the same problem. These issues may be related\nTitle Suppressed Due to Excessive Length\n37\nto the undetermined nature of GitHub Copilot. We also noticed that some problems\nwere resolved in the previous version of GitHub Copilot with the correct solutions. Still,\nin the new version of GitHub Copilot, these problems resulted in compilation errors,\nwhich were generally because of forgetting to add an import to the modules.\nOur research also showed that while the earlier version of Amazon CodeWhisperer\noffered entirely correct suggestions, the new version provided partial or incorrect answers\nfor 16 of the 40 problems. Additionally, our research revealed that the new version of\nAmazon CodeWhisperer had reduced correctness rates for 20 of the 50 partly correct\nsolutions offered by the prior version. We noticed that the following were the main\nreasons why code recommendations in the new edition were unsuccessful: Firstly, the\ntool occasionally suggested using a pass statement only, which is an incorrect response\nto the problem at hand. Second, as was already stated, it occasionally returned default\nnumbers that did not fulfill the required solution. Thirdly, it proposed a single-line\nreturn expression that was complicated and often failed to produce correct results. For\ninstance, Listing 15 presents that problem #155 wanted the function should take a\nnumber as an input parameter, and the function should return a tuple that has the\nnumber of even and odd digits, respectively. The new Amazon CodeWhisperer version\nfor this problem produced a single-line but complicated return statement which gave\na syntactic error for negative numbers. Despite having a long solution, the previous\nversion of Amazon CodeWhisperer had an 85% correctness rate.\n1 def\neven_odd_count (num):\n2\n\"\"\" Given an\ninteger. return a tuple\nthat\nhas the\nnumber of even\nand odd\ndigits\nrespectively .\n3\nExample:\n4\neven_odd_count ( -12) ==> (1, 1)\n5\neven_odd_count (123) ==> (1, 2)\n6\n\"\"\"\n7\n8\n9\n#Result of old\nversion\nof Amazon\nCodeWhisperer\nin the\noriginal\nexperiment\n10\neven = 0\n11\nodd = 0\n12\nif num < 0:\n13\nnum = -num\n14\nwhile\nnum > 0:\n15\nif num % 2 == 0:\n16\neven += 1\n17\nelse:\n18\nodd += 1\n19\nnum = num // 10\n20\nreturn\neven , odd\n21\n22\n#Result of new\nversion\nof Amazon\nCodeWhisperer\nin the\noriginal\nexperiment\n23\nreturn (len ([i for i in str(num) if int(i)%2==0]) , len ([i for i\nin str(num) if int(i)%2]))\nListing 15 Generated Code for the Example Problem ("
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_038",
    "source_id": "CodeQualityComparison2023",
    "text": "\nodd = 0\n12\nif num < 0:\n13\nnum = -num\n14\nwhile\nnum > 0:\n15\nif num % 2 == 0:\n16\neven += 1\n17\nelse:\n18\nodd += 1\n19\nnum = num // 10\n20\nreturn\neven , odd\n21\n22\n#Result of new\nversion\nof Amazon\nCodeWhisperer\nin the\noriginal\nexperiment\n23\nreturn (len ([i for i in str(num) if int(i)%2==0]) , len ([i for i\nin str(num) if int(i)%2]))\nListing 15 Generated Code for the Example Problem (ID: 155)\n38\nBurak Yeti\u015ftiren et al.\nGitHub Copilot\u2019s new version had 62% more passed-unit tests than its older\nversion. Similarly, Amazon CodeWhisperer\u2019s updated version resulted in 28%\nmore passed-unit tests than its previous version, suggesting that both tools\nhave notable improvements.\n6 Threats to Validity\n6.1 Conclusion Validity\nTrivial Solutions: In some problems, code generators generated solutions to return\nsimple statements like empty arrays or Boolean values. In this case, if there are test\ncases related to the problem, where such expressions are the desired output, those test\ncases pass by chance without any algorithm generated for the problem.\nNumber of test cases: The varying amount of test cases for the dataset may introduce\na threat to our experiment. On average there are 7.7 test cases for each problem in the\nHumanEval dataset Chen et al. (2021). Having broader test cases, both for the amount\nand the scope can be important. By extending the test cases, any potential corner case\nthat could be missed may be covered. This can be critical especially when some corner\ncases for a given problem are not involved. We plan to improve the test cases both in\nquantity and quality in our future work.\nSonarQube: We used the SonarQube code inspector to obtain results for our code\nsecurity, code maintainability, and code reliability metrics. But in our results for them,\nthere was scarce information about the possible vulnerabilities for the generated code\nin each sample, which would be discovered by SonarQube. We believe that due to the\nextent of the problems that are contained in the HumanEval Dataset, the solutions to\nthose problems consist of a small number of lines. The case could also be observed in\nthe canonical solutions provided with each question.\n6.2 Internal Validity\nOne-shot code generation: While generating code with the code generators, we used\nthe function names, parameters, the corresponding docstring containing an explanation\nof the function, and a few instances of tests for that function. Furthermore, we did not\nwrite any code to provide additional information to the code generators which would\nclarify more what our intent for that particular problem is. Therefore, in most cases,\nthe success rate could be increased if we have given hints as code snippets to the code\ngenerators.\nReproduction of the Generations: While conducting our experiments, we observed\nthat the code generators had a nondeterministic characteristic, hence they were gener-\nating different outputs for the same input in different trials. We paid great attention\nnot to include different outputs for the same input by"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_039",
    "source_id": "CodeQualityComparison2023",
    "text": " code generators, we used\nthe function names, parameters, the corresponding docstring containing an explanation\nof the function, and a few instances of tests for that function. Furthermore, we did not\nwrite any code to provide additional information to the code generators which would\nclarify more what our intent for that particular problem is. Therefore, in most cases,\nthe success rate could be increased if we have given hints as code snippets to the code\ngenerators.\nReproduction of the Generations: While conducting our experiments, we observed\nthat the code generators had a nondeterministic characteristic, hence they were gener-\nating different outputs for the same input in different trials. We paid great attention\nnot to include different outputs for the same input by generating code for our problems\nin one iteration, and saving the generated code, then conducting our evaluation on\nthe saved code. Given that the code generators have a dynamic characteristic that the\nunderlying LLM of the tools is being retrained, our results might not be fully replicated\nTitle Suppressed Due to Excessive Length\n39\ngiven our experimental setup and input.\nCode Generation Methods: With GitHub Copilot and Amazon CodeWhisperer,\none can use two different approaches to generate code. For GitHub Copilot, the first\none happens automatically as a programmer proceeds to write code, GitHub Copi-\nlot suggests code snippets that might fit into that context. In the other approach,\nwhenever the programmer wants to generate code, they press the \u2018ctrl + enter\u2019 key\ncombination to see up to 10 code generations GitHub Copilot produces. Similar to\nGitHub Copilot, the default approach for Amazon CodeWhisperer is also automatically\ngenerating code when prompted. The other approach in Amazon CodeWhisperer is\nto use the \u2018option + c\u2019 (Mac) / \u2018alt + c\u2019 (PC) key combination. In our experiment,\nwe chose the first approach whenever possible, otherwise, we implemented the second\napproach and selected the suggestion at the top of the list. There were some problems\nwhere the generators failed to generate any code after we entered the next line (af-\nter the user presses the \u2018enter\u2019 key and continues from the next line). Therefore, we\nhad to apply the key combinations to see the solutions, and we were able to obtain\ncode generations for all problems. As we had to use two different methods for code\ngeneration, we stated our practice as a possible factor to reduce the validity of our study.\nOn a further note, we also want to state the difference between the solutions that are\nautomatically generated, and the ones shown when the key combinations are applied.\nWe observed that for the same context, two methods yield different results. There-\nfore, if in both methods, the code is generated, choosing different methods for a set\nof problems may introduce possible invalidity to a study. Hence, we tried to be as\nconsistent as possible in our experiment by avoiding the latter method whenever possible.\nBlock and Line-by-Line Generation: For GitHub Copilot and Amazon CodeWhis-\nperer, for most of the cases, they managed to generate the solution of a given function\nas bulk, but there were cases, where we had to generate the solution line-by-line. As\nwe had no control over how these tools would generate the code, we had to accept\nthe method they would choose for a particular problem. We state these"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_040",
    "source_id": "CodeQualityComparison2023",
    "text": ", two methods yield different results. There-\nfore, if in both methods, the code is generated, choosing different methods for a set\nof problems may introduce possible invalidity to a study. Hence, we tried to be as\nconsistent as possible in our experiment by avoiding the latter method whenever possible.\nBlock and Line-by-Line Generation: For GitHub Copilot and Amazon CodeWhis-\nperer, for most of the cases, they managed to generate the solution of a given function\nas bulk, but there were cases, where we had to generate the solution line-by-line. As\nwe had no control over how these tools would generate the code, we had to accept\nthe method they would choose for a particular problem. We state these cases, as in\nline-by-line suggestion, the previously generated lines might have an effect on the next\nline to be generated, whereas in the first case, code is generated at once as a bulk. We\nhave not experienced this problem with ChatGPT, since it always generated the whole\nfunction in a single interaction.\nVersions of the Generators: While conducting our experiment, the latest version of\nGitHub Copilot was v1.70.8099 and of ChatGPT was 9 Jan \u201923. As explained earlier,\nAmazon does not keep the version of their CodeWhisperer, therefore we can only provide\nthe time when we conducted our experiment, which was January 2023. For any possible\nlater evaluations with the same experimental setup, the results might be different, which\nwe have proved with RQ4.\nInterval for Improvement of the Code Generators: As mentioned in Section 3.6,\nwe have made duplicate experiments on GitHub Copilot and Amazon CodeWhisperer\nto evaluate how these tools have improved over time, in terms of code correctness.\nHowever, the intervals between the two experiments for each generator are not the\nsame. In other words, the time difference between the experiments we conducted for\nGitHub Copilot was 13 months; however, this difference was two months for Amazon\n40\nBurak Yeti\u015ftiren et al.\nCodeWhisperer. Hence, the room for improvement for these tools is not the same.\nPrompting ChatGPT: GitHub Copilot and Amazon CodeWhisperer are tools that\nare integrated into an IDE. Therefore when we provided input to these tools, the tools\ngenerated code directly. However, we had to give an explanation to ChatGPT that\nour inputs should be used to generate code. We explained in a the following sentence\n\u201cGenerate code using the prompts I will provide\" to ChatGPT for code generation.\nThen we inputted the prompts one-by-one in the same chat window.\n6.3 Construct Validity\nMetrics: As explained in Section 3.3, we have evaluated the generated code using the\nCode Validity, Correctness, Security, Maintainability, and Reliability metrics. However,\nwe are aware that we could have used additional metrics, such as Readability, Cyclomatic\nComplexity, and Reusability to analyze the generated code.\n6.4 External Validity\nProblem Coverage: For our experiment, we evaluated the generated solutions for 164\ndifferent problems, contained in the HumanEval dataset. In the HumanEval dataset,\nthe subjects of the problems include algorithms, simple mathematics, reasoning, and\nlanguage comprehension Chen et al. (2021). For better"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_041",
    "source_id": "CodeQualityComparison2023",
    "text": " in the same chat window.\n6.3 Construct Validity\nMetrics: As explained in Section 3.3, we have evaluated the generated code using the\nCode Validity, Correctness, Security, Maintainability, and Reliability metrics. However,\nwe are aware that we could have used additional metrics, such as Readability, Cyclomatic\nComplexity, and Reusability to analyze the generated code.\n6.4 External Validity\nProblem Coverage: For our experiment, we evaluated the generated solutions for 164\ndifferent problems, contained in the HumanEval dataset. In the HumanEval dataset,\nthe subjects of the problems include algorithms, simple mathematics, reasoning, and\nlanguage comprehension Chen et al. (2021). For better and more insightful results, the\nnumber of problems can be increased, and the comprehension of the problems could be\nbroader. For instance, in the experimental setup proposed by Xu et al. (2022) for their\ncode generation and retrieval tool, the scope of the problems consists of basic Python,\nfile, OS, web scraping, web server & client, data analysis & ML, and data visualization.\nSuch topics could be included in our dataset to both broaden the comprehension and\nincrease the number of our problems. We consider this task as future work for our study.\nDependency on the HumanEval Dataset: As we explained in Section 3.1, we have\nused the HumanEval Benchmark Dataset for our experiment. Since we have only used\nthis dataset, it followed that we were limited to the Python programming language.\nHence, our experiment can reflect the code generation performance of the generators in\nregard to this language.\nIDE Dependency: For code generation using GitHub Copilot and Amazon Code-\nWhisperer, we used the Visual Studio Code IDE. In Table 1, we show the available\nIDEs that these tools are available on. Since we tried the code generation only on a\nsingle IDE, there might be IDE-dependent results; using the other mentioned IDEs\nmight have yielded different results.\n7 Related Work\nIn the last few years, code generation has attracted attention from researchers such as\nZhong et al. (2022); Hayati et al. (2018); Sun et al. (2020); Lyu et al. (2021). In this\nstudy, we compare some of the prevalent code generators: GitHub Copilot, Amazon\nTitle Suppressed Due to Excessive Length\n41\nCodeWhisperer, and ChatGPT, concerning their code generation capabilities, and find\nthe strengths and shortcomings of the tools by looking at the code quality metrics: Code\nValidity, Code Correctness, Code Reliability, Code Security, and Code Maintainability.\nWhen we investigated the studies similar to ours, we found more related studies about\nGitHub Copilot than other AI-based code generation tools. The reason is that GitHub\nCopilot was the first created tool among the code generators we evaluated and there\nwas more time for it to be evaluated by researchers.\nThe underlying model of GitHub Copilot, Codex, is externally developed by OpenAI\nand employed by GitHub. Some earlier versions of the current Codex model used by\nGitHub Copilot were evaluated by Chen et al. (2021). The Codex model relies on GPT\nmodels that OpenAI previously developed for natural language generation. The public\ncode available on GitHub was used here while fine-tuning"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_042",
    "source_id": "CodeQualityComparison2023",
    "text": " Security, and Code Maintainability.\nWhen we investigated the studies similar to ours, we found more related studies about\nGitHub Copilot than other AI-based code generation tools. The reason is that GitHub\nCopilot was the first created tool among the code generators we evaluated and there\nwas more time for it to be evaluated by researchers.\nThe underlying model of GitHub Copilot, Codex, is externally developed by OpenAI\nand employed by GitHub. Some earlier versions of the current Codex model used by\nGitHub Copilot were evaluated by Chen et al. (2021). The Codex model relies on GPT\nmodels that OpenAI previously developed for natural language generation. The public\ncode available on GitHub was used here while fine-tuning the model to implement the\ncode recognition and generation capabilities. This model can recognize other elements\nsuch as function signatures, code comments, etc. and it can use such elements as inputs\nand generate related outputs. They found that a success rate of 70.2% could be reached\nin terms of code correctness, by generating 100 solutions for each problem and choosing\nthe most successful one among them. The success rate was only 28.8% for the case\nwith one solution per problem, which is consistent with our initial results in Yetistiren\net al. (2022). In this research, we also examined code reliability, maintainability, and\nsecurity to provide a detailed form of this evaluation. We evaluated the three main code-\ngeneration tools as well. Additionally, in order to broaden the scope of our investigation,\nwe modified the HumanEval dataset by replacing real function names with the dummy\nname \u2018foo\u2019 and then produced new sets of results.\nThere are also empirical studies similar to ours, conducted to evaluate GitHub\nCopilot. We list the available studies in the following.\nOne such study is conducted by Sobania et al. (2022) in which the code correctness\nof GitHub Copilot is evaluated, and the tool is contrasted to the automatic program\ngenerators having the Genetic Programming (GP) architecture. They found that there\nis not a significant difference between the two approaches on the benchmark problems;\nhowever, the program synthesis approaches are not sufficient in supporting programmers\ncompared to GitHub Copilot.\nAn evaluation of GitHub Copilot in terms of the security of the generated programs\nwas implemented by Pearce et al. (2021). They evaluated the vulnerabilities in the\ncode generated by Copilot. It was determined that 40% of generated programs were\nvulnerable. These results differed from our Code Security results; we believe that the\ncharacteristics of our dataset caused the difference between the two studies.\nAnother study discusses the effects of GitHub Copilot by conducting a within-\nsubjects user study Vaithilingam et al. (2022). It was found that GitHub Copilot did\nnot cause a significant improvement in terms of speed and success rate. However, it\nwas stated that most participants preferred to use Copilot in daily programming tasks\nsince it saved the effort for the basic tasks.\nNguyen and Nadi (2022) evaluated GitHub Copilot using 33 different LeetCode\nquestions and four different programming languages (Python, Java, JavaScript, and C).\nTheir evaluation includes code correctness and code understandability for the generated\ncode. They evaluated code correctness by measuring the ratio of passed tests for each\nquestion, which is a similar approach to our study."
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_043",
    "source_id": "CodeQualityComparison2023",
    "text": " of GitHub Copilot by conducting a within-\nsubjects user study Vaithilingam et al. (2022). It was found that GitHub Copilot did\nnot cause a significant improvement in terms of speed and success rate. However, it\nwas stated that most participants preferred to use Copilot in daily programming tasks\nsince it saved the effort for the basic tasks.\nNguyen and Nadi (2022) evaluated GitHub Copilot using 33 different LeetCode\nquestions and four different programming languages (Python, Java, JavaScript, and C).\nTheir evaluation includes code correctness and code understandability for the generated\ncode. They evaluated code correctness by measuring the ratio of passed tests for each\nquestion, which is a similar approach to our study. Code understandability was measured\nby two different metrics, which are cognitive and cyclomatic complexity. In terms of\ncode correctness, Java had the highest (57%) and JavaScript had the lowest (27%) score.\n42\nBurak Yeti\u015ftiren et al.\nFor code understandability, they determined that there was no statistical significance\nbetween the programming languages.\nMastropaolo et al. (2023) presented an empirical study that focuses on the effect of\nsemantic-preserving changes in the natural language on the generated code function of\nGitHub Copilot. For this purpose, Mastropaolo et al. (2023) provided 892 non-trivial\nJava method descriptions to GitHub Copilot. Firstly, they used the original descriptions\nof methods and asked GitHub Copilot to generate them. Secondly, they paraphrased\ndescriptions manually. Thirdly, they paraphrased descriptions using automated para-\nphrasing tools. After GitHub Copilot generated all of the methods according to their\ndescriptions, they found that in 46% of cases, semantically equivalent but different\nmethod descriptions resulted in different code recommendations. Moreover, they ob-\nserved that some code recommendations were correct with only one of the semantically\nequivalent descriptions as input.\nChatGPT is the other code generator that we have chosen for our study. Since\nChatGPT is released recently, there are only a few studies similar to our work. These\nstudies are in the following.\nIn order to analyze the bug fixing performance of ChatGPT, Sobania et al. (2023)\nevaluated ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compared\nthese results with CoCoNut, Codex, and standard APR approaches. They found that\nChatGPT had a similar performance to Codex and its performance was much better\nthan standard APR approaches. When Sobania et al. (2023) used the dialogue option\nof ChatGPT and gave ChatGPT more information about the bug, they found that\nChatGPT gave an overall success rate of 77.5%. Then, they concluded that although\nChatGPT had an outstanding performance, it required mental cost to verify ChatGPT\nanswers.\nThe possible integration of ChatGPT into a well-known software testing curriculum\nis covered in another research. Jalil et al. (2023) requested that ChatGPT respond to\ntypical software testing questions. They discovered that ChatGPT could offer correct or\npartly correct responses in 44% of the cases and correct or partially correct explanations\nof answers in 57% of the cases. As a result, they noticed that"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_044",
    "source_id": "CodeQualityComparison2023",
    "text": "of ChatGPT and gave ChatGPT more information about the bug, they found that\nChatGPT gave an overall success rate of 77.5%. Then, they concluded that although\nChatGPT had an outstanding performance, it required mental cost to verify ChatGPT\nanswers.\nThe possible integration of ChatGPT into a well-known software testing curriculum\nis covered in another research. Jalil et al. (2023) requested that ChatGPT respond to\ntypical software testing questions. They discovered that ChatGPT could offer correct or\npartly correct responses in 44% of the cases and correct or partially correct explanations\nof answers in 57% of the cases. As a result, they noticed that ChatGPT failed a course\non software testing. Furthermore, ChatGPT was a poor judge of its correctness. It is\ntherefore uncertain how much benefit ChatGPT might offer to students.\nAdditionally, we have selected Amazon CodeWhisperer as the code generation tool\nfor our study\u2019s evaluation. There are no studies about Amazon CodeWhisperer that\nare comparable to our research because it is a relatively new tool, like the case with\nChatGPT.\nIn general, most of the associated works assessed the code quality produced by\nGitHub Copilot or OpenAI\u2019s Codex model. Except for the studies of Pearce et al. (2021),\nwhere the emphasis is code security, and Vaithilingam et al. (2022), where the work\nprimarily focuses on the practical usage performance of GitHub Copilot, the majority\nof the studies concentrated on the assessment of code correctness. On the other hand,\nthere aren\u2019t nearly enough studies about ChatGPT and Amazon CodeWhisperer that\nare comparable to ours. The bug-fixing performance of ChatGPT (Sobania et al., 2023)\nand the possible applicability of ChatGPT to a well-known software testing program\n(Jalil et al., 2023) were the main topics of related research about ChatGPT that we\ndiscovered.\nTo the best of our knowledge, this is the first study comparing code correctness,\ncode validity, security, maintainability, and dependability between GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT. In this regard, we think that our methods and\nTitle Suppressed Due to Excessive Length\n43\nfindings will contribute to current research on the capabilities of these common tools\nand other code-generation tools.\n8 Conclusion\nIn our study, we compared three code generation tools: GitHub Copilot, Amazon\nCodeWhisperer, and ChatGPT. We evaluated the quality of the generated code in\nterms of correctness, validity, reliability, security, and maintainability. Our results show\nthat ChatGPT, in its original setup, had the highest success rate among the evaluated\ncode generation tools. Specifically, it was able to generate correct code solutions for\n65.2% of the problems in the HumanEval problem dataset. It also produced partially\ncorrect solutions for 22.6% of the problems and incorrect solutions for 12.2% of the\nproblems.\nIn terms of code maintainability, we found repeated types of code smells among the\ncode generation tools, and we compiled a list of the code smells that we encountered. If\nthe solution to the problem contained any smells, the average time to"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_045",
    "source_id": "CodeQualityComparison2023",
    "text": " generated code in\nterms of correctness, validity, reliability, security, and maintainability. Our results show\nthat ChatGPT, in its original setup, had the highest success rate among the evaluated\ncode generation tools. Specifically, it was able to generate correct code solutions for\n65.2% of the problems in the HumanEval problem dataset. It also produced partially\ncorrect solutions for 22.6% of the problems and incorrect solutions for 12.2% of the\nproblems.\nIn terms of code maintainability, we found repeated types of code smells among the\ncode generation tools, and we compiled a list of the code smells that we encountered. If\nthe solution to the problem contained any smells, the average time to eliminate them\nwas 9.1 minutes for GitHub Copilot, 5.6 minutes for Amazon CodeWhisperer, and 8.9\nminutes for ChatGPT.\nTo evaluate the impact of input parameters\u2019 quality on the three major code\ngeneration tools, we conducted an assessment of providing only function names and\nparameters. Our findings revealed that compared to their initial setup, all three code-\ngeneration tools had lower percentages of correct answers. ChatGPT and GitHub\nCopilot achieved the best and most comparable outcomes, with correct answers for\n20%-22% of the problems, partially correct answers for 26%-27% of the problems, and\nincorrect answers for 50%-53% of the problems.\nWe also investigated the effect of dummy function names on the success of code\ngeneration tools. Our results showed that ChatGPT had the highest percentage of\ncorrect solutions among the three tools, with 61.6% of the problems examined generating\ncorrect solutions. ChatGPT also generated partially correct solutions for 25.6% of the\nproblems and incorrect solutions for 12.8% of the problems.\nBased on our findings, ChatGPT was the most successful tool, whereas Amazon\nCodeWhisperer was the least successful. We also found that providing an accurate and\nclear problem description was essential for the success of code generation tools, as we\nobserved that all tools performed worse when we eliminated docstrings from the input.\nMoreover, we observed that both Amazon CodeWhisperer and GitHub Copilot are\nimproving rapidly, suggesting their potential for various coding tasks in the future.\nIn summary, our study has made several contributions to the understanding of code\ngeneration tools:\n\u2013 We conducted a comparative analysis of GitHub Copilot, Amazon CodeWhisperer,\nand ChatGPT and provided a comprehensive comparison table (Table 1) of their\nfeatures.\n\u2013 We evaluated the code generation capabilities of these tools using the HumanEval\ndataset and proposed a pipeline to assess the quality of the generated code.\n\u2013 We analyzed the performance improvements of the new version of GitHub Copi-\nlot compared to the previous version and observed the improvement of Amazon\nCodeWhisperer between November 2022 and January 2023.\n\u2013 We highlighted the importance of providing accurate and clear problem descriptions\nfor code generation tools to improve their performance.\n44\nBurak Yeti\u015ftiren et al.\nOverall, our study contributes to the development of code generation tools and\nprovides insights into their potential for various coding tasks in the future.\nData Availability: Our data yielded from this study and the code that was utilized\ncan be found in our repository at https://github.com/miray"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_046",
    "source_id": "CodeQualityComparison2023",
    "text": "Eval\ndataset and proposed a pipeline to assess the quality of the generated code.\n\u2013 We analyzed the performance improvements of the new version of GitHub Copi-\nlot compared to the previous version and observed the improvement of Amazon\nCodeWhisperer between November 2022 and January 2023.\n\u2013 We highlighted the importance of providing accurate and clear problem descriptions\nfor code generation tools to improve their performance.\n44\nBurak Yeti\u015ftiren et al.\nOverall, our study contributes to the development of code generation tools and\nprovides insights into their potential for various coding tasks in the future.\nData Availability: Our data yielded from this study and the code that was utilized\ncan be found in our repository at https://github.com/mirayayerdem/Github-Copilot-\nAmazon-Whisperer-ChatGPT.\nConflict of Interests: The authors declare no conflicts of interest in relation to this\narticle.\nReferences\nBays\nJ\n(2022)\nAws\nannounces\namazon\ncodewhisperer\n(preview).\nURL\nhttps://aws.amazon.com/about-aws/whats-new/2022/06/\naws-announces-amazon-codewhisperer-preview/\nCerullo M (2023) Chatgpt is growing faster than tiktok. URL https://www.cbsnews.\ncom/news/chatgpt-chatbot-tiktok-ai-artificial-intelligence/\nChen M, Tworek J, Jun H, Yuan Q, Pinto HPdO, Kaplan J, Edwards H, Burda Y,\nJoseph N, Brockman G, Ray A, Puri R, Krueger G, Petrov M, Khlaaf H, Sastry\nG, Mishkin P, Chan B, Gray S, Ryder N, Pavlov M, Power A, Kaiser L, Bavarian\nM, Winter C, Tillet P, Such FP, Cummings D, Plappert M, Chantzis F, Barnes\nE, Herbert-Voss A, Guss WH, Nichol A, Paino A, Tezak N, Tang J, Babuschkin\nI, Balaji S, Jain S, Saunders W, Hesse C, Carr AN, Leike J, Achiam J, Misra V,\nMorikawa E, Radford A, Knight M, Brundage M, Murati M, Mayer K, Welinder P,\nMcGrew B, Amodei D, McCandlish S, Sutskever I, Zaremba W (2021) Evaluating\nlarge language models trained on code. DOI 10.48550/ARXIV.2107.03374, URL\nhttps://arxiv.org/abs/2107.03374\nDohmke\nT\n(2022)\nGithub\ncopilot\nis\ngenerally\navail-\nable\nto\nall\ndevelopers.\nURL\nhttps://github.blog/\n2022-06-21-github-copilot-is-generally-available-to-all-developers/\nErnst NA, Bavota G (2022) Ai-driven development is here: Should you worry? IEEE\nSoftware 39(2):106\u2013110\nFriedman N (2021) Introducing github copilot: Your ai pair programmer. URL https://\ngithub.blog/2021-06-29-introducing-github-c"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_047",
    "source_id": "CodeQualityComparison2023",
    "text": "7.03374, URL\nhttps://arxiv.org/abs/2107.03374\nDohmke\nT\n(2022)\nGithub\ncopilot\nis\ngenerally\navail-\nable\nto\nall\ndevelopers.\nURL\nhttps://github.blog/\n2022-06-21-github-copilot-is-generally-available-to-all-developers/\nErnst NA, Bavota G (2022) Ai-driven development is here: Should you worry? IEEE\nSoftware 39(2):106\u2013110\nFriedman N (2021) Introducing github copilot: Your ai pair programmer. URL https://\ngithub.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/\nHayati SA, Olivier R, Avvaru P, Yin P, Tomasic A, Neubig G (2018) Retrieval-based\nneural code generation. DOI 10.48550/ARXIV.1808.10025, URL https://arxiv.\norg/abs/1808.10025\nJalil S, Rafi S, LaToza T, Moran K, Lam W (2023) Chatgpt and software testing\neducation: Promises & perils. DOI 10.48550/arXiv.2302.03287\nLyu C, Wang R, Zhang H, Zhang H, Hu S (2021) Embedding api dependency graph\nfor neural code generation. Empirical Software Engineering 26(4):61, DOI 10.1007/\ns10664-021-09968-2, URL https://doi.org/10.1007/s10664-021-09968-2\nMastropaolo A, Pascarella L, Guglielmi E, Ciniselli M, Scalabrino S, Oliveto R, Bavota\nG (2023) On the robustness of code generation techniques: An empirical study on\ngithub copilot. DOI 10.48550/arXiv.2302.00438\nNguyen N, Nadi S (2022) An empirical evaluation of github copilot\u2019s code suggestions.\nIn: 2022 IEEE/ACM 19th International Conference on Mining Software Repositories\n(MSR), pp 1\u20135, DOI 10.1145/3524842.3528470\nTitle Suppressed Due to Excessive Length\n45\nOmar C, Yoon YS, LaToza TD, Myers BA (2012) Active code completion. In: 2012\n34th International Conference on Software Engineering (ICSE), pp 859\u2013869, DOI\n10.1109/ICSE.2012.6227133\nOpenAI\n(2023)\nIntroducing\nchatgpt\nplus.\nURL\nhttps://openai.com/blog/\nchatgpt-plus/\nPearce H, Ahmad B, Tan B, Dolan-Gavitt B, Karri R (2021) Asleep at the keyboard?\nassessing the security of github copilot\u2019s code contributions. DOI 10.48550/ARXIV.\n2108.09293, URL https://arxiv.org/abs/2108.09293\nSobania D, Briesch M, Rothlauf F (2022) Choose"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_048",
    "source_id": "CodeQualityComparison2023",
    "text": " (ICSE), pp 859\u2013869, DOI\n10.1109/ICSE.2012.6227133\nOpenAI\n(2023)\nIntroducing\nchatgpt\nplus.\nURL\nhttps://openai.com/blog/\nchatgpt-plus/\nPearce H, Ahmad B, Tan B, Dolan-Gavitt B, Karri R (2021) Asleep at the keyboard?\nassessing the security of github copilot\u2019s code contributions. DOI 10.48550/ARXIV.\n2108.09293, URL https://arxiv.org/abs/2108.09293\nSobania D, Briesch M, Rothlauf F (2022) Choose your programming copilot: A compari-\nson of the program synthesis performance of github copilot and genetic programming.\nIn: Proceedings of the Genetic and Evolutionary Computation Conference, Associ-\nation for Computing Machinery, New York, NY, USA, GECCO \u201922, p 1019\u20131027,\nDOI 10.1145/3512290.3528700, URL https://doi.org/10.1145/3512290.3528700\nSobania D, Briesch M, Hanna C, Petke J (2023) An analysis of the automatic bug fixing\nperformance of chatgpt\nSun Z, Zhu Q, Xiong Y, Sun Y, Mou L, Zhang L (2020) Treegen: A tree-based\ntransformer architecture for code generation. Proceedings of the AAAI Conference\non Artificial Intelligence 34(05):8984\u20138991, DOI 10.1609/aaai.v34i05.6430, URL\nhttps://ojs.aaai.org/index.php/AAAI/article/view/6430\nTung\nL\n(2023)\nChatgpt\ncan\nwrite\ncode.\nnow\nresearchers\nsay\nit\u2019s\ngood\nat\nfixing\nbugs,\ntoo.\nURL\nhttps://www.zdnet.com/article/\nchatgpt-can-write-code-now-researchers-say-its-good-at-fixing-bugs-too/\nVaithilingam P, Zhang T, Glassman EL (2022) Expectation vs. experience: Evaluating\nthe usability of code generation tools powered by large language models. In: Extended\nAbstracts of the 2022 CHI Conference on Human Factors in Computing Systems,\nAssociation for Computing Machinery, New York, NY, USA, CHI EA \u201922, DOI\n10.1145/3491101.3519665, URL https://doi.org/10.1145/3491101.3519665\nXu FF, Vasilescu B, Neubig G (2022) In-ide code generation from natural language:\nPromise and challenges. ACM Trans Softw Eng Methodol 31(2), DOI 10.1145/3487569,\nURL https://doi.org/10.1145/3487569\nYetistiren B, Ozsoy I, Tuzun E (2022) Assessing the quality of github copilot\u2019s code\ngeneration. In: Proceedings of the 18th International Conference on Predictive Models\nand Data Analytics in Software Engineering, pp 62\u201371\nZhong M, Liu G, Li H, Ku"
  },
  {
    "chunk_id": "CodeQualityComparison2023_chunk_049",
    "source_id": "CodeQualityComparison2023",
    "text": "1145/3491101.3519665\nXu FF, Vasilescu B, Neubig G (2022) In-ide code generation from natural language:\nPromise and challenges. ACM Trans Softw Eng Methodol 31(2), DOI 10.1145/3487569,\nURL https://doi.org/10.1145/3487569\nYetistiren B, Ozsoy I, Tuzun E (2022) Assessing the quality of github copilot\u2019s code\ngeneration. In: Proceedings of the 18th International Conference on Predictive Models\nand Data Analytics in Software Engineering, pp 62\u201371\nZhong M, Liu G, Li H, Kuang J, Zeng J, Wang M (2022) Codegen-test: An automatic\ncode generation model integrating program test information. DOI 10.48550/ARXIV.\n2202.07612, URL https://arxiv.org/abs/2202.07612\n"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_001",
    "source_id": "EnterpriseImpact2024",
    "text": "Examining the Use and Impact of an AI Code Assistant on Developer\nProductivity and Experience in the Enterprise\nJUSTIN D. WEISZ, IBM Research, USA\nSHRADDHA KUMAR\u2217, Cisco Systems, Inc., India\nMICHAEL MULLER, IBM Research, USA\nKAREN-ELLEN BROWNE, IBM Software, Ireland\nARIELLE GOLDBERG, IBM Infrastructure, USA\nELLICE HEINTZE, IBM Software, Germany\nSHAGUN BAJPAI, IBM Software, India\nAI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and\ntesting code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within\nIBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers\u2019 experiences\nwith WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their\nexpectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated\ncode. Our case study characterizes the impact of an LLM-powered assistant on developers\u2019 perceptions of productivity and it shows\nthat although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.\nCCS Concepts: \u2022 Human-centered computing \u2192Empirical studies in HCI; Field studies; \u2022 Software and its engineering \u2192\nCollaboration in software development; Automatic programming.\nAdditional Key Words and Phrases: Generative AI, LLM, software engineering, productivity, code assistant\nACM Reference Format:\nJustin D. Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. 2025.\nExamining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA \u201925), April 26-May 1, 2025, Yokohama, Japan. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3706599.3706670\n1\nIntroduction\nAI assistants powered by large language models (LLMs) are becoming increasingly prevalent in the workplace. A\nnumber of commercial and open-source coding assistants have been released for software engineers, developers, and\n\u2217Work conducted while an employee of IBM Software, Kochi, India.\nAuthors\u2019 Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen,"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_002",
    "source_id": "EnterpriseImpact2024",
    "text": "i, India.\nAuthors\u2019 Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen, Germany; Shagun Bajpai, Shagun.Bajpai@ibm.com, IBM Software, Kochi, India.\n2025. Manuscript submitted to ACM\nManuscript submitted to ACM\n1\narXiv:2412.06603v2  [cs.HC]  4 Mar 2025\n2\nWeisz et al.\ndata scientists1 who perform code-related work. These tools include GitHub Copilot2, Amazon Q Developer3, Gemini\nCode Assist4, and IBM\u2019s watsonx Code Assistant5.\nWith this rapid proliferation of AI code assistants, it is important to understand their impact on developer productivity.\nWe were provided with the opportunity to examine a new AI assistant under development within IBM in 2024 that\nsupported general developer needs in common programming languages (e.g. Python, Java, JavaScript, C++, and more).\nOur team \u2013 spanning product management, design, and research \u2013 used a mixed-methods approach to characterize the\nassistant\u2019s impact on productivity.\nIn this paper, we report selected results from two studies: a large-scale survey of WCA users and small-scale usability\ntesting. We discovered several interesting insights regarding the use of the assistant, the content it generated, and\nits impact on productivity and the developer profession. Our paper makes the following contributions to the CHI\ncommunity:\n\u2022 We characterize the user experience of an LLM-based programming assistant under development within a large\ntechnology company. Our work examines the impact of the assistant on attitudinal measures of perceptions of\nproductivity, complementing prior work that examined behavioral productivity metrics (e.g. [61, 62]). We find\nthat although the assistant did increase net productivity despite variability in the quality of its outputs, those\ngains were not evenly distributed across all users.\n\u2022 We observed that understanding code was the top use case, followed by the production of code, indicating a need\nfor further research in how AI code assistants can aid sensemaking tasks in code repositories.\n\u2022 We identify a shared responsibility between people and AI systems in mitigating the risks of generated outputs.\n2\nRelated Work\nWe outline three areas relevant to our study of AI code assistants: code-fluent LLMs and their incorporation into the\nsoftware engineering workflow; the multi-faceted nature of productivity in software engineering; and studies of AI\ncode assistants.\n2.1\nCode-fluent LLMs and software engineering assistants\nLarge language models that have been exposed to source code in their pre-training have demonstrated a high degree of\naptitude in performing a variety of tasks: converting natural language to code (e.g. [2, 16, 19, 56]),"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_003",
    "source_id": "EnterpriseImpact2024",
    "text": " assistants can aid sensemaking tasks in code repositories.\n\u2022 We identify a shared responsibility between people and AI systems in mitigating the risks of generated outputs.\n2\nRelated Work\nWe outline three areas relevant to our study of AI code assistants: code-fluent LLMs and their incorporation into the\nsoftware engineering workflow; the multi-faceted nature of productivity in software engineering; and studies of AI\ncode assistants.\n2.1\nCode-fluent LLMs and software engineering assistants\nLarge language models that have been exposed to source code in their pre-training have demonstrated a high degree of\naptitude in performing a variety of tasks: converting natural language to code (e.g. [2, 16, 19, 56]), converting code\nto natural language documentation (e.g. [16, 31]) or explanations (e.g. [37]), and converting code to code, such as by\ntranslating it from one language to another (e.g. [2, 19, 46, 57]) or by creating unit tests (e.g. [48]). The introduction of\nthe Codex model [10] and its corresponding incorporation into software developers\u2019 IDEs through GitHub Copilot6\ndemonstrated how code-fluent LLMs could revolutionize the software development workflow. New agentic design\npatterns are enabling coding assistants to perform even more complex tasks, such as converting issues into pull\nrequests [25] and new feature descriptions into specifications and implementation plans7.\n1In this paper, we use the term \u201cdeveloper\u201d as a catch-all to cover individuals who perform code-related work, including software engineers, architects,\nand data scientists.\n2GitHub Copilot: https://github.com/features/copilot\n3Amazon Q Developer: https://aws.amazon.com/q/developer/\n4Gemini Code Assist: https://cloud.google.com/gemini/docs/codeassist/overview\n5watsonx Code Assistant: https://www.ibm.com/products/watsonx-code-assistant\n6GitHub Copilot: https://github.com/features/copilot\n7GitHub Copilot Workspace: https://githubnext.com/projects/copilot-workspace\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n3\n2.2\nSoftware engineering productivity\nProductivity in software engineering is a complex, multi-faceted construct [17, 47]. It is often assessed via objective\nmetrics of productivity that capture the ratio of output to effort [47] (e.g. lines of code over time [15], function points [30]),\nthe complexity of the software system [20, 33], or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders \u201cwhen software developers perceive themselves to be productive and... unproductive\u201d [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_004",
    "source_id": "EnterpriseImpact2024",
    "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders \u201cwhen software developers perceive themselves to be productive and... unproductive\u201d [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60\u201362]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: \u201cacceleration mode\u201d in which the tool aided\ndevelopers when they knew what to do next, and \u201cexploration mode\u201d to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM\u2019s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as \u201cWCA@IBM,\u201d was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_005",
    "source_id": "EnterpriseImpact2024",
    "text": "3\nCase Study of an AI Code Assistant\nIBM\u2019s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as \u201cWCA@IBM,\u201d was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watsonx\u2122Code Assistant for Red Hat\u00ae Ansible\u00ae Lightspeed: https://www.ibm.com/products/watsonx-code-assistant-ansible-lightspeed\n9IBM watsonx\u2122Code Assistant for Z: https://www.ibm.com/products/watsonx-code-assistant-z\nManuscript submitted to ACM\n4\nWeisz et al.\nWe had the opportunity to study internal usage of this new WCA10 variant from a subjective standpoint: how did it\nimpact developers\u2019 perceptions of their own productivity? To address this question, we used a two-pronged approach:\nwe conducted a survey of internal WCA users (N=669 respondents) and we conducted unmoderated usability testing in\nwhich participants used multiple WCA features to complete small programming tasks (N=15 participants).\nDue to privacy concerns and sensitivities in data collection, we were only able to analyze attitudinal data in our\nstudies. We did not have access to product telemetry or other behavioral data regarding use of WCA, such as the number\nof queries made throughout the study period, prompts sent to the model, or outputs generated by the model.\n3.1\nSurvey\nWe developed a comprehensive survey to capture a wide range of data regarding the user experience of WCA and its\nimpact on developer productivity. It was developed to meet the needs of multiple stakeholder groups, including product\nmanagers, designers, and researchers. We outline the different categories of measures in Table 1. We used a number of\nmeasures to capture different aspects of the WCA user experience, which we detail in Appendix A.\nGiven the large number of measures included in the survey, we split it into two modules to prioritize the most\nimportant questions and help respondents avoid survey fatigue. Module 1 focused on understanding usage, usability,\nand demographics, and Module 2 focused more in-depth on motivations for use, trust, and content ownership &\nresponsibility. Each module took approximately 15-20 minutes to complete. Between Modules 1 and 2, respondents\nwere asked whether they wanted to end the survey or spend additional time to provide more in-depth feedback.\n3.1.1\nRecruitment & participants. We launched a first round of data collection (Cohort 1) in May 2024 and received 105\nresponses. We launched a second round of data collection (Cohort 2) in July 2024, targeted to a group of 564 developers\nparticipating in a WCA training program. As completing the survey was part of the program, we eliminated the optional\nnature of Module 2 for this cohort.\nA total of 669 WCA users responded to"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_006",
    "source_id": "EnterpriseImpact2024",
    "text": " 15-20 minutes to complete. Between Modules 1 and 2, respondents\nwere asked whether they wanted to end the survey or spend additional time to provide more in-depth feedback.\n3.1.1\nRecruitment & participants. We launched a first round of data collection (Cohort 1) in May 2024 and received 105\nresponses. We launched a second round of data collection (Cohort 2) in July 2024, targeted to a group of 564 developers\nparticipating in a WCA training program. As completing the survey was part of the program, we eliminated the optional\nnature of Module 2 for this cohort.\nA total of 669 WCA users responded to our survey. Despite the optional nature of Module 2 for Cohort 1, only 41\nrespondents did not complete this module (39.0% of Cohort 1; 6.1% of the total sample). Because we do not hypothesize\nany differences between the cohorts, we analyze them as a single group11.\nSurvey respondents held a wide range of developer roles, such as back-end developers (59.6%), front-end developers\n(19.9%), QA/test developers (19.3%), and more. They held a wide range of tenures with IBM (< 6 months to 31+ years),\nyears of experience working as a professional software engineer (0 to 10+ years), and hailed from a variety of geographies\n(primarily Americas and APAC). We show distributions of respondent demographics in Appendix B.\n3.2\nUnmoderated usability testing\nWe conducted an unmoderated usability test to assess specific features of WCA. This test involved solving a small\nprogramming problem using different WCA features: code generation, chat, and code autocompletion. Participants then\nused WCA to generate explanations for their code and documentation in a README file. After each task, participants\nfilled out a small questionnaire that asked about their experience. The usability test took approximately 40 minutes.\n3.2.1\nRecruitment & participants. We recruited 15 WCA users to complete the usability test based on the programming\nlanguages with which they worked and the frequency of their WCA usage. Participants came from a variety of product\n10For simplicity, we refer to the internal deployment of \u201cWCA@IBM\u201d within IBM as \u201cWCA\u201d in this paper and we note that our use of \u201cWCA\u201d is not\nintended to refer to other products in the watsonx Code Assistant family.\n11We note that, despite the gap in time between the two cohorts, no major product enhancements beyond minor bug fixes were deployed; thus, the user\nexperience between the two cohorts was equivalent.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n5\nteams, spanning areas including firmware development, security, mobile apps, databases, and more. About 43% of\nparticipants used WCA regularly in their work.\n3.3\nEthics statement\nOur research followed IBM\u2019s AI Ethics framework12. Our survey was approved by an internal review committee,\nsubject to restrictions regarding the collection of demographic information. Specifically, we were unable to collect\nage or gender identity, we collected geography and job role using specified sets of options, and we did"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_007",
    "source_id": "EnterpriseImpact2024",
    "text": " beyond minor bug fixes were deployed; thus, the user\nexperience between the two cohorts was equivalent.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n5\nteams, spanning areas including firmware development, security, mobile apps, databases, and more. About 43% of\nparticipants used WCA regularly in their work.\n3.3\nEthics statement\nOur research followed IBM\u2019s AI Ethics framework12. Our survey was approved by an internal review committee,\nsubject to restrictions regarding the collection of demographic information. Specifically, we were unable to collect\nage or gender identity, we collected geography and job role using specified sets of options, and we did not collect any\npersonally-identifiable information (PII). All survey responses were anonymous13.\n4\nResults\nOur data consists of a mix of quantitative measures from the survey and qualitative feedback from the survey and\nthe unmoderated usability testing sessions. We generally report descriptive statistics on the quantitative measures as\nour research questions did not require making statistical comparisons. To analyze qualitative data, we conducted a\nreflexive thematic analysis [12], using a deductive approach as our product management and design stakeholders had\nclear priorities for which aspects of the user experience on which to focus. We summarize our key results in Table 1.\nIn presenting our results, we refer to survey respondents as Rx.yyy, where x corresponds to their cohort (1 or 2) and\nyyy is a unique identifier. We refer to participants in the unmoderated usability study as Pxx, where xx is a unique\nidentifier. Collectively, we refer to both groups as \u201cWCA users.\u201d\nCategory\nDescription\nSummary of Findings\nSection\nMotivations, use,\nand non-use\nWhy and how WCA was used or not\nused\nTop use cases focused on code under-\nstanding; \u201coff-label\u201d usage by content\ndesigners; unmet needs for specialized\ntechnologies (e.g. DB2, Maximo)\n4.1\nUse of generated\ncontent\nWays that generated content was re-\nviewed & used\nContent modified before use; outputs\nalso used for learning and inspiration\n4.2\nImpact on\nproductivity\nImpact of WCA on various dimensions\n(effort, speed, work quality, self-efficacy)\nSmall net productivity improvement, but\nwith mixed and disparate impact\n4.3\nAuthorship &\nresponsibility\nWho deserves authorship credit and\nwho is responsible for avoiding inclu-\nsion of copyrighted IP?\nWCA deserving of authorship credit\nfor co-creative activity; users and WCA\nhave a joint responsibility to avoid inclu-\nsion of copyrighted IP\n4.4\nImpact on job role\nHow AI assistants might change the de-\nveloper profession\nAI lets developers focus on higher-level\ntasks; potential for deskilling; increased\nproductivity translates into increased ex-\npectations\n4.5\nTable 1. Categories of measures included in the user experience survey and a summary of our findings. We provide a complete listing\nof all survey questions in Appendix A.\n12IBM AI Ethics: https://www.ibm.com/impact/ai-ethics\n13For Cohort 2 respondents, we preserved the anonymity of their feedback by collecting their email address in a separate form after"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_008",
    "source_id": "EnterpriseImpact2024",
    "text": " activity; users and WCA\nhave a joint responsibility to avoid inclu-\nsion of copyrighted IP\n4.4\nImpact on job role\nHow AI assistants might change the de-\nveloper profession\nAI lets developers focus on higher-level\ntasks; potential for deskilling; increased\nproductivity translates into increased ex-\npectations\n4.5\nTable 1. Categories of measures included in the user experience survey and a summary of our findings. We provide a complete listing\nof all survey questions in Appendix A.\n12IBM AI Ethics: https://www.ibm.com/impact/ai-ethics\n13For Cohort 2 respondents, we preserved the anonymity of their feedback by collecting their email address in a separate form after they completed the\nsurvey to mark their completion of the training course. In addition, it is possible that an individual responded to both surveys. However, given that the\nnumber of respondents in Cohort 2 was vastly greater than that of Cohort 1, we were not overly concerned with the possibility of capturing repeated\nmeasures from the same set of respondents.\nManuscript submitted to ACM\n6\nWeisz et al.\n4.1\nMotivations, use, and non-use\nLiang et al. [28] examined a number of motivations for developers to use (or not use) AI programming assistants.\nCompared to their sample of AI code assistant users in the wild, our respondents were less interested in having a\ncode autocompletion feature (47% vs. 86%) or finishing their programming tasks faster (59% vs. 76%). Rather, they\nwere more interested in discovering potential ways or starting points to solve their programming challenges (63%\nvs. 50%). In addition, our respondents were interested in exploring new tools (68%), they wanted to know how their\nwork might change in the future (64%), they felt a responsibility to try new IBM products (64%), and their management\nrecommended using WCA (60%). We provide a more detailed comparison of motivations for use in Appendix C.\nWe asked respondents about the kinds of tasks they conducted with WCA within their prior two weeks of usage.\nPrior research on LLM-based code assistants suggests that the core use is to generate code [28]. By contrast, the most\npopular use cases for WCA related to understanding code, either via explanations (71.9%) or by receiving answers to\ngeneral programming questions (68.5%). R1.92 summarized the utility of WCA for code understanding: \u201cI use WCA\nfor two main things: Explaining code other people wrote that I am working with for the first time. Explaining code I have\nwritten that isn\u2019t working as expected, because sometimes WCA can tell me why it isn\u2019t working and give tips on how to fix\nit. It can often catch typo-like errors that I overlooked.\u201d R1.45 pointed out how explanations saved them time: \u201cI like its\nability to explain functions of code which could take a bit to understand. It can save a lot of time.\u201d R2.177 described their\nuse of explanations to \u201cexplore areas of code I\u2019m not familiar with.\u201d Respondents did use WCA to generate code (55.6%),\ndocumentation (39.6%), and tests (35.7%), but to a lesser extent. We detail additional purposes of use in Appendix D.\nSome respondents used W"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_009",
    "source_id": "EnterpriseImpact2024",
    "text": " that isn\u2019t working as expected, because sometimes WCA can tell me why it isn\u2019t working and give tips on how to fix\nit. It can often catch typo-like errors that I overlooked.\u201d R1.45 pointed out how explanations saved them time: \u201cI like its\nability to explain functions of code which could take a bit to understand. It can save a lot of time.\u201d R2.177 described their\nuse of explanations to \u201cexplore areas of code I\u2019m not familiar with.\u201d Respondents did use WCA to generate code (55.6%),\ndocumentation (39.6%), and tests (35.7%), but to a lesser extent. We detail additional purposes of use in Appendix D.\nSome respondents used WCA to improve specific qualities or characteristics of code, including its readability (17.9%),\nmaintainability (15.2%), performance (12.8%), and security (6.7%). Usability test participants also indicated the importance\nof qualities including readability, maintainability, reliability, scalability, and testability. These desires for AI assistance\nbeyond mere code generation mirror user needs for controlling code-fluent LLM outputs discovered by Houde et al.\n[21].\nA small number of respondents reported not making use of WCA within the prior two weeks (N=28; 4.2%). They\nindicated that it was faster to do the work themselves (39.3%), they didn\u2019t feel that WCA provided helpful suggestions\n(32.1%), they hadn\u2019t conducted any code-related work within the prior two weeks (25.0%), and WCA didn\u2019t support\ntheir functional or non-functional requirements (14.3%). Multiple respondents indicated a need for WCA to handle the\nspecialized technologies with which they worked, such as R2.279, who commented, \u201cI am a db2 developer and WCA\u2019s\nknowledge on db2 internals is poor.\u201d R2.365 also noted, \u201cit does not seem a good tool to search about IBM Products/offerings...\nsuch as Maximo Application Suite and MAS Ansible DevOps automation.\u201d In enterprise scenarios, it may be even more\nimportant to support the specialized technologies used in these environments to increase the utility of such assistants.\nOne interesting theme that stood out to us was a reluctance to use WCA because it may reflect negatively upon\nthe user. R1.49 explained, \u201cI just don\u2019t use code generated by WCA@IBM... obviously I would not want to be seen with\ngenerated code in my PRs, so embarrassing!\u201d This sentiment was echoed by R1.92, who said, \u201cI imagine some people could\nfind it embarrassing because the tool is so new and very few people on my team use it. There is an inherent suspicion against\nAI-generated code.\u201d Although only two respondents raised this issue, it may be important for organizational leaders to\nfoster a culture in which AI assistance is viewed as acceptable to garner wide-spread adoption.\nFinally, we discovered a small group of technical writers who found utility in using WCA to understand technical\nconcepts without having to disturb their developer counterparts. R1.57 remarked, \u201cMy favorite feature is to understand\nthe technical terms and code provided by the Dev. This reduces the time in understanding the API terms and code rather\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev."
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_010",
    "source_id": "EnterpriseImpact2024",
    "text": " tool is so new and very few people on my team use it. There is an inherent suspicion against\nAI-generated code.\u201d Although only two respondents raised this issue, it may be important for organizational leaders to\nfoster a culture in which AI assistance is viewed as acceptable to garner wide-spread adoption.\nFinally, we discovered a small group of technical writers who found utility in using WCA to understand technical\nconcepts without having to disturb their developer counterparts. R1.57 remarked, \u201cMy favorite feature is to understand\nthe technical terms and code provided by the Dev. This reduces the time in understanding the API terms and code rather\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n7\nthan discussing with the Dev.\u201d Multiple respondents desired using WCA to \u201ccreate customer facing documentation\u201d\n(R1.27) and \u201chelp me writ[e] drafts following IBM content guidelines.\u201d (R1.68). These \u201coff-label\u201d use cases by people in\ndeveloper-adjacent roles surprised us and indicate the importance of taking a holistic view on the potential beneficiaries\nof AI code assistants.\n4.2\nUse of generated content\nRespondents reported using generated outputs \u2013 code, documentation, unit tests, and explanations \u2013 in different ways.\nOverall, the use of generated outputs without modification was not common (2-4% of respondents reported doing\nthis, depending on output type); rather, respondents often modified outputs before using them (9-19%) or used them\nfor another purpose, such as learning something new (23-35%) or getting new ideas (24-37%). Users described how\n\u201cthe results give me new ideas\u201d (R2.180) and \u201cIt is very helpful to get started writing code in a new language\u201d (R2.626)\nby \u201crecommend[ing] an approach I haven\u2019t thought of or I wasn\u2019t even aware of\u201d (R2.405). P1 described how \u201ccreating\ndiagrams in markdown works from the code.\u201d Users also talked about how WCA helped them recall \u201cconcepts which may\nbe I have forgotten during [the] course of time\u201d (R2.296) and aiding them when \u201c[I] know what to do, but don\u2019t know how\nto do it or forgot about that.\u201d (R2.292). These uses reinforce the value that generative AI provides in helping people\nlearn [1, 6, 49, 59].\n4.3\nImpact on productivity\nWe evaluated the impact of WCA on respondents\u2019 perceptions of productivity using multiple measures: 7-point semantic\ndifferential scales14 that assessed effort, quality, and speed [53] and a 4-item scale of self-efficacy15 (derived from [44]).\nOverall, respondents felt that WCA made their work easier (M (SD) = .78 (1.45), t(609) = 13.35, p < .001, 95% CI =\n[.67, .90]), of a better quality (M (SD) = .66 (1.25), t(603) = 13.02, p < .001, 95% CI = [.56, .76]), and faster (M (SD) = .57\n(1.48), t(606) = 9.57, p"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_011",
    "source_id": "EnterpriseImpact2024",
    "text": " that assessed effort, quality, and speed [53] and a 4-item scale of self-efficacy15 (derived from [44]).\nOverall, respondents felt that WCA made their work easier (M (SD) = .78 (1.45), t(609) = 13.35, p < .001, 95% CI =\n[.67, .90]), of a better quality (M (SD) = .66 (1.25), t(603) = 13.02, p < .001, 95% CI = [.56, .76]), and faster (M (SD) = .57\n(1.48), t(606) = 9.57, p < .001, 95% CI = [.46, .69]), as indicated by means and 95% CIs > 0. However, the magnitudes of\nproductivity improvements were small, further evidenced by self-efficacy ratings falling around the scale midpoint (M\n(SD) = 3.20 (.93) of 5).\nDespite net positive ratings, the benefits of WCA were not evenly distributed: 42.6% of respondents felt that WCA\nmade them less effective (self-efficacy \u22643), whereas 57.4% of respondents felt that WCA made them more effective\n(self-efficacy > 3). Users\u2019 comments shed light on WCA\u2019s mixed impact on productivity. Some users benefited from\nusing WCA in acceleration mode: \u201cIts ability to suggest code and code autocompletion... improves my work productivity\nsignificantly.\u201d (R2.608). P11 remarked, \u201cIt saves time compared to write from scratch by myself.\u201d WCA helped R2.236 come\nup to speed faster in a new project: \u201cI used WCA to help document and explain me new functionality in different classes to\nhelp understand it quicker, thus making my productivity faster.\u201d Other users were helped in exploration mode when WCA\n\u201cprovid[es] different approaches towards the problem\u201d (P11), which R2.311 described as \u201cmy favourite feature.\u201d P6 also felt,\n\u201cI like the code generation aspect... it helps me think about the solution.\u201d\nContrarily, imperfections in the quality of WCA\u2019s output may require additional user effort to identify and correct.\nR1.25 pointed out that, \u201cSometimes WCA goes off topic and it ends up wasting time that could\u2019ve been used to finish up\nthe work. If multiple retries are needed to get the desired result, it becomes counter-productive.\u201d (R1.25). P5 noted that \u201cIt\nhallucinated\u201d during the usability test. R2.658 highlighted the relationship between correctness and trust, saying, \u201cIf\n14These scales were coded from [-3, +3] such that ratings > 0 correspond to beneficial impact, ratings of 0 indicate no impact, and ratings < 0 correspond\nto detrimental impact.\n15Items were rated on 5-point Likert scales. A confirmatory factor analysis indicated this scale was highly reliable (Cronbach\u2019s \ud835\udefc= .91).\nManuscript submitted to ACM\n8\nWeisz et al.\nit doesn\u2019t have close to 100% correctness, then I cannot trust its answers on topics that I don\u2019t actually know myself. It is\nlimited to helping me speed up"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_012",
    "source_id": "EnterpriseImpact2024",
    "text": "\u201d during the usability test. R2.658 highlighted the relationship between correctness and trust, saying, \u201cIf\n14These scales were coded from [-3, +3] such that ratings > 0 correspond to beneficial impact, ratings of 0 indicate no impact, and ratings < 0 correspond\nto detrimental impact.\n15Items were rated on 5-point Likert scales. A confirmatory factor analysis indicated this scale was highly reliable (Cronbach\u2019s \ud835\udefc= .91).\nManuscript submitted to ACM\n8\nWeisz et al.\nit doesn\u2019t have close to 100% correctness, then I cannot trust its answers on topics that I don\u2019t actually know myself. It is\nlimited to helping me speed up more routine tasks that I can do myself.\u201d R2.603 commented, \u201cYou have to spend time to\nverify it.\u201d P13 similarly felt, \u201cWCA created code need[s] to be tested,\u201d and R2.183 lamented, \u201cit is a burden to have to double\ncheck answers...I still don\u2019t have enough confidence to blindly trust the responses.\u201d\nWe assessed the quality of WCA\u2019s outputs on a 5-point scale16: Very poor (1), Poor, Acceptable, Good, Very good (5).\nRespondents rated WCA\u2019s quality in the middle of the scale (M (SD) = 3.20 (.77)), indicating an \u201cacceptable\u201d quality but\nwith room for improvement. This level of quality was good enough for some respondents, but not others:\n\u201cI\u2019m really impressed with the quality of simple python programs that can be generated.\u201d (R2.143)\n\u201cThe code generated is of poor quality and often does not meet the stated requirements.\u201d (R1.62)\nAnother factor that impacted perceptions of productivity was the speed of WCA\u2019s responses. We assessed speed on a\n5-point scale: Very slow (1), Slow, Acceptable, Fast, Very fast (5). Speed was rated slightly below the scale midpoint (M\n(SD) = 2.88 (0.86)) and many respondents commented on how it needed to be improved: \u201cthe code suggestion needs to be\nfaster. At the moment it does not keep up with my typing.\u201d (R1.61).\nIt is clear that WCA had an impact on productivity, but its impact was mixed and disparate. Several respondents\npointed out how they viewed the role of WCA as an \u201cintern\u201d or \u201cjunior developer\u201d given its current performance:\n\u201cIn some ways I feel it\u2019s like an intern that just started on the project and does training on the job. Needs a lot\nof supervision, but can be handed some set of tasks.\u201d (R1.86)\n\u201cI tend to view it as a junior developer helping me out. It can generate code much more quickly than a junior\ndeveloper, and usually of higher quality, but there are still often mistakes, edge cases, etc. that need to be\nfixed. So I always need to be mindful that the generated code has to be carefully reviewed and often manually\nupdated before it can be used.\u201d (R1.11)\n4.4\nAuthorship & responsibility\nWorking with WCA is a co-creative process [36, 43, 55] in which both human users and WCA are capable of shaping an\nartifact-under-production, source code. Given the novelty of this"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_013",
    "source_id": "EnterpriseImpact2024",
    "text": " handed some set of tasks.\u201d (R1.86)\n\u201cI tend to view it as a junior developer helping me out. It can generate code much more quickly than a junior\ndeveloper, and usually of higher quality, but there are still often mistakes, edge cases, etc. that need to be\nfixed. So I always need to be mindful that the generated code has to be carefully reviewed and often manually\nupdated before it can be used.\u201d (R1.11)\n4.4\nAuthorship & responsibility\nWorking with WCA is a co-creative process [36, 43, 55] in which both human users and WCA are capable of shaping an\nartifact-under-production, source code. Given the novelty of this form of interaction, we were interested in understanding\nhow WCA impacted developers\u2019 perceptions of their authorship17 over code co-produced with WCA.\nWe considered how respondents felt about who authored code in four different scenarios: (1) reviewing WCA-\ngenerated code but implementing the functionality themselves; (2) pasting in WCA-generated code verbatim; (3) pasting\nin WCA-generated code but then modifying it; and (4) implementing an idea suggested by WCA.\nMany respondents viewed themselves as the sole author when implementing functionality themselves (scenario 1:\n57.5%). Similarly, many respondents viewed WCA as the sole author when they pasted its outputs verbatim (scenario 2:\n53.7%). Interestingly, some respondents felt a shared sense of authorship across all scenarios: when it produced code that\nthey rewrote (scenario 1: 30.4%), when they used its outputs verbatim (scenario 2: 27.8%), when each party contributed\nto the code (scenario 3: 64.4%), and even when WCA only contributed ideas (scenario 4: 39.8%). These results suggest\nthat new mechanisms may be needed to track co-creative activities and ensure that each party\u2019s contributions \u2013 human\nand AI \u2013 are properly attributed in ways that do not \u201csteal the recognition and hardwork of the developers\u201d (R2.632).\n16Quality was assessed separately for each type of generated content: code, documentation, unit tests, explanations, and Q&A responses. These items\nformed a scale with high reliability (Cronbach\u2019s \ud835\udefc= 0.83 and thus were averaged into a single metric of quality.\n17Authorship and ownership are related, but distinct concepts. Ownership provides certain rights over the work, such as holding the copyright; as our\nusers are employed by a corporation that owns the intellectual property produced by their employees, we assessed feelings of authorship rather than\nownership.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n9\nAnother aspect of code authorship concerns the responsibility for mitigating the risks of generated outputs [22], such\nas ensuring generated code does not violate the IP rights of other parties. Given the current legal climate concerning\nthe reproduction of copyrighted works by LLMs (e.g. [9, 45]), we also wanted to understand the extent to which our\nusers felt responsible for ensuring that WCA did not reproduce copyrighted works18.\nLiang et al. [28] found that 46% of their participants had concerns that AI programming assistants may produce\n"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_014",
    "source_id": "EnterpriseImpact2024",
    "text": "ship rather than\nownership.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n9\nAnother aspect of code authorship concerns the responsibility for mitigating the risks of generated outputs [22], such\nas ensuring generated code does not violate the IP rights of other parties. Given the current legal climate concerning\nthe reproduction of copyrighted works by LLMs (e.g. [9, 45]), we also wanted to understand the extent to which our\nusers felt responsible for ensuring that WCA did not reproduce copyrighted works18.\nLiang et al. [28] found that 46% of their participants had concerns that AI programming assistants may produce\ncode that infringes on intellectual property. By contrast, most of our respondents (83.4%) expressed similar concerns\nover WCA reproducing copyrighted materials not owned by IBM. In addition, most respondents felt that they had a\nresponsibility (89.2%) and that WCA had a responsibility (96.2%) to ensure copyrighted materials were not included in\ntheir source code. R2.549 aptly summarized their own responsibilities: \u201c...if I am the person who uses the tool, it\u2019s my\nresponsibility know what is going to be part of the code or not.\u201d Conversely, R2.459 expected WCA to detect copyrighted\nmaterial: \u201cit seems like copyrighted material should be detected as copyrighted and if this is not something WCA can\ndo, it should definitely immediately do [so].\u201d Many respondents expected that, \u201call content generated or reproduced by\nWCA@IBM adheres to copyright regulations,\u201d (R2.652) and \u201cWCA needs to fully own responsibility for not reproducing\ncopyrighted code\u201d (R1.86). Thus, protecting the integrity of a codebase is seen as a shared responsibility, and one for\nwhich developers may require new kinds of support.\n4.5\nImpact on job role\nRespondents held a wide range of views on how WCA would change their job role, responsibilities, and purpose as a\ndeveloper. Some users felt that there would be \u201cNo Change\u201d (R1.13), or that \u201cIt does not [change my role]. Yet, anyway :o)\u201d\n(R1.52), because \u201cI\u2019m still better than WCA\u201d (R2.546) or \u201cI see it as a tool to better my work experience\u201d (R2.142). Other\nusers felt that their role would change, but weren\u2019t sure of how: \u201cIt confuses me on what my duty is as the boundary is\nnot clear\u201d (R2.472). Echoing sentiments of WCA\u2019s role as a \u201cjunior developer\u201d or \u201cintern,\u201d some users felt it would free\nthem up to focus on the higher-level, creative aspects of their profession:\n\u201cI believe WCA has large potential to generate boilerplate code and implement trivial pieces of code. This\nwould save developers time and effort writing a lot of repetitive and mundane code. It will allow developers to\nfocus on solving the harder, higher level, and more creative problems.\u201d (R1.24)\nR2.304 also pointed out that having an AI assistant focus on the \u201cmany dirty repeated tasks like security fixes, add unit\ntest cases/ translate the unit test cases etc.\u201d would allow them to \u201csave my"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_015",
    "source_id": "EnterpriseImpact2024",
    "text": ".472). Echoing sentiments of WCA\u2019s role as a \u201cjunior developer\u201d or \u201cintern,\u201d some users felt it would free\nthem up to focus on the higher-level, creative aspects of their profession:\n\u201cI believe WCA has large potential to generate boilerplate code and implement trivial pieces of code. This\nwould save developers time and effort writing a lot of repetitive and mundane code. It will allow developers to\nfocus on solving the harder, higher level, and more creative problems.\u201d (R1.24)\nR2.304 also pointed out that having an AI assistant focus on the \u201cmany dirty repeated tasks like security fixes, add unit\ntest cases/ translate the unit test cases etc.\u201d would allow them to \u201csave my time to focus on innovative feature design and\ndevelopment.\u201d\nUsers were sensitive to the potential deskilling aspects of AI assistance [42, 51]. R2.185 espoused a view that, \u201c[I]\n[d]on\u2019t really like to use AI, makes people lazy and promotes to concept of not to think.\u201d R2.547 expressed the same concern,\nalbeit more bluntly: \u201dI suspect we\u2019re all going to get a lot stupider, doing a worse job of maintaining larger amounts of\nworse code.\u201d However, technological acceptance takes time [14], and new technologies have a learning curve before\npeople can use them effectively; for LLMs, effective use requires learning the art of prompt engineering [29, 32]. WCA\nis no exception: \u201cVery few people on my team have tried it, and many aren\u2019t sure how to engineer prompts to get effective\nanswers yet\u201d (R1.92).\nOne final impact we noticed on the role of the developer is a consequence of AI augmenting human abilities: when a\ndeveloper is more productive with AI, the expectations of their productivity may also be increased. This sentiment was\n18Our examination of this topic is not predicated on observations of WCA actually reproducing copyrighted works; Mishra et al. [35] provide an\nexplanation of the data used to train the underlying Granite model.\nManuscript submitted to ACM\n10\nWeisz et al.\ncaptured by P6, who said, \u201cSince our team is expected to use WCA our management is expecting more work in one sprint as\ncompared to before.\u201d\n5\nDiscussion & Conclusions\nOur examination of WCA usage revealed a number of insights on the use of AI code assistants within the enterprise\nand their impact on developer productivity.\n\u2022 Code understanding is a top use case. We anticipated that code generation would be a top use case due to the\nnature of generative AI and previous examinations of AI code assistants [28]. Rather, WCA was primarily used\nto support code understanding, either via explanations of existing code or through answers to general questions.\nThis observation motivates the need for further research into how AI code assistants can help developers conduct\nsensemaking tasks in existing code repositories and bolster their knowledge of programming languages and\nconcepts.\n\u2022 Software engineering is a co-creative activity. Users generally did not accept the code produced by WCA\nwithout first reviewing and modifying it, suggesting that overreliance problems may not be as prevalent with\nAI code assistants as in other domains [3, 4, 8]. Additionally, they felt a shared sense of authorship over the\ncode produced with WCA, even in instances"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_016",
    "source_id": "EnterpriseImpact2024",
    "text": " code assistants [28]. Rather, WCA was primarily used\nto support code understanding, either via explanations of existing code or through answers to general questions.\nThis observation motivates the need for further research into how AI code assistants can help developers conduct\nsensemaking tasks in existing code repositories and bolster their knowledge of programming languages and\nconcepts.\n\u2022 Software engineering is a co-creative activity. Users generally did not accept the code produced by WCA\nwithout first reviewing and modifying it, suggesting that overreliance problems may not be as prevalent with\nAI code assistants as in other domains [3, 4, 8]. Additionally, they felt a shared sense of authorship over the\ncode produced with WCA, even in instances when WCA only provided ideas and not actual source code. This\nobservation suggests that new mechanisms for tracking authorship attribution may be necessary to give each\nparty proper credit for their work, in addition to tracking the provenance of contributions within a larger source\ncode repository.\n\u2022 Perfection is not required. Echoing the title of the paper by Weisz et al. [52], we similarly observed that (some)\ndevelopers felt productivity improvements from WCA despite variability in its quality and speed. We anticipate\nthat quality and speed improvements will help more developers receive these benefits. We also observed that\nsome developers may require additional training in effectively prompting LLMs.\n\u2022 Mitigating risks in generated outputs is a joint responsibility. LLMs have the potential to reproduce\nmaterials contained within their training data [7]; in the domain of software engineering, there is a risk of\ncontaminating a codebase with copyrighted material or code subject to specific licenses (e.g. GPL). Users felt that\nthey themselves, as well as WCA, were responsible for minimizing this risk. New socio-technical approaches\nthat combine algorithmic methods (e.g. code similarity detection [24, 39]) with intelligent, human-driven review\ninterfaces, may be needed to minimize these risks.\n\u2022 Developers are worried about deskilling and early adoption. Users were concerned that their use of AI\nassistants may result in a loss of skills. In addition, two users were reluctant to use WCA due to perceived\nnegative social consequences of being an early adopter. These findings indicate a need to more clearly articulate\nthe benefits of AI code assistants and an organization\u2019s stance on their use: the developer profession will change\nto focus on higher-level, creative aspects of the work, and clear organizational policies can help employees feel\nmore comfortable using AI code assistants in their work.\nOur case study represents the use of a specific AI code assistant at a specific point of time (the summer of 2024).\nWith the rapid pace of advancement in AI, we anticipate some issues experienced by our users, especially regarding\nquality and speed, will naturally diminish as model performance increases. What won\u2019t change is the need for human\ningenuity and insight to determine what software systems to build, even if the mechanics of how those systems are\nbuilt are increasingly aided by AI.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n11\nAcknowledgments\nWe thank Katharina Schippert and Robin Auer for their support in defining the WCA user research program and\nrecruiting participants for our studies. We also thank Keri Olson and Melissa"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_017",
    "source_id": "EnterpriseImpact2024",
    "text": "2024).\nWith the rapid pace of advancement in AI, we anticipate some issues experienced by our users, especially regarding\nquality and speed, will naturally diminish as model performance increases. What won\u2019t change is the need for human\ningenuity and insight to determine what software systems to build, even if the mechanics of how those systems are\nbuilt are increasingly aided by AI.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n11\nAcknowledgments\nWe thank Katharina Schippert and Robin Auer for their support in defining the WCA user research program and\nrecruiting participants for our studies. We also thank Keri Olson and Melissa Modjeski whose support made this research\npossible. Finally, we thank all of our users who provided us with valuable feedback.\nReferences\n[1] Ibrahim Adeshola and Adeola Praise Adepoju. 2023. The opportunities and challenges of ChatGPT in education. Interactive Learning Environments\n(2023), 1\u201314.\n[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation.\narXiv preprint arXiv:2103.06333 (2021).\n[3] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Narendra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin,\nQian Pan, Christine T Wolf, et al. 2021. Ai-assisted human labeling: Batching for efficiency without overreliance. Proceedings of the ACM on\nHuman-Computer Interaction 5, CSCW1 (2021), 1\u201327.\n[4] Zahra Ashktorab, Qian Pan, Werner Geyer, Michael Desmond, Marina Danilevsky, James M Johnson, Casey Dugan, and Michelle Bachman. 2024.\nEmerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions. arXiv\npreprint arXiv:2409.08937 (2024).\n[5] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with code-generating models.\nProceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85\u2013111.\n[6] Brett A Becker, Michelle Craig, Paul Denny, Hieke Keuning, Natalie Kiesler, Juho Leinonen, Andrew Luxton-Reilly, James Prather, and Keith Quille.\n2023. Generative ai in introductory programming. Name of Journal (2023).\n[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language\nmodels be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610\u2013623.\n[8] Zana Bu\u00e7inca, Maja Barbara Malaya, and Krzysztof Z Gajos. 202"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_018",
    "source_id": "EnterpriseImpact2024",
    "text": " Hieke Keuning, Natalie Kiesler, Juho Leinonen, Andrew Luxton-Reilly, James Prather, and Keith Quille.\n2023. Generative ai in introductory programming. Name of Journal (2023).\n[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language\nmodels be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610\u2013623.\n[8] Zana Bu\u00e7inca, Maja Barbara Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in\nAI-assisted decision-making. Proceedings of the ACM on Human-computer Interaction 5, CSCW1 (2021), 1\u201321.\n[9] Matthew Butterick. 2022. GitHub Copilot Litigation. Retrieved 04-Oct-2024 from https://githubcopilotlitigation.com\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).\n[11] Lan Cheng, Emerson Murphy-Hill, Mark Canning, Ciera Jaspan, Collin Green, Andrea Knight, Nan Zhang, and Elizabeth Kammer. 2022. What\nimproves developer productivity at google? code quality. In Proceedings of the 30th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering. 1302\u20131313.\n[12] Victoria Clarke and Virginia Braun. 2017. Thematic analysis. The journal of positive psychology 12, 3 (2017), 297\u2013298.\n[13] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot\nai pair programmer: Asset or liability? Journal of Systems and Software 203 (2023), 111734.\n[14] Fred D Davis, RP Bagozzi, and PR Warshaw. 1989. Technology acceptance model. J Manag Sci 35, 8 (1989), 982\u20131003.\n[15] Prem Devanbu, Sakke Karstu, Walcelio Melo, and William Thomas. 1996. Analytical and empirical evaluation of software reuse metrics. In Proceedings\nof IEEE 18th International Conference on Software Engineering. IEEE, 189\u2013199.\n[16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:\nA pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).\n["
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_019",
    "source_id": "EnterpriseImpact2024",
    "text": "), 982\u20131003.\n[15] Prem Devanbu, Sakke Karstu, Walcelio Melo, and William Thomas. 1996. Analytical and empirical evaluation of software reuse metrics. In Proceedings\nof IEEE 18th International Conference on Software Engineering. IEEE, 189\u2013199.\n[16] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:\nA pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).\n[17] Nicole Forsgren, Margaret-Anne Storey, Chandra Maddila, Thomas Zimmermann, Brian Houck, and Jenna Butler. 2021. The SPACE of Developer\nProductivity: There\u2019s more to it than you think. Queue 19, 1 (2021), 20\u201348.\n[18] GitClear. [n. d.]. Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality. Retrieved 24-Sep-2024 from https://www.gitclear.com/\ncoding_on_copilot_data_shows_ais_downward_pressure_on_code_quality\n[19] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020.\nGraphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366 (2020).\n[20] Maurice H Halstead. 1977. Elements of Software Science (Operating and programming systems series). Elsevier Science Inc.\n[21] Stephanie Houde, Vignesh Radhakrishna, Praneeth Reddy, Juie Darwade, Haoran Hu, Kalpesh Krishna, Mayank Agarwal, Kartik Talamadupula, and\nJustin Weisz. 2022. User and technical perspectives of controllable code generation. In Annual Conference on Neural Information Processing Systems.\n[22] IBM. 2024. AI Risk Atlas. Retrieved 07-Oct-2024 from https://www.ibm.com/docs/en/watsonx/saas?topic=ai-risk-atlas\n[23] Saki Imai. 2022. Is github copilot a substitute for human pair-programming? an empirical study. In Proceedings of the ACM/IEEE 44th International\nConference on Software Engineering: Companion Proceedings. 319\u2013321.\n[24] Sarthak Jain, Aditya Dora, Ka Seng Sam, and Prabhat Singh. 2024. LLM Agents Improve Semantic Code Search. arXiv preprint arXiv:2408.11058\n(2024).\nManuscript submitted to ACM\n12\nWeisz et al.\n[25] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models\nresolve real-world github issues? ar"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_020",
    "source_id": "EnterpriseImpact2024",
    "text": " ACM/IEEE 44th International\nConference on Software Engineering: Companion Proceedings. 319\u2013321.\n[24] Sarthak Jain, Aditya Dora, Ka Seng Sam, and Prabhat Singh. 2024. LLM Agents Improve Semantic Code Search. arXiv preprint arXiv:2408.11058\n(2024).\nManuscript submitted to ACM\n12\nWeisz et al.\n[25] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models\nresolve real-world github issues? arXiv preprint arXiv:2310.06770 (2023).\n[26] Amy J Ko and Brad A Myers. 2005. A framework and methodology for studying the causes of software errors in programming systems. Journal of\nVisual Languages & Computing 16, 1-2 (2005), 41\u201384.\n[27] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-offs for substituting a human with an agent in a pair programming\ncontext: the good, the bad, and the ugly. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u201320.\n[28] Jenny T Liang, Chenyang Yang, and Brad A Myers. 2024. A large-scale survey on the usability of ai programming assistants: Successes and challenges.\nIn Proceedings of the 46th IEEE/ACM International Conference on Software Engineering. 1\u201313.\n[29] Leo S Lo. 2023. The art and science of prompt engineering: a new literacy in the information age. Internet Reference Services Quarterly 27, 4 (2023),\n203\u2013210.\n[30] Graham C. Low and D. Ross Jeffery. 1990. Function points in the estimation and evaluation of the software process. IEEE transactions on Software\nEngineering 16, 1 (1990), 64\u201371.\n[31] Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. 2024. Repoagent: An\nllm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667 (2024).\n[32] Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. 2023. Prompt engineering in large language models. In\nInternational conference on data intelligence and cognitive informatics. Springer, 387\u2013402.\n[33] Thomas J McCabe. 1976. A complexity measure. IEEE Transactions on software Engineering 4 (1976), 308\u2013320.\n[34] Andr\u00e9 N Meyer, Thomas Fritz, Gail C Murphy, and Thomas Zimmermann. 2014. Software developers\u2019 perceptions of productivity. In Proceedings of\nthe 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 19\u201329.\n[35] Mayank Mishra,"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_021",
    "source_id": "EnterpriseImpact2024",
    "text": "wango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. 2023. Prompt engineering in large language models. In\nInternational conference on data intelligence and cognitive informatics. Springer, 387\u2013402.\n[33] Thomas J McCabe. 1976. A complexity measure. IEEE Transactions on software Engineering 4 (1976), 308\u2013320.\n[34] Andr\u00e9 N Meyer, Thomas Fritz, Gail C Murphy, and Thomas Zimmermann. 2014. Software developers\u2019 perceptions of productivity. In Proceedings of\nthe 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. 19\u201329.\n[35] Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha\nSurendran, Shivdeep Singh, et al. 2024. Granite code models: A family of open foundation models for code intelligence. arXiv preprint arXiv:2405.04324\n(2024).\n[36] Caterina Moruzzi and Solange Margarido. 2024. A user-centered framework for human-ai co-creativity. In Extended Abstracts of the CHI Conference\non Human Factors in Computing Systems. 1\u20139.\n[37] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In\nProceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1\u201313.\n[38] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub copilot\u2019s code suggestions. In Proceedings of the 19th International Conference\non Mining Software Repositories. 1\u20135.\n[39] Matija Novak, Mike Joy, and Dragutin Kermek. 2019. Source-code similarity detection and detection tools used in academia: a systematic review.\nACM Transactions on Computing Education (TOCE) 19, 3 (2019), 1\u201337.\n[40] Edson Oliveira, Eduardo Fernandes, Igor Steinmacher, Marco Cristo, Tayana Conte, and Alessandro Garcia. 2020. Code and commit metrics of\ndeveloper productivity: a study on team leaders perceptions. Empirical Software Engineering 25 (2020), 2519\u20132549.\n[41] Ruchika Pandey, Prabhat Singh, Raymond Wei, and Shaila Shankar. 2024. Transforming Software Development: Evaluating the Efficiency and\nChallenges of GitHub Copilot in Real-World Projects. arXiv preprint arXiv:2406.17910 (2024).\n[42] Janet Rafner, Dominik Dellermann, Arthur Hjorth, D\u00f3ra Veraszt\u00f3, Constance Kampf, Wendy Mackay, and Jacob Sherson. 2022. Deskilling, upskilling,\nand reskilling: a case for hybrid intelligence. Morals & Machines 1, 2 (2022), 24\u201339.\n[43] Jeba Rezwana and Mary Lou Maher. 2023. User"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_022",
    "source_id": "EnterpriseImpact2024",
    "text": ", and Shaila Shankar. 2024. Transforming Software Development: Evaluating the Efficiency and\nChallenges of GitHub Copilot in Real-World Projects. arXiv preprint arXiv:2406.17910 (2024).\n[42] Janet Rafner, Dominik Dellermann, Arthur Hjorth, D\u00f3ra Veraszt\u00f3, Constance Kampf, Wendy Mackay, and Jacob Sherson. 2022. Deskilling, upskilling,\nand reskilling: a case for hybrid intelligence. Morals & Machines 1, 2 (2022), 24\u201339.\n[43] Jeba Rezwana and Mary Lou Maher. 2023. User perspectives on ethical challenges in Human-AI co-creativity: A design fiction study. In Proceedings\nof the 15th Conference on Creativity and Cognition. 62\u201374.\n[44] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmer\u2019s assistant: Conversational\ninteraction with a large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces.\n491\u2013514.\n[45] Emma Roth. 2024. The developers suing over GitHub Copilot got dealt a major blow in court. The Verge (9 July 2024). Retrieved 04-Oct-2024 from\nhttps://www.theverge.com/2024/7/9/24195233/github-ai-copyright-coding-lawsuit-microsoft-openai\n[46] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised translation of programming languages.\nAdvances in neural information processing systems 33 (2020), 20601\u201320611.\n[47] Caitlin Sadowski and Thomas Zimmermann. 2019. Rethinking productivity in software engineering. Springer Nature.\n[48] Max Sch\u00e4fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023. An empirical evaluation of using large language models for automated unit test\ngeneration. IEEE Transactions on Software Engineering (2023).\n[49] Shamini Shetye. 2024. An Evaluation of Khanmigo, a Generative AI Tool, as a Computer-Assisted Language Learning App. Studies in Applied\nLinguistics and TESOL 24, 1 (2024).\n[50] Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022. Expectation vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1\u20137.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists\u2019 perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1\u201324.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev."
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_023",
    "source_id": "EnterpriseImpact2024",
    "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1\u20137.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists\u2019 perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1\u201324.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402\u2013412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369\u2013391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172\u2013178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24\u201329, 2021, Proceedings, Part I 23. Springer, 171\u2013190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1\u201347.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585\u20131608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot\u2019s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62\u201371.\n[59] Ramaz"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_024",
    "source_id": "EnterpriseImpact2024",
    "text": " Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585\u20131608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot\u2019s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62\u201371.\n[59] Ramazan Yilmaz and Fatma Gizem Karaoglan Yilmaz. 2023. The effect of generative artificial intelligence (AI)-based tool use on students\u2019\ncomputational thinking skills, programming self-efficacy and motivation. Computers and Education: Artificial Intelligence 4 (2023), 100147.\n[60] Beiqi Zhang, Peng Liang, Xiyu Zhou, Aakash Ahmad, and Muhammad Waseem. 2023. Practices and challenges of using github copilot: An empirical\nstudy. arXiv preprint arXiv:2303.08733 (2023).\n[61] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022.\nProductivity assessment of neural code completion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming.\n21\u201329.\n[62] Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2024.\nMeasuring GitHub Copilot\u2019s Impact on Productivity. Commun. ACM 67, 3 (2024), 54\u201363.\nManuscript submitted to ACM\n14\nWeisz et al.\nA\nSurvey instrument\nIn this section, we provide a listing of the survey questions discussed in this case study. Some questions included in\nthe survey have been omitted as their relevance to this case study is low, but their inclusion was important for other\ninternal stakeholders.\nA.1\nUsage within the past two weeks\nThe survey began by asking whether the respondent had used WCA within the past two weeks.\n1. Have you used WCA within the past 2 weeks?\n\u2022 Yes\n\u2022 No\nA.2\nMotivations for use\nIf the respondent indicated they used WCA within the past two weeks, they were asked about their motivations for using\nWCA.\n2a. For what purposes did you use WCA? Please check all that apply. Please respond based only on your usage of\nWCA within the past 2 weeks.\n\u2022 Generate code (or code snippets) in the chat\n\u2022 Generate code via autocompletion in the source editor\n\u2022 Generate documentation\n\u2022 Generate tests\n\u2022 Explain code\n\u2022 Answer questions about APIs or libraries\n\u2022 Answer general programming questions\n\u2022 Generate alternate ways to implement a method or functionality\n\u2022 Refactor code\n\u2022 Optimize code for performance (e.g."
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_025",
    "source_id": "EnterpriseImpact2024",
    "text": " No\nA.2\nMotivations for use\nIf the respondent indicated they used WCA within the past two weeks, they were asked about their motivations for using\nWCA.\n2a. For what purposes did you use WCA? Please check all that apply. Please respond based only on your usage of\nWCA within the past 2 weeks.\n\u2022 Generate code (or code snippets) in the chat\n\u2022 Generate code via autocompletion in the source editor\n\u2022 Generate documentation\n\u2022 Generate tests\n\u2022 Explain code\n\u2022 Answer questions about APIs or libraries\n\u2022 Answer general programming questions\n\u2022 Generate alternate ways to implement a method or functionality\n\u2022 Refactor code\n\u2022 Optimize code for performance (e.g. runtime or memory usage)\n\u2022 Translate code to another programming language\n\u2022 Fix code\n\u2022 Improve the readability of code\n\u2022 Improve the maintainability of code\n\u2022 Identify security issues in code\n\u2022 Leave feedback for the WCA development team\n2b. For what other purposes did you use WCA? Please respond based only on your usage of WCA within the past 2\nweeks.\n\u2022 Open-ended response\nA.3\nReasons for non-use\nIf the respondent indicated they had not used WCA in the past two weeks, they were then asked about why. These items\nwere based on Liang et al. [28] and expanded upon by us.\n3a. For what reasons have you not used WCA within the past 2 weeks? Please check all that apply.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n15\n\u2022 WCA writes code that doesn\u2019t meet functional or non-functional (e.g. security, performance) requirements that I\nneed\n\u2022 It\u2019s hard to control WCA to get code that I want\n\u2022 I spent too much time debugging or modifying code written by WCA\n\u2022 I don\u2019t think WCA provided helpful suggestions\n\u2022 I don\u2019t want WCA to have access to my code\n\u2022 I write and use code that WCA wasn\u2019t trained on which limits its ability to provide assistance\n\u2022 I found WCA\u2019s suggestions too distracting\n\u2022 I don\u2019t understand the code generated by WCA\n\u2022 I don\u2019t understand the documentation generated by WCA\n\u2022 I don\u2019t understand the explanations generated by WCA\n\u2022 Code generated by WCA doesn\u2019t perform well enough for my needs\n\u2022 It is faster to do the work myself\n\u2022 I have not done any code-related tasks in the past 2 weeks\n3b. Did you have any other reasons for not using WCA? What were they?\n\u2022 Open-ended response\nA.4\nLikes & dislikes\n4a. Please explain what you like about WCA. What is your favorite feature? Why?\n\u2022 Open-ended response\n4b. Please explain what you dislike about WCA@IBM. What is your least favorite feature? Why?\n\u2022 Open-ended response\nA.5\nSelf-efficacy\nThe items in this scale were derived from Ross et al. [44] and were expanded upon by us. Each item was rated on a 5-point\nLikert scale: Strongly disagree, Disagree, Neither disagree nor agree, Agree, Strongly agree.\n5. How would you characterize your experience using WCA?\n\u2022 WCA helps me write"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_026",
    "source_id": "EnterpriseImpact2024",
    "text": " were they?\n\u2022 Open-ended response\nA.4\nLikes & dislikes\n4a. Please explain what you like about WCA. What is your favorite feature? Why?\n\u2022 Open-ended response\n4b. Please explain what you dislike about WCA@IBM. What is your least favorite feature? Why?\n\u2022 Open-ended response\nA.5\nSelf-efficacy\nThe items in this scale were derived from Ross et al. [44] and were expanded upon by us. Each item was rated on a 5-point\nLikert scale: Strongly disagree, Disagree, Neither disagree nor agree, Agree, Strongly agree.\n5. How would you characterize your experience using WCA?\n\u2022 WCA helps me write better code\n\u2022 WCA helps me write code more quickly\n\u2022 I feel more productive when using WCA\n\u2022 I spend less time searching for information while using WCA\nA.6\nSpeed\n6. How would you characterize the speed of receiving a chat response from WCA?\n\u2022 I have not used the chat feature, Unusably slow, Very slow, Slow, Acceptable, Fast, Very fast\nA.7\nCopyright\n7a. How concerned are you that WCA may reproduce copyrighted materials not owned by IBM?\n\u2022 Not concerned at all, Slightly concerned, Moderately concerned, Concerned, Very concerned\nManuscript submitted to ACM\n16\nWeisz et al.\nRespondents were asked to rate the degree of responsibility for each party on a 5-point scale: Not at all responsible, Slightly\nresponsible, Moderately responsible, Responsible, Very responsible.\n7b. When using WCA, to what extent are the following parties responsible for ensuring that copyrighted materials\nnot owned by IBM are not included in IBM source code?\n\u2022 Me\n\u2022 WCA\nA.8\nAdditional improvements\n8. What single aspect of WCA should be immediately improved? Please explain.\n\u2022 Open-ended response\nA.9\nDemographics\n9a. How many years of service do you have at IBM (based on date of hire)\n\u2022 < 6 months, 6 months\u20131 year, 1-2 years, 3-5 years, 6-10 years, 11-15 years, 16-20 years, 21-25 years, 26-30 years, 31+\nyears\n9b. How many years have you held a role as a professional software engineer (at IBM or other companies)?\n\u2022 0-5 years, 6-10 years, 10+ years\n9c. What is your job role? Please check all that apply.\n\u2022 Back-End Developer\n\u2022 Compiler Technology Developer\n\u2022 Developer Advocate\n\u2022 DevOps Developer\n\u2022 Firmware Developer\n\u2022 Front-End Developer\n\u2022 L3 Support Engineer\n\u2022 QA/Test Developer\n\u2022 Software Architect\n\u2022 Software Performance Analyst\n\u2022 Other: free text\nA.10\nModule 2\nAt this point in the survey, respondents were asked whether they wished to continue to provide additional, in-depth feedback.\n10. Would you like to provide additional feedback on WCA?\n\u2022 Yes\n\u2022 No\nA.11\nIn-depth quality\nWe asked respondents to rate the quality of each type of generated output on a 5-point scale: Very poor, Poor, Acceptable,\nGood, Very good. These ratings were averaged to produce"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_027",
    "source_id": "EnterpriseImpact2024",
    "text": "End Developer\n\u2022 Compiler Technology Developer\n\u2022 Developer Advocate\n\u2022 DevOps Developer\n\u2022 Firmware Developer\n\u2022 Front-End Developer\n\u2022 L3 Support Engineer\n\u2022 QA/Test Developer\n\u2022 Software Architect\n\u2022 Software Performance Analyst\n\u2022 Other: free text\nA.10\nModule 2\nAt this point in the survey, respondents were asked whether they wished to continue to provide additional, in-depth feedback.\n10. Would you like to provide additional feedback on WCA?\n\u2022 Yes\n\u2022 No\nA.11\nIn-depth quality\nWe asked respondents to rate the quality of each type of generated output on a 5-point scale: Very poor, Poor, Acceptable,\nGood, Very good. These ratings were averaged to produce an overall quality score.\n11. How would you characterize the quality of WCA\u2019s outputs?\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n17\n\u2022 Generated code (within chat or the source editor)\n\u2022 Generated unit tests\n\u2022 Generated documentation\n\u2022 Generated explanations of code\n\u2022 Answers to general questions (within chat)\nA.12\nIn-depth motivations for use\nThese motivations were derived from Liang et al. [28] and were expanded upon by us. Each item was rated on a 5-point\nscale: Not important at all, Slightly important, Moderately important, Important, Very important.\n12a. What motivations did you have for using WCA?\n\u2022 To have an autocomplete or reduce the amount of keystrokes I make\n\u2022 To finish my programming tasks faster\n\u2022 To skip needing to go online to find specific code snippets, programming syntax, or API calls I\u2019m aware of, but\ncan\u2019t remember\n\u2022 To discover potential ways or starting points to write a solution to a problem I\u2019m facing\n\u2022 To find an edge case for my code I haven\u2019t considered\n\u2022 My line management recommended I use it\n\u2022 My colleagues recommended I use it\n\u2022 I like to explore new tools\n\u2022 It is my responsibility to try new IBM products\n\u2022 I wanted to know more about how my work might change in the future\n12b. Did you have any other reasons for using WCA? What were they?\n\u2022 Open-ended response\nA.13\nUse of generated content\nIf the respondent indicated they used WCA within the past two weeks, they were asked these questions about how they\nmade use of content generated by WCA.\n13a. Within the past 2 weeks, how have you used content produced by WCA? Please check all that apply. Please respond\nbased only on your usage of WCA within the past 2 weeks.\n\u2022 Code snippets (excluding unit tests)\n\u2022 Unit tests\n\u2022 Natural language explanations\n\u2022 Documentation\nThe items above were rated on the following scale: I included it in my code without modification, I included it in my code\nand made changes to it, I used it to give me new ideas, I used it to learn something new, I didn\u2019t make use of it\n13b. Within the past 2 weeks, how did you decide when to use WCA versus completing a task yourself? Please respond\nbased only on your usage of WCA within the past 2 weeks.\n\u2022 Open-ended response\nManuscript submitted to ACM\n18\nWeisz et al.\nA.14\nImpact on productivity"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_028",
    "source_id": "EnterpriseImpact2024",
    "text": " 2 weeks.\n\u2022 Code snippets (excluding unit tests)\n\u2022 Unit tests\n\u2022 Natural language explanations\n\u2022 Documentation\nThe items above were rated on the following scale: I included it in my code without modification, I included it in my code\nand made changes to it, I used it to give me new ideas, I used it to learn something new, I didn\u2019t make use of it\n13b. Within the past 2 weeks, how did you decide when to use WCA versus completing a task yourself? Please respond\nbased only on your usage of WCA within the past 2 weeks.\n\u2022 Open-ended response\nManuscript submitted to ACM\n18\nWeisz et al.\nA.14\nImpact on productivity\nThese scales were reproduced from Weisz et al. [53]. Each item was rated on a 7-point semantic differential scale.\n14. Do you feel that WCA makes your work...\n\u2022 More difficult / Easier\n\u2022 Slower / Faster\n\u2022 Of the worst quality / Of the best quality\nA.15\nIn-depth use of generated content\nThe items in this section were rated on a 5-point scale: Never, Rarely, Sometimes, Often, Always.\n15. How often do you take these actions to evaluate content generated by WCA?\n\u2022 Quickly check the generated code for specific keywords or logic structures\n\u2022 Compile, type check, lint, and/or use an in-IDE syntax checker\n\u2022 Execute the generated code\n\u2022 Examine details of the generated code\u2019s logic in depth\n\u2022 Consult API documentation\nA.16\nAuthorship\nThese items in this question were rated on a 4-point scale: Me, Both of us, WCA, I\u2019m unsure.\n16a. When using WCA, who do you consider to be the author of the code when...\n\u2022 I paste WCA-generated code into my source file verbatim\n\u2022 I paste WCA-generated code into my source file but then make edits to it\n\u2022 I review code generated by WCA, but ultimately implement the functionality myself\n\u2022 I implement an idea suggested by WCA\n16b. How does WCA change your perceptions of your job role, responsibilities, and purpose as a developer?\n\u2022 Open-ended response\nA.17\nFinal comments\n17a. Please describe your ideal vision for how WCA could help boost your productivity as a developer. Consider the\nfollowing ideas: What additional features or capabilities does it need? How would it fit into your workflow? What kinds\nof tasks would you use it for and what kinds of tasks would you not use it for?\n\u2022 Open-ended response\n17b. Is there any other feedback you would like to provide about WCA or this survey?\n\u2022 Open-ended response\nB\nRespondents & participants\nFor survey respondents, we show the distributions of their years of experience as a professional software engineer in\nFigure 1a, tenure with IBM in Figure 1b, and geography in Figure 1c (survey questions 9a-9c).\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n19\nC\nMotivations for using AI programming assistants\nTable 2 shows a detailed comparison of motivations for using or not using AI programming assistants between our survey\nrespondents and the software engineers who responded to the survey reported by Li"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_029",
    "source_id": "EnterpriseImpact2024",
    "text": " like to provide about WCA or this survey?\n\u2022 Open-ended response\nB\nRespondents & participants\nFor survey respondents, we show the distributions of their years of experience as a professional software engineer in\nFigure 1a, tenure with IBM in Figure 1b, and geography in Figure 1c (survey questions 9a-9c).\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n19\nC\nMotivations for using AI programming assistants\nTable 2 shows a detailed comparison of motivations for using or not using AI programming assistants between our survey\nrespondents and the software engineers who responded to the survey reported by Liang et al. [28] (survey question\n12a). We include a number of additional motivations (A1-10) identified as important by our product management team.\nWe also note that respondents were only asked to select which motivations explained their non-use of WCA rather than\nrating them on the 5-point scale of importance; we therefore only report the percentage of respondents who indicated\neach motivation for non-use. In addition, we did not include all motivations for non-use from Liang et al. [28] as some\nwere not relevant for our enterprise context.\nD\nPurposes of use of WCA\nFigure 3a shows the distribution of purposes of use of WCA (survey question 2a). These data were reported by\nrespondents who indicated they had used WCA within the prior two weeks (N=638).\nE\nDistributions of productivity measures\nFigure 2a shows detailed distributions of self-reported ratings of effort, quality of work, and speed (survey question 14).\nThese items were rated on 7-point semantic differential scales, centered on 0. In Figure 2b, we show the distribution of\nself-efficacy scores. In Figure 2c, we show the distribution of quality scores.\nF\nViews on code authorship across co-creative scenarios\nFigure 3b shows the distribution of respondents\u2019 views on code authorship across different co-creative scenarios (survey\nquestion 16a). Although the major trends align with our intuitions (e.g. when a party authors code, they are an author),\nwe note that across all scenarios, some proportion of respondents felt a joint ownership with WCA.\n0\u22125 years\n6\u221210 years\n10+ years\n0%\n10%\n20%\n30%\n40%\n50%\nPercentage of respondents\nYears of experience as a\nprofessional software engineer\n(a)\n< 6 months\n6 months\u22121 year\n1\u22122 years\n3\u22125 years\n6\u221210 years\n11\u221215 years\n16\u221220 years\n21\u221225 years\n26\u221230 years\n31+ years\n0%\n5%\n10%\n15%\nPercentage of respondents\nTenure with IBM\n(b)\nAmericas\nAPAC\nEMEA\nJapan\n0%\n10%\n20%\n30%\n40%\n50%\nPercentage of respondents\nGeography\n(c)\nFig. 1. Survey respondent demographics: (a) years of experience as a professional software engineer, (b) tenure with IBM, and (c)\ngeography.\nManuscript submitted to ACM\n20\nWeisz et al.\nMotivations\nLiang et al. [28]\nOur respondents"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_030",
    "source_id": "EnterpriseImpact2024",
    "text": " years\n3\u22125 years\n6\u221210 years\n11\u221215 years\n16\u221220 years\n21\u221225 years\n26\u221230 years\n31+ years\n0%\n5%\n10%\n15%\nPercentage of respondents\nTenure with IBM\n(b)\nAmericas\nAPAC\nEMEA\nJapan\n0%\n10%\n20%\n30%\n40%\n50%\nPercentage of respondents\nGeography\n(c)\nFig. 1. Survey respondent demographics: (a) years of experience as a professional software engineer, (b) tenure with IBM, and (c)\ngeography.\nManuscript submitted to ACM\n20\nWeisz et al.\nMotivations\nLiang et al. [28]\nOur respondents\nMotivations for use (Liang et al. [28])\nM1\nTo have an autocomplete or reduce the amount of keystrokes I\nmake\n86%\n6.2%\n47%\n30%\nM2\nTo finish my programming tasks faster\n76%\n12%\n59%\n21%\nM3\nTo skip needing to go online to find specific code snippets, pro-\ngramming syntax, or API calls I\u2019m aware of, but can\u2019t remember\n68%\n14%\n64%\n17%\nM4\nTo discover potential ways or starting points to write a solution\nto a problem I\u2019m facing\n50%\n24%\n64%\n17%\nM5\nTo find an edge case for my code I haven\u2019t considered\n36%\n44%\n48%\n28%\nAdditional motivations for use\nA1\nMy colleagues recommended I use it\n\u2013\n30%\n39%\nA2\nMy line management recommended I use it\n\u2013\n60%\n19%\nA3\nI like to explore new tools\n\u2013\n68%\n14%\nA4\nI wanted to know more about how my work might change in\nthe future\n\u2013\n64%\n18%\nA5\nIt is my responsibility to try new IBM products\n\u2013\n64%\n18%\nMotivations for non-use (Liang et al. [28])\nM6\nCode generation tools write code that doesn\u2019t meet functional\nor non-functional (e.g., security, performance) requirements\nthat I need\n54%\n34%\n14.3%\nM7\nIt\u2019s hard to control code generation tools to get code that I want\n48%\n36%\n14.3%\nM8\nI spend too much time debugging or modifying code written\nby code generation tools\n38%\n45%\n14.3%\nM9\nI don\u2019t think code generation tools provide helpful suggestions\n34%\n46%\n32.1%\nM10\nI don\u2019t want to use a tool that has access to my code\n30%\n51%\n\u2013\nM11\nI write and use proprietary code that code generation tools\nhaven\u2019t seen before and don\u2019t generate\n28%\n59%\n\u2013\nM12\nTo prevent potential intellectual property infringement\n26%\n66%\n\u2013\nM13\nI find the tool\u2019s suggestions too distracting\n26%\n51%\n\u2013\nM14\nI don\u2019t understand the code written by code generation tools\n16%\n76%\n3.6%\nM15\nI don\u2019t want to use open-source code\n10%\n89%\n\u2013\nAdditional motivations for non-use\nA6\nCode generated by WCA doesn\u2019t perform well enough for my\nneeds\n\u2013\n17.9%\nA7\nI don\u2019t understand"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_031",
    "source_id": "EnterpriseImpact2024",
    "text": " code\n30%\n51%\n\u2013\nM11\nI write and use proprietary code that code generation tools\nhaven\u2019t seen before and don\u2019t generate\n28%\n59%\n\u2013\nM12\nTo prevent potential intellectual property infringement\n26%\n66%\n\u2013\nM13\nI find the tool\u2019s suggestions too distracting\n26%\n51%\n\u2013\nM14\nI don\u2019t understand the code written by code generation tools\n16%\n76%\n3.6%\nM15\nI don\u2019t want to use open-source code\n10%\n89%\n\u2013\nAdditional motivations for non-use\nA6\nCode generated by WCA doesn\u2019t perform well enough for my\nneeds\n\u2013\n17.9%\nA7\nI don\u2019t understand the explanations generated by WCA\n\u2013\n3.6%\nA8\nI have not done any code-related tasks in the past 2 weeks\n\u2013\n25.0%\nA9\nI write and use code that WCA wasn\u2019t trained on which limits\nits ability to provide assistance\n\u2013\n10.7%\nA10\nIt is faster to do the work myself\n\u2013\n39.3%\nVery important\nImportant\nModerately important\nSlightly important\nNot important at all\nTable 2. Comparison of motivations for using (M1-5) and not using (M6-15) AI programming assistants with Liang et al. [28], along\nwith additional motivations (A1-10) examined in our survey. Percentages on the left-hand side (right-hand side) of each bar indicate\nthe proportion of respondents who rated each motivation as \u201cVery important\u201d or \u201cImportant\u201d (\u201cSlightly important\u201d or \u201cNot important\nat all\u201d) on a 5-point scale. Only 28 of our 669 respondents (4.2%) indicated non-use of WCA within the prior two weeks.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n21\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nMore difficult / Easier\nOf the worst / best quality\nSlower / Faster\nDo you feel that WCA makes your work...\nRating\n(a)\n0\n100\n200\n300\n1\n2\n3\n4\n5\nNumber of respondents\n1\n2\n3\n4\n5\nSelf Efficacy (M=3.20, SD=.93)\n(b)\n0\n100\n200\n300\nVery poor\nPoor\nAcceptable\nGood\nVery good\nNumber of respondents\nVery poor\nPoor\nAcceptable\nGood\nVery good\nQuality (M=3.2, SD=.77)\n(c)\nFig. 2. Distributions of productivity and quality measures: (a) self-reported ratings of effort, quality of work, and speed on 7-point\nsemantic differential scales (centered at 0), (b) distribution of self-efficacy scores, and (c) distribution of overall quality scores.\nIdentify security issues in code\nOptimize code performance\nTranslate code to another language\nGenerate code via autocompletion\nImprove code maintainability\nImprove code readability\nFix code\nRefactor code\nGen. alt. implementations\nLeave feedback for the WCA team\nGenerate tests\nAnswer questions on APIs or libs.\nGenerate documentation\nGen"
  },
  {
    "chunk_id": "EnterpriseImpact2024_chunk_032",
    "source_id": "EnterpriseImpact2024",
    "text": "\nQuality (M=3.2, SD=.77)\n(c)\nFig. 2. Distributions of productivity and quality measures: (a) self-reported ratings of effort, quality of work, and speed on 7-point\nsemantic differential scales (centered at 0), (b) distribution of self-efficacy scores, and (c) distribution of overall quality scores.\nIdentify security issues in code\nOptimize code performance\nTranslate code to another language\nGenerate code via autocompletion\nImprove code maintainability\nImprove code readability\nFix code\nRefactor code\nGen. alt. implementations\nLeave feedback for the WCA team\nGenerate tests\nAnswer questions on APIs or libs.\nGenerate documentation\nGen. code (or snippets) in chat\nAnswer gen. programming questions\nExplain code\n0%\n25%\n50%\n75%\n100%\nPercentage of respondents (who used\nWCA within the prior 2 weeks)\nFor what purposes did you use WCA?\n(a) Purposes of use of WCA for the prior two weeks.\nI implement an idea suggested\nby WCA\nI paste WCA\u2212generated code\ninto my source file but then\nmake edits to it\nI paste WCA\u2212generated code\ninto my source file verbatim\nI review code generated by\nWCA but ultimately implement\nthe functionality myself\n0%\n25%\n50%\n75%\n100%\nPercentage of respondents\nWhen using WCA, who do you consider to\nbe the author of the code when...\nMe\nBoth of us\nWCA\nI'm unsure\n(b) Views on code authorship across different co-creative scenar-\nios.\nFig. 3. Distributions of purposes of use of WCA and views on code authorship.\nManuscript submitted to ACM\n"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_001",
    "source_id": "ExpectationVsExperience2022",
    "text": "Expectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nPriyan Vaithilingam\npvaithilingam@g.harvard.edu\nHarvard University\nUSA\nTianyi Zhang\ntianyi@purdue.edu\nPurdue University\nUSA\nElena Glassman\nglassman@seas.harvard.edu\nHarvard University\nUSA\nABSTRACT\nRecent advances in Large Language Models (LLM) have made auto-\nmatic code generation possible for real-world programming tasks in\ngeneral-purpose programming languages such as Python. However,\nthere are few human studies on the usability of these tools and how\nthey fit the programming workflow. In this work, we conducted\na within-subjects user study with 24 participants to understand\nhow programmers use and perceive Copilot, a LLM-based code\ngeneration tool. We found that, while Copilot did not necessarily\nimprove the task completion time or success rate, most partici-\npants preferred to use Copilot in daily programming tasks, since\nCopilot often provided a useful starting point and saved the effort\nof searching online. However, participants did face difficulties in\nunderstanding, editing, and debugging code snippets generated\nby Copilot, which significantly hindered their task-solving effec-\ntiveness. Finally, we highlighted several promising directions for\nimproving the design of Copilot based on our observations and\nparticipants\u2019 feedback.\nCCS CONCEPTS\n\u2022 Human-centered computing \u2192Empirical studies in HCI.\nKEYWORDS\nlarge language model, github copilot\nACM Reference Format:\nPriyan Vaithilingam, Tianyi Zhang, and Elena Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Pow-\nered by Large Language Models. In CHI Conference on Human Factors in\nComputing Systems Extended Abstracts (CHI \u201922 Extended Abstracts), April\n29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 7 pages.\nhttps://doi.org/10.1145/3491101.3519665\n1\nINTRODUCTION\nAutomatic code generation has been a long-term goal for multiple\nresearch communities including Programming Languages (PL), Soft-\nware Engineering (SE), Natural Language Processing (NLP), and Ma-\nchine Learning (ML). Recent attempts to achieve this have focused\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\n\u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9156-6/22/04...$15.00\nhttps://doi.org/"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_002",
    "source_id": "ExpectationVsExperience2022",
    "text": " and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\n\u00a9 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-9156-6/22/04...$15.00\nhttps://doi.org/10.1145/3491101.3519665\non two different kinds of approaches: (1) program synthesis algo-\nrithms that search over a large program space defined by a domain-\nspecific language (DSL) [2, 7, 10, 12, 14, 19, 24, 25, 30, 31, 34, 43],\nand (2) deep learning models that are trained on a large amount\nof existing code and can generate new code given some forms of\nspecifications such as natural language descriptions or incomplete\ncode [5, 16, 17, 22, 38, 39, 48, 49]. Both kinds of approaches have\nclear drawbacks. On the one hand, existing program synthesis tech-\nniques are constrained to pre-defined DSLs and cannot scale to\ngeneral-purpose programming languages [15]. On the other hand,\nexisting generative models have a hard time learning sophisticated\nprogramming patterns from code corpora and often generate code\nwith syntactic or semantic errors [9, 29, 40]. The recent development\nof Large Language Models (LLM) such as GPT-3 [32] has opened up\nnew opportunities for addressing the limitations of existing code\ngeneration techniques. For example, Codex [50], which contains\n12 billion model parameters and is trained on 54 million software\nrepositories on GitHub, has demonstrated stunning code genera-\ntion capability\u2014solving over 70% of 164 Python programming tasks\nwith 100 samples [8].\nThe performance of LLM-based code generation tools has been\nextensively studied using benchmarks [8, 33]. However, little is\nknown about the usability and programmers\u2019 perception of such a\ntool in a real-world programming workflow. To bridge the gap, we\nconducted a within-subjects comparative study with 24 participants,\nin which participants were asked to complete Python programming\ntasks. In the experimental condition, participants wrote programs\nwith the assistance of Copilot, a Visual Studio Code (VSCode) plugin\npowered by Codex [13]. In the control condition, participants wrote\nprograms with the assistance of Intellisense, the default code com-\npletion plugin in VSCode. We investigated the following research\nquestions:\n\u2022 RQ1: How does using Copilot affect the programming expe-\nrience?\n\u2022 RQ2: How do users recognize errors in code generated by\nCopilot?\n\u2022 RQ3: What coping mechanisms do users employ when they\nfind errors in code generated by Copilot?\n\u2022 RQ4: What are the obstacles and limitations that can prevent\nad"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_003",
    "source_id": "ExpectationVsExperience2022",
    "text": " programming\ntasks. In the experimental condition, participants wrote programs\nwith the assistance of Copilot, a Visual Studio Code (VSCode) plugin\npowered by Codex [13]. In the control condition, participants wrote\nprograms with the assistance of Intellisense, the default code com-\npletion plugin in VSCode. We investigated the following research\nquestions:\n\u2022 RQ1: How does using Copilot affect the programming expe-\nrience?\n\u2022 RQ2: How do users recognize errors in code generated by\nCopilot?\n\u2022 RQ3: What coping mechanisms do users employ when they\nfind errors in code generated by Copilot?\n\u2022 RQ4: What are the obstacles and limitations that can prevent\nadoption of Copilot?\nOur key findings are: (1) the majority of the participants (19 out\nof 24) preferred using Copilot over Intellisense (Control condition);\n(2) Copilot provides a useful starting point for participants to kick\nstart the task and saved them the effort of searching online; (3) There\nis a need to identify better ways for participants to understand long\nblocks of generated code to help them edit, debug, and repair the\ncode.\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\nPriyan Vaithilingam, Tianyi Zhang, and Elena Glassman\n2\nRELATED WORK\n2.1\nAI-based Code Generation\nThere is a long history of research on automated code genera-\ntion. Some of the earliest work dates back to the 1960s, where\nWaldinger and Lee presented a program synthesizer called PROW\nthat automatically generated LISP programs based on user-provided\nspecifications in the form of a predicate calculus [41].\nThere are two main trends in modern automatic code generation:\nprogram synthesis and machine learning. Program Synthesis pri-\nmarily uses a search-based technique to generate code that fulfills\na given specification. These techniques work on a subset of the\nlanguage components relevant to the domain known as Domain-\nSpecific Languages (DSLs). More recently, program synthesis has\nbeen applied to a variety of domains, e.g., low-level bit-vector im-\nplementations [36], data manipulation in excel [14], and regular\nexpression synthesis [51]. The main limitation is that these tech-\nniques are limited to a pre-defined DSL, making it less scalable\nto programs written in general-purpose programming languages\nsuch as Java or Python. Because general-purpose programming\nlanguages include much more language features and syntax rules\ncompared with DSLs and therefore define a much bigger program\nspace to search from [15].\nThe second trend is using machine learning, especially deep\nlearning. Advances in deep learning have shown promising re-\nsults on automatically generating code for real-world programming\ntasks [5, 16, 17, 22, 38, 39, 48, 49]. For instance, Kim et al. [21] de-\nveloped a transformer architecture that is aware of code structures\nusing abstract syntax trees. Alon et al. [1] introduced structural\nlanguage models that remove any restriction on the vocabulary or\nstructure\u2014 the main limitation of program synthesis techniques.\nKarampatsis and Sutton [20] similarly introduced open-vocabulary\nmodels that can generate code with an"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_004",
    "source_id": "ExpectationVsExperience2022",
    "text": "space to search from [15].\nThe second trend is using machine learning, especially deep\nlearning. Advances in deep learning have shown promising re-\nsults on automatically generating code for real-world programming\ntasks [5, 16, 17, 22, 38, 39, 48, 49]. For instance, Kim et al. [21] de-\nveloped a transformer architecture that is aware of code structures\nusing abstract syntax trees. Alon et al. [1] introduced structural\nlanguage models that remove any restriction on the vocabulary or\nstructure\u2014 the main limitation of program synthesis techniques.\nKarampatsis and Sutton [20] similarly introduced open-vocabulary\nmodels that can generate code with an arbitrary number of tokens.\nThough these methods have shown promising results, they still\nsuffer from low accuracy and are less reliable [9, 29, 40]. For in-\nstance, Ciniselli et al. [9] show their RoBERTa-based model can\nonly produce correct solutions for 7% of the tasks from the Code-\nSearchNet benchmark [18].\nThe recent advances in large language models (LLM) such as GPT-\n3 [32] have led to a breakthrough in automated code generation\ncompared to prior state-of-the-art deep learning methods [4, 6,\n42]. For example, Codex [50], a fine-tuned version of GPT-3, can\ngenerate fully correct code for 29% of unseen programming tasks\nwith only one sample of generated programs and 72% of them with\n100 samples, while a widely used code generation tool, TabNine [39]\ncan only solve 3% and 8%, respectively [8].\nWhile there has been recent work evaluating the accuracy of\nLLM-based code generation tools [8, 33], little is known about its\nusability. With such increases in accuracy, how will programmers\ninteract with a tool that generates almost accurate yet not perfect\ncode? How easy or difficult is it for programmers to recognize\nerrors in a code snippet that is almost but not quite correct? Will\nthey simply modify the incorrect part or completely rewrite the\nentire code themselves? This motivates us to study programmers\u2019\nexpectations, coping strategies, and needs for such powerful code\ngeneration tools.\n2.2\nCoping with Imperfect AI\nPrior studies have examined how users interact with imperfect\nAI [11, 23, 26\u201328, 35, 37, 45]. Dzindolet et al. [11] showed that once\npeople observed an automated system make errors, their distrust in\nthe system increased unless an explanation was provided. However,\nthese explanations may also lead to over-reliance on the system\neven when unwarranted, signaling the importance and difficulty\nof providing explanations that help people to calibrate trust appro-\npriately. Kocielnik et al. [23] examined the effect of giving people\ncontrol over the types of errors made by a scheduling assistant,\neither by avoiding false positives or false negatives. They found\nthat even when the system was only 50% accurate, users who ex-\npected a reduction in the false positive rate had a lower perception\nof accuracy and lower acceptance of the system than the users who\nexpected a reduction in the false negative rate. [3, 52] showed that"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_005",
    "source_id": "ExpectationVsExperience2022",
    "text": " increased unless an explanation was provided. However,\nthese explanations may also lead to over-reliance on the system\neven when unwarranted, signaling the importance and difficulty\nof providing explanations that help people to calibrate trust appro-\npriately. Kocielnik et al. [23] examined the effect of giving people\ncontrol over the types of errors made by a scheduling assistant,\neither by avoiding false positives or false negatives. They found\nthat even when the system was only 50% accurate, users who ex-\npected a reduction in the false positive rate had a lower perception\nof accuracy and lower acceptance of the system than the users who\nexpected a reduction in the false negative rate. [3, 52] showed that\nconfidence scores helped calibrate users\u2019 trust, form a good mental\nmodel of the AI, and understand the error boundaries better.\nSimilar to other AI techniques, AI-based code generation tools\nalso suffer from inherent uncertainty and imperfection. They may\ninevitably generate code with errors or even code that wildly differs\nfrom users\u2019 expectations. However, unlike other domains, code\ngeneration demands a much higher level of correctness: code either\ncompiles or not, and it is either correct or contains bugs such as\nlogic errors and security vulnerabilities. Therefore, existing findings\nof other types of AI techniques may not generalize to the domain\nof code generation.\nCurrently, there are only a few studies on how programmers\nuse such imperfect code generation tools [44, 47]. Xu et al. [47] did\na user study with 31 participants to evaluate the usefulness of a\nNL-to-code plugin [46]. They found that there was no statistically\nsignificant difference in task completion time or task correctness\nscores when using or not using the NL-to-code plugin. Furthermore,\nmost participants stayed neural or somewhat positive to the NL-\nto-code plugin. The main reason for these negative results was the\ncorrectness and quality of generated code as pointed out by many\nparticipants in the post-study survey. However, these findings may\nnot hold as more recent large language models have significantly\nboosted the correctness and quality of generated code. This further\nmotivates us to conduct the user study with Copilot.\nWeisz et al. [44] interviewed 11 software engineers at IBM and\nsolicited their feedback on a neural machine translation (NMT)\nmodel for an adjacent domain\u2014translating code from one program-\nming language to another. They found that the user\u2019s acceptance\nof the NMT model was contingent on the number of errors in the\ntranslated code. They also identified several common themes in\nparticipants\u2019 feedback such as acceptance via verification and the\ndesire to provide guidance to the NMT model. Our study was de-\nsigned to complement this knowledge but for daily programming\ntasks.\n3\nSTUDY DESIGN\nTo understand how programmers use an LLM-based code genera-\ntion tool, we designed and carried out a within-subjects comparative\nstudy with 24 participants. For the control condition, each partici-\npant was asked to complete a Python programming task in Visual\nStudio Code (VSCode) IDE with the default code completion tool\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_006",
    "source_id": "ExpectationVsExperience2022",
    "text": " and the\ndesire to provide guidance to the NMT model. Our study was de-\nsigned to complement this knowledge but for daily programming\ntasks.\n3\nSTUDY DESIGN\nTo understand how programmers use an LLM-based code genera-\ntion tool, we designed and carried out a within-subjects comparative\nstudy with 24 participants. For the control condition, each partici-\npant was asked to complete a Python programming task in Visual\nStudio Code (VSCode) IDE with the default code completion tool\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\ncalled Intellisense. Intellisense suggests a drop-down list of valid\ntokens in the current code context, ordered by alphabetical order\nor relevance. The users can select the token they want and press\nthe Tab button to accept the suggested token or the Esc button to\nreject it.\nFor the experiment condition, each participant finished another\nPython programming task in VSCode with Copilot. Similar to Intel-\nlisene, Copilot can automatically suggest code based on the current\ncode context as a programmer is typing. While Intellisense only\npredicts one token at a time, Copilot is capable of generating multi-\nple lines of code. The participants can press Tab to accept the code\nsuggestion or Esc to reject. Though not required, participants can\ngive prompts to Copilot by writing comments. Henceforth, when\nwe mention prompts in the text, we refer to comments written by\nthe participants in the code specifically to guide Copilot.\n3.1\nTasks\nWe selected three real-world python programming tasks with dif-\nferent levels of difficulty from [47].\n\u2022 Task 1. Edit CSV (Easy): Write a program to read CSV data\nfrom the \u2018data.csv\u2019 file. Delete the first column and the last\ncolumn. Save it to the \u2018output.csv\u2019 file.\n\u2022 Task 2. Web Scrapping (Medium): Given the URL of a web\npage, write a program that extracts the URLs of all hyperlinks\nin the web page and save the URLs to a file named \u2018urls.txt\u2019.\n\u2022 Task 3. Graph Plotting (Hard): Write a program to draw a\nscatter plot of the data in \u2018shampoo.csv\u2019 and save it to \u2018sham-\npoo.png\u2019. The plot size should be 10 inches wide and 6 inches\nhigh. The Date column is the x-axis. The date string shown\non the plot should be in the YYYY-MM-DD format. The\nSales column is the y-axis. The graph should have the title\n\u201cShampoo Sales Trend\u201d.\n3.2\nParticipants\nWe recruited 24 participants (4 Female, 19 Male, 1 Non-binary)\nthrough mailing lists of two research universities. Ten participants\nwere undergraduate students, 5 were master\u2019s student, 8 were\nPh.D. students, and 1 was a software engineer. Regarding their\nfamiliarity with programming, only 1 participant had less than 2\nyears of programming experience, 14 participants have 2-5 years\nof experience, and 9 participants have over 5 years of experience.\nParticipants received a $20 Amazon gift card as"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_007",
    "source_id": "ExpectationVsExperience2022",
    "text": " the YYYY-MM-DD format. The\nSales column is the y-axis. The graph should have the title\n\u201cShampoo Sales Trend\u201d.\n3.2\nParticipants\nWe recruited 24 participants (4 Female, 19 Male, 1 Non-binary)\nthrough mailing lists of two research universities. Ten participants\nwere undergraduate students, 5 were master\u2019s student, 8 were\nPh.D. students, and 1 was a software engineer. Regarding their\nfamiliarity with programming, only 1 participant had less than 2\nyears of programming experience, 14 participants have 2-5 years\nof experience, and 9 participants have over 5 years of experience.\nParticipants received a $20 Amazon gift card as compensation for\ntheir time.\n3.3\nProtocol\nTo enable easy access to the code generation tools, we set up two vir-\ntual machines (VMs) in Microsoft Azure, one with Copilot installed\nand the other with IntelliSense installed. We also pre-installed VS-\nCode and several popular Python packages in both VMs. Partici-\npants can easily log into each VM from their laptop to start the user\nstudy. We recorded the audio and the screen-cast with the consent\nof each participant. In each study session, a participant completed\none of the three tasks using Copilot (i.e. the experiment condition)\nand another task with Intellisense (i.e. the control condition). To\nemulate real-world programming experience, the participants were\nallowed to use Internet search or refer to any online resources any-\ntime during the task. To mitigate the learning effect, both the order\nof task assignment and the order of tool assignment was counterbal-\nanced across participants through random assignment. Therefore,\nfor each unique combination of 3 tasks and 2 conditions, we have\n8 participant data points. Before each task, the participants were\ngiven a quick tutorial of the assigned tool. We set a time limit of 20\nminutes for each programming task. A task was considered failed\nif participants did not complete it within 20 minutes. After each\ntask, participants answered a survey to reflect on their experience\nusing the tool. After finishing both tasks, participants answered a\nfinal survey to directly compare the two conditions. The first au-\nthor performed open-coding on participants\u2019 responses to identify\nthemes and then discussed with co-authors to refine the themes\nover multiple sessions. These themes were then used to explain the\nresults in the following sessions.\n4\nRESULTS\nThis section describes both the quantitative and qualitative results\nof our study. Quantitative results include the task completion time,\ntask failure rates, and metrics from survey responses. In the qual-\nitative results subsection, we describe the common themes that\nemerged through open coding of participant comments and experi-\nmenter observations made during the study.\n4.1\nQuantitative\nParticipants using Copilot failed to complete tasks more often than\nparticipants using Intellisense. Table 1 shows individual and average\ntask completion times. Table cells in the orange background indicate\nsessions in which participants did not solve the task within 20\nminutes. When using Copilot, all 8 participants working on the\neasiest task completed it, 6 out of 8 participants working on the\nmedium-difficulty task completed it, and 5 out of 8 participants\nworking on"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_008",
    "source_id": "ExpectationVsExperience2022",
    "text": " metrics from survey responses. In the qual-\nitative results subsection, we describe the common themes that\nemerged through open coding of participant comments and experi-\nmenter observations made during the study.\n4.1\nQuantitative\nParticipants using Copilot failed to complete tasks more often than\nparticipants using Intellisense. Table 1 shows individual and average\ntask completion times. Table cells in the orange background indicate\nsessions in which participants did not solve the task within 20\nminutes. When using Copilot, all 8 participants working on the\neasiest task completed it, 6 out of 8 participants working on the\nmedium-difficulty task completed it, and 5 out of 8 participants\nworking on the hardest task completed it within the allotted time.\nIn contrast, when using Intellisense, all 8 participants in both the\neasiest and medium-difficulty task conditions completed their tasks,\nand only 2 participants failed to complete the hardest task. Overall\ntask difficulties, Intellisense users failed twice while Copilot users\nfailed 5 times. This difference is not statistically significant.\nWe analyzed the session recordings to identify the root cause of\nthese task failures. Out of the 5 task failures when using Copilot, 3\nwere caused by incorrect code generated by Copilot, which led par-\nticipants into a time-consuming debugging rabbit hole (discussed\nin Section 4.2.4). The other two were caused by the participants\u2019\ninexperience with the relevant Python libraries (graph plotting and\nHTML parsing libraries) and the debugging features of the IDE. In\ncontrast, participants using Intellisense failed to finish the 2 tasks\ndue to their inexperience with a graph plotting library.\nWhile Copilot users completed fewer tasks than Intellisense\nusers, the tasks completed with Copilot were done more quickly on\naverage (see the last row of Table 1). The overall mean difference of\ntask completion time using Copilot vs. Intellisense is about 1 min.\nYet the mean difference is not statistically significant (student t-test,\np = 0.53).\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\nPriyan Vaithilingam, Tianyi Zhang, and Elena Glassman\nTask 1 - Easy\nTask 2 - Medium\nTask 3 - Hard\nIntellisense\nCopilot\nIntellisense\nCopilot\nIntellisense\nCopilot\n9:35\n1:46\n7:48\n12:53\n13:41\n11:08\n3:50\n3:57\n15:52\n16:45\n13:43\n11:05\n4:49\n4:55\n16:28\n7:26\n22:42\n4:04\n9:04\n6:18\n14:16\n15:05\n13:06\nDNF\n5:18\n1:18\n7:35\n13:24\n23:13\n19:54\n15:54\n7:52\n12:39\nDNF\n4:48\nDNF\n5:27\n3:12\n10:47\n6:02\nDNF\nDNF\n2:09\n20:12\n"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_009",
    "source_id": "ExpectationVsExperience2022",
    "text": ":08\n3:50\n3:57\n15:52\n16:45\n13:43\n11:05\n4:49\n4:55\n16:28\n7:26\n22:42\n4:04\n9:04\n6:18\n14:16\n15:05\n13:06\nDNF\n5:18\n1:18\n7:35\n13:24\n23:13\n19:54\n15:54\n7:52\n12:39\nDNF\n4:48\nDNF\n5:27\n3:12\n10:47\n6:02\nDNF\nDNF\n2:09\n20:12\n8:30\nDNF\nDNF\n9:19\nAverage Time\n7:01\n6:11\n11:44\n11:56\n13:36\n11:06\nOverall average time for all tasks combined\n10:23\n9:18\nTable 1: Individual and average task completion times. Cells with an orange cell background indicate that the participant never\nsucceeded because they were stopped after approximately 20 minutes of trying. DNF implies the participant did not finish on\ntime.\nIn the post-study survey, 19 of 24 participants answered that\nthey preferred Copilot over Intellisense. Furthermore, 23 of 24 par-\nticipants answered that Copilot was more helpful than Intellisense.\nWe also asked participants to rate the helpfulness of code gen-\nerated by both tools on a scale of 1 (not at all helpful) to 7 (very\nhelpful). Participants found code generated by Copilot more helpful\nthan code generated by Intellisense (6.16 vs. 4.45 on average). This\ndifference is statistically significant (student t-test: p < 0.001). How-\never, only 10 participants self-reported that they felt more confident\nabout the code generated by Copilot than the code suggested by\nIntelliSense.\n4.2\nQualitative\n4.2.1\nUser Perception. Participants found Copilot helpful as it pro-\nvided a starting point for the task instead of a blank canvas they\nusually have. Even if the code generated by Copilot is incorrect, it\nalways points them towards a direction they can get started from.\nP1 said \u201cCopilot\u2019s function/line generation is a helpful reference; even\nif the generated code is not correct, it can point me in the right direc-\ntion for completing the task.\u201d This is primarily useful for the kind of\ntasks in which the user has no experience. P7 said, \u201cthe generation\nof fully formed functions that completed a task that I wasn\u2019t sure\nhow to approach/start was very cool.\u201d For four of the participants,\nCopilot auto-completed the code for almost the whole tasks, and\nparticipants did very little to no fixes to the generated code. Though\nwe did not see any significant difference in task completion time,\nseven participants explicitly mentioned that Copilot can save time\nin completing the task compared to Intellisense. P4 said \u201c[Copilot]\nwill likely save me much more time during the coding process.\u201d Partic-\nipants also considered writing comments to guide Copilot as a way\nof communicating with the AI. P24 said \u201cCop"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_010",
    "source_id": "ExpectationVsExperience2022",
    "text": " has no experience. P7 said, \u201cthe generation\nof fully formed functions that completed a task that I wasn\u2019t sure\nhow to approach/start was very cool.\u201d For four of the participants,\nCopilot auto-completed the code for almost the whole tasks, and\nparticipants did very little to no fixes to the generated code. Though\nwe did not see any significant difference in task completion time,\nseven participants explicitly mentioned that Copilot can save time\nin completing the task compared to Intellisense. P4 said \u201c[Copilot]\nwill likely save me much more time during the coding process.\u201d Partic-\nipants also considered writing comments to guide Copilot as a way\nof communicating with the AI. P24 said \u201cCopilot behaves just like a\nTA and can tell me exactly what I want by reading the comments.\u201d\nHowever, participants pointed out several concerns about adopt-\ning Copilot in practice. First, twelve participants said they found\nit hard to understand and change the code generated by Copilot.\nP1 said, \u201cCopilot generated a complete function to fulfill the full task,\nbut part of the function did not work as desired. Because I did not\nunderstand several parts of the function generated by Copilot, I did\nnot know how to debug the function. This caused me to get rid of the\nwhole function generated by copilot and start over.\u201d. Due to a lack\nof understanding, five participants perceived a loss of control over\ntheir code. P13 said, \u201cI would go with Intellisense for now since it\ngives me more control over the code I am writing\u201d. Second, seven\nparticipants expressed concerns over code reliability. P7 said \u201cAt\nthis time I probably prefer Intellisense just because I trust my own\ngoogling and understanding code examples online rather than opaque\nsuggestions from copilot.\u201d P18 felt very frustrated after observing\nCopilot continuously generate code with errors. They said, \u201cYes, I\ngot rid of the whole snippet as I didn\u2019t want to conform to the code\ngenerated by AI as it may have unwanted bugs.\u201d Third, eight partici-\npants said they only trusted participants for simple tasks. This is\ndue to multiple reasons, e.g., the difficulty to understand generated\ncode, fear of unknown bugs, failure to match the coding style, etc.\n4.2.2\nUser Interaction Patterns. While prior code completion tools\nsuch as Intellisense only suggest one token at a time, Copilot is ca-\npable of generating even multiple lines of code at a time. While such\na code generation capability is often interpreted as a powerful fea-\nture, it causes significant cognitive overload in practice, especially\nwhen the generated code has errors. A long piece of generated code\nforces the user to switch back and forth between program reading\nand writing. When the generated code has errors, the user needs\nto further enter into the debugging mode. This constant context\nswitching puts significant mental demand on the users.\nAnother common interaction pattern is to use Copilot as a sub-\nstitute for Internet search. P3 said, \u201cfor certain tasks that follow\nvery routine structures, and which I always have to look up on Stack\nOverflow, a tool like Copilot eliminates a lot of the tedious searching\non Google\u201d. However, we have to note that unlike code examples\nfrom Stack Overflow, which are"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_011",
    "source_id": "ExpectationVsExperience2022",
    "text": " causes significant cognitive overload in practice, especially\nwhen the generated code has errors. A long piece of generated code\nforces the user to switch back and forth between program reading\nand writing. When the generated code has errors, the user needs\nto further enter into the debugging mode. This constant context\nswitching puts significant mental demand on the users.\nAnother common interaction pattern is to use Copilot as a sub-\nstitute for Internet search. P3 said, \u201cfor certain tasks that follow\nvery routine structures, and which I always have to look up on Stack\nOverflow, a tool like Copilot eliminates a lot of the tedious searching\non Google\u201d. However, we have to note that unlike code examples\nfrom Stack Overflow, which are vetted by human programmers, the\ncode generated by Copilot may contain errors. P10 wrote, \u201cI\u2019m not\nfully confident that Copilot will suggest the best solution. By reading\nStack Overflow, the helpful thing is that there will always be someone\nwho would just post a better solution, and people will discuss and\ncompare. I feel like that is missing from Copilot.\u201d Since Copilot only\ngenerates one solution at a time and does not provide any explana-\ntions, programmers cannot compare multiple alternative solutions\nand assess their quality as they often do in an online search.\nFurthermore, we observed eight instances of over-reliance on\nCopilot. For example, P8 simply accepted the generated code and\nsaid, \u201cI guess I will take its word.\u201d This over-reliance also makes\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\nparticipants defer code validation. P20 said \u201cNot exactly sure what\nthis does. I\u2019ll figure it out later\u201d. Some participants later spot errors\nin the accepted code. They had to go back and spend a lot of time\ndebugging the previous code.\n4.2.3\nCoping Strategies. There are two main ways participants\ncope with incorrect code generated by Copilot. The first way is to\naccept the incorrect suggestion and attempt to repair it. Twelve\nparticipants attempted to repair the code when there was an error.\nHowever, the participants always found it difficult to repair the\ncode since the code was not written by themselves. Of these twelve\nparticipants open to repair the code, five participants were only\nwilling to repair the code if the code generated by Copilot is easy\nto read and understand. P7 said, \u201cit made debugging the code more\ndifficult as I hadn\u2019t written the code directly and didn\u2019t have an initial\nintuition about where the bugs might be. Especially with a final bug\nin my program I really had no idea why it was happening and had to\nrefactor the code.\u201d\nIn cases where the participant is unable or unwilling to repair\nthe code, they will simply get rid of the entire generated code and\nsearch for solutions online. Seven participants mentioned they will\nrewrite the whole code by themselves without any attempt to repair\nif there is an error in the code generated by Copilot. P13 said, \u201cI\nthink getting rid of the whole code is easier than reading the code\nand making the changes.\u201d P1 also said, \u201cbecause I did not"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_012",
    "source_id": "ExpectationVsExperience2022",
    "text": " as I hadn\u2019t written the code directly and didn\u2019t have an initial\nintuition about where the bugs might be. Especially with a final bug\nin my program I really had no idea why it was happening and had to\nrefactor the code.\u201d\nIn cases where the participant is unable or unwilling to repair\nthe code, they will simply get rid of the entire generated code and\nsearch for solutions online. Seven participants mentioned they will\nrewrite the whole code by themselves without any attempt to repair\nif there is an error in the code generated by Copilot. P13 said, \u201cI\nthink getting rid of the whole code is easier than reading the code\nand making the changes.\u201d P1 also said, \u201cbecause I did not understand\nseveral parts of the function generated by Copilot, I did not know how\nto debug the function. This caused me to get rid of the whole function\ngenerated by Copilot and start over.\u201d\n4.2.4\nObstacles and Limitations. During the user study, we ob-\nserved three major obstacles to using Copilot in practice. First, par-\nticipants often failed to understand and assess the correctness of the\ngenerated code. Since Copilot often generates a big chunk of code\nat a time, participants found it hard to understand and debug the\ncode. This is already discussed in Section 4.2.1. The second obstacle\nis the underestimation of the effort required to fix a bug in the code\ngenerated by Copilot. Among the five task failures by participants\nusing Copilot, three were due to incorrect suggestions by Copilot.\nWhile participants recognized these errors, they underestimated\nhow much effort it took to fix the bug and got stuck in a debugging\nrabbit hole they could not get out of. For instance, for P20, Copilot\ngenerated a regular expression based code for extracting URLs from\nHTML. It is extremely hard to get the regular expression right for\nthis task and a better solution is to parse the HTML and extract\nattributes instead. Since Copilot suggested the regular expression,\nP20 decided to stick with it and overlooked the better solution. Yet\nP20 failed to fix the regular expression after 20 minutes, leading to\na task failure. The third obstacle is the brittleness and ambiguity\nof using comments (or prompts) as a specification for Copilot. As\ndiscussed in the previous sections, participants used comments to\ndescribe the desired code that should be generated by Copilot. How-\never, Copilot is very sensitive to these comments. A little tweak in\na comment can cause Copilot to generate a significantly different\ncode snippet. P24 said, \u201cit is ambiguous to use comments to hint at\nCopilot what I want.\u201d\n5\nDISCUSSION\nThe majority of participants (19 out of 24) expressed a strong prefer-\nence to use Copilot for their day-to-day programming tasks for sev-\neral reasons. In many cases, Copilot accurately generated the code\nfrom the prompts provided by the participants. In four instances, it\neven generated the correct code for almost the whole task in one\nshot. Generating a whole block of code improves developer produc-\ntivity significantly. However, we did not see a big difference in the\ntime saved by Copilot during the study. Our observations point to a\nplausible explanation for this non-significance\u2014though"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_013",
    "source_id": "ExpectationVsExperience2022",
    "text": " \u201cit is ambiguous to use comments to hint at\nCopilot what I want.\u201d\n5\nDISCUSSION\nThe majority of participants (19 out of 24) expressed a strong prefer-\nence to use Copilot for their day-to-day programming tasks for sev-\neral reasons. In many cases, Copilot accurately generated the code\nfrom the prompts provided by the participants. In four instances, it\neven generated the correct code for almost the whole task in one\nshot. Generating a whole block of code improves developer produc-\ntivity significantly. However, we did not see a big difference in the\ntime saved by Copilot during the study. Our observations point to a\nplausible explanation for this non-significance\u2014though it is faster\nto generate code through Copilot compared to acquiring code from\nthe internet, the code generated by Copilot can be buggy, leading\nto more time spent in debugging. Whereas, code from the internet\nis generally bug-free, comes with explanations and discussion, and\ncan be composed suitably for the current task by just doing some\nminor edits like changing the variable names. Moreover, Copilot\nalso provides a useful starting point for the users to get started,\neven if the generated code was incorrect. This is especially useful\nfor users who are stuck in a problem or who do not know how to\napproach the task. Several participants request to see multiple code\nsuggestions so they can compare and compose code from different\nsnippets to suit their needs. Furthermore, we found participants\nused Copilot as a replacement for internet search. However, they\nmissed out on comparing multiple sources and community discus-\nsions. Hence, it is worthwhile integrating online search with code\ngeneration to help users compare AI-generated code with online\ncode examples and identify the best possible solution for a task.\nThis can also prevent users from getting trapped in a debugging\nrabbit hole whenever Copilot suggests an incorrect or inefficient\nsolution.\nAnother observation that is worth investigating is that partici-\npants had a hard time understanding the code generated by Copilot.\nOne way to help users understand the generated code is to provide\nexplanations using inline comments. We can highlight different\nparts of the code based on model confidence similar to the ap-\nproach suggested by [44]. We can also help users debug code by\nautomatically generating test cases and test data for users to val-\nidate generated code and identify corner cases. We would like to\nstudy this in-depth and come up with ways to make the code more\nunderstandable and help users to debug and repair generated code.\nMoreover, we observed that Copilot led to more task failures in\nmedium and hard tasks since it was hard for Copilot to generate\ncorrect code in one shot. Three participants who finished the hard\ntask approached the problem by decomposing the complex task\ninto simpler sub-tasks and wrote prompts for each sub-task for\nCopilot to solve. Such a task decomposition strategy led to higher\ntask-solving efficiency and a better user experience. Therefore, it\nis worth working on interaction mechanisms that facilitate task\ndecomposition in the future.\n6\nCONCLUSION\nThis paper presents a user study with 24 participants on the us-\nability of GitHub Copilot, a groundbreaking code generation tool\nempowered by an ultra-large language model. In particular, we\ninvestigated users\u2019 perception of Copilot, their"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_014",
    "source_id": "ExpectationVsExperience2022",
    "text": "medium and hard tasks since it was hard for Copilot to generate\ncorrect code in one shot. Three participants who finished the hard\ntask approached the problem by decomposing the complex task\ninto simpler sub-tasks and wrote prompts for each sub-task for\nCopilot to solve. Such a task decomposition strategy led to higher\ntask-solving efficiency and a better user experience. Therefore, it\nis worth working on interaction mechanisms that facilitate task\ndecomposition in the future.\n6\nCONCLUSION\nThis paper presents a user study with 24 participants on the us-\nability of GitHub Copilot, a groundbreaking code generation tool\nempowered by an ultra-large language model. In particular, we\ninvestigated users\u2019 perception of Copilot, their interaction patterns,\nand their coping strategies when the generated code is not correct.\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\nPriyan Vaithilingam, Tianyi Zhang, and Elena Glassman\nWe found that, despite all the promising results on benchmarks [8],\nCopilot did not necessarily reduce the task completion time or in-\ncrease the success rate of solving programming tasks in a real-world\nsetting. On the other hand, participants overwhelmingly preferred\nusing Copilot in their programming workflow since Copilot often\nprovided a good starting point to approach the programming task.\nFurthermore, our study shed light on several promising future di-\nrections for improving the design of Copilot. For example, instead\nof simply using Copilot as a one-shot code generation tool, there\nshould be more support for understanding and validating the gen-\nerated code, exploring multiple solutions, and task decomposition.\nACKNOWLEDGMENTS\nThis material is based upon work supported by the NSF under Grant\nNo. IIS-2107391 and Grant No. CCF-2123965.\nREFERENCES\n[1] Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. 2020. Structural Language\nModels of Code. In Proceedings of the 37th International Conference on Machine\nLearning (Proceedings of Machine Learning Research, Vol. 119), Hal Daum\u00e9 III and\nAarti Singh (Eds.). PMLR, 245\u2013256. https://proceedings.mlr.press/v119/alon20a.\nhtml\n[2] Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund\nRaghothaman, Sanjit A Seshia, Rishabh Singh, Armando Solar-Lezama, Emina\nTorlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. IEEE.\n[3] Stavros Antifakos, Nicky Kern, Bernt Schiele, and Adrian Schwaninger. 2005.\nTowards Improving Trust in Context-Aware Systems by Displaying System Con-\nfidence (MobileHCI \u201905). Association for Computing Machinery, New York, NY,\nUSA, 9\u201314. https://doi.org/10.1145/1085777.1085780\n[4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc V. Le,\nand Charles"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_015",
    "source_id": "ExpectationVsExperience2022",
    "text": " 2013. Syntax-guided synthesis. IEEE.\n[3] Stavros Antifakos, Nicky Kern, Bernt Schiele, and Adrian Schwaninger. 2005.\nTowards Improving Trust in Context-Aware Systems by Displaying System Con-\nfidence (MobileHCI \u201905). Association for Computing Machinery, New York, NY,\nUSA, 9\u201314. https://doi.org/10.1145/1085777.1085780\n[4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk\nMichalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc V. Le,\nand Charles Sutton. 2021. Program Synthesis with Large Language Models. ArXiv\nabs/2108.07732 (2021).\n[5] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin,\nand Daniel Tarlow. 2017.\nDeepCoder: Learning to Write Programs.\nArXiv\nabs/1611.01989 (2017).\n[6] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-\nNeo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. https:\n//doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using\nthese metadata..\n[7] Sarah E Chasins, Maria Mueller, and Rastislav Bodik. 2018. Rousillon: Scrap-\ning distributed hierarchical web data. In Proceedings of the 31st Annual ACM\nSymposium on User Interface Software and Technology. 963\u2013975.\n[8] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).\n[9] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys Poshyvanyk, Massimil-\niano Di Penta, and Gabriele Bavota. 2021. An Empirical Study on the Usage of\nBERT Models for Code Completion. arXiv preprint arXiv:2103.07115 (2021).\n[10] Allen Cypher. 1995. Eager: Programming repetitive tasks by example. In Readings\nin human\u2013computer interaction. Elsevier, 804\u2013810.\n[11] Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and\nHall P Beck. 2003. The role of trust in automation reliance. International journal\nof human-computer studies 58, 6 (2003), 697\u2013718.\n[12] John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure\ntransformations from input-output examples. ACM SIGPLAN Notices 50, 6 (2015),\n229\u2013239.\n[13] Github Copilot [n"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_016",
    "source_id": "ExpectationVsExperience2022",
    "text": " by example. In Readings\nin human\u2013computer interaction. Elsevier, 804\u2013810.\n[11] Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and\nHall P Beck. 2003. The role of trust in automation reliance. International journal\nof human-computer studies 58, 6 (2003), 697\u2013718.\n[12] John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure\ntransformations from input-output examples. ACM SIGPLAN Notices 50, 6 (2015),\n229\u2013239.\n[13] Github Copilot [n. d.]. Your AI pair programmer.\n[14] Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-\noutput examples. ACM Sigplan Notices 46, 1 (2011), 317\u2013330.\n[15] Sumit Gulwani, Jos\u00e9 Hern\u00e1ndez-Orallo, Emanuel Kitzelmann, Stephen H Mug-\ngleton, Ute Schmid, and Benjamin Zorn. 2015. Inductive programming meets the\nreal world. Commun. ACM 58, 11 (2015), 90\u201399.\n[16] Tong Guo and Huilin Gao. 2019. Content enhanced bert-based text-to-sql gener-\nation. arXiv preprint arXiv:1910.07179 (2019).\n[17] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, An-\nthony Tomasic, and Graham Neubig. 2018. Retrieval-based neural code generation.\narXiv preprint arXiv:1808.10025 (2018).\n[18] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic\ncode search. arXiv preprint arXiv:1909.09436 (2019).\n[19] Susmit Jha, Sumit Gulwani, Sanjit A Seshia, and Ashish Tiwari. 2010. Oracle-\nguided component-based program synthesis. In 2010 ACM/IEEE 32nd International\nConference on Software Engineering, Vol. 1. IEEE, 215\u2013224.\n[20] Rafael-Michael Karampatsis and Charles Sutton. 2019.\nMaybe deep neu-\nral networks are the best choice for modeling source code.\narXiv preprint\narXiv:1903.05734 (2019).\n[21] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code pre-\ndiction by feeding trees to transformers. In 2021 IEEE/ACM 43rd International\nConference on Software Engineering (ICSE). IEEE, 150\u2013162.\n[22] Kite - Free AI Coding Assistant and Code Auto-Complete Plugin 2020. Kite - Free\nAI Coding Assistant and Code Auto-Complete Plugin. https://www.kite.com/.\nAccessed: 2022-1-8.\n["
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_017",
    "source_id": "ExpectationVsExperience2022",
    "text": "-\nral networks are the best choice for modeling source code.\narXiv preprint\narXiv:1903.05734 (2019).\n[21] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code pre-\ndiction by feeding trees to transformers. In 2021 IEEE/ACM 43rd International\nConference on Software Engineering (ICSE). IEEE, 150\u2013162.\n[22] Kite - Free AI Coding Assistant and Code Auto-Complete Plugin 2020. Kite - Free\nAI Coding Assistant and Code Auto-Complete Plugin. https://www.kite.com/.\nAccessed: 2022-1-8.\n[23] Rafal Kocielnik, Saleema Amershi, and Paul N Bennett. 2019. Will you accept an\nimperfect ai? exploring designs for adjusting end-user expectations of ai systems.\nIn Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems.\n1\u201314.\n[24] Tessa Lau, Steven A Wolfman, Pedro Domingos, and Daniel S Weld. 2003. Pro-\ngramming by demonstration using version space algebra. Machine Learning 53,\n1 (2003), 111\u2013156.\n[25] Vu Le and Sumit Gulwani. 2014. Flashextract: A framework for data extraction by\nexamples. In Proceedings of the 35th ACM SIGPLAN Conference on Programming\nLanguage Design and Implementation. 542\u2013553.\n[26] Brian Y Lim and Anind K Dey. 2009. Assessing demand for intelligibility in\ncontext-aware applications. In Proceedings of the 11th international conference on\nUbiquitous computing. 195\u2013204.\n[27] Brian Y Lim and Anind K Dey. 2010. Toolkit to support intelligibility in context-\naware applications. In Proceedings of the 12th ACM international conference on\nUbiquitous computing. 13\u201322.\n[28] Brian Y Lim, Anind K Dey, and Daniel Avrahami. 2009. Why and why not\nexplanations improve the intelligibility of context-aware intelligent systems.\nIn Proceedings of the SIGCHI conference on human factors in computing systems.\n2119\u20132128.\n[29] Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader Palacio,\nDenys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota. 2021. Studying the\nusage of text-to-text transfer transformer to support code-related tasks. In 2021\nIEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE,\n336\u2013347.\n[30] Brad A Myers. 1990. Creating user interfaces using programming by exam-\nple, visual programming, and constraints. ACM Transactions on Programming\nLanguages and Systems (TOPLAS) 12, 2 (1990), 143\u2013177.\n[31] Brad A Myers. 1991. Graphical techniques in a spreadsheet for specifying user\ninterfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing\nSystems. 243\u2013249.\n[32] OpenAI and Ashley Pilipiszyn. 2021. GPT-3 Powers the Next Generation of Apps.\nhttps://openai.com"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_018",
    "source_id": "ExpectationVsExperience2022",
    "text": "ACM 43rd International Conference on Software Engineering (ICSE). IEEE,\n336\u2013347.\n[30] Brad A Myers. 1990. Creating user interfaces using programming by exam-\nple, visual programming, and constraints. ACM Transactions on Programming\nLanguages and Systems (TOPLAS) 12, 2 (1990), 143\u2013177.\n[31] Brad A Myers. 1991. Graphical techniques in a spreadsheet for specifying user\ninterfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing\nSystems. 243\u2013249.\n[32] OpenAI and Ashley Pilipiszyn. 2021. GPT-3 Powers the Next Generation of Apps.\nhttps://openai.com/blog/gpt-3-apps/. Accessed: 2022-1-8.\n[33] Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan\nDolan-Gavitt. 2021. Can OpenAI Codex and Other Large Language Models Help\nUs Fix Security Bugs? arXiv preprint arXiv:2112.02125 (2021).\n[34] Hila Peleg, Sharon Shoham, and Eran Yahav. 2018. Programming not only by\nexample. In 2018 IEEE/ACM 40th International Conference on Software Engineering\n(ICSE). IEEE, 1114\u20131124.\n[35] Paul Robinette, Wenchen Li, Robert Allen, Ayanna M Howard, and Alan R Wagner.\n2016.\nOvertrust of robots in emergency evacuation scenarios. In 2016 11th\nACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 101\u2013\n108.\n[36] Armando Solar-Lezama, Rodric Rabbah, Rastislav Bod\u00edk, and Kemal Ebcio\u011flu.\n2005. Programming by sketching for bit-streaming programs. In Proceedings of\nthe 2005 ACM SIGPLAN conference on Programming language design and imple-\nmentation. 281\u2013294.\n[37] Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett,\nThomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting\nmeaningfully with machine learning systems: Three experiments. International\njournal of human-computer studies 67, 8 (2009), 639\u2013662.\n[38] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020.\nTreegen: A tree-based transformer architecture for code generation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8984\u20138991.\n[39] Tabnine [n. d.]. Code Faster with AI Code Completions. https://www.tabnine.\ncom/. Accessed: 2022-1-8.\n[40] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin\nWhite, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing\npatches in the wild via neural machine translation. ACM Transactions on Software\nEngineering and Methodology (TOSEM) 28"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_019",
    "source_id": "ExpectationVsExperience2022",
    "text": "Treegen: A tree-based transformer architecture for code generation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8984\u20138991.\n[39] Tabnine [n. d.]. Code Faster with AI Code Completions. https://www.tabnine.\ncom/. Accessed: 2022-1-8.\n[40] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin\nWhite, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing\npatches in the wild via neural machine translation. ACM Transactions on Software\nEngineering and Methodology (TOSEM) 28, 4 (2019), 1\u201329.\n[41] Richard J Waldinger and Richard CT Lee. 1969. PROW: A step toward automatic\nprogram writing. In Proceedings of the 1st international joint conference on Artificial\nintelligence. 241\u2013252.\n[42] Ben Wang. 2021. Mesh-Transformer-JAX: Model-Parallel Implementation of\nTransformer Language Model with JAX. https://github.com/kingoflolz/mesh-\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI \u201922 Extended Abstracts, April 29-May 5, 2022, New Orleans, LA, USA\ntransformer-jax.\n[43] Chenglong Wang, Yu Feng, Rastislav Bodik, Alvin Cheung, and Isil Dillig. 2019.\nVisualization by example. Proceedings of the ACM on Programming Languages 4,\nPOPL (2019), 1\u201328.\n[44] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\nFernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Perfection\nNot Required? Human-AI Partnerships in Code Translation. In 26th International\nConference on Intelligent User Interfaces. 402\u2013412.\n[45] Daniel S Weld and Gagan Bansal. 2018. Intelligible artificial intelligence. ArXiv\ne-prints, March 2018 (2018).\n[46] Frank F Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and Graham\nNeubig. 2020. Incorporating external knowledge through pre-training for natural\nlanguage to code generation. arXiv preprint arXiv:2004.09015 (2020).\n[47] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2021. In-IDE Code Generation\nfrom Natural Language: Promise and Challenges. arXiv preprint arXiv:2101.11149\n(2021).\n[48] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-\npurpose code generation. arXiv preprint arXiv:1704.01696 (2017).\n[49] Pengcheng Yin and Graham Neubig. 2018. TRANX: A transition-based neural\nabstract syntax parser for semantic parsing and code generation. arXiv preprint\narXiv:1810.02720 (2018).\n"
  },
  {
    "chunk_id": "ExpectationVsExperience2022_chunk_020",
    "source_id": "ExpectationVsExperience2022",
    "text": ", Bogdan Vasilescu, and Graham Neubig. 2021. In-IDE Code Generation\nfrom Natural Language: Promise and Challenges. arXiv preprint arXiv:2101.11149\n(2021).\n[48] Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-\npurpose code generation. arXiv preprint arXiv:1704.01696 (2017).\n[49] Pengcheng Yin and Graham Neubig. 2018. TRANX: A transition-based neural\nabstract syntax parser for semantic parsing and code generation. arXiv preprint\narXiv:1810.02720 (2018).\n[50] Wojciech Zaremba, Greg Brockman, and OpenAI. 2021. OpenAI Codex. https:\n//openai.com/blog/openai-codex/. Accessed: 2022-1-8.\n[51] Tianyi Zhang, London Lowmanstone, Xinyu Wang, and Elena L Glassman. 2020.\nInteractive Program Synthesis by Augmented Examples. In Proceedings of the\n33rd Annual ACM Symposium on User Interface Software and Technology. 627\u2013648.\n[52] Yunfeng Zhang, Q Vera Liao, and Rachel KE Bellamy. 2020. Effect of confidence\nand explanation on accuracy and trust calibration in AI-assisted decision making.\nIn Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.\n295\u2013305.\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_001",
    "source_id": "SkillFormation2026",
    "text": "How AI Impacts Skill Formation\nJudy Hanwen Shen\u2217\nAlex Tamkin\u2020\nFebruary 3, 2026\nAbstract\nAI assistance produces significant productivity gains across professional domains, particularly for\nnovice workers. Yet how this assistance affects the development of skills required to effectively supervise\nAI remains unclear. Novice workers who rely heavily on AI to complete unfamiliar tasks may compromise\ntheir own skill acquisition in the process. We conduct randomized experiments to study how developers\ngained mastery of a new asynchronous programming library with and without the assistance of AI.\nWe find that AI use impairs conceptual understanding, code reading, and debugging abilities, without\ndelivering significant efficiency gains on average. Participants who fully delegated coding tasks showed\nsome productivity improvements, but at the cost of learning the library. We identify six distinct AI\ninteraction patterns, three of which involve cognitive engagement and preserve learning outcomes even\nwhen participants receive AI assistance. Our findings suggest that AI-enhanced productivity is not a\nshortcut to competence and AI assistance should be carefully adopted into workflows to preserve skill\nformation \u2013 particularly in safety-critical domains.\n1\nIntroduction\nSince the industrial revolution, skills in the labor market have continually shifted in response to the introduction\nof new technology; the role of workers often shifts from performing the task to supervising the task [Autor\net al., 2001]. For example, the automation of factory robots has enabled humans to move from manual labor\nto supervision, and accounting software has enabled professionals to move from performing raw calculations\nto identifying better bookkeeping and tax strategies. In both scenarios, humans are responsible for the quality\nof the final product and are liable for any errors [Bleher and Braun, 2022]. Even as automation changes the\nprocess of completing tasks, technical knowledge to identify and fix errors remains extremely important.\nAs AI promises to be a catalyst for automation and productivity in a wide range of applications, from\nsoftware engineering to entrepreneurship [Dell\u2019Acqua et al., 2023, Peng et al., 2023, Cui et al., 2024, Otis\net al., 2024, Brynjolfsson et al., 2025], the impacts of AI on the labor force are not yet fully understood.\nAlthough more workers rely on AI to improve their productivity, it is unclear whether the use of AI assistance\nin the workplace might hinder core understanding of concepts or prevent the development of skills necessary\nto supervise automated tasks. Although most studies have focused on the end product of AI assistance (e.g.,\nlines of code written, quality of ideas proposed), an equally important, if not more crucial question is how\nprocess of receiving AI assistance impacts workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthems"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_002",
    "source_id": "SkillFormation2026",
    "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n\u2217Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n\u2020Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_003",
    "source_id": "SkillFormation2026",
    "text": " we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring new concepts and skills, and (2) whether this use of AI reduces the\nlevel of understanding of these new concepts and skills.\n1.1\nOur Results\nMotivated by the salient setting of AI and software skills, we design a coding task and evaluation around a\nrelatively new asynchronous Python library and conduct randomized experiments to understand the impact\nof AI assistance on task completion time and skill development. We find that using AI assistance to complete\ntasks that involve this new library resulted in a reduction in the evaluation score by 17% or two grade\npoints (Cohen\u2019s d = 0.738, p = 0.010). Meanwhile, we did not find a statistically significant acceleration in\ncompletion time with AI assistance (Figure 6).\nThrough an in-depth qualitative analysis where we watch the screen recordings of every participant in our\nmain study, we explain the lack of AI productivity improvement through the additional time some participants\ninvested in interacting with the AI assistant. Some participants asked up to 15 questions or spent more\nthan 30% of the total available task time on composing queries (Figure 12). We attribute the gains in\n2\nskill development of the control group to the process of encountering and subsequently resolving errors\nindependently. We categorize AI interaction behavior into six common patterns and find three AI interaction\npatterns that best preserve skill development (Figure 11). These three patterns of interaction with AI, which\nresulted in higher scores in our skill evaluation, involve more cognitive effort and independent thinking (for\nexample, asking for explanations or asking conceptual questions only).\n2\nBackground\n2.1\nThe Impacts of AI Usage\nSince the widespread availability of ChatGPT, Copilot, Claude, and other advanced conversational assistants\nin late 2022, AI tools have been widely used in many different domains. Studies examining prompt-based\nutilization have facilitated a detailed examination of AI\u2019s real-world applications [Tamkin et al., 2024, Shen and\nGuestrin, 2025]. For example, AI tools are being used in professional domains such as software development,\neducation, design, and the sciences [Handa et al., 2025].\nProductivity Gains\nMany studies have found improvements in productivity using these AI assistants.\nFor example, Brynjolfsson et al. found that AI-based conversational assistants increased the number of\nissues call center workers were able to resolve on average by 15%.\nDell\u2019Acqua et al. find similar results\nin which consultants completed 12.2% more tasks on average with the help of AI than without it. While\nthe skill-based effects differ across studies,"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_004",
    "source_id": "SkillFormation2026",
    "text": " et al., 2024, Shen and\nGuestrin, 2025]. For example, AI tools are being used in professional domains such as software development,\neducation, design, and the sciences [Handa et al., 2025].\nProductivity Gains\nMany studies have found improvements in productivity using these AI assistants.\nFor example, Brynjolfsson et al. found that AI-based conversational assistants increased the number of\nissues call center workers were able to resolve on average by 15%.\nDell\u2019Acqua et al. find similar results\nin which consultants completed 12.2% more tasks on average with the help of AI than without it. While\nthe skill-based effects differ across studies, a consistent pattern emerges in call center work, consulting, legal\nquestion-answering, and writing: less experienced and lower-skilled workers tend to benefit most [Brynjolfsson\net al., 2025, Dell\u2019Acqua et al., 2023, Choi and Schwarcz, 2023, Noy and Zhang, 2023]. One exception was\nwhen GPT-4 was given to Kenyan small business owners, AI business advice helped high performers (by\nrevenue) improve business results while worsening the results for lower performers [Otis et al., 2024].\nFor software engineering in particular, Peng et al. found that crowd-sourced software developers using copilot\ncompleted a task 55.5% faster than the control group and novice programmers benefited more from AI coding\nassistance. Follow-up studies of developers in major software companies and found that AI-generated code\ncompletions provide a 26. 8% boost in productivity as measured by pull requests, commits, and software\nproduct builds [Cui et al., 2024]. This study also found that less experienced coders experienced greater\nboosts in productivity. While studies find that junior or less experienced developers experience greater\nproductivity uplift from using AI, these very same workers should be quickly developing new skills in the\nworkplace. Yet the effect of these tools on the skill formation of this subgroup remains unknown. Will the\nskill development of novice workers be affected significantly since they are still in the process of learning their\ntrade? We are motivated by whether this productivity comes from free or at a cost.\nCognitive Offloading\nConcerns around the impact of AI assistance and skill depletion have been highlighted\nby recent works. For example, medical professionals trained with AI assistance might not develop keen visual\nskills to identify certain conditions [Macnamara et al., 2024]. In surveys given to knowledge workers, frequent\nuse of AI has been associated with worse critical thinking abilities and increased cognitive offloading [Gerlich,\n2025]. Furthermore, knowledge workers reported a lower cognitive effort and confidence when using generative\nAI tools [Lee et al., 2025]. However, these surveys are observational and may not capture the causal effects of\nAI usage.\nSkill Retention\nAn adjacent line of inquiry to our research is how well humans retain knowledge and\nskills after AI assistance. Wu et al. find that even when generative AI improved immediate performance on\ncontent creation tasks (e.g., writing a Facebook post, writing a performance review, drafting a welcoming\nemail), the performance increase did not persist in subsequent tasks performed independently by humans\nafterward. For data science tasks"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_005",
    "source_id": "SkillFormation2026",
    "text": " has been associated with worse critical thinking abilities and increased cognitive offloading [Gerlich,\n2025]. Furthermore, knowledge workers reported a lower cognitive effort and confidence when using generative\nAI tools [Lee et al., 2025]. However, these surveys are observational and may not capture the causal effects of\nAI usage.\nSkill Retention\nAn adjacent line of inquiry to our research is how well humans retain knowledge and\nskills after AI assistance. Wu et al. find that even when generative AI improved immediate performance on\ncontent creation tasks (e.g., writing a Facebook post, writing a performance review, drafting a welcoming\nemail), the performance increase did not persist in subsequent tasks performed independently by humans\nafterward. For data science tasks, Wiles et al. described the impact of AI on non-technical consultants as\nan \u201cexoskeleton\u201d, the enhanced technical abilities enabled by AI did not persist when workers no longer had\naccess to AI. Our work asks the natural follow-up question of whether the usage of AI tools could cause worse\nlearning outcomes for the acquisition of skills on the job for technical professionals themselves.\n3\nOverreliance\nAlthough much of the literature in economics on AI-enhanced productivity implicitly assumes\nthat generations of AI are trustworthy, the reality is that generative AI can produce incorrect [Longwell\net al., 2024] or hallucinated content [Maleki et al., 2024]. When models are fallible, yet still deployed to assist\nhumans, human decisions that follow erroneous model decisions are referred to as \u201coverreliance\u201d [Bu\u00e7inca\net al., 2021, Vasconcelos et al., 2023, Klingbeil et al., 2024]. Although methods have been suggested to reduce\noverreliance, these focus mainly on decision-time information such as explanations [Vasconcelos et al., 2023,\nReingold et al., 2024] or debate [Kenton et al., 2024].\n2.2\nCS Education and AI Assistance\nMeasuring the acquisition of skills is highly domain dependent. For computer science in particular, most\nintroductory courses measure learning through multiple choice questions, code writing, and code read-\ning/explanations [Cheng et al., 2022]. More recent work has found code interviews, and active discussion of\nstudents\u2019 code to yield positive learning outcomes [Kannam et al., 2025].\nSeveral observational studies have described how students use AI tools in the context of a computer science\ncourse. Poitras et al. found, over the course of a semester, that students used AI tools to write code, fix\nerrors, and explain algorithmic concepts; students with less coding proficiency were more likely to seek AI\nassistance. Other works use surveys to find that students may be hesitant to use AI coding assistant tools\ndue to \u201cdependence worry\u201d (i.e., overreliance on coding tools) [Pan et al., 2024]. For formal methods, Prasad\net al. coded the different ways in which students used LLMs for course work and found that upper-year\nstudents taking the class did not rely on LLM assistance and only asked a few questions at the beginning.\nUser studies have also been conducted in the professional development environments. Wang et al. study\ndifferent patterns in usage between users with and without"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_006",
    "source_id": "SkillFormation2026",
    "text": " code, fix\nerrors, and explain algorithmic concepts; students with less coding proficiency were more likely to seek AI\nassistance. Other works use surveys to find that students may be hesitant to use AI coding assistant tools\ndue to \u201cdependence worry\u201d (i.e., overreliance on coding tools) [Pan et al., 2024]. For formal methods, Prasad\net al. coded the different ways in which students used LLMs for course work and found that upper-year\nstudents taking the class did not rely on LLM assistance and only asked a few questions at the beginning.\nUser studies have also been conducted in the professional development environments. Wang et al. study\ndifferent patterns in usage between users with and without chat access to AI models in completing coding\npuzzles and development tasks. They found rich interaction patterns including interactive debugging, code\ndiscussions, and asking specific questions. Participants ranged from asking ChatGPT to do then the entire\nproblem (lowest quality code output) to only asking minimal questions (highest efficiency). Other studies\nhave reported that AI tools help the software development process through easier access to documentation\nand accurate generation code for specific APIs [Pinto et al., 2024].\n3\nFramework\nProfessional Skill Acquisition\nThe \u201clearning by doing\u201d philosophy has been suggested by many learning\nframeworks such as the Kolb\u2019s experiential learning cycle, and the Problem-Based Learning (PBL) [Kolb,\n2014, Schmidt, 1994]. The frameworks connect the completion of real-world tasks with the learning of\nnew concepts and the development of new skills. Experiential learning has also been explored specifically\nin software engineering courses in higher education in order to mimic solving problems in a professional\nsetting [Gonzalez-Huerta et al., 2020]. In its simplest form, we model AI tool assistance as taking a different\nlearning path than without AI. We hypothesize that using AI tools to generate code in the development\nprocess effectively amounts to taking a shortcut to task completion without a pronounced learning stage.\nAI for Coding Usage Patterns\nPrior works have found that humans use AI in many different ways\nfor coding: from question answering to writing code, to debugging [Poitras et al., 2024, Wang et al., 2020,\nPinto et al., 2024]. In our framework, different ways of using AI assistance represent different learning paths\ntaken to reach the goals of completing the task. We analyze these different usage patterns in the qualitative\nanalysis of this work (Section 6).\nResearch Questions\nBased on this background, we focus on on-the-job learning: settings where workers\nmust acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n\u2022 RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_007",
    "source_id": "SkillFormation2026",
    "text": " acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n\u2022 RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on coding skill formation.\nFigure 3: Experiment interface: We used a online interview platform to run our experiment. The treatment\ncondition participants are prompted to use the AI assistant.\n\u2022 RQ2: How does using AI assistance affect the development of these new skills?\n4\nMethods\n4.1\nTask Selection: Learning Asynchronous Programming with Trio\nWe prototyped tasks for several different skills that junior software engineers may encounter on the job:\nfrom data analysis to plotting. We designed an experiment around the Python Trio library,1 which is\ndesigned for asynchronous concurrency and input-output processing (I/O). This library is less well known\nthan asyncio (according to the number of StackOverflow questions) and involves new concepts (e.g.,\nstructured concurrency) beyond just Python fluency. It is also explicitly designed to be easy to use \u2013 making\nit particularly suitable for a learning experiment.\nWe designed and tested five tasks that use the Trio library for asynchronous programming, a skill often\n1See documentation at: https://trio.readthedocs.io/en/stable/\n5\nlearned in a professional setting when working with large-scale data or software systems. The tasks we created\ninclude problem descriptions, starter code, and brief descriptions of the Trio concepts required to complete\nthe task. These tasks are designed to parallel the process of learning to use a new library or new software\ntool through a brief self-guided tutorial. For example, in software engineers\u2019 on-boarding materials, there is\noften a description of how to use an internal library and small tasks to build skills with the new library.\nAfter several pilot studies, we used the first two tasks in our main study; each task took 10 - 20 minutes\nduring initial testing. The first task is to write a timer that prints every passing second while other functions\nrun. This task introduces the core concepts of nurseries, starting tasks, and running functions concurrently\nin Trio. The second task involves implementing a record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants\u2019 current version of the code and can produce the full, correct code\nfor both tasks directly when"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_008",
    "source_id": "SkillFormation2026",
    "text": " record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants\u2019 current version of the code and can produce the full, correct code\nfor both tasks directly when prompted.\n4.2\nEvaluation Design\nBased on a previous meta-analysis of evaluations in computer science education [Cheng et al., 2022], we\nidentify four types of questions used to assess the mastery of coding skills. Returning to our initial motivation\nof developing and retaining the skills required for supervising automation, proficiency in some of these areas\nmay be more important than others for the oversight of AI-generated code. The four types of questions we\nconsider are the following.\n\u2022 Debugging The ability to identify and diagnose errors in code. This skill is crucial for detecting when\nAI-generated code is incorrect and understanding why it fails.\n\u2022 Code Reading The ability to read and comprehend what code does. This skill enables humans to\nunderstand and verify AI-written code before deployment.\n\u2022 Code Writing The ability to write or pick the right way to write code. Low-level code writing, like\nremembering the syntax of functions, will be less important with further integration of AI coding tools\nthan high-level system design.\n\u2022 Conceptual The ability to understand the core principles behind tools and libraries. Conceptual\nunderstanding is critical to assess whether AI-generated code uses appropriate design patterns that\nadheres to how the library should be used.\nThe two tasks in our study cover 7 core concepts from the Trio library. We designed a quiz with debugging,\ncode reading, and conceptual questions that cover these 7 concepts. We exclude code writing questions to\nreduce the impact of syntax errors in our evaluation; these errors can be easily corrected with an AI query or\nweb search. We tested 5 versions (Table 2) of the quiz in user testing and preliminary studies based on item\nresponse theory. For example, we ensure that all questions are sufficiently correlated with the overall quiz\nscore, that each question has an appropriate average score, and that the questions are split up such that there\nis no local item dependence between questions (i.e., participants could not infer the answers to a question by\nlooking at other questions). The final evaluation we used contained 14 questions for a total of 27 points. We\nsubmitted the grading rubric for the quiz in our study pre-registration before running the experiment.\n4.3\nStudy Design\nWe use a between-subjects randomized experiment to test for the effects of using AI in the coding skill\nformation process. Each participant first completed a warm-up coding task on a coding platform, where they\nneeded to add a border around a list of strings. This Python coding question takes an average of 4 minutes to\ncomplete among users of this coding platform. There are no asynchronous concepts in this coding question.\n6\nFigure 4: Overview of learning task and"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_009",
    "source_id": "SkillFormation2026",
    "text": " answers to a question by\nlooking at other questions). The final evaluation we used contained 14 questions for a total of 27 points. We\nsubmitted the grading rubric for the quiz in our study pre-registration before running the experiment.\n4.3\nStudy Design\nWe use a between-subjects randomized experiment to test for the effects of using AI in the coding skill\nformation process. Each participant first completed a warm-up coding task on a coding platform, where they\nneeded to add a border around a list of strings. This Python coding question takes an average of 4 minutes to\ncomplete among users of this coding platform. There are no asynchronous concepts in this coding question.\n6\nFigure 4: Overview of learning task and comprehension check. All participants completed a warm-up coding\ntask that did not require Trio knowledge. During the main Trio task, participants in the treatment group\ncould use AI assistance to answer questions or generate code. All participants were not allowed to use AI in\nthe comprehension check.\nTreatment (%)\nControl (%)\nDifference (%)\nYears of Coding Experience\n1-3 years\n2 (0.077)\n2 (0.077)\n0\n4-6 years\n10 (0.385)\n9 (0.346)\n1 (0.038)\n7+ years\n14 (0.538)\n15 (0.577)\n1 (0.038)\nFrequency of Python Use\nRegularly / Frequently\n18 (0.692)\n16 (0.615)\n2 (0.077)\nDaily / Extensively\n8 (0.308)\n10 (0.385)\n2 (0.077)\nPer Task Async Quiz Score\n0-2 (0-40%)\n5 (0.192)\n5 (0.192)\n0\n3-4 (60-80%)\n18 (0.692)\n15 (0.577)\n3 (0.115)\n5 (100%)\n3 (0.115)\n6 (0.231)\n3 (0.115)\nPrior Python Asyncio Usage\n18 (0.692)\n20 (0.769)\n2 (0.077)\nPre-Task Coding Time\n6.5 min\n8 min\n1.5 min\nTable 1: Balance table of main study participants (n=52).\nNo participants have access to AI while completing the warm-up stage. We use this stage to calibrate the\nPython familiarity of the participants and to help participants familiarize themselves with the interface.\nThe next stage is the Trio task stage, where participants have a maximum of 35 minutes to complete two\ncoding tasks using Trio in the same coding platform. During this stage, participants in the AI assistance\ncondition (treatment group) had access to coding help through a chat-based AI assistant (Figure 3). All\nparticipants are instructed to complete the task as fast as they could.\nAfter completing the Trio task, participants completed the evaluation stage where they take the quiz we\ndescribed in the previous section and complete a survey that consists of demographic and experiential\nquestions after the quiz.\nIn our main study, 52 participants completed the task, 26 for each of the control and treatment groups. For\nall our pilot studies and the main study, we only recruited participants who self-reported having more than\none year of Python experience, code in Python at least once a week, have tried AI coding assistance at least\na"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_010",
    "source_id": "SkillFormation2026",
    "text": " assistance\ncondition (treatment group) had access to coding help through a chat-based AI assistant (Figure 3). All\nparticipants are instructed to complete the task as fast as they could.\nAfter completing the Trio task, participants completed the evaluation stage where they take the quiz we\ndescribed in the previous section and complete a survey that consists of demographic and experiential\nquestions after the quiz.\nIn our main study, 52 participants completed the task, 26 for each of the control and treatment groups. For\nall our pilot studies and the main study, we only recruited participants who self-reported having more than\none year of Python experience, code in Python at least once a week, have tried AI coding assistance at least\na few times, and have never used the Trio library before (Table 1).\nWe use the coding platform to collect the keystrokes of the users as they code and the transcripts of their\n7\nPilot\nPlatform\nParticipants\nNo. Tasks\nChallenges\nA\nP1\nn=39\n5\nNon-Compliance: 35% of participants in the no AI condi-\ntion used AI assistance to copy the instructions and paste\nthe results. Participants also self-reported using AI assis-\ntance in the no AI condition.\nB\nP1\nn=107\n5\nContinued Non-Compliance: Even when warned about\nthe strict no AI requirements, participants continued to\nuse AI for both coding and the quiz. 25% of participants\nused AI and screen recording was not an option from the\nparticipant platform.\nC\nP2\nn=20\n5\nLocal Item Dependence:\nThrough watching screen\nrecordings, we observed participants scrolling back and forth\nbetween questions to guess the correct answer.\nD\nP2\nn=20\n2\nPython Syntax Delays: In the time limit of 35 minutes,\nonly 60% of participants in the control (no AI condition)\nfinished both tasks. The screen recordings showed several\nparticipants struggling with Python syntax issues, such as\ntry/except blocks and string formatting. These delays were\nnot germane to the Trio library.\nTable 2: Summary of pilot studies with different data providers, tasks, and evaluation design.\ninteraction with the AI coding assistant in the coding condition. We use Google Forms to collect survey\nresponses from users both before the coding task and after the coding task. Together, these tasks take\na maximum time of 1 hour and 15 minutes with an average duration of 58.5 minutes. Participants were\nrecruited through a third party crowd-worker platform and paid a flat rate of 150 USD for the task.\n5\nResults\n5.1\nPilot Studies\nNon-Compliance We conducted 4 pilot studies before running the full study (Table 2). The first two pilot\nstudies were done on a different crowdworking platform (P1). On this platform, we observed a high level\nnon-compliance (35%) both during the task and the quiz (i.e., participants used AI to complete the coding\ntask in the control group or used AI to complete the evaluation. We observed non-compliance behavior\nthrough the coding platform transcripts of when users copied the instructions or pasted code into the editor.\nWe tested different mechanisms to ensure participants in the control condition (No AI) did not use AI for the\ntask. However,"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_011",
    "source_id": "SkillFormation2026",
    "text": "\nResults\n5.1\nPilot Studies\nNon-Compliance We conducted 4 pilot studies before running the full study (Table 2). The first two pilot\nstudies were done on a different crowdworking platform (P1). On this platform, we observed a high level\nnon-compliance (35%) both during the task and the quiz (i.e., participants used AI to complete the coding\ntask in the control group or used AI to complete the evaluation. We observed non-compliance behavior\nthrough the coding platform transcripts of when users copied the instructions or pasted code into the editor.\nWe tested different mechanisms to ensure participants in the control condition (No AI) did not use AI for the\ntask. However, despite more explicit instructions, around 25% in the control group participants still used AI.\nWe conducted two pilot studies with a second crowdworking platform (P2), each with 20 participants. Using\nscreen recordings of participant progress, we verified that participants did not use AI in the control group nor\nfor the quiz.\nLocal Item Dependence In Pilot Study C, we observed Local Item Dependence in the quiz: participants\nwould compare questions and identify answers based on code snippets provided in other questions. This\nmotivated us to split the quiz into several different pages, where the questions on each page did not provide\nhints for other questions. Based on screen recordings, we observed that this reduced Local Item Dependence\nin pilot D. Additionally, we reduced the total number of tasks from five to two. This change allowed us to\nbetter isolate learning from the first two tasks while eliminating a confounding variable: participants in the\nAI condition seeing more concepts simply because they completed more tasks within the allotted time. To\nalign the quiz with this modification, we adjusted the quiz questions to cover only the first two tasks.\nBarriers to Task Completion In Pilot Study D, we included 20 participants. We found a significant\ndifference in both the task completion time and the quiz score between the AI and non-AI conditions\n8\nAI\nNo AI\nCondition\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\nTask Time (min)\np = 0.033*\nTask Time\nAI\nNo AI\nCondition\n35\n40\n45\n50\n55\n60\n65\n70\nQuiz Score %\np = 0.003**\nQuiz Score %\nTask Time\nQuiz Score\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTreatment Effect in s.d.\n*\n**\nTreatment Effect\nPilot Study D (n=20): Difference in Group Means and Treatment Effect\nFigure 5: Difference in means of overall task time and quiz score between the control (No AI) and treatment\n(AI Assistant) groups in Pilot Study D. Error bars represent 95% CI. Significance values correspond to\ntreatment effect. * p<0.05, **<0.01, ***<0.001\n(Figure 5). When we reviewed the screen recording, participants in the control (no AI) condition struggled\nwith Python syntax that was unrelated to Trio, such as try/except blocks and string formatting. The task\ncompetition rate within the 35-minute time limit was only 60% within the control"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_012",
    "source_id": "SkillFormation2026",
    "text": "Pilot Study D (n=20): Difference in Group Means and Treatment Effect\nFigure 5: Difference in means of overall task time and quiz score between the control (No AI) and treatment\n(AI Assistant) groups in Pilot Study D. Error bars represent 95% CI. Significance values correspond to\ntreatment effect. * p<0.05, **<0.01, ***<0.001\n(Figure 5). When we reviewed the screen recording, participants in the control (no AI) condition struggled\nwith Python syntax that was unrelated to Trio, such as try/except blocks and string formatting. The task\ncompetition rate within the 35-minute time limit was only 60% within the control (no AI) group compared to\na 90% completion rate in the treatment (AI) group. Since our focus was not Python syntax, we added syntax\nhints about string formatting and try/except blocks for the main study.\nFigure 5 presents the treatment effects on both outcome measures: Task Time and Quiz Score. The treatment\n(AI) group completed the Trio tasks faster (Cohen\u2019s d=1.11, p=0.03), demonstrating improved task efficiency.\nHowever, the AI group performed significantly worse on the knowledge quiz (Cohen\u2019s d=1.7, p=0.003),\nindicating reduced retention of learning. For our complete power analysis and pre-registration of the study,2\nwe assumed a conservative effect size of d = 0.85 (half of the observed learning effect) to account for the\npotential effect size inflation typical in pilot studies.\n5.2\nMain Study\n5.2.1\nParticipants\nTo recruit 50 participants, we sent our study to 58 crowd workers. Participants were balanced across the\nfollowing attributes (recorded through a separate recruitment survey): years of coding experience, years of\nPython experience, prior usage of the Python Asyncio library, frequency of Python use in the past year,\nand an asynchronous programming familiarity score (a 5-question, multiple-choice concept check). The\ndemographic breakdown of the participants, collected after the completion of the task to avoid the threat\nof stereotypes, is summarized in Figure 17. Most participants in our study hold a bachelor\u2019s degree, are\nbetween 25 and 35 years old, and work either as freelance or professional software developers. 53 participants\ncompleted all three parts of the study. Following our preregistered disqualification criteria, 1 participant was\ndisqualified after leaving four blank questions on the quiz due to not realizing that there were multiple parts\nof the quiz and subsequently running out of time.\n5.2.2\nResults\nFigure 6 shows that while using AI to complete our coding task did not significantly improve task completion\ntime, the level of skill formation gained by completing the task, measured by our quiz, is significantly reduced\n(Cohen d=0.738, p=0.01). There is a 4.15 point difference between the means of the treatment and control\ngroups. For a 27-point quiz, this translates into a 17% score difference or 2 grade points. Controlling for\nwarm-up task time as a covariate, the treatment effect remains significant (Cohen\u2019s d=0.725, p=0.016).\n2Pre-registration: https://osf.io/w49e7"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_013",
    "source_id": "SkillFormation2026",
    "text": ".2\nResults\nFigure 6 shows that while using AI to complete our coding task did not significantly improve task completion\ntime, the level of skill formation gained by completing the task, measured by our quiz, is significantly reduced\n(Cohen d=0.738, p=0.01). There is a 4.15 point difference between the means of the treatment and control\ngroups. For a 27-point quiz, this translates into a 17% score difference or 2 grade points. Controlling for\nwarm-up task time as a covariate, the treatment effect remains significant (Cohen\u2019s d=0.725, p=0.016).\n2Pre-registration: https://osf.io/w49e7\n9\nAI\nNo AI\nCondition\n21\n22\n23\n24\n25\n26\n27\n28\nTask Time (min)\np = 0.391\nTask Time\nAI\nNo AI\nCondition\n45\n50\n55\n60\n65\n70\n75\nQuiz Score %\np = 0.010*\nQuiz Score %\nTask Time\nQuiz Score\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nTreatment Effect in s.d.\n*\nTreatment Effect\nMain Study (n=52): Difference in Group Means and Treatment Effect\nFigure 6: Difference in means of overall task time and quiz score between the control (No AI) and treatment\n(AI Assistant) groups in main study (n=52). Error bars represent 95% CI. Significance values correspond to\ntreatment effect. * p<0.05, **<0.01, ***<0.001\n1-3 years\n4-6 years\n7+ years\nYears of Coding Experience\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\nTask Time (min)\nTask Time by Years of Coding Experience\n1-3 years\n4-6 years\n7+ years\nYears of Coding Experience\n30\n40\n50\n60\n70\n80\nQuiz Score %\nQuiz Score by Years of Coding Experience\nCondition\nAI\nNo AI\nMain Study: Skill Development by Years of Coding Experience\nFigure 7: Task completion time and quiz score by years of coding experience. Error bars represent 95% CI.\nThe control group (No AI) average quiz score is higher across all levels of coding experience.\n10\nTask1\nTask2\nConceptual\nDebugging\nCodeReading\nQuiz Subarea\n30\n40\n50\n60\n70\nScore %\nQuiz Score % by Subarea and Condition\nCondition\nAI\nNo AI\nFigure 8: Score breakdown by questions type relating to each task and skill area. Debugging questions\nrevealed the largest differences in average quiz score between the treatment and control groups.\nPrior works have presented mixed results on whether AI helps or hinders coding productivity [Peng et al.,\n2023, Becker et al., 2025]; our study differs from prior results in that it is designed to study how AI affects\nskill formation while performing a task requiring new knowledge. While we do observe a slightly lower average\ncompletion time in the AI group among novice programmers, due to the small group size of the 1-"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_014",
    "source_id": "SkillFormation2026",
    "text": "50\n60\n70\nScore %\nQuiz Score % by Subarea and Condition\nCondition\nAI\nNo AI\nFigure 8: Score breakdown by questions type relating to each task and skill area. Debugging questions\nrevealed the largest differences in average quiz score between the treatment and control groups.\nPrior works have presented mixed results on whether AI helps or hinders coding productivity [Peng et al.,\n2023, Becker et al., 2025]; our study differs from prior results in that it is designed to study how AI affects\nskill formation while performing a task requiring new knowledge. While we do observe a slightly lower average\ncompletion time in the AI group among novice programmers, due to the small group size of the 1-3 year\nparticipant group (n=4), the difference in task time was not significant. 4 of the 26 participants in the control\n(No AI) group did not complete the second task within the 35-minute limit, while every participant in the AI\ncondition completed the second task. Our results do not conclusively find a speed up or slow down using AI\nin this task.\nAcross all levels of prior coding experience, users scored higher on average in the control (no AI) than in\nthe treatment (AI assistance) group (Figure 7). This shows that our choice of tasks and task design did not\ncritically hinge on the participants\u2019 experience level of the but presented new skills to be acquired for every\nexperience group.\nConcept Group Analysis In exploratory data analysis (not pre-registered), the quiz score was decomposed\ninto subareas and question types (Figure 8). Each question in the quiz belonged to exactly one task (e.g.,\nTask 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). For both\ntasks, there is a gap between the quiz scores between the treatment and control groups. Among the different\ntypes of questions, the largest score gap occurs in the debugging questions and the smallest score gap in\nthe code reading questions. This outcome is expected since treatment and control groups may have similar\nexposure to reading code through the task, but the control group with no access to AI assistance encountered\nmore errors during the task and became more capable at debugging.\nTask Experience In further exploratory data analysis, we also find differences in the way participants\u2019\nexperience of completing the study. The control group (No AI) reported higher self-reported learning (on a\n7-point scale), while both groups reported high levels of enjoyment in completing the task (Figure 9). In terms\nof difficulty of the task, Figure 10 shows that although participants in the treatment group (AI Assistance)\nfound the task easier than the control group, both groups found the post-task quiz similarly challenging.\n6\nQualitative Analysis\nAlthough overall statistics on productivity and quiz score shed light on a high-level trend of how AI assistance\naffects a new learning task, a deeper analysis of how each participant completed the learning task allows us\n11\nLearning\nEnjoyment\nMetric\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nRating (Self-Reported)\nTask Learning and Enjoyment by Condition (1 to 7 Scale)\nCondition\nAI\nNo AI\nFigure 9: Self-reported enjoyment and learning\nby condition during our study.\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_015",
    "source_id": "SkillFormation2026",
    "text": " the treatment group (AI Assistance)\nfound the task easier than the control group, both groups found the post-task quiz similarly challenging.\n6\nQualitative Analysis\nAlthough overall statistics on productivity and quiz score shed light on a high-level trend of how AI assistance\naffects a new learning task, a deeper analysis of how each participant completed the learning task allows us\n11\nLearning\nEnjoyment\nMetric\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nRating (Self-Reported)\nTask Learning and Enjoyment by Condition (1 to 7 Scale)\nCondition\nAI\nNo AI\nFigure 9: Self-reported enjoyment and learning\nby condition during our study.\nWarm Up\nTrio Task\nPost Task Quiz\nTask Stage\n2.0\n2.5\n3.0\n3.5\n4.0\nMean Difficulty (1=Very Little, 5=Very High)\nMean Task Difficulty by Task Stage and Condition\nCondition\nAI\nNo AI\nFigure 10: Self-reported task difficulty by condi-\ntion during different stages of our study.\nto better understand participant heterogeneity. In the initial coding phase of our qualitative analysis, we\nmanually annotated screen recordings of the 51 participants in the main study.3 We grouped the annotations\ninto several main concepts related to task progress events such as errors, AI interactions, AI queries, and\ntask completions (Table 5). This analysis allows us to understand not just the overall productivity and\nlearning, but also how AI was used during each task in our study. We make these annotated transcripts\npublicly available for future studies.4\nAnalyzing these concepts or common patterns among participants helps supplement our quantitative ob-\nservations of skill formation and task completion in this new library. Specifically, the following axes shows\ndifferences between participants and across conditions:\n\u2022 AI Interaction Time: The lack of significant speed-up in the AI condition can be explained by how\nsome participants used AI. Several participants spent substantial time interacting with the AI assistant,\nspending up to 11 minutes composing AI queries in total (Figure 12).\n\u2022 Query Types: The study participants varied between conceptual questions only, code generation only,\nand a mixture of conceptual, debugging, and code generation queries. Participants who focused on\nasking the AI assistant debugging questions or confirming their answer spent more time on the task\n(Figure 18).\n\u2022 Encountering Errors: Participants in the control group (no AI) encountered more errors; these errors\nincluded both syntax errors and Trio errors (Figure 14). Encountering more errors and independently\nresolving errors likely improved the formation of Trio skills.\n\u2022 Active Time: Using AI decreased the amount of active coding time. Time spent coding shifted to\ntime spent interacting with AI and understanding AI generations (Figure 16).\nUsing these axes, we develop a typology of six AI interaction patterns based on query types, number of queries,\nqueries per task, and active time. As a result of this categorization, these six patterns yield different outcomes\nfor both completion time and skill formation (i.e., quiz score). Figure 11 summarizes each pattern and the\naverage task outcomes. We can divide the interaction pattern into two categories: low- and high-scoring\ninteraction patterns; the high-scoring patterns generally involve more cognitive effort and less AI reliance.\nAlthough each behavior pattern cluster is"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_016",
    "source_id": "SkillFormation2026",
    "text": " skills.\n\u2022 Active Time: Using AI decreased the amount of active coding time. Time spent coding shifted to\ntime spent interacting with AI and understanding AI generations (Figure 16).\nUsing these axes, we develop a typology of six AI interaction patterns based on query types, number of queries,\nqueries per task, and active time. As a result of this categorization, these six patterns yield different outcomes\nfor both completion time and skill formation (i.e., quiz score). Figure 11 summarizes each pattern and the\naverage task outcomes. We can divide the interaction pattern into two categories: low- and high-scoring\ninteraction patterns; the high-scoring patterns generally involve more cognitive effort and less AI reliance.\nAlthough each behavior pattern cluster is small, the difference between low-scoring clusters and high-scoring\nclusters is stark.\nLow-Scoring Interaction Patterns Low-scoring patterns generally involved a heavy reliance on AI, either\nthrough code generation or debugging. The average quiz scores in these groups are less than 40%. Participants\n3The screen recording for one participant in the AI condition was not available.\n4Details on annotation procedures can be found in the Section B and the annotated transcripts can be found at https:\n//github.com/safety-research/how-ai-impacts-skill-formation\n12\nAI Delegation\nOnly asked AI to generate code \nand pasted code as answer. \nConceptual Inquiry\nOnly asked conceptual questions; \nindependent error resolution\nIterative AI Debugging\nRepeated AI-assisted troubleshooting \nor verification (5-15 queries)\nHybrid Code-Explanation\nQuestions that include both code \ngeneration and explanation\nGeneration-Then-Comprehension\nAI code generation followed by \nunderstanding focused queries\nProgressive AI Reliance\nAsked questions for task 1, relied \non AI for task 2\nQuiz Score\nCompletion Time\n24min\n68%\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nFigure 11: The 6 AI interaction personas in the treatment (AI) condition from our study with average\ncompletion times and quiz scores.\nexhibiting these interaction patterns showed less independent thinking and more cognitive offloading [Lee\net al., 2025].\n\u2022 AI Delegation (n=4): Participants in this group wholly relied on AI to write code and complete the\ntask. This group completed the task the fastest and encountered few or no errors in the process.\n\u2022 Progressive AI Reliance (n=4): Participants in this group started by asking 1 or 2 questions and\neventually delegated all code writing to the AI assistant. This group scored poorly on the quiz largely\ndue to not mastering any of the concepts in the second task.\n\u2022 Iterative AI Debugging (n=4): Participants in this group relied on AI to debug or verify their code.\nThis group made a higher number of queries to the AI assistant, but relied on the assistant to solve\nproblems, rather than clarifying their own understanding. As a result, they scored poorly on the quiz\nand were relatively slower at completing the two tasks.\nHigh-Scoring Interaction Patterns High-scoring interaction patterns were clusters of behaviors where\nthe average quiz score is 65% or higher. Participants in these clusters used AI both for code generation,\nconceptual queries or a combination of the two.\n\u2022 Generation-Then"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_017",
    "source_id": "SkillFormation2026",
    "text": " scored poorly on the quiz largely\ndue to not mastering any of the concepts in the second task.\n\u2022 Iterative AI Debugging (n=4): Participants in this group relied on AI to debug or verify their code.\nThis group made a higher number of queries to the AI assistant, but relied on the assistant to solve\nproblems, rather than clarifying their own understanding. As a result, they scored poorly on the quiz\nand were relatively slower at completing the two tasks.\nHigh-Scoring Interaction Patterns High-scoring interaction patterns were clusters of behaviors where\nthe average quiz score is 65% or higher. Participants in these clusters used AI both for code generation,\nconceptual queries or a combination of the two.\n\u2022 Generation-Then-Comprehension (n=2): Participants in this group first generated code and then\nmanually copied or pasted the code into their work. After their code was generated, they then asked\nthe AI assistant follow-up questions to improve understanding. These participants were not particularly\nfast when using AI, but demonstrated a high level of understanding on the quiz. Importantly, this\napproach looks nearly the same as the AI delegation group, but additionally uses AI to check their own\nunderstanding.\n\u2022 Hybrid Code-Explanation (n=3): Participants in this group composed hybrid queries in which they\nasked for code generation along with explanations of the generated code. Reading and understanding\nthe explanations they asked for took more time.\n13\n\u2022 Conceptual Inquiry (n=7): Participants in this group only asked conceptual questions and relied on\ntheir improved understanding to complete the task. Although this group encountered many errors, they\nalso independently resolved these errors. On average, this mode was the fastest among high-scoring\npatterns and second fastest overall after the AI Delegation mode.\n6.1\nAI Interaction\nInteraction Time\nContrary to previous work finding significant uplift or speedup of AI assistance for\ncoding [Peng et al., 2023, Cui et al., 2024], our results do not show a significant improvement in productivity\nif we only look at the total completion time across the treatment and control groups. By analyzing how\nparticipants in the AI condition completed the task, the reason for the lack of improved productivity was\ndue to the time spent interacting with the AI assistant. Some participants in the treatment group spent\nsignificant time (up to 11 minutes) interacting with the AI Assistant. For example, by typing or thinking\nabout what to type. We capture the time invested in interacting with AI by labeling the time between when\nusers start to type a query (annotated as an AI Interaction event) and when an answer from the AI assistant\nis produced (annotated as an AI Query event).\nSince participants could ask the AI assistant as many questions as time allowed, a handful of participants\nasked more than five questions and spent up to six minutes composing a single query during this 35-minute\nassignment (Figure 12).5 Since the median completion time is only 19 minutes in the AI condition, spending\nup to 6 minutes composing a single query amounts to a significant amount of the total time spent interacting\nwith the AI assistant. Although this effect might be due to the short duration of our task, Becker et al. also\nfound a slowdown effect for expert coders on longer tasks when participants waiting for AI-written code may\nbecome distracted.\nHowever"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_018",
    "source_id": "SkillFormation2026",
    "text": " AI assistant\nis produced (annotated as an AI Query event).\nSince participants could ask the AI assistant as many questions as time allowed, a handful of participants\nasked more than five questions and spent up to six minutes composing a single query during this 35-minute\nassignment (Figure 12).5 Since the median completion time is only 19 minutes in the AI condition, spending\nup to 6 minutes composing a single query amounts to a significant amount of the total time spent interacting\nwith the AI assistant. Although this effect might be due to the short duration of our task, Becker et al. also\nfound a slowdown effect for expert coders on longer tasks when participants waiting for AI-written code may\nbecome distracted.\nHowever, from the lens of skill formation, the time spent composing queries may aid in better understanding\nthe task and, consequently, better acquisition of skills. Screen recordings show participants contemplating\nwhat to ask the AI assistant (e.g., rereading instructions and rewriting queries). As a result, some participants\ntook several minutes to compose a single query. Thus, while this time cost would be more prominent in\nchat-based assistants than agentic coding assistants, the loss in knowledge is likely even greater in an agentic\nor autocomplete setting where composing queries is not required. A more significant difference in completion\ntime due to shorter interactions with AI assistance would likely translate to an even larger negative impact\non skill formation. When we look at individual queries, not all queries involve significant thinking and time.\nThus, we analyze individual queries to better understand how participants from new skills.\nAI Queries\nWe categorized user inputs into the AI assistant, queries, into 5 broad categories: explanation,\ngeneration, debugging, capabilities questions, and appreciation (Table 3). The most common type of query\nwas explanations (q=79); users requested more information about the trio library, details about asynchronous\noperations, and high-level conceptual introductions. 21 out of 25 participants in the treatment group asked\nan explanation question; this reflects the high level of engagement among our participants. The second most\ncommon were queries asking for code to be generated (q=51); some participants asked for an entire task to\nbe completed, while other participants asked for specific functions to be implemented. Only 16 of 25 or two\nthirds of the participants used AI to generate code. 4 of these participants only asked for code generation\nand no other types of question. In fact, 3 of the 8 lowest-scoring participants asked AI to generate code\nwithout asking for explanations, suggesting that if all participants in the AI group were to use AI for solely\ngenerating code, the skill-formation differences compared to the control group would be even greater.\nA third category of common queries was debugging (q=9). Our tasks were designed to be straightforward,\nbut the participants still encountered various errors (Section 6.2). This is a broader category of queries that\nincludes errors directly pasted as input to the AI assistant as well as asking the AI assistant to confirm the\ncode written is correct. A higher fraction of debugging queries correlates with slower completion times (Figure\n18) and lower quiz scores (Figure 19). This suggests that relying on AI for debugging (e.g. repetatedly asking\nAI to check and fix things without understanding) when learning a new task is correlated with less learning.\n5Participants were instructed to"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_019",
    "source_id": "SkillFormation2026",
    "text": "formation differences compared to the control group would be even greater.\nA third category of common queries was debugging (q=9). Our tasks were designed to be straightforward,\nbut the participants still encountered various errors (Section 6.2). This is a broader category of queries that\nincludes errors directly pasted as input to the AI assistant as well as asking the AI assistant to confirm the\ncode written is correct. A higher fraction of debugging queries correlates with slower completion times (Figure\n18) and lower quiz scores (Figure 19). This suggests that relying on AI for debugging (e.g. repetatedly asking\nAI to check and fix things without understanding) when learning a new task is correlated with less learning.\n5Participants were instructed to complete the task as fast as possible and were compensated a flat fee for participation. See\nSection 4.3.\n14\n2\n4\n6\n8\n10\nTotal AI Interaction Time (minutes)\n0\n1\n2\n3\n4\n5\n6\n7\nCount\nDistribution of Total AI Interaction Time\n2\n4\n6\n8\n10\n12\n14\nNumber of AI Queries\n0\n1\n2\n3\n4\n5\n6\nCount\nDistribution of Number of AI Queries\nFigure 12: Distribution of total AI interaction time and number of AI queries. Participants spending more\nthan 6 minutes interacting with AI during the task contribute to the treatment group (AI Assistance) not\nbeing significantly faster than the control group (No AI).\nAlthough we only recruited participants who have used AI assistants before, there were still questions (q=4)\nabout whether the assistant could see the existing code and whether the assistant had knowledge of the\nspecific library. In response to these questions, the AI assistance clarified that they could see the code\nand instructions. Several participants also expressed their appreciation for the assistant after the task was\ncompleted correctly. These expressions of appreciation, even at the cost of additional task time, reflect that\npoliteness in the human-AI interaction [Druga et al., 2017, Ribino, 2023] also appears in the context of AI for\ncoding assistance.\nAdopting AI Advice: Pasting vs Manual Code Copying\nAnother pattern that differs between\nparticipants is that some participants directly paste AI-written code, while other participants manually typed\nin (i.e., copied) the the AI generated code into their own file. The differences in this AI adoption style\ncorrelate with completion time. In Figure 13, we isolate the task completion time and compare how the\nmethod of AI adoption affects task completion time and quiz score. Participants in the AI group who directly\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_020",
    "source_id": "SkillFormation2026",
    "text": "\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\ndirectly pasted AI output. This suggests that spending more time manually typing may not yield better\nconceptual understanding. Cognitive effort may be more important than the raw time spent on completing\nthe task.\n6.2\nEncountering Errors\nThe way participants encountered and resolved errors was notably different between the treatment and control\nconditions. In the platform, participants could use the run button or the terminal to run their code as often\nas they wanted. In general, most of the participants ran the code for the first time after trying to complete\nmost of the question and ran the code again only after the changes were made. We recorded every error\nencountered by each participant as we watched the screen recordings of the task progress.\n15\nQuery Type\nExample Query\nExplanation (q=79)\n\u201ccan trio.sleep use partial seconds?\u201d\n\u201cCan you remind me what the different trio async operations are?\u201d\n\u201cLooks good, can you give me a really brief overview of the general idea behind\nall of this?\u201d\nGeneration (q=51)\n\u201cgiven this instruction to trio, can you implement the missing bits of main.py?\u201d\n\u201ccomplete get_user_data\u201d\n\u201cimplement delayed_hello(). It should simply sleep for 2.1 seconds upon which\nit prints \u2019Hello World!\u2019 \u201d\nDebugging (q=9)\n\u201cDoes that look right? If so let\u2019s move on to delayed_hello()\u201d\n\u201cI\u2019m having issues getting my code to work. I\u2019m getting a notimplementederror\nfor delayed_hello\u201d\nPasted\nError\n(e.g.,\n\u201cTraceback\n(most\nrecent\ncall\nlast):\nFile\n\"/user-\ncode/FILESYSTEM/main.py3\", line 81, in... \u201d)\nCapabilities\nQues-\ntion (q=4)\n\u201cCan you see the current question?\u201d\n\u201cSo what can you do for me here? Can you write code directly into the file?\u201d\n\u201cAre you aware of how trio works? Are there parallels in its execution model to\nanother library I\u2019d be more familiar with like asyncio\u201d\nAppreciation (q=4)\n\u201cGreat job, we got the expected output on the first try.\u201d\n\u201cLooks like it worked, thanks!\u201d\n\u201cTrueeee!\u201d\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI ("
  },
  {
    "chunk_id": "SkillFormation2026_chunk_021",
    "source_id": "SkillFormation2026",
    "text": "? Are there parallels in its execution model to\nanother library I\u2019d be more familiar with like asyncio\u201d\nAppreciation (q=4)\n\u201cGreat job, we got the expected output on the first try.\u201d\n\u201cLooks like it worked, thanks!\u201d\n\u201cTrueeee!\u201d\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\nQuiz Score\nQuiz Score\ncondition\nNo AI\nAI\nPaste Behavior vs Task Time and Quiz Score\nFigure 13: Decomposing AI Coding Behavior: Participants using AI by directly pasting outputs experience\nthe most significant speed ups while participants who manually copied the AI-generated output were similar\nin pace to the control (No AI) group.\n16\nAI\nNo AI\nTotal\n1.0 (0.0-3.0)\n3.0 (2.0-5.0)\nTask 1\n0.0 (0.0-2.0)\n2.0 (0.5-3.0)\nTask 2\n0.0 (0.0-1.0)\n2.0 (0.5-2.0)\nTable 4: Number of errors encountered per participant by condition. Values are median (Q1\u2013Q3).\nError Frequency\nThe AI group encountered fewer errors than the control group: the median participant\nin the treatment group encountered only one error in the entire task, while the median for the control group\nwas three errors. Table 4 shows the difference in the error distributions. Most of the participants in the AI\ngroup were able to complete the tasks the first time they ran their code. In contrast, in the control condition,\nmost of the participants encountered several errors in the process of completing each task. Among the 12\nparticipants who completed both tasks without encountering errors, only two were in the control group.\nNameError\nAttributeError\nSyntaxError\nRuntimeWarning\nTypeError\nModuleNotFoundError\nIndentationError\nNotImplementedError\nUnboundLocalError\nValueError\nRuntimeError\nExecution limit exceeded\nError Group\n0\n5\n10\n15\n20\n25\nCount\nError Types Encountered By Participants\nError Group\nSyntax Error\nTrio Error\nFigure 14: Count of all errors\nencountered by participants by\nerror type.\nAttributeError\nSyntaxError\nNameError\nTypeError\nRuntimeWarning\nModuleNotFoundError\nIndentationError\nNotImplementedError\nUnboundLocalError\nValueError\nRuntimeError\nExecution limit exceeded\nError Type\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nCount\ncondition = No AI\nAttributeError\nSyntaxError\nNameError\nTypeError\nRuntimeWarning\nModuleNotFoundError\nIndentationError\nNotImplementedError\nUnboundLocalError\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_022",
    "source_id": "SkillFormation2026",
    "text": "\nError Types Encountered By Participants\nError Group\nSyntax Error\nTrio Error\nFigure 14: Count of all errors\nencountered by participants by\nerror type.\nAttributeError\nSyntaxError\nNameError\nTypeError\nRuntimeWarning\nModuleNotFoundError\nIndentationError\nNotImplementedError\nUnboundLocalError\nValueError\nRuntimeError\nExecution limit exceeded\nError Type\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\nCount\ncondition = No AI\nAttributeError\nSyntaxError\nNameError\nTypeError\nRuntimeWarning\nModuleNotFoundError\nIndentationError\nNotImplementedError\nUnboundLocalError\nValueError\nRuntimeError\nExecution limit exceeded\nError Type\ncondition = AI\nError Group\nSyntax Error\nTrio Error\nFigure 15: Count of errors by participant condition: AI (treatment)\nand No AI (control). The control group encountered many more errors\nrelated to key Trio concepts (e.g., TypeError and RuntimeWarning).\nErrors and Trio Skill\nNot all errors carry the same weight in skill development in our study. Certain errors\nrequire a deeper understanding of the Trio library, which may account for differences in learning outcomes.\nFigure 14 shows that the most common errors are not directly related to the Trio library: NameError and\nAttributeError are typically typos made on variable names or function names that are quickly corrected.\nOther errors are directly related to Trio: RuntimeWarning appears when a coroutine was never awaited and\nTypeError appears when a trio function gets a coroutine object instead of an async function. These errors\nforce an understanding of key concepts on how the trio library handles corountines and the usage of await\nkeywords that are tested in the evaluation. Although participants in the AI condition also encounter errors\n(Figure 15), there are much fewer Trio-related errors encountered.\nFor participants in the control group, the higher frequency of encountering errors leads to more critical thinking\nabout what is happening with the code and how to used the new library being presented. Furthermore,\nthe frequent appearance of errors specifically related to the Trio library ensures that these specific concepts\nare gained in the process of completing the task. Together, these two differences suggest that encountering\nerrors may play a significant role in the formation of coding skills. Moreover, our original motivation for the\nimportance of preserving debugging skills may hinge on the acquisition of these skills without relying on AI.\n17\n2\n4\n6\n8\n10\n12\n14\nActive Time (Minutes)\n5\n10\n15\n20\n25\nQuiz Score\nCoding Activity: Active Time vs Quiz Score\ncondition\nNo AI\nAI\nexperience\n4-6 years\n1-3 years\n7+ years\nFigure 16: Active coding time vs. quiz score: active coding time represents the amount of time actually spent\ncoding and is often a very small fraction of total task time. The No AI condition participants spent more\nactive time coding and achieved higher quiz scores.\n6.3\nShifts in Active Coding Time\nAlthough the outcome we measure in our main analysis is productivity through total task time, the actual\namount to time spent actively coding illustrates a clearer pattern. Figure 16 shows that participants in the\nAI condition spent much less active time on the task. This shift from coding to reading and understanding\nhas also been found in"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_023",
    "source_id": "SkillFormation2026",
    "text": "\nNo AI\nAI\nexperience\n4-6 years\n1-3 years\n7+ years\nFigure 16: Active coding time vs. quiz score: active coding time represents the amount of time actually spent\ncoding and is often a very small fraction of total task time. The No AI condition participants spent more\nactive time coding and achieved higher quiz scores.\n6.3\nShifts in Active Coding Time\nAlthough the outcome we measure in our main analysis is productivity through total task time, the actual\namount to time spent actively coding illustrates a clearer pattern. Figure 16 shows that participants in the\nAI condition spent much less active time on the task. This shift from coding to reading and understanding\nhas also been found in previous work [Becker et al., 2025]. When we look at quiz score, the control group\nachieves high quiz scores with a higher total active time without the use of AI. Within each condition, higher\nactive time correlates with lower quiz score, this is because the more experienced programmers spend less\ntime actively coding while having better base knowledge compared to novice programmers.\n6.4\nParticipant Feedback\nA quarter of the participants left feedback after the task and quiz were completed. In the control group (No\nAI), participants remarked that they found the task fun and that the tasks instructions were good at helping\ndevelop an understanding of Trio. In the treatment group (AI Assistance), participants remarked that they\nwished they had paid more attention to the details of the Trio library during the task, either by reading\nthe generated code or by generating explanations in more depth. Specifically, participants reported feeling\n\u2018lazy\u2019 and that \u2018there are still a lot of gaps in (their) understanding\u2019. The sentiment of participants\u2019 feedback\nsuggested a more positive experience among the control group even though the task instructions and quiz\nquestions were identical across groups (Table 6 and Table 7 provide all of the participant feedback from all\nparticipants).\n7\nDiscussion\nOur main finding is that using AI to complete tasks that require a new skill (i.e., knowledge of a new Python\nlibrary) reduces skill formation. In a randomized controlled trial, participants were assigned to the treatment\ncondition (using an AI assistant, web search, and instructions) or the control condition (completing tasks with\nweb search and instructions alone). The erosion of conceptual understanding, code reading, and debugging\nskills that we measured among participants using AI assistance suggests that workers acquiring new skills\nshould be mindful of their reliance on AI during the learning process. Among participants who use AI, we\nfind a stark divide in skill formation outcomes between high-scoring interaction patterns (65%-86% quiz\n18\nscore) vs low-scoring interaction patterns (24%-39% quiz score). The high scorers only asked AI conceptual\nquestions instead of code generation or asked for explanations to accompany generated code; these usage\npatterns demonstrate a high level of cognitive engagement.\nContrary to our initial hypothesis, we did not observe a significant performance boost in task completion\nin our main study. While using AI improved the average completion time of the task, the improvement in\nefficiency was not significant in our study, despite the AI Assistant being able to generate the complete code\nsolution when prompted. Our qualitative analysis reveals that our finding is largely due to the heterogeneity\nin how participants decide to use AI during the task. There is a group of participants"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_024",
    "source_id": "SkillFormation2026",
    "text": "18\nscore) vs low-scoring interaction patterns (24%-39% quiz score). The high scorers only asked AI conceptual\nquestions instead of code generation or asked for explanations to accompany generated code; these usage\npatterns demonstrate a high level of cognitive engagement.\nContrary to our initial hypothesis, we did not observe a significant performance boost in task completion\nin our main study. While using AI improved the average completion time of the task, the improvement in\nefficiency was not significant in our study, despite the AI Assistant being able to generate the complete code\nsolution when prompted. Our qualitative analysis reveals that our finding is largely due to the heterogeneity\nin how participants decide to use AI during the task. There is a group of participants who relied on AI to\ngenerate all the code and never asked conceptual questions or for explanations. This group finished much\nfaster than the control group (19.5 minutes vs 23 minutes), but this group only accounted for around 20%\nof the participants in the treatment group. Other participants in the AI group who asked a large number\nof queries (e.g., 15 queries), spent a long time composing queries (e.g., 10 minutes), or asked for follow-up\nexplanations, raised the average task completion time. These contrasting patterns of AI usage suggest that\naccomplishing a task with new knowledge or skills does not necessarily lead to the same productive gains as\ntasks that require only existing knowledge.\nTogether, our results suggest that the aggressive incorporation of AI into the workplace can have negative\nimpacts on the professional development workers if they do not remain cognitatively engaged. Given time\nconstraints and organizational pressures, junior developers or other professionals may rely on AI to complete\ntasks as fast as possible at the cost of real skill development. Furthermore, we found that the biggest difference\nin test scores is between the debugging questions. This suggests that as companies transition to more AI\ncode writing with human supervision, humans may not possess the necessary skills to validate and debug\nAI-written code if their skill formation was inhibited by using AI in the first place.\n7.1\nFuture Work\nOur work is a first step to understanding the impact of AI assistance on humans in the human-AI collaboration\nprocess. We hope that this work will motivate future work that addresses the following limitations:\n\u2022 Task Selection: This study focuses on a single task using a chat-based interface. This should be\na lower bound for cognitive offloading since agentic AI coding tools would require even less human\nparticipation. In our work, users who relied on AI without thinking performed the worst on the\nevaluation; a completely agentic tool would create a similar effect. Future work should investigate the\nimpacts of agentic coding tools on learning outcomes and skill development.\n\u2022 Task Length: Ideally, skill formation takes place over months to years. We measured skill formation for\na specific Python library over a one-hour period. Future work should study real-world skill development\nthrough longitudinal measurement of the impacts of AI adoption.\n\u2022 Participant Realism: While participants in our study were professional or freelance programmers,\nthere was not the same incentive to learn the library as if it were required for their actual job. Future\nstudies should aim at studying the skill acquisition fro novice workers within a real company.\n\u2022 Prompting Skills: We collect self-reported familiarity with AI coding tools, but we do not actually\nmeasure differences in prompting techniques."
  },
  {
    "chunk_id": "SkillFormation2026_chunk_025",
    "source_id": "SkillFormation2026",
    "text": " should investigate the\nimpacts of agentic coding tools on learning outcomes and skill development.\n\u2022 Task Length: Ideally, skill formation takes place over months to years. We measured skill formation for\na specific Python library over a one-hour period. Future work should study real-world skill development\nthrough longitudinal measurement of the impacts of AI adoption.\n\u2022 Participant Realism: While participants in our study were professional or freelance programmers,\nthere was not the same incentive to learn the library as if it were required for their actual job. Future\nstudies should aim at studying the skill acquisition fro novice workers within a real company.\n\u2022 Prompting Skills: We collect self-reported familiarity with AI coding tools, but we do not actually\nmeasure differences in prompting techniques. An extension to our would also involve testing the level of\nprompting fluency beyond self-report.\n\u2022 Evaluation Design: Our study measures skill formation through a comprehensive quiz. Other studies\ncould use the completion of another task or design coding as alternative evaluation strategies.\n\u2022 Human Assistance: We do not include the counterfactual of how skill formation would be impacted by\nreceiving assistance from humans. Since human assistance and feedback takes place in a diverse settings\n(e.g., classroom, pair programming, code review), future work can compare the effect of feedback from\nAI vs humans in all these settings on skill formation.\nFor novice workers in software engineering or any other industry, our study can be viewed as a small piece\nof evidence toward the value of intentional skill development despite the ubiquity of AI tools. Our study\ndemonstrates the benefits of deploying cognitive effort when encountering a learning opportunity to master a\n19\nnew tool even if barriers (e.g., errors) may be encountered in the process of mastery. Exerting cognitive effort\ncan be assisted by AI; beyond the patterns we describe, major LLM services also provide learning modes\n(e.g., ChatGPT Study Mode, Claude Code Learning / Explanatory mode). Ultimately, to accommodate\nskill development in the presence of AI, there needs to be a more expansive view of the impacts of AI on\nworkers. Participants in the new AI economy must care not only about productivity gains from AI but also\nthe long-term sustainability of expertise development amid the proliferation of new AI tools.\n8\nAcknowledgments\nWe would like to thank Ethan Perez, Miranda Zhang, and Henry Sleight for making this project possible\nthrough the Anthropic Safety Fellows Program. We would also like to thank Matthew J\u00f6rke, Juliette Woodrow,\nSarah Wu, Elizabeth Childs, Roshni Sahoo, Nate Rush, Julian Michael, and Rose Wang for experimental\ndesign feedback.\nWe like to thank Jeffrey Shen, Aram Ebtekar, Minh Le, Mateusz Piotrowski, Nate Rahn, Miles McCain,\nJessica Zhu, Alex Wang, John Hewitt, Rosanne Hu, Saffron Huang, Kyle Hsu, Sanjana Srivastava and Jennifer\nLeung for task testing and feedback.\n20\nReferences\nDavid Autor, Frank Levy, and Richard Murnane. The skill content of recent technological change: an\nempirical exploration, 2001.\nJoel Becker, Nate Rush, Elizabeth Barnes, and David Rein. Measuring the impact of early-2025 ai on\nexperienced open-source developer productivity. arXiv preprint arXiv:2507.09089, 2025.\nH"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_026",
    "source_id": "SkillFormation2026",
    "text": "am Ebtekar, Minh Le, Mateusz Piotrowski, Nate Rahn, Miles McCain,\nJessica Zhu, Alex Wang, John Hewitt, Rosanne Hu, Saffron Huang, Kyle Hsu, Sanjana Srivastava and Jennifer\nLeung for task testing and feedback.\n20\nReferences\nDavid Autor, Frank Levy, and Richard Murnane. The skill content of recent technological change: an\nempirical exploration, 2001.\nJoel Becker, Nate Rush, Elizabeth Barnes, and David Rein. Measuring the impact of early-2025 ai on\nexperienced open-source developer productivity. arXiv preprint arXiv:2507.09089, 2025.\nHannah Bleher and Matthias Braun. Diffused responsibility: attributions of responsibility in the use of\nai-driven clinical decision support systems. AI and Ethics, 2(4):747\u2013761, 2022.\nSamuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e,\nAmanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language\nmodels. arXiv preprint arXiv:2211.03540, 2022.\nErik Brynjolfsson, Danielle Li, and Lindsey Raymond. Generative ai at work. The Quarterly Journal of\nEconomics, page qjae044, 2025.\nZana Bu\u00e7inca, Maja Barbara Malaya, and Krzysztof Z Gajos. To trust or to think: cognitive forcing functions\ncan reduce overreliance on ai in ai-assisted decision-making. Proceedings of the ACM on Human-computer\nInteraction, 5(CSCW1):1\u201321, 2021.\nQingwan Cheng, Angela Tao, Huangliang Chen, and Maira Marques Samary. Design an assessment for an\nintroductory computer science course: A systematic literature review. In 2022 IEEE frontiers in education\nconference (FIE), pages 1\u20138. IEEE, 2022.\nJonathan H Choi and Daniel Schwarcz. Ai assistance in legal analysis: An empirical study. 2023.\nZheyuan Kevin Cui, Mert Demirer, Sonia Jaffe, Leon Musolff, Sida Peng, and Tobias Salz. The effects\nof generative ai on high skilled work: Evidence from three field experiments with software developers.\nAvailable at SSRN 4945566, 2024.\nFabrizio Dell\u2019Acqua, Edward McFowland III, Ethan R Mollick, Hila Lifshitz-Assaf, Katherine Kellogg, Saran\nRajendran, Lisa Krayer, Fran\u00e7ois Candelon, and Karim R Lakhani. Navigating the jagged technological\nfrontier: Field experimental evidence of the effects of ai on knowledge worker productivity and quality.\nHarvard Business School Technology & Operations Mgt. Unit Working Paper, (24-013), 2023.\nStefania Druga, Randi Williams, Cynthia Breazeal, and Mitchel Resnick. \" hey google is it ok if i eat you?\"\ninitial explorations in child-agent interaction. In Proceedings of the 2017 conference on"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_027",
    "source_id": "SkillFormation2026",
    "text": "Acqua, Edward McFowland III, Ethan R Mollick, Hila Lifshitz-Assaf, Katherine Kellogg, Saran\nRajendran, Lisa Krayer, Fran\u00e7ois Candelon, and Karim R Lakhani. Navigating the jagged technological\nfrontier: Field experimental evidence of the effects of ai on knowledge worker productivity and quality.\nHarvard Business School Technology & Operations Mgt. Unit Working Paper, (24-013), 2023.\nStefania Druga, Randi Williams, Cynthia Breazeal, and Mitchel Resnick. \" hey google is it ok if i eat you?\"\ninitial explorations in child-agent interaction. In Proceedings of the 2017 conference on interaction design\nand children, pages 595\u2013600, 2017.\nMichael Gerlich. Ai tools in society: Impacts on cognitive offloading and the future of critical thinking.\nSocieties, 15(1):6, 2025.\nJavier Gonzalez-Huerta, Jefferson Seide Moll\u00e9ri, Aivars \u0160ablis, and Ehsan Zabardast. Experiential learning\napproach for software engineering courses at higher education level. arXiv preprint arXiv:2012.14178, 2020.\nKunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Durmus, Sarah Heck, Jared Mueller, Jerry\nHong, Stuart Ritchie, Tim Belonax, et al. Which economic tasks are performed with ai? evidence from\nmillions of claude conversations. arXiv preprint arXiv:2503.04761, 2025.\nSuhas Kannam, Yuri Yang, Aarya Dharm, and Kevin Lin. Code interviews: Design and evaluation of a\nmore authentic assessment for introductory programming assignments. In Proceedings of the 56th ACM\nTechnical Symposium on Computer Science Education V. 1, pages 554\u2013560, 2025.\nZachary Kenton, Noah Siegel, J\u00e1nos Kram\u00e1r, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh\nAgarwal, David Lindner, Yunhao Tang, Noah Goodman, et al. On scalable oversight with weak llms\njudging strong llms. Advances in Neural Information Processing Systems, 37:75229\u201375276, 2024.\nArtur Klingbeil, Cassandra Gr\u00fctzner, and Philipp Schreck. Trust and reliance on ai\u2014an experimental study\non the extent and costs of overreliance on ai. Computers in Human Behavior, 160:108352, 2024.\nDavid A Kolb. Experiential learning: Experience as the source of learning and development. FT press, 2014.\n21\nHao-Ping Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson.\nThe impact of generative ai on critical thinking: Self-reported reductions in cognitive effort and confidence\neffects from a survey of knowledge workers. In Proceedings of the 2025 CHI Conference on Human Factors\nin Computing Systems, pages 1\u201322, 2025.\nJack B Longwell, Ian Hirsch, Fernando Binder, Galileo Arturo Gonzalez Conchas, Daniel Mau, Raymond\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_028",
    "source_id": "SkillFormation2026",
    "text": "108352, 2024.\nDavid A Kolb. Experiential learning: Experience as the source of learning and development. FT press, 2014.\n21\nHao-Ping Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson.\nThe impact of generative ai on critical thinking: Self-reported reductions in cognitive effort and confidence\neffects from a survey of knowledge workers. In Proceedings of the 2025 CHI Conference on Human Factors\nin Computing Systems, pages 1\u201322, 2025.\nJack B Longwell, Ian Hirsch, Fernando Binder, Galileo Arturo Gonzalez Conchas, Daniel Mau, Raymond\nJang, Rahul G Krishnan, and Robert C Grant. Performance of large language models on medical oncology\nexamination questions. JAMA Network Open, 7(6):e2417641\u2013e2417641, 2024.\nBrooke N Macnamara, Ibrahim Berber, M Cenk \u00c7avu\u015fo\u011flu, Elizabeth A Krupinski, Naren Nallapareddy,\nNoelle E Nelson, Philip J Smith, Amy L Wilson-Delfosse, and Soumya Ray. Does using artificial intelligence\nassistance accelerate skill decay and hinder skill development without performers\u2019 awareness? Cognitive\nResearch: Principles and Implications, 9(1):46, 2024.\nNegar Maleki, Balaji Padmanabhan, and Kaushik Dutta. Ai hallucinations: a misnomer worth clarifying. In\n2024 IEEE conference on artificial intelligence (CAI), pages 133\u2013138. IEEE, 2024.\nShakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of generative artificial\nintelligence. Science, 381(6654):187\u2013192, 2023.\nNicholas Otis, Rowan Clarke, Solene Delecourt, David Holtz, and Rembrand Koning. The uneven impact of\ngenerative ai on entrepreneurial performance. 2024.\nZelin Pan, Zhendong Xie, Tingting Liu, and Tiansheng Xia. Exploring the key factors influencing college\nstudents\u2019 willingness to use ai coding assistant tools: An expanded technology acceptance model. Systems,\n12(5):176, 2024.\nSida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer productivity:\nEvidence from github copilot. arXiv preprint arXiv:2302.06590, 2023.\nGustavo Pinto, Cleidson De Souza, Thayssa Rocha, Igor Steinmacher, Alberto Souza, and Edward Monteiro.\nDeveloper experiences with a contextualized ai coding assistant: Usability, expectations, and outcomes. In\nProceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for\nAI, pages 81\u201391, 2024.\nEric Poitras, Brent Crane, and Angela Siegel. Generative ai in introductory programming instruction:\nExamining the assistance dilemma with llm-based code generators. In Proceedings of the 2024 on ACM\nVirtual Global Computing Education Conference V. 1, pages 186\u2013192, 2024.\nSiddhar"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_029",
    "source_id": "SkillFormation2026",
    "text": "avo Pinto, Cleidson De Souza, Thayssa Rocha, Igor Steinmacher, Alberto Souza, and Edward Monteiro.\nDeveloper experiences with a contextualized ai coding assistant: Usability, expectations, and outcomes. In\nProceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for\nAI, pages 81\u201391, 2024.\nEric Poitras, Brent Crane, and Angela Siegel. Generative ai in introductory programming instruction:\nExamining the assistance dilemma with llm-based code generators. In Proceedings of the 2024 on ACM\nVirtual Global Computing Education Conference V. 1, pages 186\u2013192, 2024.\nSiddhartha Prasad, Ben Greenman, Tim Nelson, and Shriram Krishnamurthi. Generating programs trivially:\nstudent use of large language models. In Proceedings of the ACM Conference on Global Computing\nEducation Vol 1, pages 126\u2013132, 2023.\nOmer Reingold, Judy Hanwen Shen, and Aditi Talati. Dissenting explanations: leveraging disagreement to\nreduce model overreliance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,\npages 21537\u201321544, 2024.\nPatrizia Ribino. The role of politeness in human\u2013machine interactions: a systematic literature review and\nfuture perspectives. Artificial Intelligence Review, 56(Suppl 1):445\u2013482, 2023.\nHenk G Schmidt. Problem-based learning: An introduction. Instructional science, pages 247\u2013250, 1994.\nJudy Hanwen Shen and Carlos Guestrin.\nSocietal impacts research requires benchmarks for creative\ncomposition tasks. arXiv preprint arXiv:2504.06549, 2025.\nAlex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi, Saffron Huang, Alfred\nMountfield, Jerry Hong, Stuart Ritchie, et al. Clio: Privacy-preserving insights into real-world ai use. arXiv\npreprint arXiv:2412.13678, 2024.\nHelena Vasconcelos, Matthew J\u00f6rke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S Bernstein,\nand Ranjay Krishna. Explanations can reduce overreliance on ai systems during decision-making. Proceedings\nof the ACM on Human-Computer Interaction, 7(CSCW1):1\u201338, 2023.\n22\nDakuo Wang, Elizabeth Churchill, Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and\nQianying Wang. From human-human collaboration to human-ai collaboration: Designing ai systems that\ncan work together with people. In Extended abstracts of the 2020 CHI conference on human factors in\ncomputing systems, pages 1\u20136, 2020.\nWei Wang, Huilong Ning, Gaowei Zhang, Libo Liu, and Yi Wang. Rocks coding, not development: A\nhuman-centric, experimental evaluation of llm-supported se tasks. Proceedings of the ACM on Software\nEngineering, 1(FSE):699\u2013721, 2024.\nEmma Wiles, Lisa Krayer"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_030",
    "source_id": "SkillFormation2026",
    "text": " Pattie Maes, Xiangmin Fan, Ben Shneiderman, Yuanchun Shi, and\nQianying Wang. From human-human collaboration to human-ai collaboration: Designing ai systems that\ncan work together with people. In Extended abstracts of the 2020 CHI conference on human factors in\ncomputing systems, pages 1\u20136, 2020.\nWei Wang, Huilong Ning, Gaowei Zhang, Libo Liu, and Yi Wang. Rocks coding, not development: A\nhuman-centric, experimental evaluation of llm-supported se tasks. Proceedings of the ACM on Software\nEngineering, 1(FSE):699\u2013721, 2024.\nEmma Wiles, Lisa Krayer, Mohamed Abbadi, Urvi Awasthi, Ryan Kennedy, Pamela Mishkin, Daniel Sack,\nand Fran\u00e7ois Candelon. Genai as an exoskeleton: Experimental evidence on knowledge workers using genai\non new skills. Available at SSRN 4944588, 2024.\nSuqing Wu, Yukun Liu, Mengqi Ruan, Siyu Chen, and Xiao-Yun Xie. Human-generative ai collaboration\nenhances task performance but undermines human\u2019s intrinsic motivation. Scientific Reports, 15(1):15105,\n2025.\n23\nFreelance/contract work\nPersonal projects and hobbies\nAt work as a professional developer\nIn school/university courses\nAcademic research\nCoding Context\nCoding Context Distribution (Multi-Select) haha\n22-25\n26-30\n31-35\n35-45\n45+\nAge\nAge Distribution\n0\n20\n40\n60\n80\nPercentage %\nAssociate's degree\nBachelor's degree\nDoctoral degree\nMaster's degree\nProfessional degree (JD, MD, etc.)\nSome college (no degree)\nEducation\nEducation Distribution\n0\n20\n40\n60\n80\nPercentage %\nNo\nYes - Advanced Degree\nYes - College\nYes - Other\nStudent Status\nStudent Status Distribution\nFigure 17: Participant distribution for main study, collected after the task to avoid stereotype threat. Most\nparticipants are professional programmers.\nA\nParticipant Details\nA.1\nEthics Review\nThe protocol was reviewed and approved by internal reviewers at Anthropic. Participants were not exposed\nto any risks during this study. The benefits which may reasonably be expected to result from this study\nare learning a new software (Python) skill. We did not guarantee or promise that participants will receive\nany specific learning benefits from this study. We collected informed consent for participation in the study\nduring the prescreening stage. We gave participants the right to withdraw consent at any time without\npenalty. They will still be compensated even if they fail the attention checks, do not fully complete the task,\nor complete the task incorrectly. Quiz responses are stored in Google Drive, and coding keystrokes are stored\nin the coding Platform. All stored information is completely anonymized; only the data platform can use the\nIDs to identify the participants for payment. We further remove data platform identifiers to annotate coding\npatterns between the two groups.\nB\nQualitative Analysis Data and Details\nB.1\nAnnotation Procedure\n51/52 participants uploaded screen recordings of their work in the warm-up task, main coding task, and quiz.\nWe watched the recordings of for all participants (25 AI condition, 25 no AI condition) for"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_031",
    "source_id": "SkillFormation2026",
    "text": " without\npenalty. They will still be compensated even if they fail the attention checks, do not fully complete the task,\nor complete the task incorrectly. Quiz responses are stored in Google Drive, and coding keystrokes are stored\nin the coding Platform. All stored information is completely anonymized; only the data platform can use the\nIDs to identify the participants for payment. We further remove data platform identifiers to annotate coding\npatterns between the two groups.\nB\nQualitative Analysis Data and Details\nB.1\nAnnotation Procedure\n51/52 participants uploaded screen recordings of their work in the warm-up task, main coding task, and quiz.\nWe watched the recordings of for all participants (25 AI condition, 25 no AI condition) for the main coding\ntask. We record the time stamps of the following events:\nWe also note general themes in how participants use AI in each condition based on these codes of events in\nthe event.\nB.2\nData Availability\nWe make the annotated transcripts of each participant available at the following URL: https://github.\ncom/safety-research/how-ai-impacts-skill-formation.\n24\nEvent\nDescription\nAdditional Info\nTask Start\nUser opens each task\nAI Interaction\nUser starts typing into AI window\nDescription of interaction\nAI Query\nUser receives answer from AI\nQuery\nWebsearch\nUser queries search engine\nQuery\nPaste (Direct)\nUser pastes output of AI assistant\nCode Copying\nUser types code using AI output\nError\nCode produces error when run\nError Message\nInterface Error\nDevelopment environment or AI assistant error\nTask Completion\nCorrect output is achieved\nTask Submission\nUser submits task\nCode completion\nTable 5: Events annotated manually for each video recording of the main task.\nB.3\nParticipant Feedback Details\nWe include all participant feedback in Table 6 and 7. Since the average completion time was faster for the AI\ngroup, the AI group left more comments since they felt like they had more time at the end of the task.\nC\nEvaluation Details\nC.1\nEvaluation Design\nQuestion Types We discuss the three types of questions we used: Conceptual Understanding, Code Reading,\nand Debugging in Section 4.2.\nKnowledge Categories The evaluation covers 7 core concepts from the Trio library:\n1. Async and await keywords: When to use await keywords within async functions. For example:\n\u201cWhat happens when the await keyword is used in an async function?\u201d\n2. Starting Trio functions: Basic Trio usage including how to spawn tasks and how spawned tasks with\ndifferent durations behave.\n3. Error handling in Trio: Understanding error propagation patterns and how to catch errors in child\ntasks. For example \u201c What happens to a parent task when a child task raises an unhandled exception\nin Trio?\u201d\n4. Coroutines: When calling async functions, how to debug co-routine never awaited errors.\n5. Memory channels using Trio: Understanding that start_soon doesn\u2019t return anything and how\nto use dictionaries, lists and other memory channels to collect data when running multiple tasks in\nparallel.\n6. Opening and closing a Trio nursery: Understanding asynchronous context managers and how\nto use them. For example, a debugging question consisting of a snippet of code where the nursery is\nstarted in correctly.\n7. Sequential vs concurrent execution: The expected behavior of concurrent tasks. For example\n\u201cRead the following code and identify when each task starts"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_032",
    "source_id": "SkillFormation2026",
    "text": " example \u201c What happens to a parent task when a child task raises an unhandled exception\nin Trio?\u201d\n4. Coroutines: When calling async functions, how to debug co-routine never awaited errors.\n5. Memory channels using Trio: Understanding that start_soon doesn\u2019t return anything and how\nto use dictionaries, lists and other memory channels to collect data when running multiple tasks in\nparallel.\n6. Opening and closing a Trio nursery: Understanding asynchronous context managers and how\nto use them. For example, a debugging question consisting of a snippet of code where the nursery is\nstarted in correctly.\n7. Sequential vs concurrent execution: The expected behavior of concurrent tasks. For example\n\u201cRead the following code and identify when each task starts and completes\u201d\n25\nCondition\nFeedback\nAI\nThe first quiz (printing the *s) would have been much easier if I could\njust look up the syntax of appending to an array. I forgot it when doing\nthe task - and normally, that\u2019d take five seconds to google and would\nrequire no model assistance, but since I couldn\u2019t leave the tab, I got\nstuck.\nAI\nBy using the AI assistant, I feel like I got lazy. I didn\u2019t read the Trio\nlibrary intro and code examples as closely as I would have otherwise.\nAI\nVery cool project, hope it goes well!\nAI\nI would not have minded going into a little more depth with the assistant\nto really understand and prove out the details of the trio library. I feel\nlike I got a pretty good overview but there are still a lot of gaps in my\nunderstanding.\nAI\nI feel stupid from the warmup, but hopefully the other project demon-\nstrated what I can do. Sorry if I wasted your time.\nAI\nIt was fun and challenging. The warm up was confusing because it seemed\nthe task had some issues but overall the entire thing was a fun learning\nexperience.\nAI\nI wish I\u2019d taken the time to understand the explanations from Cosmo a\nbit more!\nAI\nI\u2019m slow. I think the time limit made me act in a way that wasn\u2019t\nrepresentative to my normal workflow, particularly in the proportion of\ntime spent building mental models vs. obtaining code progressions. I\nhad the desire to understand trio a lot more than I allowed myself in\nthe moment, because I knew I wouldn\u2019t have the time. I also think my\nattention was scattered as I tried to operate in a rush. Trio seems to work\npretty similarly to other libraries I am realizing I\u2019m not as familiar with\nas I thought. My sense after this is that there maybe be differences in the\nexecution model, but I didn\u2019t really get to dig deep enough to understand\nthem. I\u2019m surprised that something as simple as understanding the\n\u2018start_soon\u2018 method... like I picked up nothing about that in terms of\ndeeper understanding. Thanks for letting me participate!\nAI\nthis was fun! I wish i paid more attention to trio syntax when coding\nwith Cosmo\nTable 6: Feedback from Participants in the AI Condition\n26\n2\n4\n6\n8\n10\n12\n14\nNumber of User Queries\n15\n20\n25\n30\n35\n40\nCompletion Time\nAll Queries (r=0.35, p=0.088)\n0.0\n0.1\n0."
  },
  {
    "chunk_id": "SkillFormation2026_chunk_033",
    "source_id": "SkillFormation2026",
    "text": ", but I didn\u2019t really get to dig deep enough to understand\nthem. I\u2019m surprised that something as simple as understanding the\n\u2018start_soon\u2018 method... like I picked up nothing about that in terms of\ndeeper understanding. Thanks for letting me participate!\nAI\nthis was fun! I wish i paid more attention to trio syntax when coding\nwith Cosmo\nTable 6: Feedback from Participants in the AI Condition\n26\n2\n4\n6\n8\n10\n12\n14\nNumber of User Queries\n15\n20\n25\n30\n35\n40\nCompletion Time\nAll Queries (r=0.35, p=0.088)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRatio of User Queries\nCompletion Time\nDebugging Queries (r=0.43, p=0.033)\nAI Queries and Completion Time\nFigure 18: As the number of queries the total completion time increases, users who ask\na high fraction of debugging queries also tend to use more time to complete the task.\n2\n4\n6\n8\n10\n12\n14\nNumber of User Queries\n0\n5\n10\n15\n20\n25\nQuiz Score\nAll Queries (r=-0.22, p=0.293)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nRatio of User Queries\nQuiz Score\nDebugging Queries (r=-0.41, p=0.043)\nAI Queries and Quiz Score\nFigure 19: There is no clear pattern between the number of total queries and quiz score.\nHowever, users who heavily rely on AI to debug tend to have lower quiz scores.\nD\nTask Details\n27\nCondition\nFeedback\nNo AI\nThis was a lot of fun but the recording aspect can be cumbersome on\nsome systems and cause a little bit of anxiety especially when you can\u2019t\ngo back if you messed up the recording.\nNo AI\nIt was fun to learn about asynchronous programming, which I had not\nencountered before. I think I could have done much better if I could have\naccessed the coding tasks I did at part 2 during the quiz for reference,\nbut I still tried my best. I ran out of time as the bug-finding questions\nwere quite challenging for me.\nNo AI\nThis was fun\nNo AI\nThe programming tasks were very fun and did a good job of helping me\nunderstand how Trio works despite never having used it before. I spent\ntoo much time on this quiz, but that was due to my time management.\nEven if I hadn\u2019t spent too much time on the first part, though, it still\nwould have been a tight finish for me in the 30 minute window I think.\nTable 7: Feedback from Participants in the No AI (control) Condition\nSample Question Types\nCode Reading\nDebugging\nConceptual Question\nFigure 20: Example question types from our evaluation. We designed the evaluation to test three different\nsoftware skills: conceputal understanding, code reading, and code writing.\n28\nFigure 21: Pledge taken by control group partic-\nipants: participants agree to not using AI assis-\ntance.\nFigure 22: Pledge taken by treatment group par-\nticipants.\n"
  },
  {
    "chunk_id": "SkillFormation2026_chunk_034",
    "source_id": "SkillFormation2026",
    "text": " to my time management.\nEven if I hadn\u2019t spent too much time on the first part, though, it still\nwould have been a tight finish for me in the 30 minute window I think.\nTable 7: Feedback from Participants in the No AI (control) Condition\nSample Question Types\nCode Reading\nDebugging\nConceptual Question\nFigure 20: Example question types from our evaluation. We designed the evaluation to test three different\nsoftware skills: conceputal understanding, code reading, and code writing.\n28\nFigure 21: Pledge taken by control group partic-\nipants: participants agree to not using AI assis-\ntance.\nFigure 22: Pledge taken by treatment group par-\nticipants.\nFigure 23: Instructions given to control group\nparticipants. We heavily emphasize not using AI\ntools.\nFigure 24: Instructions given to treatment group\nparticipants. This group was encouraged to use\nthe AI assistant to complete the task as quickly\nas possible.\n29\nFigure 25: Screenshot of the task platform in the control condition. The instructions are on the left and the\ncoding editor is on the right.\nFigure 26: Screenshot of the task platform in the AI condition. The instructions are on the left and the\ncoding editor is on the right. There is a nudge to use the AI assistant on the left tool plane.\n30\nFigure 27: Screenshot of the task platform when interacting with AI Assistant.\n31\n"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_001",
    "source_id": "CopilotCACM2022",
    "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot\u2019s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n\u02fd AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n\u02fd The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n\u02fd Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n\u02fd While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys\u00ad\ntems is challenging. Offline evalua\u00ad\ntion, where the system is shown a par\u00ad\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad\u00ad\nditional step taken by some research\u00ad\ners3,21,29 is to use online evaluation \nand track the frequency of real us\u00ad\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer\u2019s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con\u00ad\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review\u00ad\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan\u00ad\nguage models was first proposed in \nHindle et al.,9 and today neural syn\u00ad\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user\u2019s productivity. Developer pro\u00ad\nductivity has many aspects, and a re\u00ad\ncent study has shown that tools like \nthese are helpful in ways"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_002",
    "source_id": "CopilotCACM2022",
    "text": "higher its benefit. The validity of this \nassumption is not obvious when con\u00ad\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review\u00ad\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan\u00ad\nguage models was first proposed in \nHindle et al.,9 and today neural syn\u00ad\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user\u2019s productivity. Developer pro\u00ad\nductivity has many aspects, and a re\u00ad\ncent study has shown that tools like \nthese are helpful in ways that are only \npartially reflected by measures such \nas completion times for standardized \ntasks.23,a Alternatively, we can leverage \nthe developers themselves as expert \nassessors of their own productivity. \nThis meshes well with current think\u00ad\ning in software engineering research \nsuggesting \nmeasuring \nproductiv\u00ad\nity on multiple dimensions and using \nself-reported data.6 Thus, we focus on \nstudying perceived productivity.\nHere, we investigate whether usage \nmeasurements of developer interac\u00ad\ntions with GitHub Copilot can predict \nperceived productivity as reported \nby developers. We analyze \u200b2,631\u200b sur\u00ad\na\t Nevertheless, such completion times are \ngreatly reduced in many settings, often by \nmore than half.16\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    55\nILLUSTRATION BY JUSTIN METZ\nresearch\nFigure 1. GitHub Copilot\u2019s code completion funnel.\nFigure 2. Demographic composition of survey respondents.\nThink of \nthe language \nyou have used \nthe most with \nOurTool. \nHow pro\ufb01cient\nare you in \nthat language?\nBeginner\nIntermediate\nAdvanced\nStudent/Learning\n0\u20132 Years Prof. Experience\n3\u20135 Years Prof. Experience\n6\u201310 Years Prof. Experience\n11\u201315 Years Prof. Experience\n16+ Years Prof. Experience\nStudent\nProfessional\nHobbyist\nConsultant/Freelancer\nResearcher\nOther\nPython\nJavaScript\nTypeScript\nJava\nRuby\nGo\nC#\nRust\nHTML\nOther\nWhich best\ndescribes your\nprogramming\nexperience?\nWhich of \nthe following \nbest describes \nwhat you do?\nWhat\nprogramming\nlanguages\ndo you usually \nuse? Choose\nup to three\nfrom the list.\n0%\n25%\n50%\n75%\n100%\nAverage number of events per survey user active hour\n50\n170\nmostly unchanged\nCompletion\nunchanged\n40\n30\n20\n24\n6.6\ncompletion\nopportunity\ncompletion\nshown\ncompletion\naccepted\nafter 30\nseconds\nafter 2\nminutes\nafter 5\nminutes\nafter 10\nminutes\n5.6\n4.3\n5.3\n5.1\n5\n3.1\n3.4\n3.8\n10\n0\nvey responses from developers using \nGitHub Copilot and match their re\u00ad\nsponses to measurements collected \nfrom the IDE. We consider acceptance \ncounts and more detailed measures \nof contribution, such as the amount"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_003",
    "source_id": "CopilotCACM2022",
    "text": "%\n100%\nAverage number of events per survey user active hour\n50\n170\nmostly unchanged\nCompletion\nunchanged\n40\n30\n20\n24\n6.6\ncompletion\nopportunity\ncompletion\nshown\ncompletion\naccepted\nafter 30\nseconds\nafter 2\nminutes\nafter 5\nminutes\nafter 10\nminutes\n5.6\n4.3\n5.3\n5.1\n5\n3.1\n3.4\n3.8\n10\n0\nvey responses from developers using \nGitHub Copilot and match their re\u00ad\nsponses to measurements collected \nfrom the IDE. We consider acceptance \ncounts and more detailed measures \nof contribution, such as the amount \nof code contributed by GitHub Copilot \nand persistence of accepted comple\u00ad\ntions in the code. We find that accep\u00ad\ntance rate of shown suggestions is a \nbetter predictor of perceived produc\u00ad\ntivity than the alternative measures. \nWe also find that acceptance rate var\u00ad\nies significantly over our developer \npopulation as well as over time, and \npresent a deeper dive into some of \nthese variations.\nOur results support the principle \nthat acceptance rate can be used for \ncoarse-grained monitoring of the per\u00ad\nformance of a neural code synthesis \nsystem. This ratio of shown sugges\u00ad\ntions being accepted correlates better \nthan more detailed measures of con\u00ad\ntribution. However, other approaches \nremain necessary for fine-grained \ninvestigation due to the many human \nfactors involved.\nBackground\nOffline evaluation of code completion \ncan have shortcomings even in tracta\u00ad\nble circumstances where completions \ncan be labeled for correctness. For ex\u00ad\nample, a study of \u200b15,000\u200b completions by \n66 developers in Visual Studio found sig\u00ad\nnificant differences between synthetic \nbenchmarks used for model evaluation \nand real-world usage.7 The evaluation \nof context-aware API completion for Vi\u00ad\nsual Studio IntelliCode considered Re\u00ad\ncall@5\u2014the proportion of completions \nfor which the correct method call was in \nthe top five suggestions. This metric fell \nfrom \u200b90%\u200b in offline evaluation to \u200b70%\u200b \nwhen used online.21\nDue to the diversity of potential \nsolutions to a multi-line completion \ntask, researchers have used software \ntesting to evaluate the behavior of \ncompletions. Competitive program\u00ad\nming sites have been used as a source \nof such data8,11 as well as handwrit\u00ad\nten programming problems.5 Yet, it \nis unclear how well performance on \nprogramming competition data gen\u00ad\neralizes to interactive development in \nan IDE.\nIn this work, we define acceptance \nrate as the fraction of completions \n56    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nshown to the developer that are subse\u00ad\nquently accepted for inclusion in the \nsource file. The IntelliCode Compose \nsystem uses the term click through rate \n(CTR) for this and reports a value of \u200b\n10%\u200b in online trials.20 An alternative \nmeasure is that of daily completions \naccepted per user (DCPU) for which a \nvalue of around 20 has been report\u00ad\ned.3,29 To calculate"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_004",
    "source_id": "CopilotCACM2022",
    "text": " interactive development in \nan IDE.\nIn this work, we define acceptance \nrate as the fraction of completions \n56    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nshown to the developer that are subse\u00ad\nquently accepted for inclusion in the \nsource file. The IntelliCode Compose \nsystem uses the term click through rate \n(CTR) for this and reports a value of \u200b\n10%\u200b in online trials.20 An alternative \nmeasure is that of daily completions \naccepted per user (DCPU) for which a \nvalue of around 20 has been report\u00ad\ned.3,29 To calculate acceptance rate \none must, of course, normalize DCPU \nby the time spent coding each day. For \ncontext, in our study, GitHub Copilot \nhas an acceptance rate of \u200b27%\u200b and a \nmean DCPU in excess of 312 (See Fig\u00ad\nure 1).b These differences are presum\u00ad\nably due to differences in the kinds \nof completion offered, or perhaps to \nuser-interface choices. We discuss \nlater how developer objectives, choice \nof programming language, and even \ntime of day seem to affect our data. \nSuch discrepancies highlight the dif\u00ad\nficulty in using acceptance rate to un\u00ad\nderstand the value of a system.\nThere is some evidence that accep\u00ad\ntance rate (and indeed correctness) \nmight not tell the whole story. One sur\u00ad\nvey of developers considered the use \nof AI to support translation between \nprogramming languages and found \nindications that developers tolerated, \nand in some cases valued, erroneous \nsuggestions from the model.26\nMeasuring \ndeveloper \nproductiv\u00ad\nity through activity counts over time (a \ntypical definition of productivity bor\u00ad\nrowed from economics) disregards the \ncomplexity of software development, \nas they account for only a subset of \ndeveloper outputs. A more holistic pic\u00ad\nture is formed by measuring perceived \nproductivity \nthrough \nself-reported \ndata across various dimensions6 and \nsupplementing it with automatically \nmeasured data.4 We used the SPACE \nframework6 to design a survey that \ncaptures \nself-reported \nproductivity \nand paired the self-reported data with \nusage telemetry.\nTo the best of our knowledge, this \nis the first study of code suggestion \ntools establishing a clear link between \nusage measurements and developer \nproductivity or happiness. A previ\u00ad\nous study comparing GitHub Copilot \nagainst IntelliCode with 25 partici\u00ad\npants found no significant correlation \nbetween task completion times and \nsurvey responses.22\nData and Methodology\nUsage measurements. GitHub Copilot \nprovides code completions using Ope\u00ad\nnAI language models. It runs within \nthe IDE and at appropriate points \nsends a completion request to a cloud-\nhosted instance of the neural model. \nGitHub Copilot can generate comple\u00ad\ntions at arbitrary points in code rath\u00ad\ner than, for example, only being trig\u00ad\ngered when a developer types a period \nfor invoking a method on an object. A \nvariety of rules determine when to re\u00ad\nquest a completion, when to abandon \nrequests if the developer has moved \non before the model is ready with a \n"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_005",
    "source_id": "CopilotCACM2022",
    "text": "between task completion times and \nsurvey responses.22\nData and Methodology\nUsage measurements. GitHub Copilot \nprovides code completions using Ope\u00ad\nnAI language models. It runs within \nthe IDE and at appropriate points \nsends a completion request to a cloud-\nhosted instance of the neural model. \nGitHub Copilot can generate comple\u00ad\ntions at arbitrary points in code rath\u00ad\ner than, for example, only being trig\u00ad\ngered when a developer types a period \nfor invoking a method on an object. A \nvariety of rules determine when to re\u00ad\nquest a completion, when to abandon \nrequests if the developer has moved \non before the model is ready with a \ncompletion, and how much of the re\u00ad\nsponse from the model to surface as a \ncompletion.\nAs stated in our terms of usage,b the \nGitHub Copilot IDE extension records \nthe events shown in Table\u00a01 for all us\u00ad\ners. We make usage measurements \nfor each developer by counting those \nevents.\nOur measures of persistence go fur\u00ad\nther than existing work, which stops at \nacceptance. The intuition here is that a \ncompletion which is accepted into the \nsource file but then subsequently turns \nout to be incorrect can be considered \nto have wasted developer time both in \nreviewing it and then having to go back \nand delete it. We also record mostly un\u00ad\nchanged completions: A large comple\u00ad\ntion requiring a few edits might still be \na positive contribution. It is not clear \nhow long after acceptance one should \nconfirm persistence, so we consider a \nrange of options.\nThe events pertaining to comple\u00ad\ntions form a funnel which we show \nquantitatively in Table 1. We include \na summary of all data in Appendix \nA.c (All appendices for this article can \nbe found online at https://dl.acm.org/\ndoi/10.1145/3633453).\nWe normalize these measures \nagainst each other and write X _\nper _ Y to indicate we have normal\u00ad\nized metric X by metric Y. For example: \naccepted _ per _ hour is calculat\u00ad\ned as the total number of accepted \nevents divided by the total number of \n(active) hour events.\nTable\u00a02 defines the core set of met\u00ad\nb\t See https://bit.ly/3S7oqZV\nc\t Appendices can be found in the arXiv version \nhttps://arxiv.org/pdf/2205.06537.pdf.\nIt is unclear how \nwell performance \non programming \ncompetition \ndata generalizes \nto interactive \ndevelopment \nin an IDE.\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    57\nresearch\nTable 2. The core set of measurements considered in this article.\nNatural name\nExplanation\nShown rate\nRatio of completion opportunities that resulted in a completion being \nshown to the user\nAcceptance rate\nRatio of shown completions accepted by the user\nPersistence rate\nRatio of accepted completions unchanged after 30, 120, 300, and 600 \nseconds\nFuzzy persistence rate\nRatio of accepted completions mostly unchanged after 30, 120, 300, \nand 600 seconds\nEfficiency\nRatio of completion opportunities"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_006",
    "source_id": "CopilotCACM2022",
    "text": "in an IDE.\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    57\nresearch\nTable 2. The core set of measurements considered in this article.\nNatural name\nExplanation\nShown rate\nRatio of completion opportunities that resulted in a completion being \nshown to the user\nAcceptance rate\nRatio of shown completions accepted by the user\nPersistence rate\nRatio of accepted completions unchanged after 30, 120, 300, and 600 \nseconds\nFuzzy persistence rate\nRatio of accepted completions mostly unchanged after 30, 120, 300, \nand 600 seconds\nEfficiency\nRatio of completion opportunities that resulted in a completion \naccepted and unchanged after 30, 120, 300, and 600 seconds\nContribution speed\nNumber of characters in accepted completions per distinct, active hour\nAcceptance frequency\nNumber of accepted completions per distinct, active hour\nPersistence frequency\nNumber of unchanged completions per distinct, active hour\nTotal volume\nTotal number of completions shown to the user\nLoquaciousness\nNumber of shown completions per distinct, active hour\nEagerness\nNumber of shown completions per opportunity\nTable 1. Developer usage events collected by GitHub Copilot.\nOpportunity\nA heuristic-based determination by the IDE and the plug-in that a completion \nmight be appropriate at this point in the code (for example, the cursor is not in \nthe middle of a word)\nShown\nCompletion shown to the developer\nAccepted\nCompletion accepted by the developer for inclusion in the source file\nAccepted char\nThe number of characters in an accepted completion\nMostly \nunchanged X\nCompletion persisting in source code with limited modifications (Levenshtein \ndistance less than 33%) after X seconds, where we consider a duration of 30, \n120, 300, and 600 seconds\nUnchanged X\nCompletion persisting in source code unmodified after X seconds.\n(Active) hour\nAn hour during which the developer was using their IDE with the plug-in active\nwhen using GitHub Copilot.\u201d For each \nself-reported productivity measure, \nwe encoded its five ordinal response \nvalues to numeric labels (1 = Strongly \nDisagree, \u200b...\u200b, 5 = Strongly Agree). We \ninclude the full list of questions and \ntheir coding to the SPACE framework \nin Appendix\u00a0C. For more information \non the SPACE framework and how the \nempirical software engineering com\u00ad\nmunity has been discussing developer \nproductivity, please see the following \nsection.\nEarly in our analysis, we found that \nthe usage metrics we describe in the \nUsage Measurements section\u00a0 corre\u00ad\nsponded similarly to each of the mea\u00ad\nsured dimensions of productivity, and \nin turn these dimensions were highly \ncorrelated to each other (Figure 3). We \ntherefore added an aggregate produc\u00ad\ntivity score calculated as the mean of \nall 12 individual measures (excluding \nskipped questions). This serves as a \nrough proxy for the much more com\u00ad\nplex concept of productivity, facili\u00ad\ntating recognition of overall trends, \nwhich may be less discernible on indi\u00ad\nvidual variables due to higher statisti\u00ad\ncal variation. The full dataset of these \naggregate productivity"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_007",
    "source_id": "CopilotCACM2022",
    "text": " found that \nthe usage metrics we describe in the \nUsage Measurements section\u00a0 corre\u00ad\nsponded similarly to each of the mea\u00ad\nsured dimensions of productivity, and \nin turn these dimensions were highly \ncorrelated to each other (Figure 3). We \ntherefore added an aggregate produc\u00ad\ntivity score calculated as the mean of \nall 12 individual measures (excluding \nskipped questions). This serves as a \nrough proxy for the much more com\u00ad\nplex concept of productivity, facili\u00ad\ntating recognition of overall trends, \nwhich may be less discernible on indi\u00ad\nvidual variables due to higher statisti\u00ad\ncal variation. The full dataset of these \naggregate productivity scores togeth\u00ad\ner with the usage measurements con\u00ad\nsidered in this article is available at \nhttps://bit.ly/47HVjAM.\nGiven it has been impossible to pro\u00ad\nduce a unified definition or metric(s) \nfor developer productivity, there have \nbeen attempts to synthesize the fac\u00ad\ntors that impact productivity to de\u00ad\nscribe it holistically, include various \nrelevant factors, and treat developer \nproductivity as a composite mea\u00ad\nsure17,19,24 In addition, organizations \noften use their own multidimensional \nframeworks to operationalize produc\u00ad\ntivity, which reflects their engineering \ngoals\u2014for example, Google uses the \nQUANTS framework, with five compo\u00ad\nnents of productivity.27 In this article, \nwe use the SPACE framework,6 which \nbuilds on synthesis of extensive and \ndiverse literature by expert research\u00ad\ners and practitioners in the area of de\u00ad\nveloper productivity.\nSPACE is an acronym of the five di\u00ad\nmensions of productivity:\n\t\n\u02f2 S (Satisfaction and well being): \nThis dimension is meant to reflect \ndevelopers\u2019 fulfillment with the work \nthey do and the tools they use, as well \non Mar. 6, 2022.\nThe survey contained multiple-\nchoice questions regarding demo\u00ad\ngraphic information (see Figure\u00a0 2) \nand Likert-style questions about dif\u00ad\nferent aspects of productivity, which \nwere randomized in their order of ap\u00ad\npearance to the user. Figure\u00a0 2 shows \nthe demographic composition of our \nrespondents. We note the significant \nproportion of professional program\u00ad\nmers who responded.\nThe SPACE framework6 defines five \ndimensions of productivity: Satisfac\u00ad\ntion and well-being, Performance, Ac\u00ad\ntivity, Communication and collabora\u00ad\ntion, and Efficiency and flow. We use \nfour of these (S,P,C,E), since self re\u00ad\nporting on (A) is generally considered \ninferior to direct measurement. We \nincluded 11 statements covering these \nfour dimensions in addition to a sin\u00ad\ngle statement: \u201cI am more productive \nrics we feel have a natural interpreta\u00ad\ntion in this context. We note there are \nalternatives, and we incorporate these \nin our discussion where relevant.\nProductivity survey. To understand \nusers\u2019 experience with GitHub Co\u00ad\npilot, we emailed a\u00a0link to an online \nsurvey to\u00a0 \u200b17,\u2009420\u200b users. These were \nparticipants of the unpaid technical \npreview using GitHub Copilot with \ntheir everyday programming tasks. \nThe only selection criterion was"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_008",
    "source_id": "CopilotCACM2022",
    "text": ",E), since self re\u00ad\nporting on (A) is generally considered \ninferior to direct measurement. We \nincluded 11 statements covering these \nfour dimensions in addition to a sin\u00ad\ngle statement: \u201cI am more productive \nrics we feel have a natural interpreta\u00ad\ntion in this context. We note there are \nalternatives, and we incorporate these \nin our discussion where relevant.\nProductivity survey. To understand \nusers\u2019 experience with GitHub Co\u00ad\npilot, we emailed a\u00a0link to an online \nsurvey to\u00a0 \u200b17,\u2009420\u200b users. These were \nparticipants of the unpaid technical \npreview using GitHub Copilot with \ntheir everyday programming tasks. \nThe only selection criterion was hav\u00ad\ning previously opted in to receive com\u00ad\nmunications. A vast majority of survey \nusers (more than 80%) filled out the \nsurvey within the first two days, on or \nbefore February 12, 2022. We there\u00ad\nfore focus on data from the four-week \nperiod leading up to this point (\"the \nstudy period\"). We received a total of \n2,047 responses we could match to \nusage data from the study period, the \nearliest on Feb. 10, 2022 and the latest \n58    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\ndocumentation or the speed of an\u00ad\nswering questions, or the onboard\u00ad\ning time and processing of new team \nmembers.\n\t\n\u02f2 E (Efficiency and flow): This di\u00ad\nmension reflects the ability to com\u00ad\nplete work or make progress with little \ninterruption or delay. It is important \nto note that delays and interruptions \ncan be caused either by systems or hu\u00ad\nmans, and it is best to monitor both \nself-reported and observed measure\u00ad\nments\u2014for example, use self-reports \nof the ability to do uninterrupted work, \nas well as measure wait time in engi\u00ad\nneering systems).\n\t\n\u02f2 A (Activity): This is the count of \noutputs\u2014for example, the number of \npull requests closed by a developer. As \na result, this dimension is best quanti\u00ad\nfied via system data. Given the variety \nof developers\u2019 activities as part of their \nwork, it is important that the activ\u00ad\nity dimension accounts for more than \ncoding activity\u2014for instance, writing \ndocumentation, creating design specs, \nand so on.\n\t\n\u02f2 C (Communication and collabora\u00ad\ntion): This dimension aims to capture \nthat modern software development \nhappens in teams and is, therefore, \nimpacted by the discoverability of \nas how healthy and happy they are \nwith the work they do. This dimension \nreflects some of the easy-to-overlook \ntrade-offs involved when looking ex\u00ad\nclusively at velocity acceleration\u2014for \nexample, when we target faster turn\u00ad\naround of code reviews without con\u00ad\nsidering workload impact or burnout \nfor developers.\n\t\n\u02f2 P (Performance): This dimension \naims to quantify outcomes rather than \noutput. Example metrics that capture \nperformance relate to quality and re\u00ad\nliability, as well as further-removed \nmetrics such as customer adoption or \nsatisfaction.\nFigure 3."
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_009",
    "source_id": "CopilotCACM2022",
    "text": " modern software development \nhappens in teams and is, therefore, \nimpacted by the discoverability of \nas how healthy and happy they are \nwith the work they do. This dimension \nreflects some of the easy-to-overlook \ntrade-offs involved when looking ex\u00ad\nclusively at velocity acceleration\u2014for \nexample, when we target faster turn\u00ad\naround of code reviews without con\u00ad\nsidering workload impact or burnout \nfor developers.\n\t\n\u02f2 P (Performance): This dimension \naims to quantify outcomes rather than \noutput. Example metrics that capture \nperformance relate to quality and re\u00ad\nliability, as well as further-removed \nmetrics such as customer adoption or \nsatisfaction.\nFigure 3. Correlation between metrics. Metrics are ordered by similarity based on distance in the correlation matrix, except for manu\u00ad\nally fixing the aggregate productivity and acceptance rate at the end for visibility.\nSpearman\nCorrelation\n1.00\n0.75\n0.50\n0.25\n0.00\naccepted_per_shown\nunchanged_600_per_active_hour\nshown\nshown_per_opportunity\nmostly_unchanged_120_per_accepted\nunchanged_600_per_accepted\nunchanged_30_per_accepted\nunchanged_300_per_accepted\nlearn_from\nless_time_searching\nunfamiliar_progress\nrepetitive_faster\nless_effort_repetitive\nbetter_code\nless_frustrated\nfocus_satisfying\nmore_ful\ufb01lled\nstay_in_\ufb02ow\ntasks_faster\naggregate_productivity\nmostly_unchanged_300_per_accepted\nmostly_unchanged_30_per_accepted\nmostly_unchanged_600_per_accepted\nunchanged_120_per_accepted\nshown_per_active_hour\naccepted_char_per_active_hour\naccepted_per_active_hour\nunchanged_30_per_active_hour\nunchanged_120_per_active_hour\naccepted_per_opportunity\nunchanged_600_per_opportunity\nunchanged_300_per_opportunity\nunchanged_30_per_opportunity\nunchanged_120_per_opportunity\nunchanged_300_per_active_hour\naccepted_per_shown\nunchanged_600_per_active_hour\nshown\nshown_per_opportunity\nmostly_unchanged_120_per_accepted\nunchanged_600_per_accepted\nunchanged_30_per_accepted\nunchanged_300_per_accepted\nlearn_from\nless_time_searching\nunfamiliar_progress\nrepetitive_faster\nless_effort_repetitive\nbetter_code\nless_frustrated\nfocus_satisfying\nmore_ful\ufb01lled\nstay_in_\ufb02ow\ntasks_faster\naggregate_productivity\nmostly_unchanged_300_per_accepted\nmostly_unchanged_30_per_accepted\nmostly_unchanged_600_per_accepted\nunchanged_120_per_accepted\nshown_per_active_hour\naccepted_char_per_active_hour\naccepted_per_active_hour\nunchanged_30_per_active_hour\nunchanged_120_per_active_hour\naccepted_per_opportunity\nunchanged_600_per_opportunity\nunchanged_300_per_opportunity\nunchanged_30_per_opportunity\nunchanged_120_per_opportunity\nunchanged_300_per_active_hour\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    59\nresearch\nTable"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_010",
    "source_id": "CopilotCACM2022",
    "text": "aggregate_productivity\nmostly_unchanged_300_per_accepted\nmostly_unchanged_30_per_accepted\nmostly_unchanged_600_per_accepted\nunchanged_120_per_accepted\nshown_per_active_hour\naccepted_char_per_active_hour\naccepted_per_active_hour\nunchanged_30_per_active_hour\nunchanged_120_per_active_hour\naccepted_per_opportunity\nunchanged_600_per_opportunity\nunchanged_300_per_opportunity\nunchanged_30_per_opportunity\nunchanged_120_per_opportunity\nunchanged_300_per_active_hour\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    59\nresearch\nTable 3. Effects of experience on facets of productivity where result of linear regression \nwas a statistically significant covariate.\nProductivity measure\ncoeff\nProficiency\nBetter code\n\u22120.061*\nProficiency\nStay in flow\n0.069*\nProficiency\nFocus satisfying\n0.067*\nProficiency\nLess effort repetitive\n0.072**\nProficiency\nRepetitive faster\n0.055***\nYears\nBetter code\n\u22120.087*\nYears\nLess frustrated\n\u22120.103**\nYears\nRepetitive faster\n\u22120.054*\nYears\nUnfamiliar progress\n0.081*\n(*: p \u00a1 0.05, **: p \u00a1 0.01, ***: p \u00a1 0.001.)\nis intuitive in the sense that shorter \nperiods move the measure closer to \nacceptance rate. We also expect that \nat some point after accepting the com\u00ad\npletion it becomes simply part of the \ncode, so any changes (or not) after that \npoint will not be attributed to GitHub \nCopilot. All persistence measures \nwere less well correlated than accep\u00ad\ntance rate.\nTo assess the different metrics in \na single model, we ran a regression \nusing projection on latent structures \n(PLS). The choice of PLS, which cap\u00ad\ntures the common variation of these \nvariables as is linearly connected to \nthe aggregate productivity,28 is due to \nthe high collinearity of the single met\u00ad\nrics. The first component, to which \nevery metric under consideration con\u00ad\ntributes positively, explains \u200b43\u2009.\u20092%\u200b of \nthe variance. The second component \ncaptures the acceptance rate/change \nrate dichotomy; it explains a further \u200b\n13\u2009.\u20091%\u200b. Both draw most strongly from \nacceptance rate.\nThis strongly points to acceptance \nrate being the most immediate indica\u00ad\ntor of perceived productivity, although \nit is beneficial to combine with others \nto get a fuller picture.\nExperience\nTo understand how different types of \ndevelopers interact with Copilot, our \nsurvey asked respondents to self-report \ntheir level of experience in two ways:\n\t\n\u02f2 \"Think of the language you have \nused the most with Copilot. How pro\u00ad\nficient are you in that language?\" with \nWhat Drives Perceived Productivity?\nTo examine the relationship between \nobjective measurements of user be\u00ad\nhavior and self-reported perceptions of \nproductivity, we used our set of core us\u00ad\nage measurements (Table\u00a02). We then \ncalculated Pearson\u2019s R correlation co\u00ad\nefficient and the corresponding p-val\u00ad\nue"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_011",
    "source_id": "CopilotCACM2022",
    "text": ", although \nit is beneficial to combine with others \nto get a fuller picture.\nExperience\nTo understand how different types of \ndevelopers interact with Copilot, our \nsurvey asked respondents to self-report \ntheir level of experience in two ways:\n\t\n\u02f2 \"Think of the language you have \nused the most with Copilot. How pro\u00ad\nficient are you in that language?\" with \nWhat Drives Perceived Productivity?\nTo examine the relationship between \nobjective measurements of user be\u00ad\nhavior and self-reported perceptions of \nproductivity, we used our set of core us\u00ad\nage measurements (Table\u00a02). We then \ncalculated Pearson\u2019s R correlation co\u00ad\nefficient and the corresponding p-val\u00ad\nue of the F-statistic between each pair \nof usage measurement and perceived \nproductivity metric. We also computed \na PLS regression from all usage mea\u00ad\nsurements jointly.\nWe summarize these results in \nFigure\u00a0 3, showing the correlation co\u00ad\nefficients between all measures and \nsurvey questions. The full table of \nall results is included in Appendix\u00a0 B, \navailable online.\nWe find acceptance rate (accept\u00ad\ned _ per _ shown) most positively \npredicts users\u2019 perception of produc\u00ad\ntivity, although, given the confound\u00ad\ning and human factors, there is still \nnotable unexplained variance.\nOf all usage measurements, accep\u00ad\ntance rate correlates best with aggregate \nproductivity (\u200b\u03c1\u2007 =\u2007 0\u2009.\u200924\u200b, \u200bP\u2007 <\u2007 0\u2009.\u20090001\u200b). \nThis measurement is also the best per\u00ad\nforming for at least one survey ques\u00ad\ntion in each of the SPACE dimensions. \nThis correlation is high confidence but \nleaves considerable unexplained vari\u00ad\nance. Later, we explore improvements \nfrom combining multiple usage mea\u00ad\nsurements together.\nLooking at the more detailed met\u00ad\nrics around persistence, we see that it \nis generally better over shorter time \nperiods than over longer periods. This \nDeveloper productivity has been \na controversial topic in software \nengineering research over the \nyears. We point readers to excellent \npresentations of the existing discourse \nin the community in Meyer et al.12 \nand Murphy-Hill et al.;15 however \nwe summarize the key points of \ndiscussion below:\n\t\n\u02f2 Inspired by economics \ndefinitions of productivity as output \nper unit of input, some research has \ndefined developer productivity in the \nsame terms\u2014for example, numbers of \nlines of code per day, function points \nper sprint, and so on. However, such \nmeasures are not connected to goals \n(for instance, it is not the goal of a \ndeveloper to write the most lines of \ncode), they may motivate developers to \ngame the system, they do not account \nfor the quality of the output, and they \nare in tension with other metrics (for \nexample, a higher number of commits \nor PRs will create a higher need for \ncode reviews).\n\t\n\u02f2 Observational studies of \ndevelopers reveal that developers \nspent more than half their working \nday on activities other than coding.13 \nGiven this, the view of developer \nproductivity as inputs and outputs, \nor using metrics that strictly focus on \ncoding, ignores the reality of the work \n"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_012",
    "source_id": "CopilotCACM2022",
    "text": " However, such \nmeasures are not connected to goals \n(for instance, it is not the goal of a \ndeveloper to write the most lines of \ncode), they may motivate developers to \ngame the system, they do not account \nfor the quality of the output, and they \nare in tension with other metrics (for \nexample, a higher number of commits \nor PRs will create a higher need for \ncode reviews).\n\t\n\u02f2 Observational studies of \ndevelopers reveal that developers \nspent more than half their working \nday on activities other than coding.13 \nGiven this, the view of developer \nproductivity as inputs and outputs, \nor using metrics that strictly focus on \ncoding, ignores the reality of the work \ndevelopers do.\n\t\n\u02f2 In addition, developers\u2019 \nperspective on what affects their \nproductivity12 and what metrics might \nreflect it14 differs from the inputs/\noutputs view. When asked when they \nare productive and how they measure \nproductivity, developers do not cite \nlines of code or function points per \nsprint, but rather completing tasks, \nbeing free of interruptions, usefulness \nof their work, success of the feature \nthey worked on, and more.\n\t\n\u02f2 To sum up, after many studies \nand many definitions, measurements, \nand approaches to productivity, \nthe empirical software engineering \nresearch community has concluded \nthat developer productivity is a \nmultidimensional topic that cannot \nbe summarized by a single metric.10 \nBoth objective and subjective \napproaches to measurement have \nbeen tried, leading to the conclusion \nthat they both have advantages and \ndisadvantages.\nDeveloper \nProductivity \nand the \nSPACE \nFramework\n60    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nFigure 5. Linear regressions between acceptance rate and aggregate productivity by \nsubgroup defined through years of professional experience or programming language \nuse. Dashed lines denote averages. The x-axis is clipped at (0, 0.5), and 95% of respon\u00ad\ndents fall into that range.\n0.0\n0.1\n4.25\nnone\nexperience\nexperience\n\u2264 2 y\n3\u20135 y\n6\u201310 y\n11\u201315 y\n\u2265 16 y\n4.00\n3.75\n3.50\n0.2\n0.3\n0.4\n0.5 0.0\nacceptance rate\naggregate productivity\n0.1\n0.2\n0.3\n0.4\n0.5\nlanguage\nlanguage\nJavaScript\nTypeScript\nPython\nother\nFigure 4. Different metrics clustering in latent structures predicting perceived pro\u00ad\nductivity. We color the following groups: flawless suggestions (counting the number of \nunchanged suggestions), persistence rate (ratio of accepted suggestions that are un\u00ad\nchanged), and fuzzy persistence rate (accepted suggestions that are mostly unchanged).\nMetric\n0.03\n0.02\n0.01\n0.00\n\u20130.01\n0.000\n0.005\nProjection on \ufb01rst latent structure\nProjection on second latent structure\n0.010\nacceptance rate\nacceptance frequency\namount contribution (char)\n\ufb02awless suggestion frequency\npersistence rate\nfuzzy"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_013",
    "source_id": "CopilotCACM2022",
    "text": "language\nJavaScript\nTypeScript\nPython\nother\nFigure 4. Different metrics clustering in latent structures predicting perceived pro\u00ad\nductivity. We color the following groups: flawless suggestions (counting the number of \nunchanged suggestions), persistence rate (ratio of accepted suggestions that are un\u00ad\nchanged), and fuzzy persistence rate (accepted suggestions that are mostly unchanged).\nMetric\n0.03\n0.02\n0.01\n0.00\n\u20130.01\n0.000\n0.005\nProjection on \ufb01rst latent structure\nProjection on second latent structure\n0.010\nacceptance rate\nacceptance frequency\namount contribution (char)\n\ufb02awless suggestion frequency\npersistence rate\nfuzzy persistence rate\nshown overall\nshown rate\nTable 4. Correlations of acceptance rate \nwith aggregate productivity broken down \nby subgroup.\noptions \u2018Beginner\u2019, \u2018Intermediate\u2019, and \n\u2018Advanced\u2019.\n\t\n\u02f2 \"Which best describes your pro\u00ad\ngramming experience?\" with options \nstarting with \"Student\" and ranging \nfrom \"0\u20132 years\" to \"16+ years\" in two-\nyear intervals.\nWe compute correlations with pro\u00ad\nductivity metrics for both experience \nvariables and include these two vari\u00ad\nables as covariates in a multivariate re\u00ad\ngression analysis. We find that both are \nnegatively correlated with our aggre\u00ad\ngate productivity measure (proficien\u00ad\ncy: \u200b\u03c1\u2007 =\u2007 \u2212\u200a0\u2009.\u2009095\u200b, \u200bP\u2007 =\u2007 0\u2009.\u20090001\u200b; years of \nexperience: \u200b\u03c1\u2007 =\u2007 \u2212\u200a0\u2009.\u2009161\u200b, \u200bP\u2007 <\u2007 0\u2009.\u20090001\u200b). \nHowever, in multivariate regressions \npredicting productivity from usage \nmetrics while controlling for demo\u00ad\ngraphics, proficiency had a non-sig\u00ad\nnificant positive effect (\u200bcoeff\u2007 =\u2007 0\u2009.\u2009021\u200b, \u200b\nP\u2007 =\u2007 0\u2009.\u2009213\u200b), while years of experience \nhad a non-significant negative effect \n(\u200bcoeff\u2007 =\u2007 \u2212\u200a0\u2009.\u2009032\u200b, \u200bP\u2007 =\u2007 0\u2009.\u2009122\u200b).\nLooking further at individual mea\u00ad\nsures of productivity, (Table 3) we find \nthat both language proficiency and \nyears of experience negatively predict \ndevelopers agreeing that Copilot helps \nthem write better code. However, pro\u00ad\nficiency positively predicts developers \nagreeing that Copilot helps them stay \nin the flow, focus on more satisfying \nwork, spend less effort on repetitive \ntasks, and perform repetitive tasks \nfaster. Years of experience negatively \npredicts developers feeling less frus\u00ad\ntrated in coding sessions and per\u00ad\nforming repetitive tasks faster while \nusing Copilot, but positively predicts \nsubgroup\ncoeff\nn\nnone\n0.135*\n344\n\u2264 2y\n0.178**\n451\n3 \u2013 5 y\n0.255***\n358\n6 \u2013 10 y\n0.265***\n251\n11 \u2013 15 y\n0.171*\n162\n\u2265 16 y\n0"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_014",
    "source_id": "CopilotCACM2022",
    "text": "\nficiency positively predicts developers \nagreeing that Copilot helps them stay \nin the flow, focus on more satisfying \nwork, spend less effort on repetitive \ntasks, and perform repetitive tasks \nfaster. Years of experience negatively \npredicts developers feeling less frus\u00ad\ntrated in coding sessions and per\u00ad\nforming repetitive tasks faster while \nusing Copilot, but positively predicts \nsubgroup\ncoeff\nn\nnone\n0.135*\n344\n\u2264 2y\n0.178**\n451\n3 \u2013 5 y\n0.255***\n358\n6 \u2013 10 y\n0.265***\n251\n11 \u2013 15 y\n0.171*\n162\n\u2265 16 y\n0.153*\n214\nJavaScript\n0.227***\n1184 \nTypeScript \n0.165***\n654\nPython\n0.172***\n716\nother\n0.178***\n1829\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    61\nresearch\nuntil mornings 7:00 am PST, where the \naverage acceptance rate is also rather \nhigh at \u200b23%\u200b.\n\t\n\u02f2 Typical working hours during the \nweek from 7:00 am PST to 4:00 pm PST, \nwhere the average acceptance rate is \nmuch lower at \u200b21\u2009.\u20092%\u200b.\nConclusions\nWhen we set out to connect the pro\u00ad\nductivity benefit of GitHub Copilot to \nusage measurements from developer \nactivity, we collected measurements \nabout acceptance of completions in \nline with prior work, but also devel\u00ad\noped persistence metrics, which ar\u00ad\nguably capture sustained and direct \nimpact on the resulting code. We \nwere surprised to find acceptance rate \n(number of acceptances normalized \nby the number of shown completions) \nto be better correlated with reported \nproductivity than our measures of \npersistence.\nIn hindsight, this makes sense. \nCoding is not typing, and GitHub Co\u00ad\npilot\u2019s central value lies not in being \nthe way users enter most of their code. \nInstead, it lies in helping users to make \nthe best progress toward their goals. A \nsuggestion that serves as a useful tem\u00ad\nplate to tinker with may be as good or \nbetter than a perfectly correct (but ob\u00ad\nvious) line of code that only saves the \nuser a few keystrokes.\nThis suggests that a narrow focus \non the correctness of suggestions \nwould not tell the whole story for these \nkinds of tooling. Instead, one could \nview code suggestions inside an IDE to \nbe more akin to a conversation. While \nchatbots such as ChatGPT are already \nused for programming tasks, they are \nexplicitly structured as conversations. \nHere, we hypothesize that interactions \nwith Copilot, which is not a chatbot, \nshare many characteristics with natu\u00ad\nral-language conversations.\nWe see anecdotal evidence of this \nin comments posted about GitHub \nCopilot online (see Appendix\u00a0 E for \nexamples), in which users talk about \nsequences of interactions. A conver\u00ad\nsation turn in this context consists of \nthe prompt in the completion request \nand the reply as the completion itself. \nThe developer\u2019s response to the com\u00ad\npletion arises from the subsequent \n"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_015",
    "source_id": "CopilotCACM2022",
    "text": " an IDE to \nbe more akin to a conversation. While \nchatbots such as ChatGPT are already \nused for programming tasks, they are \nexplicitly structured as conversations. \nHere, we hypothesize that interactions \nwith Copilot, which is not a chatbot, \nshare many characteristics with natu\u00ad\nral-language conversations.\nWe see anecdotal evidence of this \nin comments posted about GitHub \nCopilot online (see Appendix\u00a0 E for \nexamples), in which users talk about \nsequences of interactions. A conver\u00ad\nsation turn in this context consists of \nthe prompt in the completion request \nand the reply as the completion itself. \nThe developer\u2019s response to the com\u00ad\npletion arises from the subsequent \nchanges incorporated in the next \nprompt to the model. There are clear \ndevelopers \nmaking \nprogress faster when \nworking in an unfa\u00ad\nmiliar language. These \nfindings suggest that \nexperienced developers \nwho are already highly \nskilled are less likely \nto write better code with Copilot, but \nCopilot can assist their productivity in \nother ways,  particularly when engag\u00ad\ning with new areas and automating \nroutine work.\nJunior developers not only report \nhigher productivity gains; they also \ntend to accept more suggestions. How\u00ad\never, the connection observed in the \nsection \"What Drives Perceived Pro\u00ad\nductivity\" is not solely due to differing \nexperience levels. In fact, the connec\u00ad\ntion persists in every single experience \ngroup, as shown in Figure 5.\nVariation over Time\nIts connection to perceived productiv\u00ad\nity motivates a closer look at the accep\u00ad\ntance rate and what factors influence \nit. Acceptance rate typically increases \nover the board when the model or un\u00ad\nderlying prompt-crafting techniques \nare improved. But even if these con\u00ad\nditions are held constant (the study \nperiod did not see changes to either), \nthere are more fine-grained temporal \npatterns emerging.\nFor coherence of the cultural impli\u00ad\ncations of time of day and weekdays, \nall data in this section was restricted \nto users from the U.S. (whether in \nthe survey or not). We used the same \ntime frame as for the investigation in \nthe previous section. In the absence \nof more fine-grained geolocation, we \nused the same time zone to interpret \ntimestamps and for day boundaries \n(PST), recognizing this will introduce \nsome level of noise due to the inhomo\u00ad\ngeneity of U.S. time zones.\nNevertheless, we observe strong \nregular patterns in overall acceptance \nrate (Figure 6). These lead us to distin\u00ad\nguish three different time regimes, all \nof which are statistically significantly \ndistinct at \u200bp\u2007 <\u2007 0\u2009.\u2009001%\u200b (using boot\u00ad\nstrap resampling):\n\t\n\u02f2 The weekend: Saturdays and Sun\u00ad\ndays, where the average acceptance \nrate is comparatively high at \u200b23\u2009.\u20095%\u200b.\n\t\n\u02f2 Typical non-working hours during \nthe week: evenings after 4:00 pm PST \nExperienced \ndevelopers who \nare already highly \nskilled are less \nlikely to write \nbetter code with \nCopilot, but Copilot \ncan assist their \nproductivity in other \n"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_016",
    "source_id": "CopilotCACM2022",
    "text": " (Figure 6). These lead us to distin\u00ad\nguish three different time regimes, all \nof which are statistically significantly \ndistinct at \u200bp\u2007 <\u2007 0\u2009.\u2009001%\u200b (using boot\u00ad\nstrap resampling):\n\t\n\u02f2 The weekend: Saturdays and Sun\u00ad\ndays, where the average acceptance \nrate is comparatively high at \u200b23\u2009.\u20095%\u200b.\n\t\n\u02f2 Typical non-working hours during \nthe week: evenings after 4:00 pm PST \nExperienced \ndevelopers who \nare already highly \nskilled are less \nlikely to write \nbetter code with \nCopilot, but Copilot \ncan assist their \nproductivity in other \nways.\n\u2022 more online\nAll appendices \nfor this article \ncan be found \nin the online \nsupplemental \nfile at https://\ndl.acm.org/\ndoi/10.1145/\n3633453.\n62    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\n20.\tSvyatkovskiy, A., Deng, S.K., Fu, S., and Sundaresan, \nN. Intellicode compose: Code generation using \ntransformer. In \u00a0Proceedings of the 28th ACM Joint \nEuropean Software Eng. Conf. and Symp. on the \nFoundations of Software Eng., P. Devanbu, M.B. \nCohen, and T. Zimmermann (eds). ACM, (Nov. 2020), \n1433\u20131443; 10.1145/3368089.3417058\n21.\t Svyatkovskiy, A. et al. Fast and memory-efficient \nneural code completion. In Proceedings of the \n18th IEEE/ACM Intern. Conf. on Mining Software \nRepositories,\u00a0(May 2021, 329\u2013340; 10.1109/\nMSR52588.2021.00045\n22.\t Vaithilingam, P., Zhang, T., and Glassman, E. \nExpectation vs. experience: Evaluating the usability \nof code generation tools powered by large language \nmodels. In Proceedings of the 2022 Conf. on Human \nFactors in Computing Systems.\n23.\t Vaithilingam, P., Zhang, T., and Glassman, E.L. \nExpectation vs. experience: Evaluating the usability \nof code generation tools powered by large language \nmodels. In Proceedings of the CHI Conf. on \nHuman Factors in Computing Systems,\u00a0Association \nfor Computing Machinery,\u00a0Article 332\u00a0(2022), 7; \n10.1145/3491101.3519665\n24.\t Wagner, S. and Ruhe, M. A systematic review of \nproductivity factors in software development. arXiv \npreprint arXiv:1801.06475\u00a0(2018).\n25.\t Wang, D. et al. From human-human collaboration to \nhuman-AI collaboration: Designing AI systems that \ncan work together with people. In Proceedings of \nthe 2020 CHI Conf. on Human Factors in Computing \nSystems\u00a0(2020), 1\u20136.\n26.\t Weisz, J.D. et al."
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_017",
    "source_id": "CopilotCACM2022",
    "text": " \nfor Computing Machinery,\u00a0Article 332\u00a0(2022), 7; \n10.1145/3491101.3519665\n24.\t Wagner, S. and Ruhe, M. A systematic review of \nproductivity factors in software development. arXiv \npreprint arXiv:1801.06475\u00a0(2018).\n25.\t Wang, D. et al. From human-human collaboration to \nhuman-AI collaboration: Designing AI systems that \ncan work together with people. In Proceedings of \nthe 2020 CHI Conf. on Human Factors in Computing \nSystems\u00a0(2020), 1\u20136.\n26.\t Weisz, J.D. et al. Perfection not required? Human-AI \npartnerships in code translation. In Proceedings of \nthe 26th Intern. Conf. on Intelligent User Interfaces,\u00a0T. \nHammond et al (eds). ACM, (April 2021), 402\u2013412; \n10.1145/3397481.3450656\n27.\t Winters, T., Manshreck, T., and Wright, H. Software \nEngineering at Google: Lessons Learned from \nProgramming Over Time. O\u2019Reilly Media\u00a0(2020).\n28.\t Wold, S., Sj\u00f6str\u00f6m, M., and Eriksson, L. PLS-regression: \nA basic tool of chemometrics. Chemometrics and \nIntelligent Laboratory Systems 58, 2 (2001), 109\u2013130; \n10.1016/S0169-7439(01)00155-1.\n29.\t Zhou, W., Kim, S., Murali, V., and Ari Aye, G. Improving \ncode autocompletion with transfer learning. \nCoRR abs/2105.05991\u00a0(2021); https://arxiv.org/\nabs/2105.05991\nAlbert Ziegler (wunderalbert@github.com ) is a principal \nresearcher at GitHub, Inc., San Francisco, CA, USA.\nEirini Kalliamvakou is a staff researcher at GitHub, Inc., \nSan Francisco, CA, USA.\nX. Alice Li is a staff researcher for Machine Learning at \nGitHub,\u00a0San Francisco, CA, USA.\nAndrew Rice is a principal researcher at GitHub, Inc., San \nFrancisco, CA, USA.\nDevon Rifkin is a principal research engineer at GitHub, \nInc., San Francisco, CA, USA.\nShawn Simister is a staff software engineer at GitHub, \nInc., San Francisco, CA, USA.\nGanesh Sittampalam is a principal software engineer at \nGitHub, Inc.,\u00a0San Francisco, CA, USA.\nEdward Aftandilian is a principal researcher at GitHub, \nInc., San Francisco, CA, USA.\nFigure 6. Average acceptance rate during the week. Each point represents the average \nfor a one-hour period, whereas the shaded ribbon shows the min-max variation during \nthe observed four-week period.\nSaturday\n12:00\n26%\noff hours\nDaily and weekly patterns in acceptance rate in the US\n(all users between 2022-01-15 and 2022-02-12)\nweekend\nworking hours\n24%\n22%\n20%\nSunday\n12:00\nMonday\n12:00\nTuesday"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_018",
    "source_id": "CopilotCACM2022",
    "text": "ittampalam is a principal software engineer at \nGitHub, Inc.,\u00a0San Francisco, CA, USA.\nEdward Aftandilian is a principal researcher at GitHub, \nInc., San Francisco, CA, USA.\nFigure 6. Average acceptance rate during the week. Each point represents the average \nfor a one-hour period, whereas the shaded ribbon shows the min-max variation during \nthe observed four-week period.\nSaturday\n12:00\n26%\noff hours\nDaily and weekly patterns in acceptance rate in the US\n(all users between 2022-01-15 and 2022-02-12)\nweekend\nworking hours\n24%\n22%\n20%\nSunday\n12:00\nMonday\n12:00\nTuesday\n12:00\nweekday and time (PST)\nacceptance rate\nWednesday\n12:00\nThursday\n12:00\nFriday\n12:00\n960\u2013970; 10.1109/ICSE.2019.00101\n8.\t Hendrycks, D. et al. Measuring coding challenge \ncompetence with APPS. CoRR abs/2105.09938, \n(2021); https://arxiv.org/abs/2105.09938\n9.\t Hindle, A. et al. On the naturalness of software. In 34th \nIntern. Conf. on Software Engineering,\u00a0M. Glinz, G.C. \nMurphy, and M. Pezz\u00e8 (eds). IEEE Computer Society, \nJune 2012, 837\u2013847; 10.1109/ICSE.2012.6227135\n10.\t Jaspan, C. and Sadowski, C. No single metric captures \nproductivity. Rethinking Productivity in Software \nEngineering, (2019), 13\u201320.\n11.\t Kulal, S. et al. Spoc: Search-based pseudocode to code. \nIn Proceedings of Advances in Neural Information \nProcessing Systems 32,\u00a0H.M. Wallach et al (eds), Dec. \n2019, 11883\u201311894; https://bit.ly/3H7YLtF\n12.\t Meyer, A.N., Barr, E.T., Bird, C., and Zimmermann, \nT. Today was a good day: The daily life of software \ndevelopers. IEEE Transactions on Software \nEngineering 47, 5 (2019), 863\u2013880.\n13.\t Meyer, A.N. et al. The work life of developers: Activities, \nswitches and perceived productivity. IEEE Transactions \non Software Engineering 43, 12 (2017), 1178\u20131193.\n14.\t Meyer, A.N., Fritz, T., Murphy, G.C., and Zimmermann, \nT. Software developers\u2019 perceptions of productivity. In \nProceedings of the 22nd ACM SIGSOFT Intern. Symp. \non Foundations of Software Engineering\u00a0(2014), 19\u201329.\n15.\t Murphy-Hill, E. et al. What predicts software \ndevelopers\u2019 productivity? IEEE Transactions on \nSoftware Engineering 47, 3 (2019), 582\u2013594.\n16.\t Peng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. \nThe impact of AI on developer productivity"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_019",
    "source_id": "CopilotCACM2022",
    "text": "7), 1178\u20131193.\n14.\t Meyer, A.N., Fritz, T., Murphy, G.C., and Zimmermann, \nT. Software developers\u2019 perceptions of productivity. In \nProceedings of the 22nd ACM SIGSOFT Intern. Symp. \non Foundations of Software Engineering\u00a0(2014), 19\u201329.\n15.\t Murphy-Hill, E. et al. What predicts software \ndevelopers\u2019 productivity? IEEE Transactions on \nSoftware Engineering 47, 3 (2019), 582\u2013594.\n16.\t Peng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. \nThe impact of AI on developer productivity: Evidence \nfrom GitHub Copilot. arXiv:2302.06590 [cs.SE]\u00a0(2014)\n17.\t Ram\u00edrez, Y.W. and Nembhard, D.A. Measuring \nknowledge worker productivity: A taxonomy. J. of \nIntellectual Capital 5, 4 (2004), 602\u2013628.\n18.\t See, A., Roller, S., Kiela, D., and Weston, J. What makes \na good conversation? How controllable attributes \naffect human judgments. In Proceedings of the 2019 \nConf. of the North American Chapter of the Assoc. \nfor Computational Linguistics: Human Language \nTechnologies 1,\u00a0J. Burstein, C. Doran, and T. Solorio \n(eds). Assoc. for Computational Linguistics, (June \n2019), 1702\u20131723; 10.18653/v1/n19-1170\n19.\t Storey, M. et al. Towards a theory of software \ndeveloper job satisfaction and perceived productivity. \nIn\u00a0Proceedings of the IEEE Trans. on Software \nEngineering 47, 10 (2019), 2125\u20132142.\nprogramming parallels to factors \nsuch as specificity and repetition that \nhave been identified to affect human \njudgements of conversation quality.18 \nResearchers have already investigated \nthe benefits of natural-language feed\u00ad\nback to guide program synthesis,2\u00a0so \nthe conversational framing of coding \ncompletions is not a radical proposal. \nBut neither is it one we have seen fol\u00ad\nlowed yet.\b\nReferences\n1.\t Amann, S., Proksch, S., Nadi, S., and Mezini, M. A \nstudy of visual studio usage in practice. In IEEE 23rd \nIntern. Conf. on Software Analysis, Evolution, and \nReengineering 1. IEEE Computer Society, (March \n2016), 124\u2013134; 10.1109/SANER.2016.39\n2.\t Austin, J. et al. Program synthesis with large language \nmodels. CoRR abs/2108.07732\u00a0(2021); https://arxiv.\norg/abs/2108.07732\n3.\t Ari Aye, G., Kim, S., and Li, H. Learning autocompletion \nfrom real-world datasets. In\u00a0Proceedings of the 43rd \nIEEE/ACM Intern. Conf. on Software Engineering: \nSoftware Engineering in Practice,\u00a0(May 2021), \n131\u2013139; 10.1109/IC"
  },
  {
    "chunk_id": "CopilotCACM2022_chunk_020",
    "source_id": "CopilotCACM2022",
    "text": " Computer Society, (March \n2016), 124\u2013134; 10.1109/SANER.2016.39\n2.\t Austin, J. et al. Program synthesis with large language \nmodels. CoRR abs/2108.07732\u00a0(2021); https://arxiv.\norg/abs/2108.07732\n3.\t Ari Aye, G., Kim, S., and Li, H. Learning autocompletion \nfrom real-world datasets. In\u00a0Proceedings of the 43rd \nIEEE/ACM Intern. Conf. on Software Engineering: \nSoftware Engineering in Practice,\u00a0(May 2021), \n131\u2013139; 10.1109/ICSE-SEIP52600.2021.00022\n4.\t Beller, M., Orgovan, V., Buja, S., and Zimmermann, \nT. Mind the gap: On the relationship between \nautomatically measured and self-reported \nproductivity. IEEE Software 38, 5 (2020), 24\u201331.\n5.\t Chen, M. et al. Evaluating large language models \ntrained on code. CoRR abs/2107.03374\u00a0(2021); \nhttps://arxiv.org/abs/2107.03374\n6.\t Forsgren, N. et al. The SPACE of developer \nproductivity: There\u2019s more to it than you think. Queue \n19, 1 (2021), 20\u201348.\n7.\t\nHellendoorn, V.J., Proksch, S., Gall, H.C., and Bacchelli, \nA. When code completion fails: A case study on \nreal-world completions. In Proceedings of the 41st \nIntern. Conf. on Software Engineering,\u00a0J.M. Atlee, T. \nBultan, and J. Whittle (eds). IEEE/ACM, (May 2019), \nThis work is licensed under a \nhttp://creativecommons.org/licenses/by/4.0/\nWatch the authors discuss \nthis work in the exclusive \nCommunications video. \nhttps://cacm.acm.org/videos/\nmeasuring-github-copilot\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    63\n"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_001",
    "source_id": "CopilotRobustness2023",
    "text": "On the Robustness of Code Generation Techniques:\nAn Empirical Study on GitHub Copilot\nAntonio Mastropaolo\u2217, Luca Pascarella\u2217, Emanuela Guglielmi\u2020, Matteo Ciniselli\u2217\nSimone Scalabrino\u2020, Rocco Oliveto\u2020, Gabriele Bavota\u2217\n\u2217SEART @ Software Institute, Universit\u00e0 della Svizzera italiana (USI), Switzerland\n\u2020University of Molise, Italy\nAbstract\u2014Software engineering research has always being\nconcerned with the improvement of code completion approaches,\nwhich suggest the next tokens a developer will likely type while\ncoding. The release of GitHub Copilot constitutes a big step\nforward, also because of its unprecedented ability to automati-\ncally generate even entire functions from their natural language\ndescription. While the usefulness of Copilot is evident, it is\nstill unclear to what extent it is robust. Speci\ufb01cally, we do\nnot know the extent to which semantic-preserving changes in\nthe natural language description provided to the model have\nan effect on the generated code function. In this paper we\npresent an empirical study in which we aim at understanding\nwhether different but semantically equivalent natural language\ndescriptions result in the same recommended function. A negative\nanswer would pose questions on the robustness of deep learning\n(DL)-based code generators since it would imply that developers\nusing different wordings to describe the same code would obtain\ndifferent recommendations. We asked Copilot to automatically\ngenerate 892 Java methods starting from their original Javadoc\ndescription. Then, we generated different semantically equivalent\ndescriptions for each method both manually and automatically,\nand we analyzed the extent to which predictions generated by\nCopilot changed. Our results show that modifying the description\nresults in different code recommendations in \u223c46% of cases.\nAlso, differences in the semantically equivalent descriptions might\nimpact the correctness of the generated code (\u00b128%).\nIndex Terms\u2014Empirical Study, Recommender Systems\nI. INTRODUCTION\nOne of the long lasting dreams in software engineering re-\nsearch is the automated generation of source code. Towards this\ngoal, several approaches have been proposed. The \ufb01rst attempts\ntargeted the relatively simpler problem of code completion,\nthat has been tackled exploiting historical information [50],\ncoding patterns mined from software repositories [21], [42],\n[56], [10], [41], [45], [19] and, more recently, Deep Learning\n(DL) models [62], [27], [29], [8], [53], [15].\nThe release of GitHub Copilot [14] pushed the capabilities\nof these tools to whole new levels. The large-scale training\nperformed on the OpenAI\u2019s Codex model allows Copilot to\nnot limit its recommendations to few code tokens/statements\nthe developer is likely to write: Copilot is able to automatically\nsynthesize entire functions just starting from their signature\nand natural language descriptions.\nThis new generation of code recommender systems has the\npotential to change the way in which developers write code\n[18] and comes with a number of questions concerning how to\neffectively exploit them to maximize developers\u2019 productivity.\nIntuitively, the ability of the developer to provide \u201cproper\u201d in-\nputs to the model will become central to boost the effectiveness\nof its recommendations. In the concrete"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_002",
    "source_id": "CopilotRobustness2023",
    "text": " capabilities\nof these tools to whole new levels. The large-scale training\nperformed on the OpenAI\u2019s Codex model allows Copilot to\nnot limit its recommendations to few code tokens/statements\nthe developer is likely to write: Copilot is able to automatically\nsynthesize entire functions just starting from their signature\nand natural language descriptions.\nThis new generation of code recommender systems has the\npotential to change the way in which developers write code\n[18] and comes with a number of questions concerning how to\neffectively exploit them to maximize developers\u2019 productivity.\nIntuitively, the ability of the developer to provide \u201cproper\u201d in-\nputs to the model will become central to boost the effectiveness\nof its recommendations. In the concrete example of GitHub\nCopilot, the natural language description provided to the model\nto automatically generate a code function could substantially\nin\ufb02uence the model output. This means that two developers\nproviding different natural language descriptions for the same\nfunction they would like to automatically generate could receive\ntwo different recommendations. While this would be \ufb01ne in\ncase the two descriptions are actually different in the semantics\nof what they describe, receiving different recommendations for\nsemantically equivalent natural language descriptions would\npose questions on the robustness and usability of DL-based\ncode recommenders.\nThis is the main research question we investigate in this\npaper: We study the extent to which different semantically\nequivalent natural language descriptions of a function result in\ndifferent recommendations (i.e., different synthesized functions)\nby GitHub Copilot. The latter is selected as representative of\nDL-based code recommenders since it is the de facto state-of-\nthe-art tool when it comes to code generation.\nWe collected from an initial set of 1,401 open source projects\na set of 892 Java methods that are (i) accompanied by a Doc\nComment for the Javadoc tool, and (ii) exercised by a test\nsuite written by the project\u2019s contributors. Then, as done in\nthe literature [23], [32], we considered the \ufb01rst sentence of\nthe Doc Comments as a \u201cnatural language description\u201d of the\nmethod. We refer to this sentence as the \u201coriginal\u201d description.\nWe preliminarily checked whether existing automated para-\nphrasing techniques are suitable for robustness testing, i.e., if\nthey can be used to create semantically equivalent descriptions\nof the methods to generate. We validated two state-of-the-\nart approaches in this scenario: PEGASUS [66], a DL-based\nparaphrasing tool, and Translation Pivoting (TP), a heuristic-\nbased approach. We used both techniques to generate a\nparaphrase for each original description in our dataset. Then,\nwe manually inspected the obtained paraphrases and classi\ufb01ed\nthem as semantically equivalent or not. We obtained positive\nresults for both the approaches, with TP being the best\nperforming one with 77% of valid paraphrases.\nThen, to answer our main research question, we generated\ndifferent paraphrases for each original description.\narXiv:2302.00438v1  [cs.SE]  1 Feb 2023\nWe used the two previously described automated approaches,\ni.e., PEGASUS and TP, and we also manually generated\nparaphrases by distributing the original descriptions among four\nof the authors, each of which"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_003",
    "source_id": "CopilotRobustness2023",
    "text": "paraphrase for each original description in our dataset. Then,\nwe manually inspected the obtained paraphrases and classi\ufb01ed\nthem as semantically equivalent or not. We obtained positive\nresults for both the approaches, with TP being the best\nperforming one with 77% of valid paraphrases.\nThen, to answer our main research question, we generated\ndifferent paraphrases for each original description.\narXiv:2302.00438v1  [cs.SE]  1 Feb 2023\nWe used the two previously described automated approaches,\ni.e., PEGASUS and TP, and we also manually generated\nparaphrases by distributing the original descriptions among four\nof the authors, each of which was in charge of paraphrasing a\nsubset of them.\nTherefore, for each original description, we obtained a set of\nsemantically equivalent paraphrased descriptions. We provided\nboth the original and the paraphrased descriptions as input to\nCopilot, asking it to generate the corresponding method body.\nWe analyze the percentage of cases in which the paraphrased\ndescriptions result in a different code prediction as compared\nto the original one, with a particular focus on the impact\non the prediction quality, e.g., cases in which the original\ndescription resulted in the recommendation of a method passing\nits associated test cases while switching to a paraphrased\ndescription made Copilot recommending a method failing its\nrelated tests.\nOur results show that paraphrasing a description results\nin a change in the code recommendation in \u223c46% of cases.\nThe resulting changes also cause substantial variations in\nthe percentage of correct predictions. Such \ufb01ndings indicate\nthe central role played by the model\u2019s input in the code\nrecommendation and the need for testing and improving the\nrobustness of DL-based code generators.\nData and code used in our study are publicly available [6].\nII. STUDY DESIGN\nThe goal of our study is to understand how robust is a state-\nof-the-art DL-based code completion approach (i.e., GitHub\nCopilot). We aim at answering the following research questions:\nRQ0: To what extent can automated paraphrasing\ntechniques be used to test the robustness of DL-based\ncode generators? Not always natural language processing\ntechniques can be used out of the box on software-related\ntext [35]. Therefore, with this preliminary RQ, we want\nto understand whether existing automated techniques for\ngenerating natural language paraphrases are suitable for SE\ntask at hand (i.e., paraphrasing a function description).\nRQ1: To what extent is the output of GitHub Copilot\nin\ufb02uenced by the code description provided as input by the\ndeveloper? This RQ aims at understanding whether Copilot,\nas a representative of DL-based code generators, is likely to\ngenerate different recommendations for different semantically\nequivalent natural language descriptions provided as input.\nIn the following we detail the context for our study (Sec-\ntion II-A) and how we collected (Section II-B) and analyzed\n(Section II-C) the data needed to answer our RQs.\nA. Context Selection\nThe context of our study is represented by 892 Java methods\ncollected through the following process. We selected all GitHub\nJava repositories having at least 300 commits, 50 contributors,\nand 25 stars. These \ufb01lters"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_004",
    "source_id": "CopilotRobustness2023",
    "text": "enced by the code description provided as input by the\ndeveloper? This RQ aims at understanding whether Copilot,\nas a representative of DL-based code generators, is likely to\ngenerate different recommendations for different semantically\nequivalent natural language descriptions provided as input.\nIn the following we detail the context for our study (Sec-\ntion II-A) and how we collected (Section II-B) and analyzed\n(Section II-C) the data needed to answer our RQs.\nA. Context Selection\nThe context of our study is represented by 892 Java methods\ncollected through the following process. We selected all GitHub\nJava repositories having at least 300 commits, 50 contributors,\nand 25 stars. These \ufb01lters have been used in an attempt to\nexclude personal/toy projects.\nWe also excluded forked projects to avoid duplicates. The\ndecision to focus on a single programming language aimed\ninstead at simplifying the non-trivial toolchain needed to run\nour study. The whole repositories selection process has been\nperformed using the GitHub search tool by Dabic et al. [17].\nAt this stage, we obtained 1,401 repositories.\nIn our experimental design, we use the passing/failing tests as\na proxy to assess the correctness of the predictions generated by\nCopilot. Thus, we need the projects to use a testing framework\nand to be compilable. We selected all projects that used Maven\nas build automation tool and for which the build of their latest\nrelease succeeded. We obtained 214 repository. By parsing\nthe POM (Project Object Model) \ufb01le1 we only considered\nprojects having as dependencies both jUnit [4] \u2014 a well-\nknown unit testing framework \u2014 and Jacoco [2] \u2014 a code\ncoverage library. We analyzed the Jacoco reports and selected\nas methods subject of our experiment those having at least\n75% of statement coverage. This gives us con\ufb01dence that the\nrelated test cases exercise an acceptable number of behaviors\nand, therefore, could allow to spot cases in which different\ngenerated functions for semantically-equivalent descriptions\nactually behave differently. We are aware that passing tests does\nnot imply correctness. We discuss this aspect in Section IV.\nGiven our goal to use the method\u2019s description as input for\nCopilot, we also exclude methods not having any associated\nDoc Comment for the Javadoc tool. Then, we process the\nDoc Comment of each method in our dataset to extract from\nit the \ufb01rst sentence (i.e., from the beginning to the \ufb01rst \u201c.\u201d).\nThis is the same approach used in the literature when building\ndatasets aimed at training DL-based techniques for Java code\nsummarization (see e.g., [23], [32]), with the training set\ncomposed by pairs <method, code_description>, with\nthe latter being the \ufb01rst sentence of the Doc Comment. To\nensure that the extracted sentence contains enough wording for\nthe code description, we exclude all methods having less than\n10 tokens in the extracted \ufb01rst sentence, since their description\nmay not be suf\ufb01cient for synthesizing the method.\nTABLE I\nOUR DATASET OF 892 METHODS FROM 33 REPOSITORIES\nAvg\nMedian\nSt. Dev.\n# Tokens\n154.3\n92.0\n218.2\n# Parameters\n1.6\n1.0\n"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_005",
    "source_id": "CopilotRobustness2023",
    "text": " techniques for Java code\nsummarization (see e.g., [23], [32]), with the training set\ncomposed by pairs <method, code_description>, with\nthe latter being the \ufb01rst sentence of the Doc Comment. To\nensure that the extracted sentence contains enough wording for\nthe code description, we exclude all methods having less than\n10 tokens in the extracted \ufb01rst sentence, since their description\nmay not be suf\ufb01cient for synthesizing the method.\nTABLE I\nOUR DATASET OF 892 METHODS FROM 33 REPOSITORIES\nAvg\nMedian\nSt. Dev.\n# Tokens\n154.3\n92.0\n218.2\n# Parameters\n1.6\n1.0\n1.2\n# Cyclomatic Complexity\n5.3\n3.0\n7.6\n% Coverage\n96.1\n100.0\n6.7\nThe above-described process resulted in the collection of\n892 Java methods. Table I shows descriptive statistics about\ntheir characteristics in terms of number of tokens, parameters\nand cyclomatic complexity. These three together provide an\nidea about the complexity of the task Copilot was asked to\nperform (i.e., the complexity of the methods it had to generate).\n1POM \ufb01les are used in Maven to declare dependencies towards libraries.\npublic class Hook implements Resultsable {\n \n     // Start: attributes from JSON file report\n     private final Result result = null;\n     private final Match match = null;\n \n     @JsonDeserialize(using = OutputsDeserializer.class)\n     @JsonProperty(\"output\")\n     private final Output[] outputs = new Output[0];\n \n     // foe Ruby reports\n     private final Embedding[] embeddings = new Embedding[0];\n     // End: attributes from JSON file report\n \n     @Override\n     public Result getResult() {\n         return result;\n     }\n \n     /** Return the embedding vector */\n     public Embedding[] getEmbeddings() {\n          |\n     }\n  \n     /** Checks if the hook has content meaning as it has at least \n       * attachment or result with error\n       * message. \n     */\n     public boolean hasContent() {\n         if (embeddings.length > 0) {\n           return true;\n         }\n         if (StringUtils.isNotBlank(result.getErrorMessage())) {\n             return true;\n         }\n         // TODO: hook with 'output' should be treated \n         /  as empty or not?\n         return false;\n     }\n }\nFull Context\nMethod to be predicted\npublic class Hook implements Resultsable {\n \n     // Start: attributes from JSON file report\n     private final Result result = null;\n     private final Match match = null;\n \n     @JsonDeserialize(using = OutputsDeserializer.class)\n     @JsonProperty(\"output\")\n     private final Output[] outputs = new Output[0];\n \n     // foe Ruby reports\n     private final Embedding[] embeddings = new Embedding[0];\n     // End: attributes from JSON file report\n \n     @Override\n     public Result getResult() {\n         return result;\n     }\n \n     /** Return the embedding vector */\n     public Embedding[] getEmbeddings() {\n          |\n     }\n }\nNon Full Context\nMethod to be predicted\nFig. 1. GitHub Copilot\u2019s input for both code context representations\nStatistics about the coverage show, instead, the by-design high\nstatement coverage we ensure for the included methods.\nB. Data Collection\nTo address R"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_006",
    "source_id": "CopilotRobustness2023",
    "text": "Deserialize(using = OutputsDeserializer.class)\n     @JsonProperty(\"output\")\n     private final Output[] outputs = new Output[0];\n \n     // foe Ruby reports\n     private final Embedding[] embeddings = new Embedding[0];\n     // End: attributes from JSON file report\n \n     @Override\n     public Result getResult() {\n         return result;\n     }\n \n     /** Return the embedding vector */\n     public Embedding[] getEmbeddings() {\n          |\n     }\n }\nNon Full Context\nMethod to be predicted\nFig. 1. GitHub Copilot\u2019s input for both code context representations\nStatistics about the coverage show, instead, the by-design high\nstatement coverage we ensure for the included methods.\nB. Data Collection\nTo address RQ0, we experiment with two state-of-the-art\nparaphrasing techniques. The \ufb01rst is named PEGASUS [66],\nand it is a sequence-to-sequence DL model pre-trained using\nself-supervised objectives speci\ufb01cally tailored for abstractive\ntext summarization and \ufb01ne-tuned for the task of paraphrasing\n[5]. As for the second technique, we opted for Translation\nPivoting (TP).\nSuch a technique relies on natural language translation\nservices to translates the original description o from English\ninto a foreign language (i.e., French), obtaining oE \u2192F. Then,\noE \u2192F is translated back in the original language (oE\u2192F \u2192E)\nobtaining a paraphrase.\nWe provide each technique with the original description\nas input. TP failed to generate a valid paraphrase (i.e., a\nsentence different from the original one) in 100 cases (out\nof 892), while this only happened once with PEGASUS. We\nmanually analyzed whether the valid paraphrases we obtained\nwere actually semantically equivalent to the original description.\nFor such a process, each of the 1,683 paraphrases (892 for\neach of the two tools minus the 101 invalid ones) has been\nindependently inspected by two authors who classi\ufb01ed it as\nsemantically equivalent or not. Con\ufb02icts, that arisen in 11.9%\n(PEGASUS) and 16.54% (TP) of cases, have been solved by\na third author not involved in the \ufb01rst place.\nConcerning RQ1, we start from the original description\nand we generate semantically equivalent descriptions by (i)\nusing the two automated tools, i.e., PEGASUS [5] and TP,\nand (ii) manually generating paraphrases. For the manual\nparaphrasing, we split the 892 methods together with their\noriginal description into four sets and assigned each of\nthem to one author. Each author was in charge of writing a\nsemantically equivalent but different description of the method\nby looking at its code and original description. This resulted\nin a dataset (available in [6]) in which, for each subject\nmethod, we have its original and paraphrased description. In\nthe end, for each original sentence, we had between one and\nthree paraphrases: paraphrasedPEGASUS, paraphrasedTP, and\nparaphrasedmanual. While paraphrasedmanual is available for\nall the methods, paraphrasedPEGASUS and paraphrasedTP are\n"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_007",
    "source_id": "CopilotRobustness2023",
    "text": " we split the 892 methods together with their\noriginal description into four sets and assigned each of\nthem to one author. Each author was in charge of writing a\nsemantically equivalent but different description of the method\nby looking at its code and original description. This resulted\nin a dataset (available in [6]) in which, for each subject\nmethod, we have its original and paraphrased description. In\nthe end, for each original sentence, we had between one and\nthree paraphrases: paraphrasedPEGASUS, paraphrasedTP, and\nparaphrasedmanual. While paraphrasedmanual is available for\nall the methods, paraphrasedPEGASUS and paraphrasedTP are\nnot. Indeed, we exclude the cases in which each of such tools\nfailed to generate paraphrases (1 and 100, respectively) and the\nones that were not considered as semantically equivalent in our\nmanual check (based on the results of RQ0). The maximum\nnumber of semantically equivalent paraphrases is 2,575 (up to\n891 with PEGASUS, up to 792 with TP, and 892 manually).\nThe paraphrases, as well as the original description, have\nbeen used as input to Copilot, simulating developers asking it\nto synthesize the same Java method by using different natural\nlanguage descriptions. At the time of our study, Copilot does not\nprovide open APIs to access its services. The only way to use\nit is through a plugin for one of the supported IDEs. Manually\ninvoking Copilot for the thousands of times needed (up to 6,934,\nas we will explain later) was clearly not an option. For this\nreason, we developed a toolchain able to automatically invoke\nCopilot on the subject instances: We exploit the AppleScript\nlanguage to automate this task on a MacBook Pro, simulating\nthe developer\u2019s interaction with Visual Studio Code (vscode).\nFor each method mi in our dataset, we created up to four\ndifferent versions of the Java \ufb01le containing it (one for each\nof the experimented descriptions). In all such versions, we\n(i) emptied mi\u2019s body, just leaving the opening and closing\ncurly bracket delimiting it; and (ii) removed the Doc Comment,\nreplacing it with one of the four code descriptions we prepared.\nStarting from these \ufb01les, the automation script we imple-\nmented (available in our replication package [6]) performs the\nfollowing steps on each \ufb01le Fi.\nFirst, it opens Fi in vscode and moves the cursor within the\ncurly brackets of the method mi of interest. Then, it presses\n\u201creturn\u201d to invoke Copilot, waiting up to 20 seconds for its\nrecommendation. Finally, it stores the received recommendation,\nthat could possibly be empty (i.e., no recommendation received).\nTo better understand this process, the top part of Fig. 1 depicts\nhow the invocation of Copilot is performed. The gray box\nrepresents the whole Java \ufb01le (i.e., the context used by Copilot\nfor the prediction). The emptied method (i.e.,getEmbeddings)\nis framed with a black border, with the cursor indicating the\nposition in which Copilot is invoked. The green comment on\ntop of the method represents one of the descriptions we"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_008",
    "source_id": "CopilotRobustness2023",
    "text": " method mi of interest. Then, it presses\n\u201creturn\u201d to invoke Copilot, waiting up to 20 seconds for its\nrecommendation. Finally, it stores the received recommendation,\nthat could possibly be empty (i.e., no recommendation received).\nTo better understand this process, the top part of Fig. 1 depicts\nhow the invocation of Copilot is performed. The gray box\nrepresents the whole Java \ufb01le (i.e., the context used by Copilot\nfor the prediction). The emptied method (i.e.,getEmbeddings)\nis framed with a black border, with the cursor indicating the\nposition in which Copilot is invoked. The green comment on\ntop of the method represents one of the descriptions we created.\nAs it can be seen, Fig. 1 includes for the same Java \ufb01le two\ndifferent scenarios, named Full context and Non-full context. In\nthe Full context scenario (top part of Fig. 1) we provide Copilot\nwith the code preceding and following the emptied method,\nsimulating a developer adding a new method in an already\nexisting Java \ufb01le. In the Non-full context scenario, instead, we\nonly provide as context the code preceding the emptied method\n(bottom part of Fig. 1), simulating a developer writing a Java\n\ufb01le sequentially and implementing a new method.\nThe basic idea behind these two scenarios is that the\ncontextual information provided to Copilot can play a role\nin its ability to predict the emptied method. Overall, the\nmaximum number of Copilot invocations needed for our study\nis 6,934 (892 original descriptions plus up to 2,575 paraphrases,\neach of which for 2 context scenarios). After having collected\nCopilot\u2019s recommendations, we found out that sometimes they\ndid not only include the method we asked to generate, but\nalso additional code (e.g., other methods). To simplify the data\nanalysis and to make sure we only consider one recommended\nmethod, we wrote a simple parsing tool to only extract from\nthe generated recommendation the \ufb01rst valid method (if any).\nC. Data Analysis\nConcerning RQ0, we report the number and the percentage\nof 892 methods for which automatically generated paraphrases\n(i.e., those generated by PEGASUS and by TP) have been\nclassi\ufb01ed as semantically equivalent to the original description.\nThis provides an idea of how reliable these tools are when\nused for testing the robustness of DL-based code generators.\nAlso, this analysis allows to exclude from RQ1 automatically\ngenerated paraphrases that are not semantically equivalent.\nTo answer RQ1, we preliminarily assess how far the\nparaphrased descriptions are from the original ones (i.e., the\npercentage of changed words) by computing the normalized\ntoken-level Levenshtein distance [31] (NTLev) between the\noriginal (do) and any paraphrased description (dp):\nNTLev(do, dp) =\nTLev(do, dp)\nmax({|do|, |dp|})\nwith TLev representing the token-level Levenshtein distance\nbetween the two descriptions.\nWhile the original Levenshtein distance works at character-\nlevel, it can be easily generalized at token-level (each unique\ntoken is represented as a speci\ufb01c character). In this"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_009",
    "source_id": "CopilotRobustness2023",
    "text": " we preliminarily assess how far the\nparaphrased descriptions are from the original ones (i.e., the\npercentage of changed words) by computing the normalized\ntoken-level Levenshtein distance [31] (NTLev) between the\noriginal (do) and any paraphrased description (dp):\nNTLev(do, dp) =\nTLev(do, dp)\nmax({|do|, |dp|})\nwith TLev representing the token-level Levenshtein distance\nbetween the two descriptions.\nWhile the original Levenshtein distance works at character-\nlevel, it can be easily generalized at token-level (each unique\ntoken is represented as a speci\ufb01c character). In this case, a token\nis a word in the text. The normalized token-level Levenshtein\ndistance provides an indication of the percentage of words\nthat must be changed in the original description to obtain a\nparaphrased one.\nThen, we analyze the percentage of methods for which the\nparaphrased descriptions result in a different method prediction\nas compared to the original one. When they are different, we\nalso assess how far the methods obtained by using a given\nparaphrased description is from the method recommended\nwhen providing the original description as input. Also in this\ncase we use the token-level Levenshtein distance as metric. The\nlatter is computed with the same formula previously reported\nfor the natural text descriptions; in this case, however, the\ntokens are not the words but the Java syntactic tokens. Thus,\nNTLev indicates in this case the percentage of code tokens\nthat must be changed to convert the method obtained through\nthe original description into the one recommended with one\nof the paraphrases.\nFinally, we study the \u201cquality\u201d of the recommendations\nobtained using the different descriptions both in the Full\ncontext and Non-full context scenarios. Given the sets of\nmethods generated from the original description and each of the\nparaphrasing approach considered, we present the percentages\nof methods for which Copilot: (i) synthesized a method passing\nall the related test cases (PASS); (ii) synthesized a method that\ndoes not pass at least one of the test cases (FAIL); (iii) generated\nan invalid method (i.e., with syntactic errors) (ERROR); (iv)\ndid not generate any method (EMPTY). Syntactic errors have\nbeen identi\ufb01ed as recommendations for which Java Parser [3]\ndid not manage to identify a valid recommended method (i.e.,\ncases in which Java Parser fails to identify a method node in\nthe AST generated for the obtained recommendation). On top\nof the passing/failing methods, we also compute the token-level\nLevenshtein distance and the CodeBLEU [49] between each\nsynthesized method and the target one (i.e., the one originally\nimplemented by the developers). CodeBLEU measures how\nsimilar two methods are. Differently from the BLEU score\n[46], CodeBLEU evaluates the predicted code considering not\nonly the overlapping n-grams but also syntactic and semantic\nmatch of the two pieces of code (predicted and reference) [49].\nD. Replication Package\nThe code and data used in our study are publicly available\n[6]. In particular, we provide (i) the dataset of"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_010",
    "source_id": "CopilotRobustness2023",
    "text": " the obtained recommendation). On top\nof the passing/failing methods, we also compute the token-level\nLevenshtein distance and the CodeBLEU [49] between each\nsynthesized method and the target one (i.e., the one originally\nimplemented by the developers). CodeBLEU measures how\nsimilar two methods are. Differently from the BLEU score\n[46], CodeBLEU evaluates the predicted code considering not\nonly the overlapping n-grams but also syntactic and semantic\nmatch of the two pieces of code (predicted and reference) [49].\nD. Replication Package\nThe code and data used in our study are publicly available\n[6]. In particular, we provide (i) the dataset of manually de\ufb01ned\nand automatically generated paraphrases; (ii) the AppleScript\ncode used to automate the Copilot triggering; (iii) the code used\nto compute the CodeBLEU and the Levenshtein distance; (iv)\nthe dataset of 892 methods and related tests used in our study;\n(v) the scripts used to automatically generate the paraphrased\ndescriptions using PEGASUS and TP; and (vi) all raw data\noutput of our experiments.\nResults Achieved With the Original and the Manually Paraphrased Descriptions\nUnit Test Results\n652\n644\n122\n112\n99\n32\n27\nOriginal\nParaphrased\nPASS\nERROR\nEMPTY\nFAIL\n96\nCodeBLEU\nALL\nFAIL\nPASS\n0\n1,779\n0\n271\nLevenshtein Distance on Code\nALL\nFAIL\nPASS\n2,721\n0\n1\n1,779\n2,721\n1\n0\n249\nMin:\nMax:\nFig. 2. Results achieved by Copilot when considering the Full context code representation on paraphrasesmanual.\nIII. RESULTS DISCUSSION\nAs previously explained, in RQ1 we conducted our exper-\niments both in the Full context and in the Non-full context\nscenario. Since the obtained \ufb01ndings are similar, due to space\nlimitations we only discuss in the paper the results achieved\nin the Full context scenario (i.e., the case in which we provide\nCopilot with all code preceding and following the method\nobject of the prediction). The results achieved in the Non-full\ncontext scenario are available in our replication package [6].\nA. RQ0: Evaluation of Automated Praphrase Generators\nTABLE II\nNUMBER OF SEMANTICALLY EQUIVALENT OR NONEQUIVALENT\nPARAPHRASED DESCRIPTIONS OBTAINED USING PEGASUS AND TP.\nEquivalent\nNonequivalent\nInvalid\nPEGASUS\n666 (74.7%)\n225 (25.2%)\n1 (0.1%)\nTP\n688 (77.1%)\n104 (11.7%)\n100 (11.2%)\nTable II reports the number of semantically equivalent and\nnonequivalent descriptions obtained using the two state-of-the-\nart paraphrasing techniques, namely PEGASUS and Translation\nPivoting (TP), together with the number of invalid paraphrases\ngenerated. Out of the 892 original descriptions on which they\nhave been run, PEGASUS generated 666 (75%) semantically\nequivalent descriptions, while TP went up to 688 (77"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_011",
    "source_id": "CopilotRobustness2023",
    "text": "Equivalent\nNonequivalent\nInvalid\nPEGASUS\n666 (74.7%)\n225 (25.2%)\n1 (0.1%)\nTP\n688 (77.1%)\n104 (11.7%)\n100 (11.2%)\nTable II reports the number of semantically equivalent and\nnonequivalent descriptions obtained using the two state-of-the-\nart paraphrasing techniques, namely PEGASUS and Translation\nPivoting (TP), together with the number of invalid paraphrases\ngenerated. Out of the 892 original descriptions on which they\nhave been run, PEGASUS generated 666 (75%) semantically\nequivalent descriptions, while TP went up to 688 (77%). If\nwe do not consider the invalid paraphrases, i.e., the cases for\nwhich the techniques do not actually provide any paraphrase,\nthe latter obtains \u223c87% of correctly generated paraphrases.\nThese \ufb01ndings suggest that the two paraphrasing techniques\ncan be adopted as testing tools to assess the robustness of\nDL-based code recommenders. In particular, once established\na reference description (e.g., the original description in our\nstudy), these tools can be applied to paraphrase it and verify\nwhether, using the reference and the paraphrased descriptions,\nthe code recommenders generate different predictions.\nAnswer to RQ0. State-of-the-art paraphrasing techniques\ncan be used as starting point to test the robustness of DL-\nbased code recommenders, since they are able to generate\nsemantically equivalent descriptions of a reference text in\nup to 77% of cases.\nB. RQ1: Robustness of GitHub Copilot\nPerformance of Copilot when using the original and the\nparaphrased description as input. Fig. 2 summarizes the\nperformance achieved by Copilot when using the original\ndescription (light blue) and the manually generated paraphrased\ndescription (dark blue) as input. Similarly, we report in Fig. 3\nthe performance obtained when considering the paraphrases\ngenerated with the two automated techniques, i.e., PEGASUS\nand TP (top and bottom of Fig. 3, respectively). It is worth\nnoticing that, in the latter, we only considered in the analysis\nthe paraphrases manually considered as equivalent in RQ0, i.e.,\n666 for PEGASUS and 688 for TP.\nA \ufb01rst interesting result is that, as it can be noticed\nfrom Fig. 2 and Fig. 3, the results obtained with the three\nmethodologies are very similar. For this reason, to avoid\nrepetitions, in the following, we will mainly focus on the\nresults obtained with the manually generated paraphrases.\nResults Achieved With the Original and the Automatically Generated Paraphrased Descriptions\nUnit Test Results\n479\n463\n92\n88\n87\n26\n24\nOriginal\nPegasus\nPASS\nERROR\nEMPTY\nFAIL\n73\nCodeBLEU\nALL\nFAIL\nPASS\nLevenshtein Distance on Code\nALL\nFAIL\nPASS\n1,441\n2,747\n271\n2,747\n1,441\n283\nMax:\n0\n1\n0\n0\n1\n0\nMin:\nUnit Test Results\nOriginal\n495\n87\n77\n83\n25\n23\nPASS"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_012",
    "source_id": "CopilotRobustness2023",
    "text": ", in the following, we will mainly focus on the\nresults obtained with the manually generated paraphrases.\nResults Achieved With the Original and the Automatically Generated Paraphrased Descriptions\nUnit Test Results\n479\n463\n92\n88\n87\n26\n24\nOriginal\nPegasus\nPASS\nERROR\nEMPTY\nFAIL\n73\nCodeBLEU\nALL\nFAIL\nPASS\nLevenshtein Distance on Code\nALL\nFAIL\nPASS\n1,441\n2,747\n271\n2,747\n1,441\n283\nMax:\n0\n1\n0\n0\n1\n0\nMin:\nUnit Test Results\nOriginal\n495\n87\n77\n83\n25\n23\nPASS\nERROR\nEMPTY\nFAIL\n77\nTranslation-Pivoting\n509\nCodeBLEU\nPASS\nALL\nFAIL\nLevenshtein Distance on Code\nALL\nFAIL\nPASS\n1\n2,625\n0\n165\n1\n1,779\n175\n0\n0\n1,779\nMin:\nMax:\n0\n2,625\nFig. 3. Results achieved by Copilot when considering the Full context code representation on paraphrasesPEGASUS and paraphrasesTP.\nAlso, as we will discuss, the quality of Copilot\u2019s recom-\nmendations is very similar when using the original and the\nparaphrased descriptions.\nIn Fig. 2, the bar chart in the left side reports the number of\nmethods recommended by Copilot (out of 892) that resulted in\nfailing tests, passing tests, syntactic errors, and no (i.e., empty)\nrecommendation. Looking at such a chart, the \ufb01rst thing that\nleaps to the eyes is the high percentage of Java methods (\u223c73%\nfor the original and \u223c72% for the paraphrased description)\nfor which Copilot was not able to synthesize a method passing\nthe related unit tests.\nOnly \u223c13% of instances (112 and 122 depending on the\nused description) resulted in test-passing methods. While such\na result seems to indicate limited performance of Copilot,\nit must be considered the dif\ufb01culty of the code generation\ntasks involved in our study. Indeed, we did not ask Copilot to\ngenerate simple methods possibly implementing quite popular\nroutines (e.g., a method to generate an MD5 hash from a string)\nbut rather randomly selected methods that, as shown in Table I,\nare composed, on average, by more than 150 tokens (median =\n92) and have an average cyclomatic complexity of 5.3 (median\n= 3.0).\npublic void removeListener(IChemObjectListener col){\n    if (chemObjectListeners == null) {\n        return;\n    }\n    List<IChemObjectListener> listeners = lazyChemObjectListeners();\n    if (listeners.contains(col)) {\n        listeners.remove(col);\n    }\n}\nTarget Method\nRecommended method starting from the original description\npublic void removeListener(IChemObjectListener col){\n     if (chemObjectListeners == null) {\n        return;\n     }\n     lazyChemObjectListeners().remove(col);\n}\nCodeBLEU: 0.45\nPASS\nFig. 4. Example of recommended method that passes the unit tests but reports\na low CodeBLEU score compared to the oracle (i.e., target method).\n"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_013",
    "source_id": "CopilotRobustness2023",
    "text": "3.0).\npublic void removeListener(IChemObjectListener col){\n    if (chemObjectListeners == null) {\n        return;\n    }\n    List<IChemObjectListener> listeners = lazyChemObjectListeners();\n    if (listeners.contains(col)) {\n        listeners.remove(col);\n    }\n}\nTarget Method\nRecommended method starting from the original description\npublic void removeListener(IChemObjectListener col){\n     if (chemObjectListeners == null) {\n        return;\n     }\n     lazyChemObjectListeners().remove(col);\n}\nCodeBLEU: 0.45\nPASS\nFig. 4. Example of recommended method that passes the unit tests but reports\na low CodeBLEU score compared to the oracle (i.e., target method).\nThus, we consider the successful generation of more than\n110 of these methods a quite impressive result for a code\nrecommender. The remaining \u223c15% of instances resulted\neither in a parsing error (\u223c100 methods) or in an empty\nrecommendation (\u223c30 methods).\nThe box plot in the middle part of Fig. 2 depicts the results\nachieved in terms of CodeBLEU [49] computed between\nthe recommended methods and the target one (i.e., the one\nimplemented by the original developers). Higher values indicate\nhigher similarity between the compared methods. Instead, in the\nright box plot, we show the normalized Levenshtein distance,\nfor which lower values indicate higher similarity.\nFor both metrics, we depict the distributions when con-\nsidering all generated predictions, the ones failing tests, and\nthe ones passing tests. As expected, higher (lower) values of\nCodeBLEU (Levenshtein distance) are associated with test-\npassing methods. Indeed, for the latter, the median CodeBLEU\nis \u223c0.80 (Levenshtein = \u223c0.10) as compared to the \u223c0.40\n(Levenshtein = \u223c0.58) of test-failing methods. Despite such\nan expected \ufb01nding, it is interesting to notice that 25% of\ntest-passing methods have a rather low CodeBLEU <0.50.\nFig. 4 shows an example of recommended method having\na CodeBLEU with the target method of 0.45 and passing the\nrelated tests. The recommended method, while substantially\ndifferent from the target, captures the basic logic imple-\nmented in it. The target method \ufb01rst checks if the object\nchemObjectListeners is null and, if not, it proceeds re-\nmoving from the listeners list the element matching the one\nprovided as parameter (i.e.,col). The method synthesized by\nCopilot avoids the second if statement by directly performing\nthe remove operation after the null check.\nNote that there the two implementations are equivalent: The\nremove method of java.util.List preliminarily checks\nwhether the passed element is contained in the list before\nremoving it. While the check in the original method has\nno functional role, together with the introduction of the\nlisteners variable, it might have been introduced to make\nthe method more readable and self-explanatory.\npublic static void translateAllPositive(IAtomContainer atomCon){\n    double minX = Double.MAX_VALUE;\n    double minY = Double.MAX_VALUE;\n    Iterator<IAtom> atoms = atomCon.atoms().iterator();\n    while (atoms.hasNext"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_014",
    "source_id": "CopilotRobustness2023",
    "text": "). The method synthesized by\nCopilot avoids the second if statement by directly performing\nthe remove operation after the null check.\nNote that there the two implementations are equivalent: The\nremove method of java.util.List preliminarily checks\nwhether the passed element is contained in the list before\nremoving it. While the check in the original method has\nno functional role, together with the introduction of the\nlisteners variable, it might have been introduced to make\nthe method more readable and self-explanatory.\npublic static void translateAllPositive(IAtomContainer atomCon){\n    double minX = Double.MAX_VALUE;\n    double minY = Double.MAX_VALUE;\n    Iterator<IAtom> atoms = atomCon.atoms().iterator();\n    while (atoms.hasNext()) {\n        IAtom atom = (IAtom) atoms.next();\n        if (atom.getPoint2d() != null) {\n            if (atom.getPoint2d().x < minX) {\n                minX = atom.getPoint2d().x;\n            }\n            if (atom.getPoint2d().y < minY) {\n                minY = atom.getPoint2d().y;\n            }\n        }\n    }\n    logger.debug(\"Translating: minx=\" + minX + \", minY=\" + minY);\n    translate2D(atomCon, minX * -1, minY * -1);\n}\nTarget Method\nRecommended method starting from the original description\nLevenshtein distance: 165\nPASS\npublic static void translateAllPositive\n     (IAtomContainer atomCon){\n        double minX = Double.POSITIVE_INFINITY;\n        double minY = Double.POSITIVE_INFINITY;\n        double minZ = Double.POSITIVE_INFINITY;\n        for (IAtom atom : atomCon.atoms()) {\n            if (atom.getPoint2d() != null) {\n                minX = Math.min(minX, atom.getPoint2d().x);\n                minY = Math.min(minY, atom.getPoint2d().y);\n            }\n            if (atom.getPoint3d() != null) {\n                minX = Math.min(minX, atom.getPoint3d().x);\n                minY = Math.min(minY, atom.getPoint3d().y);\n                minZ = Math.min(minZ, atom.getPoint3d().z);\n            }\n        }\n        for (IAtom atom : atomCon.atoms()) {\n            if (atom.getPoint2d() != null) {\n                 atom.setPoint2d(new Point2d(\n                                 atom.getPoint2d().x - minX, \n                                 atom.getPoint2d().y - minY));\n            }\n            if (atom.getPoint3d() != null) {\n                atom.setPoint3d(new Point3d(\n                                atom.getPoint3d().x - minX, \n                                atom.getPoint3d().y - minY, \n                                atom.getPoint3d().z - minZ));\n            }\n        }\n}\nFig. 5. Example of recommended methods that pass the unit tests but would\nrequire 165 edit actions to match the target method.\nSimilarly, Fig. 5 shows an example of prediction passing the\ntests but that, accordingly to the Levenshtein distance, would\nrequire 165 token-level edits to match the target prediction\n(NTLev=63%). Differently from the previous example, it is\nclear that, in this case, the two methods do not have the same\nbehavior since the recommended one also"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_015",
    "source_id": "CopilotRobustness2023",
    "text": "                                atom.getPoint3d().x - minX, \n                                atom.getPoint3d().y - minY, \n                                atom.getPoint3d().z - minZ));\n            }\n        }\n}\nFig. 5. Example of recommended methods that pass the unit tests but would\nrequire 165 edit actions to match the target method.\nSimilarly, Fig. 5 shows an example of prediction passing the\ntests but that, accordingly to the Levenshtein distance, would\nrequire 165 token-level edits to match the target prediction\n(NTLev=63%). Differently from the previous example, it is\nclear that, in this case, the two methods do not have the same\nbehavior since the recommended one also treats 3D points,\nwhile the original one only 2D points. In other words, the tests\nfail to capture the difference in the behavior.\nThese examples provide two interesting observations. The\n\ufb01rst is that, metrics such as CodeBLEU and Levenshtein\ndistance may result in substantially wrong assessments of\nthe quality of a prediction. Indeed, while the discussed\npredictions have low CodeBLEU/high Levenshtein values and,\nthus, would be considered as unsuccessful predictions in most\nof the empirical evaluations, it is clear that they are valuable\nrecommendations for a developer, even when not 100% correct\n(see Fig. 5). This poses questions on the usage of these metrics\nin the evaluation of code recommenders. Second, also the\ntesting-based evaluation shows, as expected, some limitations\nas in the second example, in which the two methods do not\nimplement the same behavior but both pass the tests.\nAs a \ufb01nal note, it is also interesting to observe as 25%\nof test-failing predictions exhibit high values (>\u223c0.60) of\nCodeBLEU, indicating a high code similarity that, however,\ndoes not re\ufb02ect in test-passing recommendations.\nImpact of paraphrasing the input descriptions. Out of\nthe 892 manually paraphrased descriptions, 408 (46%) result\nin different code recommendations as compared to the original\ndescription. This means that Copilot synthesizes different\nmethods when it is provided as input with the original\ndescription and with the manually paraphrased description,\nwhich are supposed to summarize the same piece of code.\nNote that at this stage we are not focusing on the \u201cquality\u201d\nof the obtained predictions in any way. We are just observing\nthat different input descriptions have indeed an impact on\nthe recommended code. This implies that developers using\ndifferent wordings to describe a needed method may end\nup with different recommendations. Such differences also\nresult in the potential loss of correct recommendations. Indeed,\nout of the 112 test-passing predictions obtained with the\noriginal description and the 122 obtained with the manually\nparaphrased description, only 98 are in overlap, indicating that\nthere are 38 correct recommendations only generated either by\nthe original (14) or the paraphrased (24) description.\nTo have a deeper look into the 408 different predictions\ngenerated by Copilot with the original and the paraphrased\ndescription, the left part of Fig. 6 (light blue) shows the\nnormalized token-level Levenshtein distance between (i) the\noriginal description and the paraphrased description (see\nthe box"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_016",
    "source_id": "CopilotRobustness2023",
    "text": " Such differences also\nresult in the potential loss of correct recommendations. Indeed,\nout of the 112 test-passing predictions obtained with the\noriginal description and the 122 obtained with the manually\nparaphrased description, only 98 are in overlap, indicating that\nthere are 38 correct recommendations only generated either by\nthe original (14) or the paraphrased (24) description.\nTo have a deeper look into the 408 different predictions\ngenerated by Copilot with the original and the paraphrased\ndescription, the left part of Fig. 6 (light blue) shows the\nnormalized token-level Levenshtein distance between (i) the\noriginal description and the paraphrased description (see\nthe boxplot labeled with \u201cDescription\u201d), and (ii) the method\nobtained using the original description and that recommended\nusing the paraphrased description (\u201cCode\u201d). The \u201cDescription\u201d\nboxplot depicts the percentage of words that must be changed\nto convert the paraphrased description into the original one.\nAs it can be seen, while describing the same method, the\nparaphrased descriptions can be substantially different as\ncompared to the original ones, with 50% of them requiring\nchanges to more than 70% of their words. Similarly, the\ndifferent methods recommended in the 408 cases under analysis,\ncan be substantially different, with a median of \u223c30% of code\ntokes that must be changed to convert the recommendation\nobtained with the original description into the one obtained\nusing the paraphrased description (see the \u201cCode\u201d boxplot).\nThese \ufb01ndings are con\ufb01rmed for the automatically para-\nphrased descriptions (see the middle and the right part of Fig. 6\nfor the results achieved with the PEGASUS and TP paraphrases,\nrespectively). As it can be seen, the main difference as\ncompared to the results of the manually paraphrased description\n(left part of Fig. 6) is that TP changes a substantially lower\nnumber of words in the original description as compared to\nPEGASUS and to the manual paraphrasing. Such a \ufb01nding\nis expected considering that TP just translates the original\ndescription back and forth from English to French, thus rarely\nadding new words to the sentence, something that is likely\nto happen using PEGASUS or by paraphrasing the sentence\nmanually.\nAnswer to RQ1. Different (but semantically equivalent)\nnatural language descriptions of the same method are likely\nto result in different code recommendations generated by\nDL-based code generation models. Such differences can\nresult in a loss of correct recommendations (\u223c28% of\ntest-passing methods can only be obtained either with the\noriginal or the paraphrased descriptions). These \ufb01ndings\nsuggest that testing the robustness of DL-based code\nrecommenders may play an important role in ensuring\ntheir usability and in de\ufb01ning possible guidelines for the\ndevelopers using them.\nIV. THREATS TO VALIDITY\nThreats to construct validity concern the relationship between\nthe theory and what we observe. Concerning the performed\nmeasurements, we exploit the passing tests as a proxy for the\ncorrectness of the recommendations generated by Copilot. We\nacknowledge that passing tests does not imply code correctness.\nHowever, this it can provide hints about the"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_017",
    "source_id": "CopilotRobustness2023",
    "text": " recommendations (\u223c28% of\ntest-passing methods can only be obtained either with the\noriginal or the paraphrased descriptions). These \ufb01ndings\nsuggest that testing the robustness of DL-based code\nrecommenders may play an important role in ensuring\ntheir usability and in de\ufb01ning possible guidelines for the\ndevelopers using them.\nIV. THREATS TO VALIDITY\nThreats to construct validity concern the relationship between\nthe theory and what we observe. Concerning the performed\nmeasurements, we exploit the passing tests as a proxy for the\ncorrectness of the recommendations generated by Copilot. We\nacknowledge that passing tests does not imply code correctness.\nHowever, this it can provide hints about the code behavior.\nTo partially address this threat we focused our study on\nmethods having high statement coverage (median = 100%).\nAlso, we complemented this analysis with the CodeBLEU and\nthe normalized token-level Levenshtein distance. As for the\nexecution of our study, we automatically invoked Copilot rather\nthan using it as actual developers would do: We automatically\naccepted the whole recommendations and did not simulate a\nscenario in which a developer selects only parts of the provided\nrecommendations. In other words, while our automated script\nsimulates a developer invoking Copilot for help, it cannot\nsimulate the different usages a developer can make of the\nreceived code recommendation.\nThreats to internal validity concern factors, internal to our\nstudy, that could affect our results. While in RQ2 we had\nmultiple authors inspecting the semantic equivalence of the\nparaphrasing generated by the automated tools, in RQ1 we\nrelied on a single author to paraphrase the original description.\nThis introduces some form of subjectivity bias. However, the\nwhole point of our paper is that, indeed, subjectivity plays\na role in the natural language description of a function to\ngenerate and we are con\ufb01dent that the written descriptions\nwere indeed semantically equivalent to the original one. Indeed,\nthe authors involved in the manual paraphrasing have an\naverage of seven years of experience in Java. Also related\nto internal validity is our choice of using the \ufb01rst sentence of\nthe Doc Comments as the original natural language description.\nThese sentences may be of low quality and not representative\nof how a developer would describe a method they want to\nautomatically generate. This could substantially in\ufb02uence our\n\ufb01ndings, especially in terms of the effectiveness of Copilot (i.e.,\nits ability to generate test-passing methods). However, such\na threat is at least mitigated by the fact that Copilot has also\nbeen invoked using the manually written descriptions, showing\na similar effectiveness. A \ufb01nal threat regards the projects used\nfor our study.\nDescription\nL\ne\nv\ne\nn\ns\nh\nt\ne\ni\nn\nD\ni\ns\nt\na\nn\nc\ne\n327 out of 666 Pegasus Paraphrased Descriptions Resulted in \nChanges of the Recommended Code \nCode\nDescription\nL\ne\nv\ne\nn\ns\nh\nt\ne\ni\nn\nD\ni\ns\nt\na\nn\nc\ne\nCode\n328 out of 688 TP Paraphr"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_018",
    "source_id": "CopilotRobustness2023",
    "text": " that Copilot has also\nbeen invoked using the manually written descriptions, showing\na similar effectiveness. A \ufb01nal threat regards the projects used\nfor our study.\nDescription\nL\ne\nv\ne\nn\ns\nh\nt\ne\ni\nn\nD\ni\ns\nt\na\nn\nc\ne\n327 out of 666 Pegasus Paraphrased Descriptions Resulted in \nChanges of the Recommended Code \nCode\nDescription\nL\ne\nv\ne\nn\ns\nh\nt\ne\ni\nn\nD\ni\ns\nt\na\nn\nc\ne\nCode\n328 out of 688 TP Paraphrased Descriptions Resulted in \nChanges of the Recommended Code \nL\ne\nv\ne\nn\ns\nh\nt\ne\ni\nn\nD\ni\ns\nt\na\nn\nc\ne\nDescription\nCode\n408 out of 892 Manually Paraphrased Descriptions Resulted in \nChanges of the Recommended Code \nFig. 6. Levenshtein distance between the original description and (i) the manually paraphrased descriptions (left part) and (ii) the descriptions automatically\nparaphrased by PEGASUS (middle part) and Translate Pivoting (right). Similarly, we report the Levenshtein distance between the method recommended using\nthe original description and the three paraphrases. The latter is only computed for recommendations in which the obtained output differs.\nThose are open-source projects from GitHub, and it is likely\nthat at least some of them have been used for training Copilot\nitself. In other words, the absolute actual effectiveness reported\nmight not be reliable. However, the objective of our study is to\nunderstand the differences when different paraphrases are used\nrather than the absolute performance of Copilot, like previous\nstudies did (e.g., [43]).\nThreats to external validity are related to the possibility to\ngeneralize our results. Our study has been run on 892 methods\nwe carefully selected as explained in Section II-A. Rather than\ngoing large-scale, we preferred to focus on methods having\na high test coverage and a verbose \ufb01rst sentence in the Doc\nComment. Larger investigations are needed to corroborate\nor contradict our \ufb01ndings. Similarly, we only focused on\nJava methods, given the effort required to implement the\ntoolchain needed for our study, and in particular the script\nto automatically invoke Copilot and parse its output. Running\nthe same experiment with other languages is part of our future\nagenda.\nV. RELATED WORK\nRecommender systems for software developers are tools\nsupporting practitioners in daily activities [38], [51], such\nas documentation writing and retrieval [64], [39], [40], [24],\nrefactoring [11], [55], bug triaging [54], [63], bug \ufb01xing [30],\n[58], [34], etc. Among those, code recommenders, such as code\ncompletion tools, have became a crucial feature of modern\nIntegrated Development Environments (IDEs) and support in\nspeeding up code development by suggesting the developers\ncode they are likely to write [12], [29], [16]. Given the empirical\nnature of our work"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_019",
    "source_id": "CopilotRobustness2023",
    "text": " our future\nagenda.\nV. RELATED WORK\nRecommender systems for software developers are tools\nsupporting practitioners in daily activities [38], [51], such\nas documentation writing and retrieval [64], [39], [40], [24],\nrefactoring [11], [55], bug triaging [54], [63], bug \ufb01xing [30],\n[58], [34], etc. Among those, code recommenders, such as code\ncompletion tools, have became a crucial feature of modern\nIntegrated Development Environments (IDEs) and support in\nspeeding up code development by suggesting the developers\ncode they are likely to write [12], [29], [16]. Given the empirical\nnature of our work, that focuses on investigating a speci\ufb01c\naspect of code recommenders, in this section we do not discuss\nall pervious works proposing novel or improving existing code\nrecommenders (see e.g., [64], [39], [40], [30], [58], [34], [61],\n[44], [36], [28], [7], [29], [27], [60], [57]). Instead, we focus on\nempirical studies looking at code recommenders from different\nperspectives (Section V-A) and on studies speci\ufb01cally focused\non GitHub Copilot (Section V-B).\nA. Empirical Studies on Code Recommenders\nProksch et al. [48] conducted an empirical study aimed\nat evaluating the performance of code recommenders when\nsuggesting method calls. Their study has been run on a real-\nworld dataset composed of developers\u2019 interactions captured\nin the IDE. Results showed that commonly used evaluation\ntechniques based on synthetic datasets extracted by mining\nreleased code underperform due to a context miss.\nOn a related research thread, Hellendoorn et al. [20]\ncompared code completion models on both real-world and\nsynthetic datasets. Con\ufb01rming what observed by Proksch et al.,\nthey found that the evaluated tools are less accurate on the\nreal-world dataset, thus concluding that synthetic benchmarks\nare not representative enough. Moreover, they found that\nthe accuracy of code completion tools substantially drops in\nchallenging completion scenarios, in which developers would\nneed them the most.\nM\u02d8ar\u02d8as,oiu et al. [37] analyzed how practitioners rely on\ncode completion during software development. The results\nshowed that the users actually ignore many synthesized\nsuggestions. Such a \ufb01nding has been corroborated by Arrebola\nand Junior [9], who stressed the need for augmenting code\nrecommender systems with the development\u2019s context.\nJin and Servant [26] and Li et al. [33] investigated the hidden\ncosts of code recommendations. Jin and Servant found that\nIntelliSense, a code completion tool, sometimes underperforms\nby providing the suitable recommendation far from the top of\nthe recommended list of solutions. Consequently, developers\nare discouraged from picking the right suggestion. Li et al.,\naware of this potential issue, conducted a coding experiment in\nwhich they try to predict whether correct results are generated\nby code completion models, showing that their approach can\nreduce the percentage of false positives up to 70%.\nPrevious studies also assessed the actual usefulness of these\ntools. Xu et al"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_020",
    "source_id": "CopilotRobustness2023",
    "text": "commender systems with the development\u2019s context.\nJin and Servant [26] and Li et al. [33] investigated the hidden\ncosts of code recommendations. Jin and Servant found that\nIntelliSense, a code completion tool, sometimes underperforms\nby providing the suitable recommendation far from the top of\nthe recommended list of solutions. Consequently, developers\nare discouraged from picking the right suggestion. Li et al.,\naware of this potential issue, conducted a coding experiment in\nwhich they try to predict whether correct results are generated\nby code completion models, showing that their approach can\nreduce the percentage of false positives up to 70%.\nPrevious studies also assessed the actual usefulness of these\ntools. Xu et al. [65] ran a controlled experiment with 31\ndevelopers who were asked to complete implementation tasks\nwith and without the support of two code recommenders. They\nfound a marginal gain in developers\u2019 productivity when using\nthe code recommenders.\nCiniselli et al. [15] empirically evaluated the performance of\ntwo state-of-the-art Transformer-based models in challenging\ncoding scenarios, for example, when the code recommender\nis required to generate an entire code block (e.g., the body\nof a for loop). The two experimented models, RoBERTa\nand Text-To-Text Transfer Transformer (T5), achieved good\nperformance (\u223c69% of accuracy) in the more classic code\ncompletion scenario (i.e., predicting few tokens needed to\n\ufb01nalize a statement), while reported a substantial drop of\naccuracy (\u223c29%) when dealing with the previously described\nmore complex block-level completions.\nOur study is complementary to the ones discussed above.\nIndeed, we investigate the robustness of DL-based code\nrecommenders supporting what it is know in the literature as\n\u201cnatural language to source code translation\u201d. We show that\nsemantically equivalent code descriptions can result in different\nrecommendations, thus posing questions on the usability of\nthese tools.\nB. Empirical Studies on GitHub Copilot\nGitHub Copilot has been recently introduced as the state-\nof-the-art code recommender, and advertised as an \u201cAI pair\nprogrammer\u201d [1], [22]. Since its release, researchers started\ninvestigating its capabilities.\nMost of the previous research aimed at evaluating the\nimpact of GitHub Copilot on developers\u2019 productivity and its\neffectiveness (in terms of correctness of the provided solutions).\nImai [25] investigated to what extent Copilot is actually a\nvalid alternative to a human pair programmer. They observed\nthat Copilot results in increased productivity (i.e., number of\nadded lines of code), but decreased quality in the produced\ncode. Ziegler et al. [67] conducted a case study in which they\ninvestigated whether usage measurements about Copilot can\npredict developers\u2019 productivity. They found that the acceptance\nrate of the suggested solutions is the best predictor for perceived\nproductivity. Vaithilingam et al. [59] ran an experiment with\n24 developers to understand how Copilot can help developers\ncomplete programming tasks. Their results show that Copilot\ndoes not improve the task completion time and success rate.\nHowever, developers report that they prefer to use Copilot\nbecause it recommends code that can be used as a starting\npoint and saves the effort of searching online.\nNguyen and Nadi [43]"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_021",
    "source_id": "CopilotRobustness2023",
    "text": " decreased quality in the produced\ncode. Ziegler et al. [67] conducted a case study in which they\ninvestigated whether usage measurements about Copilot can\npredict developers\u2019 productivity. They found that the acceptance\nrate of the suggested solutions is the best predictor for perceived\nproductivity. Vaithilingam et al. [59] ran an experiment with\n24 developers to understand how Copilot can help developers\ncomplete programming tasks. Their results show that Copilot\ndoes not improve the task completion time and success rate.\nHowever, developers report that they prefer to use Copilot\nbecause it recommends code that can be used as a starting\npoint and saves the effort of searching online.\nNguyen and Nadi [43] used LeetCode questions as input\nto Copilot to evaluate the solutions provided for several\nprogramming languages in terms of correctness \u2014 by running\nthe test cases available in LeetCode \u2014 and understandability\n\u2014 by computing their Cyclomatic Complexity and Cognitive\nComplexity [13]. They found notable differences among the\nprogramming languages in terms of correctness (between\n57%, for Java, and 27%, for JavaScript). On the other\nhand, Copilot generates solutions with low complexity for\nall the programming languages. While we also measure the\neffectiveness of the solutions suggested by Copilot, our main\nfocus is on understanding its robustness when different inputs\nare provided.\nTwo previous studies aimed at evaluating the security of\nthe solutions recommended by Copilot. Hammond et al. [47]\ninvestigated the likelihood of receiving from Copilot recom-\nmendations including code affected by security vulnerabilities.\nThey observed that vulnerable code is recommended in 40%\nof cases out of the completion scenarios they experimented\nwith. On a similar note, Sobania et al. [52] evaluated GitHub\nCopilot on standard program synthesis benchmark problems\nand compared the achieved results with those from the genetic\nprogramming literature. The authors found that the performance\nof the two approaches are comparable. However, approaches\nbased on genetic programming are not mature enough to be\ndeployed in practice, especially due to the time they require to\nsynthesize solutions. In our study, we do not focus on security,\nbut only on the correctness of the suggested solutions.\nAlbert Ziegler, in a blog post about GitHub Copilot2\ninvestigated the extent to which the tool suggestions are copied\nfrom the training set they used. Ziegler reports that Copilot\nrarely recommends verbatim copies of code taken from the\ntraining set.\nVI. CONCLUSIONS AND FUTURE WORK\nWe investigated the extent to which DL-based code recom-\nmenders tend to synthesize different code components when\nstarting from different but semantically equivalent natural\nlanguage descriptions. We selected GitHub Copilot as the tool\nrepresentative of the state-of-the-art and asked it to generate 892\nnon-trivial Java methods starting from their natural language\ndescription. For each method in our dataset we asked Copilot\nto synthesize it using: (i) the original description, extracted\nas the \ufb01rst sentence in the Javadoc; and (ii) paraphrased\ndescriptions. We did this both by manually modifying the\noriginal description and by using automated paraphrasing tools,\nafter having assessed their reliability in this context.\nWe found that in \u223c46% of cases semantically equivalent but\ndifferent method descriptions result in different code recom-\n"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_022",
    "source_id": "CopilotRobustness2023",
    "text": "starting from different but semantically equivalent natural\nlanguage descriptions. We selected GitHub Copilot as the tool\nrepresentative of the state-of-the-art and asked it to generate 892\nnon-trivial Java methods starting from their natural language\ndescription. For each method in our dataset we asked Copilot\nto synthesize it using: (i) the original description, extracted\nas the \ufb01rst sentence in the Javadoc; and (ii) paraphrased\ndescriptions. We did this both by manually modifying the\noriginal description and by using automated paraphrasing tools,\nafter having assessed their reliability in this context.\nWe found that in \u223c46% of cases semantically equivalent but\ndifferent method descriptions result in different code recom-\nmendations. We observed that some correct recommendations\ncan only be obtained using one of the semantically equivalent\ndescriptions as input.\nOur results highlight the importance of providing a proper\ncode description when asking DL-based recommenders to\nsynthesize code. In the new era of AI-supported programming,\ndevelopers must learn how to properly describe the code\ncomponents they are looking for to maximize the effectiveness\nof the AI support.\nOur future work will focus on answering our \ufb01rst research\nquestion in vivo rather than in silico. In other words, we aim\nat running a controlled experiment with developers to assess\nthe impact of the different code descriptions they write on\nthe received recommendations. Also, we will investigate how\nto customize the automatic paraphrasing techniques to further\nimprove their performance on software-related text (such as\nmethods\u2019 descriptions).\n2https://docs.github.com/en/github/copilot/research-recitation\nACKNOWLEDGMENTS\nThis project has received funding from the European\nResearch Council (ERC) under the European Union\u2019s Horizon\n2020 research and innovation programme (grant agreement No.\n851720).\nREFERENCES\n[1] \u201cGithub copilot https://copilot.github.com.\u201d\n[2] Jacoco, https://www.eclemma.org/jacoco/.\n[3] Java Parser, https://github.com/javaparser/javaparser.\n[4] jUnit, https://junit.org/junit5/.\n[5] PEGASUS \ufb01ne-tuned for paraphrasing, https://huggingface.co/tuner007/\npegasus_paraphrase.\n[6] Replication\npackage,\nhttps://github.com/antonio-mastropaolo/\nrobustness-copilot.\n[7] M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, \u201cLearning natural coding\nconventions,\u201d in Proceedings of the 22nd ACM SIGSOFT International\nSymposium on Foundations of Software Engineering, ser. FSE 2014,\n2014, pp. 281\u2013293.\n[8] U. Alon, R. Sadaka, O. Levy, and E. Yahav, \u201cStructural language models\nof code,\u201d arXiv, pp. arXiv\u20131910, 2019.\n[9] F. V. Arrebola and P. T. A. Junior, \u201cOn source code completion assistants\nand the need of a context-aware approach,\u201d in International Conference\non Human Interface and the Management of Information.\nSpringer,\n2017, pp. 191\u2013201.\n[10] M. Asaduzzaman, C. K. Roy, K."
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_023",
    "source_id": "CopilotRobustness2023",
    "text": " of Software Engineering, ser. FSE 2014,\n2014, pp. 281\u2013293.\n[8] U. Alon, R. Sadaka, O. Levy, and E. Yahav, \u201cStructural language models\nof code,\u201d arXiv, pp. arXiv\u20131910, 2019.\n[9] F. V. Arrebola and P. T. A. Junior, \u201cOn source code completion assistants\nand the need of a context-aware approach,\u201d in International Conference\non Human Interface and the Management of Information.\nSpringer,\n2017, pp. 191\u2013201.\n[10] M. Asaduzzaman, C. K. Roy, K. A. Schneider, and D. Hou, \u201cContext-\nsensitive code completion tool for better api usability,\u201d in 2014 IEEE\nInternational Conference on Software Maintenance and Evolution, 2014,\npp. 621\u2013624.\n[11] G. Bavota, A. D. Lucia, A. Marcus, and R. Oliveto, \u201cAutomating extract\nclass refactoring: an improved method and its evaluation,\u201d Empir. Softw.\nEng., vol. 19, no. 6, pp. 1617\u20131664, 2014.\n[12] M. Bruch, M. Monperrus, and M. Mezini, \u201cLearning from examples\nto improve code completion systems,\u201d in Proceedings of the 7th Joint\nMeeting of the European Software Engineering Conference and the ACM\nSIGSOFT Symposium on The Foundations of Software Engineering, ser.\nESEC/FSE 2009, 2009, pp. 213\u2013222.\n[13] G. A. Campbell, \u201cCognitive complexity: An overview and evaluation,\u201d\nin Proceedings of the 2018 international conference on technical debt,\n2018, pp. 57\u201358.\n[14] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman et al., \u201cEvaluating large\nlanguage models trained on code,\u201d arXiv preprint arXiv:2107.03374,\n2021.\n[15] M. Ciniselli, N. Cooper, L. Pascarella, A. Mastropaolo, E. Aghajani,\nD. Poshyvanyk, M. D. Penta, and G. Bavota, \u201cAn empirical study on the\nusage of transformer models for code completion,\u201d IEEE Transactions\non Software Engineering, no. 01, pp. 1\u20131, 5555.\n[16] M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. Di Penta, and\nG. Bavota, \u201cAn empirical study on the usage of bert models for code\ncompletion,\u201d in Proceedings of the 18th Working Conference on Mining\nSoftware Repositories, ser. MSR \u201921, 2021, p. To Appear.\n[17] O. Dabic, E. Aghajani, and G. Bavota, \u201cSampling projects in github\nfor msr studies,\u201d in 2021 IEEE/ACM"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_024",
    "source_id": "CopilotRobustness2023",
    "text": " IEEE Transactions\non Software Engineering, no. 01, pp. 1\u20131, 5555.\n[16] M. Ciniselli, N. Cooper, L. Pascarella, D. Poshyvanyk, M. Di Penta, and\nG. Bavota, \u201cAn empirical study on the usage of bert models for code\ncompletion,\u201d in Proceedings of the 18th Working Conference on Mining\nSoftware Repositories, ser. MSR \u201921, 2021, p. To Appear.\n[17] O. Dabic, E. Aghajani, and G. Bavota, \u201cSampling projects in github\nfor msr studies,\u201d in 2021 IEEE/ACM 18th International Conference on\nMining Software Repositories (MSR).\nIEEE, 2021, pp. 560\u2013564.\n[18] N. A. Ernst and G. Bavota, \u201cAi-driven development is here: Should you\nworry?\u201d IEEE Softw., vol. 39, no. 2, pp. 106\u2013110, 2022.\n[19] V. J. Hellendoorn and P. Devanbu, \u201cAre deep neural networks the best\nchoice for modeling source code?\u201d in Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering, ser. ESEC/FSE 2017,\n2017, p. 763?773.\n[20] V. J. Hellendoorn, S. Proksch, H. C. Gall, and A. Bacchelli, \u201cWhen\ncode completion fails: A case study on real-world completions,\u201d in\n2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE).\nIEEE, 2019, pp. 960\u2013970.\n[21] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, \u201cOn the\nnaturalness of software,\u201d in Proceedings of the 34th International\nConference on Software Engineering, ser. ICSE 2012.\nIEEE Press,\n2012, pp. 837\u2013847.\n[22] G. D. Howard, \u201cGithub copilot: Copyright, fair use, creativity, transfor-\nmativity, and algorithms,\u201d 2021.\n[23] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, \u201cDeep code comment generation,\u201d\nin Proceedings of the 26th Conference on Program Comprehension, ICPC\n2018, Gothenburg, Sweden, May 27-28, 2018, F. Khomh, C. K. Roy, and\nJ. Siegmund, Eds.\nACM, 2018, pp. 200\u2013210.\n[24] \u2014\u2014, \u201cDeep code comment generation,\u201d ser. ICPC \u201918, 2018.\n[25] S. Imai, \u201cIs github copilot a substitute for human pair-programming?\nan empirical study,\u201d in 2022 IEEE/ACM 44th International Conference\non Software Engineering: Companion Proceedings (ICSE-Companion).\nIEEE, 2022, pp. 319\u2013321.\n[26] X. Jin and F. Servant, \u201cThe hidden cost of code completion: Understand"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_025",
    "source_id": "CopilotRobustness2023",
    "text": " 2018, F. Khomh, C. K. Roy, and\nJ. Siegmund, Eds.\nACM, 2018, pp. 200\u2013210.\n[24] \u2014\u2014, \u201cDeep code comment generation,\u201d ser. ICPC \u201918, 2018.\n[25] S. Imai, \u201cIs github copilot a substitute for human pair-programming?\nan empirical study,\u201d in 2022 IEEE/ACM 44th International Conference\non Software Engineering: Companion Proceedings (ICSE-Companion).\nIEEE, 2022, pp. 319\u2013321.\n[26] X. Jin and F. Servant, \u201cThe hidden cost of code completion: Understand-\ning the impact of the recommendation-list length on its ef\ufb01ciency,\u201d in\nProceedings of the 15th International Conference on Mining Software\nRepositories, 2018, pp. 70\u201373.\n[27] R. Karampatsis and C. A. Sutton, \u201cMaybe deep neural networks are\nthe best choice for modeling source code,\u201d CoRR, vol. abs/1903.05734,\n2019. [Online]. Available: http://arxiv.org/abs/1903.05734\n[28] J. Kim, S. Lee, S. Hwang, and S. Kim, \u201cAdding examples into java\ndocuments,\u201d in 2009 IEEE/ACM International Conference on Automated\nSoftware Engineering, 2009, pp. 540\u2013544.\n[29] S. Kim, J. Zhao, Y. Tian, and S. Chandra, \u201cCode prediction by feeding\ntrees to transformers,\u201d arXiv preprint arXiv:2003.13848, 2020.\n[30] C. Le Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer, \u201cA systematic\nstudy of automated program repair: Fixing 55 out of 105 bugs for $8\neach,\u201d in 2012 34th International Conference on Software Engineering\n(ICSE), 2012, pp. 3\u201313.\n[31] V. I. Levenshtein et al., \u201cBinary codes capable of correcting deletions,\ninsertions, and reversals,\u201d in Soviet physics doklady, vol. 10, no. 8. Soviet\nUnion, 1966, pp. 707\u2013710.\n[32] B. Li, M. Yan, X. Xia, X. Hu, G. Li, and D. Lo, \u201cDeepcommenter: a\ndeep code comment generation tool with hybrid lexical and syntactical\ninformation,\u201d in ESEC/FSE \u201920: 28th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software\nEngineering, Virtual Event, USA, November 8-13, 2020, P. Devanbu,\nM. B. Cohen, and T. Zimmermann, Eds.\nACM, 2020, pp. 1571\u20131575.\n[33] J. Li, R. Huang, W. Li, K. Yao, and W. Tan, \u201cToward less hidden\ncost of code completion with acceptance and ranking models,\u201d in 2021\nIEEE International Conference on Software Maintenance and Evolution\n(ICSME).\nIEEE, "
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_026",
    "source_id": "CopilotRobustness2023",
    "text": " with hybrid lexical and syntactical\ninformation,\u201d in ESEC/FSE \u201920: 28th ACM Joint European Software\nEngineering Conference and Symposium on the Foundations of Software\nEngineering, Virtual Event, USA, November 8-13, 2020, P. Devanbu,\nM. B. Cohen, and T. Zimmermann, Eds.\nACM, 2020, pp. 1571\u20131575.\n[33] J. Li, R. Huang, W. Li, K. Yao, and W. Tan, \u201cToward less hidden\ncost of code completion with acceptance and ranking models,\u201d in 2021\nIEEE International Conference on Software Maintenance and Evolution\n(ICSME).\nIEEE, 2021, pp. 195\u2013205.\n[34] Y. Li, S. Wang, and T. N. Nguyen, \u201cDl\ufb01x: Context-based code\ntransformation learning for automated program repair,\u201d in Proceedings of\nthe ACM/IEEE 42nd International Conference on Software Engineering,\nser. ICSE \u201920, 2020, p. 602?614.\n[35] B. Lin, F. Zampetti, G. Bavota, M. D. Penta, M. Lanza, and R. Oliveto,\n\u201cSentiment analysis for software engineering: how far can we go?\u201d\nin Proceedings of the 40th International Conference on Software\nEngineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018,\npp. 94\u2013104.\n[36] F. Liu, G. Li, Y. Zhao, and Z. Jin, \u201cMulti-task learning based pre-\ntrained language model for code completion,\u201d in Proceedings of the 35th\nIEEE/ACM International Conference on Automated Software Engineering,\nser. ASE 2020.\nAssociation for Computing Machinery, 2020.\n[37] M. M\u02d8ar\u02d8as,oiu, L. Church, and A. Blackwell, \u201cAn empirical investigation\nof code completion usage by professional software developers,\u201d in Pro-\nceedings of the 26th Annual Workshop of the Psychology of Programming\nInterest Group, 2015.\n[38] C. McMillan, D. Poshyvanyk, M. Grechanik, Q. Xie, and C. Fu,\n\u201cPortfolio: Searching for relevant functions and their usages in millions\nof lines of code,\u201d ACM Trans. Softw. Eng. Methodol., vol. 22, no. 4, pp.\n37:1\u201337:30, 2013.\n[39] L. Moreno, G. Bavota, M. Di Penta, R. Oliveto, and A. Marcus, \u201cHow can\ni use this method?\u201d in Proceedings of the 37th International Conference\non Software Engineering - Volume 1, ser. ICSE \u201915, 2015, p. 880?890.\n[40] L. Moreno, G. Bavota, M. D. Penta, R. Oliveto, A. Marcus, and\nG. Canfora, \u201cArena: An approach for the automated generation of release\nnotes,\u201d IEEE Transactions on Software Engineering, vol. 43, no. 2, pp.\n106\u2013127, 2017.\n[41"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_027",
    "source_id": "CopilotRobustness2023",
    "text": "39] L. Moreno, G. Bavota, M. Di Penta, R. Oliveto, and A. Marcus, \u201cHow can\ni use this method?\u201d in Proceedings of the 37th International Conference\non Software Engineering - Volume 1, ser. ICSE \u201915, 2015, p. 880?890.\n[40] L. Moreno, G. Bavota, M. D. Penta, R. Oliveto, A. Marcus, and\nG. Canfora, \u201cArena: An approach for the automated generation of release\nnotes,\u201d IEEE Transactions on Software Engineering, vol. 43, no. 2, pp.\n106\u2013127, 2017.\n[41] A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, \u201cA large-scale study on\nrepetitiveness, containment, and composability of routines in open-source\nprojects,\u201d in Proceedings of the IEEE/ACM 13th Working Conference on\nMining Software Repositories (MSR 2016), 2016, pp. 362\u2013373.\n[42] A. T. Nguyen, T. T. Nguyen, H. A. Nguyen, A. Tamrawi, H. V. Nguyen,\nJ. Al-Kofahi, and T. N. Nguyen, \u201cGraph-based pattern-oriented, context-\nsensitive source code completion,\u201d in 2012 34th International Conference\non Software Engineering (ICSE), 2012, pp. 69\u201379.\n[43] N. Nguyen and S. Nadi, \u201cAn empirical evaluation of github copilot\u2019s\ncode suggestions,\u201d in 2022 IEEE/ACM 19th International Conference on\nMining Software Repositories (MSR).\nIEEE, 2022, pp. 1\u20135.\n[44] T. Nguyen, P. C. Rigby, A. T. Nguyen, M. Karan\ufb01l, and T. N. Nguyen,\n\u201cT2api: Synthesizing api code usage templates from english texts with\nstatistical translation,\u201d in Proceedings of the 2016 24th ACM SIGSOFT\nInternational Symposium on Foundations of Software Engineering, ser.\nFSE 2016, 2016, p. 1013?1017.\n[45] H. Niu, I. Keivanloo, and Y. Zou, \u201cApi usage pattern recommendation\nfor software development,\u201d Journal of Systems and Software, vol. 129,\npp. 127\u2013139, 2017.\n[46] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for\nautomatic evaluation of machine translation,\u201d in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311\u2013318.\n[47] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, \u201cAn\nempirical cybersecurity evaluation of github copilot\u2019s code contributions,\u201d\narXiv preprint arXiv:2108.09293, 2021.\n[48] S. Proksch, S. Amann, S. Nadi, and M. Mezini, \u201cEvaluating the\neval"
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_028",
    "source_id": "CopilotRobustness2023",
    "text": " Ward, and W.-J. Zhu, \u201cBleu: a method for\nautomatic evaluation of machine translation,\u201d in Proceedings of the 40th\nannual meeting of the Association for Computational Linguistics, 2002,\npp. 311\u2013318.\n[47] H. Pearce, B. Ahmad, B. Tan, B. Dolan-Gavitt, and R. Karri, \u201cAn\nempirical cybersecurity evaluation of github copilot\u2019s code contributions,\u201d\narXiv preprint arXiv:2108.09293, 2021.\n[48] S. Proksch, S. Amann, S. Nadi, and M. Mezini, \u201cEvaluating the\nevaluations of code recommender systems: a reality check,\u201d in 2016 31st\nIEEE/ACM International Conference on Automated Software Engineering\n(ASE).\nIEEE, 2016, pp. 111\u2013121.\n[49] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,\nM. Zhou, A. Blanco, and S. Ma, \u201cCodebleu: a method for automatic\nevaluation of code synthesis,\u201d CoRR, vol. abs/2009.10297, 2020.\n[Online]. Available: https://arxiv.org/abs/2009.10297\n[50] R. Robbes and M. Lanza, \u201cImproving code completion with program\nhistory,\u201d Automated Software Engineering, vol. 17, no. 2, pp. 181\u2013212,\n2010.\n[51] M. P. Robillard, W. Maalej, R. J. Walker, and T. Zimmermann,\nRecommendation Systems in Software Engineering.\nSpringer Publishing\nCompany, Incorporated, 2014.\n[52] D. Sobania, M. Briesch, and F. Rothlauf, \u201cChoose your programming\ncopilot: A comparison of the program synthesis performance of github\ncopilot and genetic programming,\u201d arXiv preprint arXiv:2111.07875,\n2021.\n[53] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, \u201cIntelli-\ncode compose: Code generation using transformer,\u201d arXiv preprint\narXiv:2005.08025, 2020.\n[54] A. Tamrawi, T. T. Nguyen, J. M. Al-Kofahi, and T. N. Nguyen, \u201cFuzzy\nset and cache-based approach for bug triaging,\u201d in Proceedings of the\n19th ACM SIGSOFT Symposium and the 13th European Conference\non Foundations of Software Engineering, ser. ESEC/FSE \u201911, 2011, p.\n365?375.\n[55] N. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou, \u201cTen years of jdeodor-\nant: Lessons learned from the hunt for smells,\u201d in 25th International\nConference on Software Analysis, Evolution and Reengineering, SANER\n2018, R. Oliveto, M. D. Penta, and D. C. Shepherd, Eds. IEEE Computer\nSociety, 2018, pp. "
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_029",
    "source_id": "CopilotRobustness2023",
    "text": " approach for bug triaging,\u201d in Proceedings of the\n19th ACM SIGSOFT Symposium and the 13th European Conference\non Foundations of Software Engineering, ser. ESEC/FSE \u201911, 2011, p.\n365?375.\n[55] N. Tsantalis, T. Chaikalis, and A. Chatzigeorgiou, \u201cTen years of jdeodor-\nant: Lessons learned from the hunt for smells,\u201d in 25th International\nConference on Software Analysis, Evolution and Reengineering, SANER\n2018, R. Oliveto, M. D. Penta, and D. C. Shepherd, Eds. IEEE Computer\nSociety, 2018, pp. 4\u201314.\n[56] Z. Tu, Z. Su, and P. Devanbu, \u201cOn the localness of software,\u201d in\nProceedings of the 22nd ACM SIGSOFT International Symposium on\nFoundations of Software Engineering, ser. FSE 2014.\nNew York,\nNY, USA: Association for Computing Machinery, 2014, p. 269\u2013280.\n[Online]. Available: https://doi.org/10.1145/2635868.2635875\n[57] M. Tufano, D. Drain, A. Svyatkovskiy, and N. Sundaresan, \u201cGenerating\naccurate assert statements for unit test cases using pretrained transformers,\u201d\nCoRR, vol. abs/2009.05634, 2020.\n[58] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and\nD. Poshyvanyk, \u201cAn empirical study on learning bug-\ufb01xing patches\nin the wild via neural machine translation,\u201d ACM Trans. Softw. Eng.\nMethodol., vol. 28, no. 4, pp. 19:1\u201319:29, 2019.\n[59] P. Vaithilingam, T. Zhang, and E. L. Glassman, \u201cExpectation vs.\nexperience: Evaluating the usability of code generation tools powered\nby large language models,\u201d in CHI Conference on Human Factors in\nComputing Systems Extended Abstracts, 2022, pp. 1\u20137.\n[60] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk, \u201cOn\nlearning meaningful assert statements for unit test cases,\u201d in Proceedings\nof the 42nd International Conference on Software Engineering, ICSE\n2020, 2020, p. To Appear.\n[61] F. Wen, E. Aghajani, C. Nagy, M. Lanza, and G. Bavota, \u201cSiri, write the\nnext method,\u201d in 43rd IEEE/ACM International Conference on Software\nEngineering, ICSE 2021, Madrid, Spain, 22-30 May 2021.\nIEEE, 2021,\npp. 138\u2013149.\n[62] M. White, C. Vendome, M. Linares-V\u00e1squez, and D. Poshyvanyk,\n\u201cToward deep learning software repositories,\u201d in Proceedings of the\n12th Working Conference on Mining Software Repositories, ser."
  },
  {
    "chunk_id": "CopilotRobustness2023_chunk_030",
    "source_id": "CopilotRobustness2023",
    "text": "0, p. To Appear.\n[61] F. Wen, E. Aghajani, C. Nagy, M. Lanza, and G. Bavota, \u201cSiri, write the\nnext method,\u201d in 43rd IEEE/ACM International Conference on Software\nEngineering, ICSE 2021, Madrid, Spain, 22-30 May 2021.\nIEEE, 2021,\npp. 138\u2013149.\n[62] M. White, C. Vendome, M. Linares-V\u00e1squez, and D. Poshyvanyk,\n\u201cToward deep learning software repositories,\u201d in Proceedings of the\n12th Working Conference on Mining Software Repositories, ser. MSR\n\u201915.\nPiscataway, NJ, USA: IEEE Press, 2015, pp. 334\u2013345. [Online].\nAvailable: http://dl.acm.org/citation.cfm?id=2820518.2820559\n[63] X. Xia, D. Lo, Y. Ding, J. M. Al-Kofahi, T. N. Nguyen, and X. Wang,\n\u201cImproving automated bug triaging with specialized topic model,\u201d IEEE\nTransactions on Software Engineering, vol. 43, no. 3, pp. 272\u2013297, 2017.\n[64] T. Xie and J. Pei, \u201cMapo: Mining api usages from open source\nrepositories,\u201d ser. MSR \u201906, 2006.\n[65] F. F. Xu, B. Vasilescu, and G. Neubig, \u201cIn-ide code generation from\nnatural language: Promise and challenges,\u201d 2021.\n[66] J. Zhang, Y. Zhao, M. Saleh, and P. J. Liu, \u201cPegasus: Pre-training with\nextracted gap-sentences for abstractive summarization,\u201d 2019.\n[67] A. Ziegler, E. Kalliamvakou, X. A. Li, A. Rice, D. Rifkin, S. Simister,\nG. Sittampalam, and E. Aftandilian, \u201cProductivity assessment of neural\ncode completion,\u201d in Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, 2022, pp. 21\u201329.\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_001",
    "source_id": "PerformanceAnalysis2026",
    "text": "RESEARCH PAPERS\nReceived: 5 March 2025 / Accepted: 11 November 2025\n\u00a9 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2026\nCommunicated by: Lei Ma, Roberto Pietrantuono.\nExtended author information available on the last page of the article\nPerformance analysis of AI-generated code: A case study \nof Copilot, Copilot Chat, CodeLlaMa, and DeepSeek-Coder \nmodels\nShuang\u00a0Li1\u00a0\u00b7 Yuntao\u00a0Cheng1\u00a0\u00b7 Jinfu\u00a0Chen1,2\n\u00a0\u00b7 Jifeng\u00a0Xuan1\u00a0\u00b7 Sen\u00a0He3\u00a0\u00b7 Weiyi\u00a0Shang4\nEmpirical Software Engineering           (2026) 31:62 \nhttps://doi.org/10.1007/s10664-025-10776-1\nAbstract\nThe integration of Large Language Models (LLMs) into software development tools like \nGitHub Copilot and Copilot Chat, along with the advancement of code generation models \nlike DeepSeek-Coder and CodeLlama, hold the promise of transforming code generation \nprocesses. While AI-driven code generation presents numerous advantages for software \ndevelopment, code generated by LLMs may introduce challenges related to security, pri\u00ad\nvacy, and copyright issues. However, the performance implications of AI-generated code \nremain insufficiently explored. This study conducts an empirical analysis focusing on the \nperformance regressions of code generated by GitHub Copilot, Copilot Chat, CodeLlama, \nand Deepseek-Coder across four distinct datasets: HumanEval, AixBench, MBPP, and \nthe performance-oriented benchmark EvalPerf. We adopt a comprehensive methodology \nencompassing static and dynamic performance analyses to assess the effectiveness of the \ngenerated code. Our findings reveal that although the generated code is functionally correct, \nit frequently exhibits performance regressions compared to code solutions crafted by hu\u00ad\nmans. We further investigate the code-level root causes responsible for these performance \nregressions. We identify four major root causes, i.e., inefficient function calls, inefficient \nlooping, inefficient algorithms, and inefficient use of language features. We further identify \na total of eleven sub-categories of root causes attributed to the performance regressions \nof generated code. Additionally, we explore prompt engineering including few-shot and \nChain-of-Thought (CoT) prompting as a potential strategy for optimizing performance. \nThe outcomes demonstrate that few-shot prompting, grounded in identified root causes of \ncode performance regressions, can improve the performance of generated code by guiding \nmodels toward performance-oriented generation. In contrast, CoT prompting proves less \neffective, and in some cases detrimental, suggesting that reasoning-oriented strategies do \nnot necessarily enhance performance. Across both general-purpose and efficiency-oriented \nbenchmarks, our analysis reveals that performance regressions persist regardless of dataset \nscope, underscoring the necessity of treating performance as a first-class dimension of \ncode quality. This research provides valuable insights that contribute to a more compre\u00ad\nhensive understanding of AI-assisted code generation.\n1\u202f3\nEmpirical Software Engineering           (2026) 31:62 \nKeywords\u2002 Code Generation\u00a0\u00b7 Software Performance\u00a0\u00b7 Program Analysis\u00a0\u00b7 Performance \nEngineering\u00a0\u00b7 Large Language Models\n1\u2002 Introduction\nThe integration of LLMs into"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_002",
    "source_id": "PerformanceAnalysis2026",
    "text": " contrast, CoT prompting proves less \neffective, and in some cases detrimental, suggesting that reasoning-oriented strategies do \nnot necessarily enhance performance. Across both general-purpose and efficiency-oriented \nbenchmarks, our analysis reveals that performance regressions persist regardless of dataset \nscope, underscoring the necessity of treating performance as a first-class dimension of \ncode quality. This research provides valuable insights that contribute to a more compre\u00ad\nhensive understanding of AI-assisted code generation.\n1\u202f3\nEmpirical Software Engineering           (2026) 31:62 \nKeywords\u2002 Code Generation\u00a0\u00b7 Software Performance\u00a0\u00b7 Program Analysis\u00a0\u00b7 Performance \nEngineering\u00a0\u00b7 Large Language Models\n1\u2002 Introduction\nThe integration of LLMs into software development tools has introduced a new era of AI-\npowered coding assistants. These LLM-based tools, such as GitHub Copilot, Copilot Chat, \nChatGPT, and CodeWhisperer, as well as code generation models like CodeLlama and \nDeepSeek-Coder, are redefining how developers write code. One typical example is GitHub \nCopilot, a tool that leverages LLMs to aid programmers by suggesting code completions \nand functionalities. Its extension, GitHub Copilot Chat, provides an interactive conversa\u00ad\ntional interface that enables developers to request explanations and debugging assistance, \nthereby further integrating LLMs into the software development workflow. The LLM-based \ncode generation tools offer the potential to enhance developer productivity and streamline \ndevelopment processes. Similarly, emerging models like CodeLlama and DeepSeek-Coder \nfurther expand the applicability of LLMs in code generation, significantly influencing the \nmethodologies and workflows of software engineers.\nWhile tools like Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder offer the \npotential to enhance developer productivity, ensuring the quality of the generated code \nremains a crucial area of investigation. Prior research has extensively studied and reported \nchallenges related to correctness\u00a0(Nguyen and Nadi 2022; Yetistiren et\u00a0al. 2022, 2023; Wang \net\u00a0al. 2025; Mayer et\u00a0al. 2024), security\u00a0(Pearce et\u00a0al. 2022; Fu et\u00a0al. 2025; Khoury et\u00a0al. \n2023; Zhang et\u00a0al. 2024), and privacy\u00a0(Niu et\u00a0al. 2023; Huang et\u00a0al. 2023; Yang et\u00a0al. 2023; \nLuo et\u00a0al. 2024a) associated with code generated by LLMs. These studies highlight the need \nfor continuous improvement in the overall quality of LLM-generated code. However, the \nperformance implications of AI-generated code, particularly its variability across different \ncode generation models, are a critical yet unexplored area.\nPerformance is a critical aspect of software quality. Software performance regressions \nmay affect application responsiveness, resource consumption, and overall user experience. \nCode efficiency can be particularly crucial in performance-sensitive domains such as high-\nfrequency trading, real-time systems, and large-scale data processing. Given the potential \nfor AI-generated code to either enhance or degrade performance, it is imperative to evalu\u00ad\nate its performance characteristics, including the risk of performance regressions. There \nis a gap in understanding whether these AI assistants can facilitate the generation"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_003",
    "source_id": "PerformanceAnalysis2026",
    "text": " need \nfor continuous improvement in the overall quality of LLM-generated code. However, the \nperformance implications of AI-generated code, particularly its variability across different \ncode generation models, are a critical yet unexplored area.\nPerformance is a critical aspect of software quality. Software performance regressions \nmay affect application responsiveness, resource consumption, and overall user experience. \nCode efficiency can be particularly crucial in performance-sensitive domains such as high-\nfrequency trading, real-time systems, and large-scale data processing. Given the potential \nfor AI-generated code to either enhance or degrade performance, it is imperative to evalu\u00ad\nate its performance characteristics, including the risk of performance regressions. There \nis a gap in understanding whether these AI assistants can facilitate the generation of high-\nperforming code.\nTo fill this gap, we design an experimental setup that involves generating code using \nmainstream code generation models such as GitHub Copilot, Copilot Chat, CodeLlama, and \nDeepSeek-Coder and evaluating the performance regressions comprehensively using both \nstatic analysis tools and dynamic profiling. Through this approach, we aim to quantify the \nperformance regression differences among various models in the code generation process. \nTo ensure the broad applicability of our findings, we select four diverse and representa\u00ad\ntive datasets, i.e., HumanEval, AixBench, MBPP, and the performance-oriented benchmark \nEvalPerf. The static analysis is supported by tools such as Qodana, Spotbugs, and PMD, \nwhich are adept at identifying a variety of performance regression code issues. For dynamic \n1\u202f3\n   62 \n\u2003\nPage 2 of 52\nEmpirical Software Engineering           (2026) 31:62 \nanalysis, we choose cProfile, tracemalloc, and Psutil to measure critical performance met\u00ad\nrics such as runtime, memory usage, and CPU utilization.\nOur study finds that AI-generated code, although functionally correct, often exhibits \nperformance regressions when compared to canonical solutions. We identify several root \ncauses that contribute to these regressions, including inefficient function calls, inefficient \nlooping, inefficient algorithm, and inefficient use of language features. To address these per\u00ad\nformance regressions, we explore prompt engineering as a mitigation strategy. In particular, \nwe propose a few-shot prompting approach that incorporates concrete examples reflect\u00ad\ning the identified root causes, guiding models toward generating more efficient code. We \nalso evaluate CoT prompting to examine whether reasoning-oriented strategies can enhance \nperformance optimization. The results show that few-shot prompting could mitigate the \nperformance regressions of code generation models, whereas CoT prompting yields limited \nor inconsistent benefits, suggesting that explicit reasoning does not directly translate into \nefficiency gains.\nTo facilitate research reproducibility, we make the original datasets, scripts, profiling \nresults, and analysis rules available in our replication package1. Our contributions are sum\u00ad\nmarized below.\n\t\u2013\nPerformance assessment of AI-generated code: We systematically evaluated the \nperformance regressions between code generated by GitHub Copilot, Copilot Chat, \nCodeLlama, and DeepSeek-Coder and canonical solutions. Using static analysis tools \nand dynamic profiling, we analyzed the performance regressions of these code genera\u00ad\ntion models across different tasks and datasets including HumanEval, MBPP, AixBench, \nand performance-oriented benchmark EvalPerf. The study revealed that performance \n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_004",
    "source_id": "PerformanceAnalysis2026",
    "text": " explicit reasoning does not directly translate into \nefficiency gains.\nTo facilitate research reproducibility, we make the original datasets, scripts, profiling \nresults, and analysis rules available in our replication package1. Our contributions are sum\u00ad\nmarized below.\n\t\u2013\nPerformance assessment of AI-generated code: We systematically evaluated the \nperformance regressions between code generated by GitHub Copilot, Copilot Chat, \nCodeLlama, and DeepSeek-Coder and canonical solutions. Using static analysis tools \nand dynamic profiling, we analyzed the performance regressions of these code genera\u00ad\ntion models across different tasks and datasets including HumanEval, MBPP, AixBench, \nand performance-oriented benchmark EvalPerf. The study revealed that performance \nregression is a common phenomenon in AI-generated code. Compared to human-writ\u00ad\nten code, the generated code exhibits significant inefficiencies in terms of execution \nefficiency and resource utilization with the proportion of performance regressions being \neven more pronounced on the performance-oriented benchmark EvalPerf.\n\t\u2013\nPerformance regression root causes of AI-generated code: We qualitatively analyzed \nthe root causes of performance regression in the generated code. Four major catego\u00ad\nries of inefficiencies were identified: inefficient function calls, inefficient loops, inef\u00ad\nficient algorithms, and suboptimal use of language features. These were further refined \ninto eleven specific subcategories. We conducted this root cause analysis for all models \nacross all datasets, highlighting how different models exhibit distinct patterns of perfor\u00ad\nmance regressions depending on the dataset. These findings offer valuable insights into \nthe potential shortcomings of current AI-driven coding assistants and establish a solid \nfoundation for devising strategies to optimize the performance of AI-generated code.\n\t\u2013\nPrompt engineering for performance optimization: We explored prompt engineering \nincluding few-shot and CoT prompting as a technique for optimizing the performance \nof AI-generated code and proposed a few-shot prompt engineering optimization strat\u00ad\negy. By embedding specific root causes from the identified subcategories into the few-\nshot prompt design, we enhanced the performance of the generated code in most cases, \nalthough the improvement was limited or inconsistent for certain models and datasets. \nIn contrast, CoT prompting, while reasoning-oriented, does not consistently improve \nperformance and can sometimes degrade performance. These findings suggest that the \n1\u2009https:\u200b//gith\u200bub.\u200bcom/he\u200bbena/Perfor\u200bmance\u200b-An\u200bal\u200bys\u200bis-of-AI-\u200bGenerated-Code\n1\u202f3\nPage 3 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \neffectiveness of prompt engineering is highly model- and dataset-dependent. This high\u00ad\nlights the importance of aligning prompt design with concrete performance optimization \nobjectives and provides actionable insights for future work on efficiency-aware code \ngeneration with LLMs.\nThis work systematically extends our previous work\u00a0Li et\u00a0al. (2024b). First, we add more \nAI-powered coding assistants, i.e., Copilot Chat, CodeLlama, and DeepSeek-Coder, to more \ncomprehensively evaluate the performance of AI-generated code and investigate the perfor\u00ad\nmance discrepancy among different models in code generation tasks. Second, we extend our \nevaluation with a newly introduced performance-oriented dataset, EvalPerf, which provides \na"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_005",
    "source_id": "PerformanceAnalysis2026",
    "text": " \neffectiveness of prompt engineering is highly model- and dataset-dependent. This high\u00ad\nlights the importance of aligning prompt design with concrete performance optimization \nobjectives and provides actionable insights for future work on efficiency-aware code \ngeneration with LLMs.\nThis work systematically extends our previous work\u00a0Li et\u00a0al. (2024b). First, we add more \nAI-powered coding assistants, i.e., Copilot Chat, CodeLlama, and DeepSeek-Coder, to more \ncomprehensively evaluate the performance of AI-generated code and investigate the perfor\u00ad\nmance discrepancy among different models in code generation tasks. Second, we extend our \nevaluation with a newly introduced performance-oriented dataset, EvalPerf, which provides \na richer set of benchmarks for assessing performance regressions in AI-generated code. \nThird, we investigate prompt engineering strategies, including few-shot and CoT prompt\u00ad\ning, to study their impact on performance regressions across different models and datasets. \nIn addition, we incorporated detailed profiling analyses and manual inspections of static \nanalysis results to uncover dominant sources of performance regressions, and included sta\u00ad\ntistical testing to ensure the robustness of observed effects. Finally, we provide a discussion \nof model-specific performance differences, architectural implications, and prompt sensi\u00ad\ntivity, offering insights into how design choices, model characteristics, and prompt strate\u00ad\ngies jointly shape the performance of AI-generated code, providing actionable guidance for \nfuture research on performance-aware AI-assisted code generation.\nThe rest of this paper is organized as follows. Section\u00a02 introduces how to generate code \nusing GitHub Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder and a motivation \nexample. Section\u00a03 presents our case study setup. Section\u00a04 presents detailed results of \nour research questions and findings. Section\u00a05 discusses the implications of our findings. \nSection\u00a06 discusses the threats to the validity of our study. Section\u00a07 presents related prior \nstudies. Section\u00a08 summarizes the conclusion of our study.\n2\u2002 Background and Motivating Example\nIn this section, we first introduce four AI-powered code generation models, GitHub Copilot, \nCopilot Chat, CodeLlama, and DeepSeek-Coder. We then present a motivating example of \nperformance challenges in AI-generated code.\n2.1\u2002 AI-Powered Code Generation Models\nGitHub Copilot is an AI-assisted programming tool that enhances developer productivity \nby providing code generation services. GitHub Copilot empowers programmers by offer\u00ad\ning various forms of code completion. This functionality can be particularly beneficial in \nscenarios where developers have a clear understanding of the desired outcome but require \nassistance in translating that concept into functional code. Copilot offers two primary meth\u00ad\nods for code completion:\n\t\n\u25cf\nDevelopers can select a specific section of code and request Copilot to automatically \ncomplete it. This functionality leverages the surrounding code context to generate rel\u00ad\nevant suggestions.\n1\u202f3\n   62 \n\u2003\nPage 4 of 52\nEmpirical Software Engineering           (2026) 31:62 \n\t\n\u25cf\nDevelopers can use natural language comments to describe their desired functionality. \nCopilot then analyzes these comments and suggests code that aligns with the described \nrequirements.\nGitHub Copilot Chat builds upon GitHub Copilot and represents a more advanced evolu\u00ad\ntion of AI-powered coding"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_006",
    "source_id": "PerformanceAnalysis2026",
    "text": " in translating that concept into functional code. Copilot offers two primary meth\u00ad\nods for code completion:\n\t\n\u25cf\nDevelopers can select a specific section of code and request Copilot to automatically \ncomplete it. This functionality leverages the surrounding code context to generate rel\u00ad\nevant suggestions.\n1\u202f3\n   62 \n\u2003\nPage 4 of 52\nEmpirical Software Engineering           (2026) 31:62 \n\t\n\u25cf\nDevelopers can use natural language comments to describe their desired functionality. \nCopilot then analyzes these comments and suggests code that aligns with the described \nrequirements.\nGitHub Copilot Chat builds upon GitHub Copilot and represents a more advanced evolu\u00ad\ntion of AI-powered coding assistants. Unlike Copilot\u2019s inline code completion, Copilot Chat \nprovides an interactive conversational interface that leverages AI models to support a wide \nrange of developer interactions. This interface enables developers not only to request code \ncompletions, but also to ask for explanations, debugging guidance, and refactoring sugges\u00ad\ntions\u00a0(GitHub Docs 2025a).\nIn addition to Copilot Chat, other AI-powered code generation models, such as CodeL\u00ad\nlama and DeepSeek-Coder have emerged in recent years. These models incorporate more \nadvanced language understanding capabilities and diverse architectural designs, achieving \ncontinuous breakthroughs in code generation accuracy and efficiency. For instance:\n\t\n\u25cf\nCodeLlama, built on the Llama 2 architecture, specializes in code generation and offers \nfeatures such as infilling capabilities, support for large input contexts, and zero-shot \ninstruction-following for programming tasks. It supports multiple programming lan\u00ad\nguages, including Python, C++, Java, PHP, TypeScript, C#, and Bash\u00a0(Rozi\u00e8re et\u00a0al. \n2023).\n\t\n\u25cf\nDeepSeek-Coder provides a range of models scaling from 1B to 33B parameters, fo\u00ad\ncusing on code and natural language tasks. Its features include code completion, code \ninsertion, and repository-level code suggestions\u00a0(Guo et\u00a0al. 2024).\nThis study evaluates the performance and optimization potential of the code generated by \nGitHub Copilot , Copilot Chat, CodeLlama, and DeepSeek-Coder. These tools support vari\u00ad\nous popular programming languages, including Python, Java, TypeScript, and C# \u00a0(GitHub \nDocs 2025b; Rozi\u00e8re et\u00a0al. 2023; Guo et\u00a0al. 2024). In this study, we leverage four models\u2019 \ncapabilities to generate code for datasets encompassing two specific languages: Python and \nJava, including both general-purpose benchmarks and the performance-oriented bench\u00ad\nmark. By generating code across these datasets and languages, we conducted a detailed \nanalysis of the performance impact in a language-specific context, providing insights into \nhow model- and language-specific characteristics affect code performance and optimization \nopportunities.\n2.2\u2002 A Motivating Example of Performance Challenges in AI-Generated Code\nWhile AI-powered code generation models offer significant potential for developers, \na critical aspect to consider is the performance of the generated code. Here, we present \na motivating example highlighting this challenge. One wants to develop a function to \ndetermine if a given integer is a prime number. To achieve this, one uses GitHub Copilot \nwithin the Visual Studio"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_007",
    "source_id": "PerformanceAnalysis2026",
    "text": ", including both general-purpose benchmarks and the performance-oriented bench\u00ad\nmark. By generating code across these datasets and languages, we conducted a detailed \nanalysis of the performance impact in a language-specific context, providing insights into \nhow model- and language-specific characteristics affect code performance and optimization \nopportunities.\n2.2\u2002 A Motivating Example of Performance Challenges in AI-Generated Code\nWhile AI-powered code generation models offer significant potential for developers, \na critical aspect to consider is the performance of the generated code. Here, we present \na motivating example highlighting this challenge. One wants to develop a function to \ndetermine if a given integer is a prime number. To achieve this, one uses GitHub Copilot \nwithin the Visual Studio Code\u00a0(GitHub, Inc. 2025) environment to generate the function. The \ngenerated code, while functionally correct, exhibits performance regressions. As illustrated \nin Fig.\u00a01, the generated code employs a naive approach that iterates from 2 to the original \nnumber, leading to inefficiency. This issue is not exclusive to Copilot. As shown in Fig.\u00a02, \nCodeLlama, for instance, generates code that iterates from 2 to the given number plus one. \n1\u202f3\nPage 5 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nSimilarly, both DeepSeek-Coder and Copilot Chat adopt a comparable strategy, repeatedly \ndividing the number while incrementing the divisor, effectively iterating up to the original \ninput size. It is also worth noting the difference between Copilot and Copilot Chat. While \nboth ultimately produce inefficient factorization routines, Copilot\u2019s implementation simply \nloops through possible divisors up to the original number, whereas Copilot Chat nests an \nadditional inner loop to repeatedly divide by the same factor before incrementing. Although \n# DeepSeek-Coder\n1 def factorize(n: int) -> List[int]:\n2\nfactors = []\n3\ni = 2\n4\nwhile i <= n:\n5\nif n % i == 0:\n6\nfactors.append(i)\n7\nn //= i\n8\nelse:\n9\ni += 1\n10\nreturn factors\n# CodeLlama\n1 def factorize(n: int) -> List[int]:\n2\nfactors = []\n3\nwhile n > 1:\n4\nfor i in range(2, n + 1):\n5\nif n % i == 0:\n6\nfactors.append(i)\n7\nn //= i\n8\nbreak\n9\nreturn factors\n# Human-written Solution \n1 def factorize(n: int) -> List[int]:\n2\nimport math\n3\nfact = []\n4\ni = 2\n5\nwhile i <= int(math.sqrt(n) + 1):\n6\nif n % i == 0:\n7\nfact.append(i)\n8\nn //= i\n9\nelse:\n10\ni += 1\n11\n12\nif n > 1:\n13\nfact.append(n)\n14\nreturn fact\n# Copilot Chat\n1 def factorize(n: int) -> List[int]:\n2\ni = 2\n3\nres = []\n4\nwhile n > 1:\n5\nwhile n % i == 0:\n6\nres.append(i"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_008",
    "source_id": "PerformanceAnalysis2026",
    "text": " \n1 def factorize(n: int) -> List[int]:\n2\nimport math\n3\nfact = []\n4\ni = 2\n5\nwhile i <= int(math.sqrt(n) + 1):\n6\nif n % i == 0:\n7\nfact.append(i)\n8\nn //= i\n9\nelse:\n10\ni += 1\n11\n12\nif n > 1:\n13\nfact.append(n)\n14\nreturn fact\n# Copilot Chat\n1 def factorize(n: int) -> List[int]:\n2\ni = 2\n3\nres = []\n4\nwhile n > 1:\n5\nwhile n % i == 0:\n6\nres.append(i)\n7\nn //= i\n8\ni += 1\n9\nreturn res\nFig. 2\u2002 The generated code for the function to determine all prime factors of a given number from Huma\u00ad\nnEval-25.py by CodeLlama, DeepSeek-Coder, and Copilot Chat, as well as the human-written solution\n \nFig. 1\u2002 An example of generating the function to determine all prime factors of a given number from \nHumanEval-25.py by Copilot\n \n1\u202f3\n   62 \n\u2003\nPage 6 of 52\nEmpirical Software Engineering           (2026) 31:62 \nfunctionally correct, this nested structure introduces redundant checks and can further \nexacerbate performance regressions. This highlights not only the tendency of AI-generated \nsolutions to overlook efficiency but also the variability in generated implementations, \nwhich may differ structurally while sharing the same fundamental inefficiency. In contrast, \na manually written solution optimizes performance by iterating only to the square root of \nthe target number, significantly reducing computational overhead and improving efficiency. \nThis example emphasizes the potential for AI-generated code to introduce performance \nregressions. While these tools and models are capable of producing functionally correct \ncode, they do not consistently prioritize the most efficient implementation. Our study \naims to bridge this gap by analyzing the performance characteristics of code generated by \nGitHub Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder, and exploring techniques \nsuch as few-shot prompt engineering to optimize the efficiency and resource utilization of \nAI-generated code. By understanding these characteristics, we can develop best practices \nand techniques to optimize performance and unlock the full potential of AI-powered coding \nassistants and models.\n3\u2002 Case Study Setup\nIn this section, we present our case study setup. In particular, we present the datasets used \nin our case study and our experimental setup to collect code generated by Copilot, Copilot \nChat, CodeLlama, and DeepSeek-Coder, and performance regression analysis data.\n3.1\u2002 Dataset\nEvaluating the quality and effectiveness of LLMs in code generation requires specialized \ndatasets designed to assess correctness, executability, and performance. These datasets typi\u00ad\ncally include unique identifiers (IDs), natural language descriptions, function names (or \nspecifications), and corresponding test cases. Our study explores four widely used datasets \nfor code generation evaluation, focusing on both Python and Java programming languages:\n\t\u2013\nHumanEval: This handcrafted Python dataset by\u00a0Chen et\u00a0al. (2021) consists of 164 \nproblems. Each problem provides a function name"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_009",
    "source_id": "PerformanceAnalysis2026",
    "text": " by Copilot, Copilot \nChat, CodeLlama, and DeepSeek-Coder, and performance regression analysis data.\n3.1\u2002 Dataset\nEvaluating the quality and effectiveness of LLMs in code generation requires specialized \ndatasets designed to assess correctness, executability, and performance. These datasets typi\u00ad\ncally include unique identifiers (IDs), natural language descriptions, function names (or \nspecifications), and corresponding test cases. Our study explores four widely used datasets \nfor code generation evaluation, focusing on both Python and Java programming languages:\n\t\u2013\nHumanEval: This handcrafted Python dataset by\u00a0Chen et\u00a0al. (2021) consists of 164 \nproblems. Each problem provides a function name, function body, associated test cases, \nand canonical solution. These problems focus on core programming skills like semantic \nunderstanding, algorithm design, and basic math.\n\t\u2013\nMBPP: MBPP is a Python dataset containing 974 problems\u00a0(Austin et\u00a0al. 2021). Each \nproblem is presented with a brief description and corresponding test cases.\n\t\u2013\nAixbench: Designed for Java code generation, Aixbench\u00a0(Hao et\u00a0al. 2022) offers 187 \nproblems, along with function signatures and test cases.\n\t\u2013\nEvalPerf: EvalPerf\u00a0(Liu et\u00a0al. 2024a) is a Python dataset with 118 tasks focused on \nexecution efficiency. It uses computation-intensive inputs and a relative scoring mecha\u00ad\nnism to distinguish solutions by performance rather than correctness.\nThe overview of datasets is shown in Table\u00a01. HumanEval and MBPP datasets are chosen \nfor their broad acceptance within the research community, as highlighted by \u00a0Zheng et\u00a0al. \n(2023) and \u00a0Zan et\u00a0al. (2022). These datasets are widely recognized for their robustness and \n1\u202f3\nPage 7 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nrelevance in code generation. The canonical solutions are authored by senior developers, \nrepresenting high-quality and efficient solutions for specific problems, as documented by \nprior studies \u00a0(Chen et\u00a0al. 2021; Austin et\u00a0al. 2021).\nIn addition, we include the EvalPerf dataset, which focuses specifically on performance-\noriented programming tasks. EvalPerf contains 118 carefully designed challenges, each with \nhigh-computation test inputs, aimed at assessing the efficiency of generated code rather than \nmerely its correctness. By leveraging the Differential Performance Evaluation framework, \nEvalPerf compares new solutions against a set of reference solutions with varying efficiency \nlevels using a cluster-based scoring mechanism, allowing for finer-grained and stable evalu\u00ad\nation of code performance across different tasks and platforms. This makes EvalPerf par\u00ad\nticularly suitable for benchmarking AI-generated code in terms of execution efficiency. By \nincluding EvalPerf alongside general-purpose benchmarks, we ensure that our study covers \nboth functional correctness and performance-critical aspects of AI-generated code.\n3.2\u2002 Experimental Setup\nIn this subsection, we describe the process of collecting the generated code and performance \ndata for each question in our study datasets. Figure\u00a03 illustrates the overall process of our \napproach. We follow four steps to collect the needed data."
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_010",
    "source_id": "PerformanceAnalysis2026",
    "text": " new solutions against a set of reference solutions with varying efficiency \nlevels using a cluster-based scoring mechanism, allowing for finer-grained and stable evalu\u00ad\nation of code performance across different tasks and platforms. This makes EvalPerf par\u00ad\nticularly suitable for benchmarking AI-generated code in terms of execution efficiency. By \nincluding EvalPerf alongside general-purpose benchmarks, we ensure that our study covers \nboth functional correctness and performance-critical aspects of AI-generated code.\n3.2\u2002 Experimental Setup\nIn this subsection, we describe the process of collecting the generated code and performance \ndata for each question in our study datasets. Figure\u00a03 illustrates the overall process of our \napproach. We follow four steps to collect the needed data. In the first step, we prepare \nprompts by parsing the four datasets, i.e., HumanEval, AixBench, MBPP, and Evalperf. In \nthe second step, for each prepared prompt, we feed the prepared prompt to GitHub Copilot, \nCopilot Chat, CodeLlama, and DeepSeek-Coder to generate code. In the third step, we filter \nthe generated code using test cases. Finally, we analyze the performance regressions of the \ngenerated code.\nFig. 3\u2002 An overview of our approach to collecting data\n \nDataset\nLanguage\n#Instances\nYear\nReference\nHumanEval\nPython\n164\n2021\nChen et\u00a0al. (2021)\nAixBench\nJava\n187\n2022\nHao et\u00a0al. (2022)\nMBPP\nPython\n974\n2021\nAustin et\u00a0al. (2021)\nEvalPerf\nPython\n118\n2024\nLiu et\u00a0al. (2024a)\nTable 1\u2002 Overview of datasets \nused in our study\n \n1\u202f3\n   62 \n\u2003\nPage 8 of 52\nEmpirical Software Engineering           (2026) 31:62 \nStep 1: Preparing prompt. The data from HumanEval, MBPP, Evalperf, and Aixbench \ndatasets are stored in JSON files. We first parse these files to extract relevant code infor\u00ad\nmation. New files are created in either .py or .java format for each code snippet requir\u00ad\ning completion. As an example in Fig.\u00a04, the 4.py file is extracted from the corresponding \nHumanEval JSON file. The extracted function name, parameters, and comments are used as \nthe prompt for Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder.\nStep 2: Generating code. In this step, we use code generation models to generate code \nfor each prompt of each question. We designed specific parameter configurations for dif\u00ad\nferent code generation models and datasets to ensure stability and efficiency during the \ngeneration process.\n\t\u2013\nGitHub Copilot: We leverage Copilot within the VSCode\u00a0(Microsoft 2025) environ\u00ad\nment to generate code for each prompt. For each newly created file, we open the com\u00ad\nmand panel using Ctrl+Shift+P and execute GitHub Copilot: Open Completions Panel \nto activate the suggestions panel. Copilot provides 1-10 code completion suggestions. \nWe consistently accept the first suggestion (see Fig.\u00a01) for consistency. GitHub Copi\u00ad\nlot was used with its"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_011",
    "source_id": "PerformanceAnalysis2026",
    "text": " code generation models to generate code \nfor each prompt of each question. We designed specific parameter configurations for dif\u00ad\nferent code generation models and datasets to ensure stability and efficiency during the \ngeneration process.\n\t\u2013\nGitHub Copilot: We leverage Copilot within the VSCode\u00a0(Microsoft 2025) environ\u00ad\nment to generate code for each prompt. For each newly created file, we open the com\u00ad\nmand panel using Ctrl+Shift+P and execute GitHub Copilot: Open Completions Panel \nto activate the suggestions panel. Copilot provides 1-10 code completion suggestions. \nWe consistently accept the first suggestion (see Fig.\u00a01) for consistency. GitHub Copi\u00ad\nlot was used with its default settings, without any modification of configurable param\u00ad\neters\u00a0(Visual Studio Code Docs 2025)\n\t\u2013\nCodeLlama: We selected the CodeLlama-7B model, reflecting a balance between \nmodel capacity and computational overhead: although larger models generally offer \nhigher accuracy, they also demand substantially more computing resources. Conversely, \nsmaller models often exhibit diminished performance on code generation benchmarks \nand may have more performance regressions. We set the temperature to \u201c0.0\u201d and \n\u201ctop_p\u201d to \u201c0.9\u201d to balance randomness and determinism in the output. Additionally, \nwe configured \u201cmax_gen_len=1024\u201d and \u201cmax_batch_size=4\u201d to enable the model to \ngenerate sufficiently long code while supporting efficient batch processing. To accom\u00ad\nmodate the varying prompt lengths across the HumanEval, MBPP, and AixBench data\u00ad\nsets, we adjusted the \u201cmax_seq_len\u201d parameter to 2048, 5120, and 512, respectively. \nFig. 4\u2002 Examples of prompt preparation, corresponding canonical solution, and the test case from the \nHumanEval dataset\n \n1\u202f3\nPage 9 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nSince EvalPerf is derived from HumanEval and MBPP, we use the same parameters as \nMBPP to ensure consistency in evaluation. This ensured that the model could operate \nproperly and effectively utilize contextual information.\n\t\u2013\nDeepSeek-Coder: We selected the deepseek-coder-6.7b-base model, whose parameter \nscale closely approximates that of CodeLlama-7B. This choice ensures a fair comparison \nby minimizing performance regressions arising from differences in model size. Addi\u00ad\ntionally, we set \u201cmax_new_tokens\u201d to 1024, allowing the model to adapt to prompts of \ndifferent lengths. This choice was based on experimental observations, ensuring that the \ngenerated code was of adequate length to meet the requirements of programming tasks \nin the selected datasets, while maintaining fluency and consistency when processing \nlonger prompts.\n\t\u2013\nCopilot Chat: We utilize the Copilot Chat feature in VSCode, which directly inter\u00ad\nacts with the Copilot Chat agent (default GPT-4.1). Input files are passed to Copilot \nChat to generate code suggestions. Since Copilot Chat is a production-level tool with \nadvanced internal optimizations, we retain the default settings without manual tuning. \nThis ensures a realistic and practical evaluation of its code generation capabilities across \ndatasets.\nAcross all dataset experiments, we followed a"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_012",
    "source_id": "PerformanceAnalysis2026",
    "text": " \ndifferent lengths. This choice was based on experimental observations, ensuring that the \ngenerated code was of adequate length to meet the requirements of programming tasks \nin the selected datasets, while maintaining fluency and consistency when processing \nlonger prompts.\n\t\u2013\nCopilot Chat: We utilize the Copilot Chat feature in VSCode, which directly inter\u00ad\nacts with the Copilot Chat agent (default GPT-4.1). Input files are passed to Copilot \nChat to generate code suggestions. Since Copilot Chat is a production-level tool with \nadvanced internal optimizations, we retain the default settings without manual tuning. \nThis ensures a realistic and practical evaluation of its code generation capabilities across \ndatasets.\nAcross all dataset experiments, we followed a consistent generation procedure for all \nfiles requiring completion within the same model and dataset. The generated files were \nsequentially named, such as HumanEval-0 through -163, MBPP-1 through -974, and \nAixBench-0 through -186. For EvalPerf, we retained its original task identifier format, \nwhich follows HumanEval_# and Mbpp_#, to ensure consistency with the dataset \ndesign.\nStep 3. Filtering generated code. In this step, we filter the generated code from the \nlast step by executing the corresponding test case. We execute the corresponding test \ncases on each generated code to evaluate correctness. Code that compiles successfully \nand passes the tests is retained. The correctness rates of this filtering process are shown \nin Table\u00a02.\nStep 4. Analyzing performance regressions. To comprehensively evaluate the perfor\u00ad\nmance regressions of generated code, we employ both static and dynamic analyses to exam\u00ad\nine the aforementioned filtered generated code. Static analysis is used to identify factors \nthat may lead to performance regressions, while dynamic analysis observes differences in \nruntime, CPU usage, and memory consumption.\nDataset\n#Instances Copilot\nCodeLlama\nDeep\u00ad\nSeek-\nCoder\nCopilot \nChat\nHumanE\u00ad\nval\n164\n134 \n(81.7%)\n110 (67.1%) 84 \n(51.2%)\n151 \n(92.1%)\nAixBench\n175\n88 \n(50.3%)\n71 (40.6%)\n66 \n(37.7%)\n114 \n(65.1%)\nMBPP\n974\n856 \n(87.9%)\n723 (74.2%) 815 \n(83.7%)\n828 \n(85.0%)\nEvalPerf\n118\n103 \n(87.3%)\n97 (82.2%)\n91 \n(77.1%)\n109 \n(92.4%)\nTable 2\u2002 The correctness rates of \nCopilot, CodeLlama, DeepSeek-\nCoder, and Copilot Chat\n \n1\u202f3\n   62 \n\u2003\nPage 10 of 52\nEmpirical Software Engineering           (2026) 31:62 \n3.2.1\u2002 Static Performance Regression Analysis\nWe use three static analysis tools: Qodana\u00a0 (JetBrains s.r.o 2024),\u00a0 SpotBugs (2024), \nand\u00a0PMD (2024) to investigate potential performance regression issues in the generated \ncode. Qodana is designed for Python programs. It integrates with CI processes and provides \nin"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_013",
    "source_id": "PerformanceAnalysis2026",
    "text": "%)\n109 \n(92.4%)\nTable 2\u2002 The correctness rates of \nCopilot, CodeLlama, DeepSeek-\nCoder, and Copilot Chat\n \n1\u202f3\n   62 \n\u2003\nPage 10 of 52\nEmpirical Software Engineering           (2026) 31:62 \n3.2.1\u2002 Static Performance Regression Analysis\nWe use three static analysis tools: Qodana\u00a0 (JetBrains s.r.o 2024),\u00a0 SpotBugs (2024), \nand\u00a0PMD (2024) to investigate potential performance regression issues in the generated \ncode. Qodana is designed for Python programs. It integrates with CI processes and provides \nin-depth inspections across multiple languages, identifying errors, code smells, and stan\u00ad\ndard violations. Spotbugs is an open-source tool for Java, which focuses on detecting bugs \nand vulnerabilities, with an emphasis on runtime errors and non-standard practices. PMD \ncan identify potential flaws and complexity issues, promote code style consistency, and \nfocus on optimization opportunities.\nSince built-in rules in the three tools might not comprehensively cover performance \nregression, we develop custom rules based on extensive literature research and industry \ndocumentation. We use keywords such as \u201cperformance degradation\u201d, \u201cperformance regres\u00ad\nsion\u201d, \u201canti-pattern\u201d, and \u201ccode smell\u201d to search in the ACM Digital Library, IEEE Xplore \nDigital Library, Springer Link Digital Library, and Google Scholar. Based on the relevant \nliterature retrieved from the above paper databases, we manually filter and obtain the code \nperformance regression rules. In addition, we find some code performance regression rules \nin industrial documentation, such as\u00a0SonarQube (2024). This search result covers anti-pat\u00ad\nterns, code smells, and predefined PMD rules related to performance. The customized rules \nare categorized into six aspects: Performance Regression, Bad Practice, Dodgy Code, Error \nProne, Bad Design, and Multithreading, to provide a structured approach to identifying per\u00ad\nformance regression. Finally, we identify a total of 159 rules related to performance regres\u00ad\nsions, i.e., \u201cManually copying data between two arrays is inefficient. Please use a more \nefficient native array assignment method instead\"\u00a0(PMD 2024). The complete definitions \nand configurations of these custom rules are publicly available in our replication package.\nFor Spotbugs, we use its Idea-based plugin. Initially, we open a given project in Idea for \ninspection. Subsequently, we scan the project using the Spotbugs plugin. We can then obtain \nthe performance regression results of Spotbugs detection. For PMD, we first create a new \nXML file and include all custom rules within the <ruleset> element. We then define a \n<rule> element for each rule and set various attributes for the rule within this element. \nNext, we write XPath expressions or Java classes to implement the matching logic for corre\u00ad\nsponding rules. For Qodana, we first create new projects on Qodana Cloud. We then upload \nthe generated HumanEval, MBPP, and EvalPerf datasets to projects. In this way, Qodana \ncan automatically identify and highlight potential performance regression issues.\n3.2.2\u2002 Dynamic Performance Regression Analysis\nDynamic analysis is conducted on Python datasets from HumanEval,"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_014",
    "source_id": "PerformanceAnalysis2026",
    "text": ". For PMD, we first create a new \nXML file and include all custom rules within the <ruleset> element. We then define a \n<rule> element for each rule and set various attributes for the rule within this element. \nNext, we write XPath expressions or Java classes to implement the matching logic for corre\u00ad\nsponding rules. For Qodana, we first create new projects on Qodana Cloud. We then upload \nthe generated HumanEval, MBPP, and EvalPerf datasets to projects. In this way, Qodana \ncan automatically identify and highlight potential performance regression issues.\n3.2.2\u2002 Dynamic Performance Regression Analysis\nDynamic analysis is conducted on Python datasets from HumanEval, MBPP , and EvalPerf, \nfocusing on runtime, memory usage, and CPU utilization. We conduct this analysis using \nthree profiling tools:\ncProfile\u00a0(Python Software Foundation 2025b): A Python library for performance analy\u00ad\nsis, providing metrics like the number of function calls and time spent in functions.\ntracemalloc\u00a0(Python Software Foundation 2025c): A module for tracing memory blocks \nallocated by Python, providing detailed statistics on memory allocation, including trace\u00ad\nbacks, memory usage per file/line, and the ability to detect memory leaks by comparing \nsnapshots.\n1\u202f3\nPage 11 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nPsutil\u00a0(Python Software Foundation 2025a): A library for process and system utilization \ninformation, particularly useful for CPU monitoring.\nThe dynamic analysis involves running the generated code and collecting data on its \nperformance metrics. This data will be used to evaluate the performance characteristics of \nthe code. The hardware configuration for the experiment includes an Intel Core i9-13900K \nprocessor, 128 GB of RAM, 1 TB SSD for primary storage, and 8 TB HDD for secondary \nstorage, and the system operates on Ubuntu 22.04.4 LTS. By following the experimental \nsetup, we aim to achieve a thorough and systematic evaluation of the performance regres\u00ad\nsions of AI-generated code, considering both static and dynamic aspects, and providing \nactionable insights for code optimization and tool improvement.\n4\u2002 Case Study Results\nIn this section, we present our case study results by answering three research questions. For \neach research question, we show the motivation, approach, and corresponding results of the \nresearch question.\n4.1\u2002 RQ1: How Prevalent are Performance Regressions in AI-Generated Code?\nMotivation\u2002 While prior research has focused on evaluating the correctness and security \nof AI-generated code, performance regression has received less attention. However, intui\u00ad\ntively, AI-generated code models may not fully grasp the developer\u2019s performance goals, \npotentially leading to code that prioritizes functionality over efficiency. On the other hand, \ntraining data for code generation models might not explicitly emphasize performance con\u00ad\nsiderations, impacting the models\u2019 ability to generate efficient code. Given these potential \nshortcomings, it\u2019s crucial to investigate the prevalence of performance regressions in AI-\ngenerated code. Understanding the scope of this issue will inform future research directions \nand development efforts for AI-assisted coding tools and code generation models.\nApproach\u2002 Our approach involves two strategies"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_015",
    "source_id": "PerformanceAnalysis2026",
    "text": " has focused on evaluating the correctness and security \nof AI-generated code, performance regression has received less attention. However, intui\u00ad\ntively, AI-generated code models may not fully grasp the developer\u2019s performance goals, \npotentially leading to code that prioritizes functionality over efficiency. On the other hand, \ntraining data for code generation models might not explicitly emphasize performance con\u00ad\nsiderations, impacting the models\u2019 ability to generate efficient code. Given these potential \nshortcomings, it\u2019s crucial to investigate the prevalence of performance regressions in AI-\ngenerated code. Understanding the scope of this issue will inform future research directions \nand development efforts for AI-assisted coding tools and code generation models.\nApproach\u2002 Our approach involves two strategies to evaluate the performance of AI-gener\u00ad\nated code. In particular, for static performance regression analysis, we employ industry-\nstandard tools, i.e., Spotbugs and PMD, to scan the generated Java code in the AixBench \ndataset. These tools are equipped with pre-defined rules that can detect performance regres\u00ad\nsions within the code. The detailed configuration of these rules is available in the replication \npackage we provided. For Python code in the HumanEval, MBPP, and EvalPerf datasets, \nwe use Qodana, a cloud-based static analysis platform, to identify potential performance \nregressions specific to Python code. To facilitate efficient analysis, we create new proj\u00ad\nects and establish a dedicated scan workflow within Qodana Cloud. This workflow enables \nQodana to automatically identify and highlight potential performance-related code issues \nwithin the generated Python code.\nFor dynamic performance regression analysis, we compare the generated code with \ncanonical solutions from the HumanEval and MBPP datasets. For the EvalPerf dataset, \nno explicit canonical solution is provided. Instead, each problem in EvalPerf is associated \nwith multiple reference implementations, each annotated with an efficiency score. Since a \n1\u202f3\n   62 \n\u2003\nPage 12 of 52\nEmpirical Software Engineering           (2026) 31:62 \nhigher efficiency score indicates better performance\u00a0(Liu et\u00a0al. 2024a), we select the refer\u00ad\nence implementation with the highest efficiency score as the canonical solution for that \nproblem. This enables us to conduct performance regression analysis in a manner consistent \nwith the HumanEval and MBPP datasets. Using the dynamic performance regression detec\u00ad\ntion modules, we conduct dynamic performance regression analysis on the generated and \ncanonical code sets of these three datasets. The Python scripts generated for the HumanEval \nand MBPP datasets are typically short and have brief single-run execution times. To gener\u00ad\nate more robust performance data, we adopt a technique called repetitive iteration mea\u00ad\nsurement\u00a0(Laaber and Leitner 2018; Ding et\u00a0al. 2020; Jangali et\u00a0al. 2023). This technique \nextends the runtime of the generated code by increasing the number of iterations within \nthe test cases, allowing profiling tools to capture more comprehensive performance data. \nWe achieve this extension by adding a for loop at the beginning of the test cases. This loop \ncauses the existing test cases to be executed repeatedly. Figure\u00a05 illustrates this modifica\u00ad\ntion for the script shown in Fig.\u00a04. Once the iterations have been increased, we encapsulate \neach"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_016",
    "source_id": "PerformanceAnalysis2026",
    "text": " gener\u00ad\nate more robust performance data, we adopt a technique called repetitive iteration mea\u00ad\nsurement\u00a0(Laaber and Leitner 2018; Ding et\u00a0al. 2020; Jangali et\u00a0al. 2023). This technique \nextends the runtime of the generated code by increasing the number of iterations within \nthe test cases, allowing profiling tools to capture more comprehensive performance data. \nWe achieve this extension by adding a for loop at the beginning of the test cases. This loop \ncauses the existing test cases to be executed repeatedly. Figure\u00a05 illustrates this modifica\u00ad\ntion for the script shown in Fig.\u00a04. Once the iterations have been increased, we encapsulate \neach script from the HumanEval and MBPP datasets within a function. We then use the \ncProfile module to profile these functions. Extracting the \u201ccumtime\u201d metric from the profil\u00ad\ning results reveals the script\u2019s overall runtime. For the EvalPerf dataset, we do not perform \niterative repetition, since its test cases are already performance-oriented and comparatively \nlarger, making additional iterations unnecessary. Instead, each script is directly profiled in \nits original form.\nWe use both domain-level performance metrics, i.e., execution time, and physical-level \nperformance metrics, i.e., CPU and memory usage, as measurements of performance regres\u00ad\nsions. Memory usage is monitored via the tracemalloc module, which reports both the cur\u00ad\nrent memory consumption at the end of execution and the peak memory consumption during \nexecution. CPU utilization is measured using psutil in combination with cProfile, convert\u00ad\ning runtime statistics into CPU usage ratios to provide a system-level view of resource \nutilization\u00a0(Tanenbaum 2015; Psutil Documentation Team 2025).\nResults\u2002 Performance regressions are not rare instances in code generated by Copilot, Copi\u00ad\nlot Chat, CodeLlama, and DeepSeek-Coder.\nFor Java code, static analysis using SpotBugs and PMD revealed notable performance \nregressions across all four models. Specifically, Copilot-generated code contained 8 low-\nperformance instances identified by SpotBugs and 41 detected by PMD. In comparison, \nCodeLlama produced a similar number of regressions, with 8 identified by SpotBugs and 46 \nby PMD, while DeepSeek-Coder exhibited relatively fewer regressions, with 5 identified by \nSpotBugs and 22 by PMD. Copilot Chat showed the highest number of flagged issues, with \n6 low-performance cases from SpotBugs and up to 74 cases from PMD, indicating more \nsevere static performance concerns compared to the other models. These findings align with \nFig. 5\u2002 An example of increasing the number of iterations within the test case of Fig.\u00a04\n \n1\u202f3\nPage 13 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nthe differences in functional pass rates on the AixBench dataset, where GitHub Copilot \nachieved 50.3%, CodeLlama 40.6%, DeepSeek-Coder 37.7%, and Copilot Chat 65.1%. \nAlthough DeepSeek-Coder produced the lowest pass rate, its static performance regression \ncounts were lower, suggesting relatively better structural"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_017",
    "source_id": "PerformanceAnalysis2026",
    "text": " \nsevere static performance concerns compared to the other models. These findings align with \nFig. 5\u2002 An example of increasing the number of iterations within the test case of Fig.\u00a04\n \n1\u202f3\nPage 13 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nthe differences in functional pass rates on the AixBench dataset, where GitHub Copilot \nachieved 50.3%, CodeLlama 40.6%, DeepSeek-Coder 37.7%, and Copilot Chat 65.1%. \nAlthough DeepSeek-Coder produced the lowest pass rate, its static performance regression \ncounts were lower, suggesting relatively better structural efficiency despite weaker func\u00ad\ntional correctness. Conversely, Copilot Chat exhibited the best functional correctness on \nAixBench but suffered from the highest number of performance regressions, highlighting a \ntrade-off between functional accuracy and performance efficiency.\nFor Python code, static analysis using Qodana identified substantial performance regres\u00ad\nsions across the HumanEval, MBPP, and Evalperf datasets. Copilot-generated code had 14, \n274 and 26 instances of potential performance regressions in these datasets, respectively. \nCodeLlama performed slightly better, with 8, 188 and 17 instances identified. DeepSeek-\nCoder exhibited the lowest number of issues in the HumanEval dataset (5 instances) but \nshowed significantly more in the MBPP(262) and EvalPerf (23) datasets. Copilot Chat \nreported 7 regressions on HumanEval, 66 on MBPP, and 3 on EvalPerf. When considering \nboth regressions and the number of passed instances, Copilot Chat consistently achieved \nthe most favorable balance. Specifically, it reached the higher pass counts (151 on Huma\u00ad\nnEval, 828 on MBPP, and 109 on EvalPerf), which corresponded to the lowest regression \nrates of 4.6%, 8.0%, and 2.8%, respectively. In comparison, GitHub Copilot and DeepSeek-\nCoder showed high regression rates on MBPP (32.0% and 32.1%) and EvalPerf (25.2% \nand 25.3%). CodeLlama achieved moderate regression rates (7.3% on HumanEval, 26.0% \non MBPP, and 17.5% on EvalPerf). These variations highlight differences in how AI mod\u00ad\nels handle performance optimization across datasets. While GitHub Copilot demonstrated \nbroad applicability in functional implementation, it showed relatively more instances of \nperformance regressions in Qodana\u2019s static analysis. CodeLlama exhibited better perfor\u00ad\nmance in Qodana\u2019s analysis, although its code accuracy was occasionally lower than that of \nCopilot. DeepSeek-Coder\u2019s performance in detecting instances of performance regressions \nfell between the two, with slightly better results than Copilot on the HumanEval dataset. \nBy contrast, Copilot Chat achieved the most favorable balance in Qodana\u2019s static analy\u00ad\nsis, combining the highest pass counts with the lowest potential regression rates across all \ndatasets.\nDynamic analysis further confirmed that AI-generated code often lags behind human-\nwritten solutions. To determine whether observed performance differences constitute mean\u00ad\ningful regressions, we adopt a statistical testing approach similar"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_018",
    "source_id": "PerformanceAnalysis2026",
    "text": "ions in Qodana\u2019s static analysis. CodeLlama exhibited better perfor\u00ad\nmance in Qodana\u2019s analysis, although its code accuracy was occasionally lower than that of \nCopilot. DeepSeek-Coder\u2019s performance in detecting instances of performance regressions \nfell between the two, with slightly better results than Copilot on the HumanEval dataset. \nBy contrast, Copilot Chat achieved the most favorable balance in Qodana\u2019s static analy\u00ad\nsis, combining the highest pass counts with the lowest potential regression rates across all \ndatasets.\nDynamic analysis further confirmed that AI-generated code often lags behind human-\nwritten solutions. To determine whether observed performance differences constitute mean\u00ad\ningful regressions, we adopt a statistical testing approach similar to prior work\u00a0(Chen et\u00a0al. \n2022). For each generated code and its canonical code, we execute each script 11 times, \ndiscard the first run to eliminate warm-up effects, and use the results from the remaining \n10 runs for statistical analysis. Specifically, we compare performance metrics using the \nMann-Whitney U test to identify statistically significant differences (p-value < 0.05). To \ncomplement significance testing, we calculate Cliff\u2019s \u03b4 to quantify effect sizes. Only differ\u00ad\nences with medium or large effect sizes (0.33 < Cli\ufb00\u2019s \u03b4 \u22640.474 for medium, \u03b4 > 0.474 \nfor large) are considered indicative of performance regression. This approach ensures that \nour regression analysis accounts for both statistical significance and practical impact, miti\u00ad\ngating misleading conclusions that could arise from trivial differences or small sample sizes.\nTo further strengthen the causal interpretation of detected performance regressions and \nmitigate confounding effects arising from code structure differences (e.g., imports, logging, \nor unused statements), we employ a two-stage profiling design. In the first stage, we perform \nstandard dynamic profiling on all generated and canonical implementations across the three \n1\u202f3\n   62 \n\u2003\nPage 14 of 52\nEmpirical Software Engineering           (2026) 31:62 \nPython datasets to collect runtime, memory, and CPU utilization statistics. This stage iden\u00ad\ntifies cases exhibiting statistically significant performance regressions based on the afore\u00ad\nmentioned tests. In the second stage, for those regression cases only, we conduct refined \nprofiling focused exclusively on the critical execution path of the generated implementation. \nInstead of profiling the entire script\u2013which may include initialization or I/O overhead\u2013we \nisolate the execution of the core verification function and measure function-level runtime \ncontributions. This refined profiling provides a fine-grained view of which internal func\u00ad\ntions dominate execution time and clarifies whether the observed regressions stem from \nalgorithmic inefficiencies rather than incidental structural variations.\nIn the HumanEval dataset, among 134 functionally correct scripts generated by Copi\u00ad\nlot, 38 showed significant runtime discrepancies. Additionally, 10 scripts exhibited notable \nmemory usage discrepancies, and 56 scripts displayed substantial CPU utilization gaps. \nOverall, 77 scripts demonstrated significant regression in at least one performance metric \ncompared to the human-written code. The MBPP dataset presents a different set of dispari\u00ad\nties. 142, 68, and 333 scripts contain performance regression in runtime, memory usage, and \nCPU utilization, respectively. The EvalPerf dataset shows"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_019",
    "source_id": "PerformanceAnalysis2026",
    "text": " func\u00ad\ntions dominate execution time and clarifies whether the observed regressions stem from \nalgorithmic inefficiencies rather than incidental structural variations.\nIn the HumanEval dataset, among 134 functionally correct scripts generated by Copi\u00ad\nlot, 38 showed significant runtime discrepancies. Additionally, 10 scripts exhibited notable \nmemory usage discrepancies, and 56 scripts displayed substantial CPU utilization gaps. \nOverall, 77 scripts demonstrated significant regression in at least one performance metric \ncompared to the human-written code. The MBPP dataset presents a different set of dispari\u00ad\nties. 142, 68, and 333 scripts contain performance regression in runtime, memory usage, and \nCPU utilization, respectively. The EvalPerf dataset shows more inefficiencies. 74, 36, and \n51 scripts exhibited significant performance regressions in execution time, memory usage, \nand CPU utilization, respectively. Table\u00a03 presents the performance regressions observed in \nCopilot-generated code across these datasets, showing the number of notable performance \nregressions among scripts that passed the test cases.\nCompared to Copilot, CodeLlama exhibited slightly fewer performance regressions in \nHumanEval and MBPP, but more regressions in EvalPerf. Among 110 functionally correct \nscripts in HumanEval, 14 had significant runtime differences, 2 displayed memory inef\u00ad\nficiencies, and 31 showed CPU utilization regressions. In total, 43 scripts (39%) demon\u00ad\nstrated substantial regressions in at least one metric. Similar trends were observed in MBPP, \nwhere 113 scripts had runtime regressions, 51 had memory inefficiencies, and 215 exhibited \nCPU utilization regressions. In the EvalPerf dataset, among 97 functionally correct scripts, \n72, 43, and 49 scripts showed regressions in execution time, memory usage, and CPU uti\u00ad\nlization, respectively. Although Copilot has more functionally correct scripts (103) and \nthus a higher absolute number of regressions, the regression rates of Codellama are higher, \nindicating worse overall performance. DeepSeek-Coder displayed further regressions in \nthe HumanEval dataset. Among 84 functionally correct scripts in HumanEval, 27 had sig\u00ad\nnificant runtime differences, 8 displayed memory inefficiencies, and 29 showed excessive \nCPU utilization. 49 scripts exhibited regressions in at least one metric. Its performance \nin the MBPP dataset was slightly better compared to Copilot and CodeLlama, with 115, \n42, and 286 scripts demonstrating regressions in runtime, memory usage, and CPU utiliza\u00ad\ntion, respectively. However, on the EvalPerf dataset, its performance was worse than both \nCopilot and CodeLlama, with 70, 35, and 47 scripts showing regressions in execution time, \nmemory usage, and CPU utilization, respectively.\nCopilot Chat exhibited the strongest static performance across all datasets, achieving \nthe highest pass rates (92.1% on HumanEval, 85.0% on MBPP, and 92.4% on EvalPerf) \nand the lowest Qodana regression counts (7, 66, and 3, respectively). However, dynamic \nprofiling revealed the opposite trend. On HumanEval, 50 scripts showed execution time \nregressions, 45 memory inefficiencies,"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_020",
    "source_id": "PerformanceAnalysis2026",
    "text": ", on the EvalPerf dataset, its performance was worse than both \nCopilot and CodeLlama, with 70, 35, and 47 scripts showing regressions in execution time, \nmemory usage, and CPU utilization, respectively.\nCopilot Chat exhibited the strongest static performance across all datasets, achieving \nthe highest pass rates (92.1% on HumanEval, 85.0% on MBPP, and 92.4% on EvalPerf) \nand the lowest Qodana regression counts (7, 66, and 3, respectively). However, dynamic \nprofiling revealed the opposite trend. On HumanEval, 50 scripts showed execution time \nregressions, 45 memory inefficiencies, and 73 CPU utilization regressions. The disparities \nbecame more pronounced in MBPP, with 393 scripts affected by execution time regres\u00ad\nsions, 275 by memory usage, and 355 by CPU utilization. EvalPerf also showed ineffi\u00ad\n1\u202f3\nPage 15 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nciencies, with 82, 39, and 62 regressions in execution time, memory, and CPU utilization, \nrespectively. These results suggest that while Copilot Chat tends to generate functionally \ncorrect and statically robust code, it is more prone to runtime inefficiencies compared \nto other models. The dynamic performance analysis of Copilot, CodeLlama, DeepSeek-\nCoder, and Copilot Chat consistently revealed notable performance regressions, with \nvarying degrees of severity depending on the dataset and performance metric. Overall, \nCopilot, CodeLlama, and DeepSeek-Coder exhibited moderate levels of performance \nregressions on HumanEval and MBPP. In contrast, Copilot Chat showed significantly \nhigher inefficiencies across all dynamic metrics. EvalPerf proved to be the most demand\u00ad\ning dataset overall, with all four models exhibiting higher regression rates, and Copilot \nTable 3\u2002 Comparison of static and dynamic performance regression analysis for different models: GitHub \nCopilot, CodeLlama, DeepSeek-Coder, and Copilot Chat\nStatic performance regression \nanalysis\nDynamic performance \nregression analysis\n Model\nDataset\n#Passed \ninstances \n(%)\nSpotBugs\nPMD\nQodana\nEx\u00ad\necu\u00ad\ntion \ntime\nMem\u00ad\nory \nusage\nCPU \nUtiliza\u00ad\ntion\nHumanEval\n134 \n(81.7%)\nN/A\nN/A\n14\n38\n10\n56\nGitHub Copilot\nMBPP\n856 \n(87.9%)\nN/A\nN/A\n274\n142\n68\n333\nEvalPerf\n103 \n(87.3%)\nN/A\nN/A\n26\n74\n36\n51\nAixBench\n88 \n(50.3%)\n8\n41\nN/A\nN/A\nN/A\nN/A\nHumanEval\n110 \n(67.1%)\nN/A\nN/A\n8\n14\n2\n31\nCodeLlama\nMBPP\n723 \n(74.2%)\nN/A\nN/A\n188\n113\n51\n215\nEvalPerf\n97 \n(82.2%)\nN/A\nN/A\n17\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_021",
    "source_id": "PerformanceAnalysis2026",
    "text": " \n(87.9%)\nN/A\nN/A\n274\n142\n68\n333\nEvalPerf\n103 \n(87.3%)\nN/A\nN/A\n26\n74\n36\n51\nAixBench\n88 \n(50.3%)\n8\n41\nN/A\nN/A\nN/A\nN/A\nHumanEval\n110 \n(67.1%)\nN/A\nN/A\n8\n14\n2\n31\nCodeLlama\nMBPP\n723 \n(74.2%)\nN/A\nN/A\n188\n113\n51\n215\nEvalPerf\n97 \n(82.2%)\nN/A\nN/A\n17\n72\n43\n49\nAixBench\n71 \n(40.6%)\n8\n46\nN/A\nN/A\nN/A\nN/A\nHumanEval\n84 \n(51.2%)\nN/A\nN/A\n5\n27\n8\n29\nDeepSeek-Coder\nMBPP\n815 \n(83.7%)\nN/A\nN/A\n262\n115\n42\n286\nEvalPerf\n91 \n(77.1%)\nN/A\nN/A\n23\n70\n35\n47\nAixBench\n66 \n(37.7%)\n5\n22\nN/A\nN/A\nN/A\nN/A\nHumanEval\n151 \n(92.1%)\nN/A\nN/A\n7\n50\n45\n73\nCopilot Chat\nMBPP\n828 \n(85.0%)\nN/A\nN/A\n66\n393\n275\n355\nEvalPerf\n109 \n(92.4%)\nN/A\nN/A\n3\n82\n39\n62\nAixBench\n114 \n(65.1%)\n6\n74\nN/A\nN/A\nN/A\nN/A\n1\u202f3\n   62 \n\u2003\nPage 16 of 52\nEmpirical Software Engineering           (2026) 31:62 \nChat again ranking the lowest. These results indicate that although Copilot Chat generates \nthe most correct and statically robust code, this comes at the cost of runtime efficiency.\nPhysical performance metrics are important complementary indicators of per\u00ad\nformance regressions in generated code. We use the two physical performance metrics, \ni.e., CPU utilization and memory usage, to measure performance regression. Our findings \nindicate that when considering physical performance metrics, we can identify additional \ninstances of performance regression that might have been overlooked if we had relied solely \non execution time. Specifically, within the HumanEval dataset generated by Copilot, we \nfind 37 and 3 code instances exhibiting performance regressions in terms of CPU utilization \nand memory usage, even though their execution time seemed acceptable. These findings \nemphasize the importance of considering a multi-faceted approach to performance evalua\u00ad\ntion during code generation.\nTo further ensure that the identified performance regressions genuinely originate from \nalgorithmic inefficiencies rather than superficial structural variations, we conducted a \nrefined profiling analysis on the subset of generated code that exhibited significant per\u00ad\nformance regressions. Instead of profiling the entire script\u2013which may include imports, \ninitialization routines, or logging overhead\u2013we restricted the analysis to the invocation of \nthe verification function (check"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_022",
    "source_id": "PerformanceAnalysis2026",
    "text": " the HumanEval dataset generated by Copilot, we \nfind 37 and 3 code instances exhibiting performance regressions in terms of CPU utilization \nand memory usage, even though their execution time seemed acceptable. These findings \nemphasize the importance of considering a multi-faceted approach to performance evalua\u00ad\ntion during code generation.\nTo further ensure that the identified performance regressions genuinely originate from \nalgorithmic inefficiencies rather than superficial structural variations, we conducted a \nrefined profiling analysis on the subset of generated code that exhibited significant per\u00ad\nformance regressions. Instead of profiling the entire script\u2013which may include imports, \ninitialization routines, or logging overhead\u2013we restricted the analysis to the invocation of \nthe verification function (check(candidate)), thereby capturing only the execution \npath of the generated implementation itself. For each profiled case, function-level statistics \nsuch as the number of calls, total self-time, and cumulative execution time were collected \nand ranked by cumulative cost. We further computed each function\u2019s relative contribution \nto the total runtime, quantifying the proportion of overall execution time attributable to \neach code component. By filtering out external library calls and module-level overhead, \nthis analysis highlights the internal functions within the generated code that dominate run\u00ad\ntime performance. The resulting critical-path profiles provide stronger causal evidence that \nthe observed performance regressions are driven by inefficiencies inherent to the gener\u00ad\nated algorithms rather than incidental structural factors. This refinement complements the \naggregate performance metrics reported earlier and enhances the validity of the prevalence \nanalysis presented.\nAs shown in Table\u00a04, the majority of execution time is concentrated in the task-specific \nfunction filter_by_substring, which accounts for approximately 75% of the total \nruntime within the critical path. Built-in operations such as append contribute only mar\u00ad\nginally. This observation indicates that the primary performance bottleneck stems from the \ngenerated function\u2019s internal logic rather than from external or structural code components, \nreinforcing the causal interpretation of the observed performance regressions. Furthermore, \nwe have applied the same critical-path profiling procedure to all cases identified as perfor\u00ad\nFunction\nLocation\nCalls Cumulative \nTime (s)\nPro\u00ad\npor\u00ad\ntion \n(%)\ncheck\ncandidate \nfile\n1\n6.79\u00d710\u22125\n100.0\nfilter_by_substring candidate \nfile\n120\n5.08\u00d710\u22125\n74.9\nappend (list)\nbuilt-in\n270\n1.50\u00d710\u22125\n22.1\ndisable (Profiler)\nbuilt-in\n1\n5.84\u00d710\u22126\n8.6\nTable 4\u2002 Representative \ncritical-path profiling result for \na performance-regressed case \n(HumanEval_7 generated by \nCopilot)\n \n1\u202f3\nPage 17 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nmance regressions across Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder on the \nHumanEval, MBPP, and EvalPerf datasets. The complete profiling results are included in \nour replication package.\n4.2\u2002 RQ2: What are the Root Causes in the Generated Code that Lead to Performance \nRegression?\nMotivation\u2002 In RQ1, we find that there are prevalent performance regressions in the AI-\ngenerated code. While identifying"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_023",
    "source_id": "PerformanceAnalysis2026",
    "text": "a performance-regressed case \n(HumanEval_7 generated by \nCopilot)\n \n1\u202f3\nPage 17 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nmance regressions across Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder on the \nHumanEval, MBPP, and EvalPerf datasets. The complete profiling results are included in \nour replication package.\n4.2\u2002 RQ2: What are the Root Causes in the Generated Code that Lead to Performance \nRegression?\nMotivation\u2002 In RQ1, we find that there are prevalent performance regressions in the AI-\ngenerated code. While identifying these regressions is crucial, a more profound understand\u00ad\ning of the root causes is essential for mitigating their impact. By pinpointing the reasons and \npatterns that lead to performance regressions, we can not only provide valuable insights that \ncan inform the development of coding assistants like Copilot but also potentially guide more \nAI-powered code generation models toward generating more efficient code. We can also \ninform developers with guidance on how to recognize potential performance pitfalls during \nthe code generation process with coding assistants and code generation models.\nApproach\u2002 To investigate the underlying root causes responsible for performance regres\u00ad\nsions in AI-generated code, we employ a qualitative research approach known as open cod\u00ad\ning. This method involves manually examining code samples, allowing us to uncover root \ncauses that contribute to performance regressions. Given the observed performance regres\u00ad\nsion instances across all four models, we conducted open coding for the generated code \nfrom all models. This approach enables a comprehensive identification of the root causes \nof performance regressions. Such an in-depth analysis not only provides targeted optimiza\u00ad\ntion insights for practical development scenarios but also serves as a valuable reference for \nimproving the performance of code generation models in general. We recognize that directly \nanalyzing code root causes can introduce potential subjectivity and bias. To mitigate this \nconcern, we follow a rigorous process inspired by prior research methodologies\u00a0(Zeng et\u00a0al. \n2019; Ding et\u00a0al. 2020):\n\t\u2013\nDual coding. Two authors independently analyze a consistent set of generated code \ninstances with identified performance regressions of at least one performance metric, \nalongside their canonical code. This step is critical for surfacing any inconsistencies in \nthe interpretation of code root causes.\n\t\u2013\nDisagreement resolution. When discrepancies arise between the initial analyses, a \nthird author facilitates a discussion. This collaborative review refines and aligns the \nidentified root causes for performance regressions.\n1\u202f3\n   62 \n\u2003\nPage 18 of 52\nEmpirical Software Engineering           (2026) 31:62 \n\t\u2013\nIterative analysis. The examination is repeated in an iterative process until no new root \ncauses of code-related performance regressions are discovered, indicating that a com\u00ad\nprehensive understanding of the prevalent root causes has been achieved.\nTo quantify the reliability of our dual coding analysis, we calculate the Cohen\u2019s Kappa sta\u00ad\ntistic, which yielded a considerable agreement score of 0.87\u00a0(McHugh 2012).\nResults\u2002 We identify four major root causes of performance regressions from the code \ngenerated by all models. Our in"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_024",
    "source_id": "PerformanceAnalysis2026",
    "text": " regressions.\n1\u202f3\n   62 \n\u2003\nPage 18 of 52\nEmpirical Software Engineering           (2026) 31:62 \n\t\u2013\nIterative analysis. The examination is repeated in an iterative process until no new root \ncauses of code-related performance regressions are discovered, indicating that a com\u00ad\nprehensive understanding of the prevalent root causes has been achieved.\nTo quantify the reliability of our dual coding analysis, we calculate the Cohen\u2019s Kappa sta\u00ad\ntistic, which yielded a considerable agreement score of 0.87\u00a0(McHugh 2012).\nResults\u2002 We identify four major root causes of performance regressions from the code \ngenerated by all models. Our in-depth analysis of code samples exhibiting performance \nregressions yields four key categories of root causes at the code level, along with eleven \nsub-categories. These categories are detailed in Tables\u00a05, 6, 7 and 8. Below, we discuss each \nroot cause category with corresponding code examples for illustration.\n4.2.1\u2002 R1 Inefficient Function Calls\nPerformance regression is often attributable to suboptimal function call choices, including \nthe use of inefficient APIs, excessive recursion leading to deep stack issues, and unnecessary \nfunction abstraction.\nR1-1 Inefficient API Usage\u2002 Selecting the correct functions and APIs has a significant \nimpact on performance. The generated code often opts for less efficient functions \nwhen more optimal methods are available to enhance efficiency. For example, in the \ncode snippet shown in Fig.\u00a06(a) from AixBench-11, the generated code uses the Math.\nrandom() API to generate random double numbers and then convert them to int type. \nThis approach is less efficient in terms of performance compared to directly using the \nrandom.nextInt() API.\nR1-2 Excessive Recursion\u2002 Recursive functions are used in the generated code. In code imple\u00ad\nmentation, employing recursive functions can lead to excessive stack depth when handling \nlarge data ranges, thus impacting performance. For example, in the code shown in Fig.\u00a06(b) \nfrom HumanEval-76, the function is_simple_power(x, n) is designed to check if the number \nx is the power of another number n. However, this recursive function may suffer from per\u00ad\nformance regression due to increased recursion depth when faced with very large values of \nx or when n is close to 1.\nR1-3 Unnecessary Function Abstraction\u2002 Simple operations are unnecessarily encapsu\u00ad\nlated into helper functions, leading to additional function call overhead without pro\u00ad\nviding real modularity or reuse benefits. For example, in the code snippet shown in \nFig.\u00a06(c) from HumanEval-137 by Copilot Chat, the generated code defines a helper \nfunction to_float() only to replace commas with dots and cast to float. This abstraction \nis repeatedly invoked, while the same logic could be implemented in-line with signifi\u00ad\ncantly lower overhead. Such unnecessary function abstractions degrade runtime perfor\u00ad\nmance without offering meaningful software engineering advantages such as readability \nor reuse.\n1\u202f3\nPage 19 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nTable 5\u2002 Root causes of performance regression from the Copilot-generated Code\nDataset\nIn"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_025",
    "source_id": "PerformanceAnalysis2026",
    "text": " example, in the code snippet shown in \nFig.\u00a06(c) from HumanEval-137 by Copilot Chat, the generated code defines a helper \nfunction to_float() only to replace commas with dots and cast to float. This abstraction \nis repeatedly invoked, while the same logic could be implemented in-line with signifi\u00ad\ncantly lower overhead. Such unnecessary function abstractions degrade runtime perfor\u00ad\nmance without offering meaningful software engineering advantages such as readability \nor reuse.\n1\u202f3\nPage 19 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nTable 5\u2002 Root causes of performance regression from the Copilot-generated Code\nDataset\nInefficient Function Calls\nInefficient Looping\nInefficient Algorithm\nInefficient Use of Language Features\n Ineffi\u00ad\ncient API \nusage\nExcessive \nRecursion\nALL\nString \nConcat\u00ad\nenation in \nLoops\nNested \nLooping\nObject \nCre\u00ad\nation in \nLoops\nALL\nMissed \nMathematical \nOptimizations\nSubopti\u00ad\nmal Con\u00ad\nditional \nLogic\nALL\nUnderuti\u00ad\nlization of \nLanguage \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n8\n1\n9\n2\n4\n1\n7\n2\n1\n3\n5\n0\n0\n5\nAixBench\n9\n1\n10\n1\n4\n1\n6\n1\n1\n2\n3\n1\n7\n11\nMBPP\n53\n6\n59\n16\n40\n10\n66\n53\n23\n76\n54\n6\n5\n65\nEvalPerf\n18\n1\n19\n3\n15\n6\n24\n7\n8\n15\n13\n0\n0\n13\n1\u202f3\n   62 \n\u2003\nPage 20 of 52\nEmpirical Software Engineering           (2026) 31:62 \nTable 6\u2002 Root causes of performance regression from the CodeLlama-generated Code\nDataset\nInefficient Function Calls\nInefficient Looping\nInefficient Algorithm\nInefficient Use of Language Features\nIneffi\u00ad\ncient API \nusage\nExcessive \nRecursion\nALL\nString \nConcat\u00ad\nenation in \nLoops\nNested \nLooping\nObject \nCre\u00ad\nation in \nLoops\nALL\nMissed \nMathematical \nOptimizations\nSubopti\u00ad\nmal Con\u00ad\nditional \nLogic\nALL\nUnderuti\u00ad\nlization of \nLanguage \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n1\n0\n1\n0\n0\n0\n0\n4\n0\n4\n2\n0\n0\n2\nAixBench\n6\n0\n6\n2\n3\n0\n5\n1\n3\n4\n6\n1\n2\n9\nMBPP\n49\n4\n53\n1\n8\n1\n10\n14\n11\n25\n3\n0\n0\n3\nEvalPerf\n16\n1\n17\n3\n15\n7\n25\n5\n6\n11\n8\n0\n0\n8\n1\ufffd"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_026",
    "source_id": "PerformanceAnalysis2026",
    "text": "lization of \nLanguage \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n1\n0\n1\n0\n0\n0\n0\n4\n0\n4\n2\n0\n0\n2\nAixBench\n6\n0\n6\n2\n3\n0\n5\n1\n3\n4\n6\n1\n2\n9\nMBPP\n49\n4\n53\n1\n8\n1\n10\n14\n11\n25\n3\n0\n0\n3\nEvalPerf\n16\n1\n17\n3\n15\n7\n25\n5\n6\n11\n8\n0\n0\n8\n1\u202f3\nPage 21 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nTable 7\u2002 Root causes of performance regression from the DeepSeek-Coder-generated Code\nDataset\nInefficient Function Calls\nInefficient Looping\nInefficient Algorithm\nInefficient Use of Language Features\nIneffi\u00ad\ncient API \nusage\nExcessive \nRecursion\nALL\nString \nConcat\u00ad\nenation in \nLoops\nNested \nLooping\nObject \nCre\u00ad\nation in \nLoops\nALL\nMissed \nMathematical \nOptimizations\nSubopti\u00ad\nmal Con\u00ad\nditional \nLogic\nALL\nUnderuti\u00ad\nlization of \nLanguage \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n2\n3\n5\n1\n0\n2\n3\n1\n2\n3\n1\n1\n0\n2\nAixBench\n6\n2\n8\n0\n2\n0\n2\n0\n0\n0\n5\n0\n0\n5\nMBPP\n37\n4\n41\n4\n16\n1\n21\n19\n6\n25\n3\n0\n0\n3\nEvalPerf\n18\n0\n18\n3\n6\n2\n11\n10\n7\n17\n1\n0\n1\n2\n1\u202f3\n   62 \n\u2003\nPage 22 of 52\nEmpirical Software Engineering           (2026) 31:62 \nTable 8\u2002 Root causes of performance regression from the Copilot-Chat-generated Code\nDataset\nInefficient Function Calls\nInefficient Looping\nInefficient Algorithm\nInefficient Use of Language Features\nInefficient \nAPI usage\nExcessive \nRecursion\nUnnecessary \nFunction \nAbstraction\nALL\nString Con\u00ad\ncatenation in \nLoops\nNested \nLooping\nObject \nCreation in \nLoops\nALL\nMissed Mathematical \nOptimizations\nSuboptimal \nConditional \nLogic\nALL\nUnderutilization \nof Language \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n14\n0\n3\n17\n0\n1\n15\n16\n2\n9\n11\n9\n0\n0\n9\nAixBench\n23\n5\n0\n28\n1\n5\n6\n12\n2\n4\n6\n3\n0\n11\n14\nMBPP\n109\n4\n0\n113\n1\n15\n15\n31\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_027",
    "source_id": "PerformanceAnalysis2026",
    "text": "catenation in \nLoops\nNested \nLooping\nObject \nCreation in \nLoops\nALL\nMissed Mathematical \nOptimizations\nSuboptimal \nConditional \nLogic\nALL\nUnderutilization \nof Language \nFeatures\nUnused \nVariables\nInefficient \nException \nHandling\nALL\nHumanEval\n14\n0\n3\n17\n0\n1\n15\n16\n2\n9\n11\n9\n0\n0\n9\nAixBench\n23\n5\n0\n28\n1\n5\n6\n12\n2\n4\n6\n3\n0\n11\n14\nMBPP\n109\n4\n0\n113\n1\n15\n15\n31\n53\n4\n57\n2\n0\n0\n2\nEvalPerf\n14\n1\n0\n15\n0\n1\n1\n2\n10\n8\n18\n0\n0\n0\n0\n1\u202f3\nPage 23 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \n4.2.2\u2002 R2 Inefficient Looping\nPerformance regressions are frequently linked to inefficient looping. For instance, opera\u00ad\ntions such as string concatenation within loops, multiple nested loops, and object creation \ninside loops can lead to significant performance declines. These root causes particularly \naffect the efficiency of code when handling large amounts of data.\nR2-1 String Concatenation in Loops\u2002 Strings are immutable, and using the + operator to con\u00ad\ncatenate strings creates a new string object. Performing such operations frequently within a \nloop, especially when n is large, can lead to substantial memory allocation and release, thus \npotentially reducing the efficiency of code execution. An example of this code is shown in \nFig.\u00a07(a) from HumanEval-15.\nR2-2 Nested Looping\u2002 The generated code exhibits issues with nested loops. In the code \nsnippet shown in Fig.\u00a07(b) from MBPP-12, a double loop is used for comparing and swap\u00ad\nFig. 6\u2002 Examples of inefficient function calls\n \n1\u202f3\n   62 \n\u2003\nPage 24 of 52\nEmpirical Software Engineering           (2026) 31:62 \nping rows of a matrix. This nested looping structure, when handling larger matrices, leads to \na time complexity of O(n2), resulting in significant performance regression.\nR2-3 Object Creation in Loops\u2002 Creating objects repeatedly within loops can negatively \nimpact performance due to extensive memory allocation and frequent garbage collection. In \nFig.\u00a07(c) from MBPP-363, the code example demonstrates a performance regression, where \na new temp list is created within each iteration of a loop.\n4.2.3\u2002 R3 Inefficient Algorithm\nOur analysis finds that the generated code may employ algorithms that are less efficient than \ncanonical solutions. In particular, we identify two sub-categories of inefficient algorithms.\nR3-1 Missed Mathematical Optimizations\u2002 The generated code may not always leverage \nmathematical optimizations to their full potential. In Fig.\u00a08(a) from MBPP-335, the gener\u00ad\nated code iterates over the array through the loop and accumulates them one by one. How\u00ad\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_028",
    "source_id": "PerformanceAnalysis2026",
    "text": " garbage collection. In \nFig.\u00a07(c) from MBPP-363, the code example demonstrates a performance regression, where \na new temp list is created within each iteration of a loop.\n4.2.3\u2002 R3 Inefficient Algorithm\nOur analysis finds that the generated code may employ algorithms that are less efficient than \ncanonical solutions. In particular, we identify two sub-categories of inefficient algorithms.\nR3-1 Missed Mathematical Optimizations\u2002 The generated code may not always leverage \nmathematical optimizations to their full potential. In Fig.\u00a08(a) from MBPP-335, the gener\u00ad\nated code iterates over the array through the loop and accumulates them one by one. How\u00ad\never, from a mathematical perspective, the sum of an arithmetic sequence can be directly \ncalculated using a formula, which is more efficient and avoids unnecessary loops.\nR3-2 Suboptimal Conditional Logic\u2002 The use of complex or unnecessary conditional state\u00ad\nments (if-else) can introduce performance regression. As shown in Fig.\u00a08(b) from Huma\u00ad\nFig. 7\u2002 Examples of inefficient looping\n \n1\u202f3\nPage 25 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nnEval-13, when calculating the greatest common divisor (GCD), the generated code \nimplementation reduces the difference between the two numbers by repeatedly subtracting \nthe smaller number from the larger one, increasing the runtime. This inefficient conditional \nlogic results in unnecessary performance regression.\n4.2.4\u2002 R4 Inefficient Use of Language Features\nThe generated code may exhibit shortcomings in performance in its utilization of the pro\u00ad\ngramming language\u2019s built-in features and functionalities.\nR4-1 Underutilization of Language Features\u2002 Sometimes the generated code fails to effec\u00ad\ntively leverage the features of the programming language, as demonstrated in Fig.\u00a09(a) from \nMBPP-688. When calculating the magnitude of a complex number, the generated code \nmanually computes it, not fully utilizing Python\u2019s built-in capabilities for handling complex \nnumbers. Using Python\u2019s cmath module or the built-in complex type and abs function could \noffer performance benefits, as built-in operations are typically closer to the hardware level \nand more optimized.\nR4-2 Unused Variables\u2002 During the code generation process, unnecessary or redundant code \nis sometimes produced. For example, variables may be assigned values even though they \nare not subsequently read or utilized in the program. This not only increases the complex\u00ad\nity of the code but can also affect its execution efficiency. In Fig.\u00a09(b) from MBPP-45, the \nexample of the generated code shows that although the main purpose of the code is to com\u00ad\npute the greatest common divisor (GCD) of a list of numbers, the code includes operations \nfor assigning initial values to num1 and num2.\nR4-3 Inefficient Exception Handling\u2002 The generated code includes improper exception han\u00ad\ndling, which can become a performance bottleneck in scenarios requiring frequent calls \nFig. 8\u2002 Examples of inefficient algorithm\n \n1\u202f3\n   62 \n\u2003\nPage 26 of 52\nEmpirical Software Engineering           (2026) 31:62 \n(such as in loops or core processing"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_029",
    "source_id": "PerformanceAnalysis2026",
    "text": " In Fig.\u00a09(b) from MBPP-45, the \nexample of the generated code shows that although the main purpose of the code is to com\u00ad\npute the greatest common divisor (GCD) of a list of numbers, the code includes operations \nfor assigning initial values to num1 and num2.\nR4-3 Inefficient Exception Handling\u2002 The generated code includes improper exception han\u00ad\ndling, which can become a performance bottleneck in scenarios requiring frequent calls \nFig. 8\u2002 Examples of inefficient algorithm\n \n1\u202f3\n   62 \n\u2003\nPage 26 of 52\nEmpirical Software Engineering           (2026) 31:62 \n(such as in loops or core processing logic). In Fig.\u00a09(c) from AixBench-72, the generated \ncode contains issues with exception handling, which may lead to performance regression. \nThe code frequently throws and catches specific exceptions such as InstantiationException \nand IllegalAccessException, which respectively indicate problems with class instantiation \nand access. These exceptions are rewrapped and thrown as RuntimeException, a practice \nthat obscures the specific cause of the errors, affecting the performance of the code.\nRoot Causes of Performance Regressions Vary Across Models and Datasets\u2002 Our analysis of \nthe root causes of performance regressions (Table\u00a05\u20138) reveals both model-specific char\u00ad\nacteristics and cross-model regularities in code generation. Across all evaluated models, \ninefficient function calls emerge as the most frequent source of performance regressions, \nparticularly on the MBPP dataset, where Copilot (53), CodeLlama (49), DeepSeek-Coder \n(37), and Copilot Chat (109) all exhibit substantial API misuse. This suggests that LLMs \nFig. 9\u2002 Examples of inefficient use of language features\n \n1\u202f3\nPage 27 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \ntend to prioritize functional correctness and syntactic completeness over execution effi\u00ad\nciency, reflecting limited reasoning about API complexity and call overhead. Inefficient \nalgorithms, especially missed mathematical optimizations, form the second most frequent \ncategory, with substantial counts on MBPP\u2013Copilot (76), CodeLlama (25), DeepSeek-\nCoder (25), and Copilot Chat (57). Inefficient looping appears less frequently but remains \nnotable on MBPP, particularly in Copilot (66) and Copilot Chat (31). Together, these results \nindicate that current LLMs tend to prioritize functional correctness and structural complete\u00ad\nness over runtime efficiency.\nPerformance-oriented datasets such as EvalPerf systematically expose optimization \nweaknesses across all models, despite their smaller size, with frequent issues in function \ncalls (15\u201319), looping (2\u201325), and algorithms (11\u201318). In contrast, HumanEval shows mini\u00ad\nmal inefficiencies(typically single-digit instances per category), consistent with its simpler, \ncorrectness-focused tasks. The Java-based AixBench dataset uniquely reveals inefficiencies \ntied to static language features\u2013particularly inefficient exception handling (0\u201311 instances \nper model) and unused variables (0\u20131)\u2013which are largely absent in Python benchmarks. \nPython datasets, by contrast, primarily expose dynamic inefficiencies such as missed math\u00ad\nematical optimizations and nested looping"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_030",
    "source_id": "PerformanceAnalysis2026",
    "text": " as EvalPerf systematically expose optimization \nweaknesses across all models, despite their smaller size, with frequent issues in function \ncalls (15\u201319), looping (2\u201325), and algorithms (11\u201318). In contrast, HumanEval shows mini\u00ad\nmal inefficiencies(typically single-digit instances per category), consistent with its simpler, \ncorrectness-focused tasks. The Java-based AixBench dataset uniquely reveals inefficiencies \ntied to static language features\u2013particularly inefficient exception handling (0\u201311 instances \nper model) and unused variables (0\u20131)\u2013which are largely absent in Python benchmarks. \nPython datasets, by contrast, primarily expose dynamic inefficiencies such as missed math\u00ad\nematical optimizations and nested looping, indicating insufficient exploitation of built-in \noptimizations like list comprehensions and generator expressions. These patterns reveal that \nLLMs face distinct challenges depending on the target language paradigm: static type sys\u00ad\ntems in Java versus runtime performance idioms in Python.\nAt the model level, heterogeneous regression patterns highlight distinct performance \ncharacteristics across code generation. Although all models share the same high-level per\u00ad\nformance regression categories, their quantitative profiles differ markedly. The Copilot-\nbased models (Copilot and Copilot Chat) exhibit the highest frequency of performance \nregressions, primarily driven by inefficient function calls and algorithmic inefficiencies. \nSpecifically, on the MBPP dataset, Copilot Chat generates 113 code instances with func\u00ad\ntion call issues and 57 instances with algorithmic issues, indicating that its enhanced com\u00ad\npleteness and modularity come at the cost of runtime efficiency. The model also introduces \nunique regressions like unnecessary function abstraction (appearing 3 times on HumanE\u00ad\nval), indicating an overemphasis on structural modularity. Copilot follows a similar pat\u00ad\ntern with slightly fewer function call regressions (59) but higher looping inefficiencies (66 \ninstances), suggesting that it tends to overuse iterative structures. In contrast, CodeLlama \ndemonstrates a more balanced regression profile, with moderate counts across categories \n(53 instances with function call issues and 25 with algorithm issues on MBPP) but a spe\u00ad\ncific weakness in nested looping on EvalPerf (15 instances), revealing limited awareness \nof loop-level computational overhead in performance-sensitive contexts. DeepSeek-Coder \nperforms most robustly overall, exhibiting the lowest total regression counts (41 instances \nwith function call issues and 25 algorithm issues on MBPP), indicating relatively efficient \ncode generation. Nevertheless, even DeepSeek-Coder shows non-trivial regressions on per\u00ad\nformance-critical tasks (18 instances with function call issues and 17 with algorithm issues \non EvalPerf), suggesting that no current model fully internalizes computational efficiency \nprinciples. Overall, these quantitative profiles reveal that while larger, instruction-tuned \nmodels like Copilot Chat achieve superior functional correctness, this improvement often \ncoincides with higher performance regressions, reflecting a systematic trade-off between \nstructural completeness and execution efficiency.\n1\u202f3\n   62 \n\u2003\nPage 28 of 52\nEmpirical Software Engineering           (2026) 31:62 \nQuantitatively, inefficient API usages (ranging from 1\u2013109 instances depending on \nmodel and dataset) and missed mathematical optimizations (0\u201353) dominate the distribu\u00ad\ntion of regressions"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_031",
    "source_id": "PerformanceAnalysis2026",
    "text": " with algorithm issues \non EvalPerf), suggesting that no current model fully internalizes computational efficiency \nprinciples. Overall, these quantitative profiles reveal that while larger, instruction-tuned \nmodels like Copilot Chat achieve superior functional correctness, this improvement often \ncoincides with higher performance regressions, reflecting a systematic trade-off between \nstructural completeness and execution efficiency.\n1\u202f3\n   62 \n\u2003\nPage 28 of 52\nEmpirical Software Engineering           (2026) 31:62 \nQuantitatively, inefficient API usages (ranging from 1\u2013109 instances depending on \nmodel and dataset) and missed mathematical optimizations (0\u201353) dominate the distribu\u00ad\ntion of regressions, accounting for over 70% of all identified cases, suggesting that models \nfundamentally struggle with reasoning about computational complexity and API overhead. \nIn contrast, some inefficiency types follow a long-tail distribution\u2013such as inefficient excep\u00ad\ntion handling (appearing almost exclusively in Java code with 5\u201311 instances on AixBench \nbut 0 on Python datasets), excessive recursion (0\u20136 instances), and unnecessary function \nabstraction (uniquely 3 instances in Copilot Chat on HumanEval)\u2013that occur less frequently \nbut can still introduce significant performance penalties when they occur, such as expo\u00ad\nnential time complexity or substantially degraded runtime performance. This distribution \npattern provides clear priorities: enhanced training on API complexity reasoning and math\u00ad\nematical optimization patterns would address the most prevalent regressions, while targeted \ninterventions for language-specific and context-specific long-tail patterns would handle \nhigh-impact edge cases.\nWhile we employ static analysis tools (Qodana, SpotBugs, and PMD) and extend them \nwith custom rules, we acknowledge that static rule matching is inherently imprecise. \nMany rules are based on code smells or heuristics, which do not always correspond \nto actual performance regressions. To assess the practical relevance of static warnings, \nwe randomly sampled 50 flagged instances across all datasets and tools for manual \nvalidation. To ensure that our manual inspection of static warnings is representative, we \nemployed a stratified and weighted sampling strategy. Specifically, we first stratified the \nwarnings by static analysis tools (Qodana, SpotBugs, PMD) and datasets (HumanEval, \nMBPP, EvalPerf, AixBench). Within each stratum, we allocated samples proportionally \nto the number of warnings generated, while ensuring that each tool\u2013dataset pair was \nrepresented by at least one instance. In total, 46 instances were selected following \nthis proportional allocation. To further strengthen the validity of the evaluation, \nwe additionally included 4 purposeful samples where static and dynamic analyses \nproduced divergent results (e.g., static warnings not confirmed by profiling, or dynamic \nregressions missed by static tools). This yielded 50 instances in total. Each instance was \nindependently inspected by two human experts, who judged whether the flagged code \nwas likely to cause real performance degradation, considering both canonical code and \navailable dynamic profiling results. Disagreements were resolved through discussion to \nreach a consensus. Our analysis revealed that only about 46% of the sampled warnings \ncorresponded to real performance regressions, confirming the limited precision of static \nrule-based detection (Table "
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_032",
    "source_id": "PerformanceAnalysis2026",
    "text": " proportional allocation. To further strengthen the validity of the evaluation, \nwe additionally included 4 purposeful samples where static and dynamic analyses \nproduced divergent results (e.g., static warnings not confirmed by profiling, or dynamic \nregressions missed by static tools). This yielded 50 instances in total. Each instance was \nindependently inspected by two human experts, who judged whether the flagged code \nwas likely to cause real performance degradation, considering both canonical code and \navailable dynamic profiling results. Disagreements were resolved through discussion to \nreach a consensus. Our analysis revealed that only about 46% of the sampled warnings \ncorresponded to real performance regressions, confirming the limited precision of static \nrule-based detection (Table 9).\nOur manual inspection revealed that static analysis warnings differ in their practical rel\u00ad\nevance to performance, even within the same rule category. For example, the rule \u201cExplicit \nreturn statement expected\" sometimes flagged issues that led to inefficient execution paths \n(e.g., prolonged loop iterations), which were confirmed to cause measurable performance \ndegradation. However, in other cases, the same warning merely indicated stylistic or cor\u00ad\nrectness-related concerns without observable runtime impact. Similarly, warnings such as \n\u201cShadows built-in name \u2018str\u201d\u2019 or \u201cShadows name \u2018x\u2019 from outer scope\" were purely cos\u00ad\nmetic and unrelated to performance, while warnings like \u201cSimplify chained comparison\" \nor \u201cInconsistent return statements\" were more consistently linked with inefficient code pat\u00ad\nterns. These findings highlight that static rules cannot be uniformly interpreted as indicators \nof performance regressions; their impact depends on context and requires careful validation. \n1\u202f3\nPage 29 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nThis underscores the importance of combining static analysis with dynamic profiling and \nmanual inspection to distinguish truly performance-relevant issues from benign code smells \n(Table 10).\n4.3\u2002 RQ3: Can Prompt Engineering Optimize AI-Generated Code for Performance?\nMotivation\u2002 Building upon the significant prevalence of performance regressions identified \nin AI-generated code in RQ1 and RQ2, this research question aims to investigate the poten\u00ad\ntial mitigation strategies. Modifying the underlying architectures of code generation models \nsuch as GitHub Copilot,Copilot Chat, CodeLlama, and DeepSeek-Coder presents technical \nchallenges. Instead, we investigated prompt engineering as a feasible approach to improving \nthe performance characteristics of generated code. Our goal is to leverage prompts to guide \ncode generation models in producing functionally correct and performance-optimized code. \nPrompt engineering does not require alterations to the internal mechanisms of the mod\u00ad\nels, making it more practical for application to existing code generation systems. Further\u00ad\nmore, prompt engineering offers significant flexibility. By tailoring prompt content based on \nTool\nValidated \nsamples\nConfirmed \n(Yes)\nFalse posi\u00ad\ntives (No)\nPreci\u00ad\nsion\nQodana\n38\n16\n22\n42.1%\nSpotBugs\n4\n2\n2\n50.0%\nPMD\n8\n5\n3\n62.5%\nOverall\n50\n23\n27\n46.0%\n\u201cYes\u201d indicates confirmed correlation with actual performance \nregressions, \u201cNo\u201d indicates false positives\nTable"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_033",
    "source_id": "PerformanceAnalysis2026",
    "text": "Prompt engineering does not require alterations to the internal mechanisms of the mod\u00ad\nels, making it more practical for application to existing code generation systems. Further\u00ad\nmore, prompt engineering offers significant flexibility. By tailoring prompt content based on \nTool\nValidated \nsamples\nConfirmed \n(Yes)\nFalse posi\u00ad\ntives (No)\nPreci\u00ad\nsion\nQodana\n38\n16\n22\n42.1%\nSpotBugs\n4\n2\n2\n50.0%\nPMD\n8\n5\n3\n62.5%\nOverall\n50\n23\n27\n46.0%\n\u201cYes\u201d indicates confirmed correlation with actual performance \nregressions, \u201cNo\u201d indicates false positives\nTable 10\u2002 Manual validation \nresults of 50 sampled static \nwarnings\n \nTool\nDataset\nGitHub \nCopilot\nCodeLlama\nDeepSeek-Coder\nCo\u00ad\npilot \nChat\nQo\u00ad\ndana\nHumanE\u00ad\nval\n1\n1\n1\n1\nMBPP\n8\n6\n8\n2\nEvalPerf\n2\n1\n2\n1\nSpot\u00ad\nBugs\nAixBench\n1\n1\n1\n1\nPMD AixBench\n2\n2\n1\n3\nConflict cases \n(static vs. dy\u00ad\nnamic mismatch)\n1\n1\n1\n1\nTotal\n15\n12\n14\n9\nTable 9\u2002 Distribution of 50 \nsampled static warnings for \nmanual validation\nQodana is applied to \nHumanEval, MBPP, and \nEvalPerf; SpotBugs and PMD \nare applied to AixBench\nConflict cases are purposeful \nsamples where static and \ndynamic analyses disagree\n \n1\u202f3\n   62 \n\u2003\nPage 30 of 52\nEmpirical Software Engineering           (2026) 31:62 \ninsights from open coding analyses of root causes of performance regression, performance \nconsiderations can be more effectively integrated into the code generation process.\nApproach\u2002 We follow two steps to evaluate prompt engineering for performance in AI-gen\u00ad\nerated code.\nFirst, we designed few-shot prompts based on the four major root causes and eleven sub\u00ad\ncategories of performance regressions identified in RQ2. Specifically, these subcategories \nserved as exemplar instances in our few-shot prompt, with each instance comprising an inef\u00ad\nficient example (i.e., AI-generated code exhibiting performance regressions) and an efficient \nexample (i.e., the corresponding human-written code). As shown in Fig.\u00a010, the few-shot \nprompt consists of two primary components: a task description and a series of specific \nexamples. The task description delineates the objective of performance optimization, while \nthe accompanying specific examples meticulously highlight the observed performance \nregressions alongside their corresponding improvements, thus effectively guiding the code \ngeneration models to produce functionally correct and performance-optimized code.\nListing 1 CoT Prompt Example\n1\n#\nWrite\na\nfunction\nto\nimplement\nthe\nfollowing\nrequirements .\nPlease\nprovide\nonly\nthe\ncode ,\nwithout\nany\nother\ntext .\n2\n3\n#\nChain\nof\nThought:\nPerformance - Optimized\nImplementation\n4\n5\nI\nneed\nto\nimplement\nthis\nfunction\nwith\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_034",
    "source_id": "PerformanceAnalysis2026",
    "text": " \nprompt consists of two primary components: a task description and a series of specific \nexamples. The task description delineates the objective of performance optimization, while \nthe accompanying specific examples meticulously highlight the observed performance \nregressions alongside their corresponding improvements, thus effectively guiding the code \ngeneration models to produce functionally correct and performance-optimized code.\nListing 1 CoT Prompt Example\n1\n#\nWrite\na\nfunction\nto\nimplement\nthe\nfollowing\nrequirements .\nPlease\nprovide\nonly\nthe\ncode ,\nwithout\nany\nother\ntext .\n2\n3\n#\nChain\nof\nThought:\nPerformance - Optimized\nImplementation\n4\n5\nI\nneed\nto\nimplement\nthis\nfunction\nwith\noptimal\nperformance .\nLet\nme\nthink\nstep\nby\nstep :\n6\n7\n##\nStep\n1:\nUnderstanding\nthe\nRequirements\n8\n-\nWhat\nexactly\ndoes\nthis\nfunction\nneed\nto\ndo\nbased\non\nthe\ndocstring\nand\nexamples ?\n9\n-\nWhat\nare\nthe\ninput\nconstraints\nand\nexpected\noutputs ?\n10\n-\nWhat\nedge\ncases\ndo\nI\nneed\nto\nhandle?\n11\n12\n##\nStep\n2:\nAlgorithm\nDesign\nConsiderations\n13\n-\nWhat \u2019s the  most  efficient  approach  to  solve this  \nproblem?\n14\n- Can  I avoid unnecessary  operations  or  memory \nallocations ?\n15\n- Are  there built -in functions  or  data  structures  that  \nwould be more  efficient ?\n16\n- What \u2019s\nthe\noptimal\ntime\nand\nspace\ncomplexity\nI\ncan\nachieve?\n17\n18\n##\nStep\n3:\nPerformance\nOptimization\nStrategy\n19\n-\nHow\ncan\nI\nminimize\nthe\nnumber\nof\noperations ?\n20\n-\nCan\nI\nreduce\nmemory\nusage\nor\navoid\ncreating\ntemporary\nobjects?\n21\n-\nAre\nthere\nlanguage - specific\noptimizations\nI\ncan\napply?\n22\n-\nCan\nI\neliminate\nredundant\ncomputations ?\n23\n24\n##\nStep\n4:\nImplementation\n25\nNow\nI\u2019ll read  the  following  test  code  and the  \nrequirements  and  implement  the most  efficient  \nsolution  and  provide  the code  only , without  any other\n text :\n1\u202f3\nPage 31 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nFig. 10\u2002 Part of the few-shot prompt used in our prompt engineering for optimizing code performance\n \n1\u202f3\n   62 \n\u2003\nPage 32 of 52\nEmpirical Software Engineering           (2026) 31:62 \nTo further explore the effectiveness of prompt engineering in mitigating performance \nregressions, we additionally experimented with CoT prompts. CoT prompting has been \nwidely used to enhance reasoning and decision-making in generative tasks\u00a0(Wei et\u00a0al. \n2022). Our goal was to examine whether explicitly guiding models through a structured \nreasoning process could improve performance optimization in AI-generated code. \nThe CoT prompt was designed to guide the model step by step through the"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_035",
    "source_id": "PerformanceAnalysis2026",
    "text": ". 10\u2002 Part of the few-shot prompt used in our prompt engineering for optimizing code performance\n \n1\u202f3\n   62 \n\u2003\nPage 32 of 52\nEmpirical Software Engineering           (2026) 31:62 \nTo further explore the effectiveness of prompt engineering in mitigating performance \nregressions, we additionally experimented with CoT prompts. CoT prompting has been \nwidely used to enhance reasoning and decision-making in generative tasks\u00a0(Wei et\u00a0al. \n2022). Our goal was to examine whether explicitly guiding models through a structured \nreasoning process could improve performance optimization in AI-generated code. \nThe CoT prompt was designed to guide the model step by step through the process \nof understanding the requirements, considering algorithmic efficiency, and applying \noptimization strategies before producing the final implementation. As illustrated in \nListing\u00a01, the CoT prompt consists of four stages\u2013requirement understanding, algorithm \ndesign considerations, performance optimization strategy, and implementation\u2013while \nstill enforcing code-only output for evaluation consistency. Compared with the exemplar-\nbased few-shot prompt, the CoT prompt does not rely on specific inefficient\u2013efficient code \npairs but instead leverages structured reasoning instructions to encourage performance-\naware generation.\nSecond, we applied these prompts to generate code using Copilot, Copilot Chat, CodeL\u00ad\nlama, and DeepSeek-Coder across the HumanEval, AixBench, EvalPerf, and MBPP datas\u00ad\nets. To ensure consistency, we maintained all model parameters from previous experiments, \nmodifying only the prompt content. This approach maintained consistency in parameters \nsuch as temperature, top_p, and max_gen_len, ensuring fairness and reproducibility of the \ncomparison results. Subsequently, we conduct both static and dynamic analyses of the gen\u00ad\nerated code. Finally, we compare the performance metrics obtained in this stage with the \nprevious baseline results to assess and compare the efficacy of the few-shot prompts and \nCoT prompts in improving the performance of the generated code. To maintain fairness in \nevaluation, we only analyzed scripts that successfully passed functional correctness checks \nunder both few-shot and CoT prompting. As shown in Table\u00a011, the number of passing \nscripts decreased due to potential compilation failures or test case failures post-prompting. \nThis ensured that only scripts that compiled and passed all test cases in both prompting \ngenerations were included in the final analysis, providing an objective evaluation of the \nprompts\u2019 impact on performance optimization.\nResults\u2002 Table\u00a011 presents the changes in performance regressions before and after apply\u00ad\ning CoT and few-shot prompt engineering to the Copilot, CodeLlama, DeepSeek-Coder, \nand Copilot Chat models. The results, which include both static and dynamic analyses, \nindicate that prompt engineering can lead to reductions in performance regressions, but the \nmagnitude and direction of improvement depend on the specific model, dataset, and metric. \nFor Copilot, the effects of prompt engineering varied across datasets and metrics. As shown \nin Table\u00a011, both CoT and few-shot prompts influenced static and dynamic performance \nregressions, but no single prompt type consistently outperformed the other. For instance, in \nAixBench, which primarily involves static analysis, CoT eliminated regressions flagged by \nSpotBugs (1 \u2192 0), whereas few-shot returned"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_036",
    "source_id": "PerformanceAnalysis2026",
    "text": "Llama, DeepSeek-Coder, \nand Copilot Chat models. The results, which include both static and dynamic analyses, \nindicate that prompt engineering can lead to reductions in performance regressions, but the \nmagnitude and direction of improvement depend on the specific model, dataset, and metric. \nFor Copilot, the effects of prompt engineering varied across datasets and metrics. As shown \nin Table\u00a011, both CoT and few-shot prompts influenced static and dynamic performance \nregressions, but no single prompt type consistently outperformed the other. For instance, in \nAixBench, which primarily involves static analysis, CoT eliminated regressions flagged by \nSpotBugs (1 \u2192 0), whereas few-shot returned results similar to the baseline (1). For PMD, \nhowever, CoT increased regressions (15 \u2192 25), while few-shot remained close to the base\u00ad\nline (15 \u2192 14). In HumanEval, CoT substantially reduced static regressions (Qodana: 16 \u2192 \n1), while few-shot also improved them, but to a lesser extent (16 \u2192 10). Dynamic analysis \nrevealed a different trend: few-shot produced clear benefits in execution time and CPU utili\u00ad\nzation (Execution time: 35 \u2192 5; CPU: 45 \u2192 28), and also reduced memory regressions (10 \n\u2192 1), whereas CoT increased them (10 \u2192 27). A similar divergence was observed in MBPP. \nCoT was most effective in reducing static regressions (Qodana: 228 \u2192 36), but its dynamic \n1\u202f3\nPage 33 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nStatic performance regression \nanalysis\nDynamic performance regression analysis\n Model\nDataset\n#Instances (intersection, %)\nPrompt\nSpotBugs\nPMD\nQodana\nExecution time\nMemory usage\nCPU Utilization\nGitHub Copilot\nHumanEval\n114 (69.5%)\nBefore\nN/A\nN/A\n16\n35\n10\n45\nCoT\nN/A\nN/A\n1\n27\n27\n45\nFew-shot\nN/A\nN/A\n10\n5\n1\n28\nMBPP\n 699 (71.8%)\nBefore\nN/A\nN/A\n228\n127\n57\n268\nCoT\nN/A\nN/A\n36\n229\n214\n284\nFew-shot\nN/A\nN/A\n176\n55\n56\n225\nEvalPerf\n 96 (81.3%)\nBefore\nN/A\nN/A\n23\n67\n30\n48\nCoT\nN/A\nN/A\n8\n57\n39\n58\nFew-shot\nN/A\nN/A\n16\n69\n31\n46\nAixBench\n 62 (35.4%)\nBefore\n1\n15\nN/A\nN/A\nN/A\nN/A\nCoT\n0\n25\nN/A\nN/A\nN/A\nN/A\nFew-shot\n1\n14\nN/A\nN/A\nN/A\nN/A\nCodeLlama\nHumanEval\n 92 (56.1%)\nBefore\nN/A\nN/A\n7\n12\n1\n30\nCo"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_037",
    "source_id": "PerformanceAnalysis2026",
    "text": "\nN/A\nN/A\n23\n67\n30\n48\nCoT\nN/A\nN/A\n8\n57\n39\n58\nFew-shot\nN/A\nN/A\n16\n69\n31\n46\nAixBench\n 62 (35.4%)\nBefore\n1\n15\nN/A\nN/A\nN/A\nN/A\nCoT\n0\n25\nN/A\nN/A\nN/A\nN/A\nFew-shot\n1\n14\nN/A\nN/A\nN/A\nN/A\nCodeLlama\nHumanEval\n 92 (56.1%)\nBefore\nN/A\nN/A\n7\n12\n1\n30\nCoT\nN/A\nN/A\n7\n6\n2\n11\nFew-shot\nN/A\nN/A\n7\n7\n1\n12\nMBPP\n 637 (65.4%)\nBefore\nN/A\nN/A\n154\n97\n38\n188\nCoT\nN/A\nN/A\n167\n99\n24\n186\nFew-shot\nN/A\nN/A\n173\n63\n15\n58\nEvalPerf\n 85 (72.0%)\nBefore\nN/A\nN/A\n14\n64\n39\n42\nCoT\nN/A\nN/A\n16\n76\n37\n40\nFew-shot\nN/A\nN/A\n16\n62\n35\n41\nAixBench\n 58 (33.1%)\nBefore\n8\n37\nN/A\nN/A\nN/A\nN/A\nCoT\n3\n29\nN/A\nN/A\nN/A\nN/A\nFew-shot\n6\n37\nN/A\nN/A\nN/A\nN/A\nDeepSeek-Coder\nHumanEval\n 42 (25.6%)\nBefore\nN/A\nN/A\n1\n17\n6\n13\nCoT\nN/A\nN/A\n1\n7\n9\n10\nFew-shot\nN/A\nN/A\n1\n4\n4\n6\nTable 11\u2002 Comparison of static and dynamic performance regression analysis before and after CoT prompt and few-shot prompt engineering for different models and datasets\n1\u202f3\n   62 \n\u2003\nPage 34 of 52\nEmpirical Software Engineering           (2026) 31:62 \nStatic performance regression \nanalysis\nDynamic performance regression analysis\n Model\nDataset\n#Instances (intersection, %)\nPrompt\nSpotBugs\nPMD\nQodana\nExecution time\nMemory usage\nCPU Utilization\nMBPP\n 634 (65.1%)\nBefore\nN/A\nN/A\n186\n89\n29\n213\nCoT\nN/A\nN/A\n145\n48\n35\n80\nFew-shot\nN/A\nN/A\n114\n28\n7\n81\nEvalPerf\n 67 (56.8%)\nBefore\nN/A\nN/A\n14\n50\n23\n31\nCoT\nN/A\nN/A\n10\n60\n23\n34\nFew-shot\nN/A\nN/A\n12\n51\n22\n39\nAixBench\n 46 (26.3%)\nBefore\n5\n13\nN/A\nN/A\nN/A\n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_038",
    "source_id": "PerformanceAnalysis2026",
    "text": "PP\n 634 (65.1%)\nBefore\nN/A\nN/A\n186\n89\n29\n213\nCoT\nN/A\nN/A\n145\n48\n35\n80\nFew-shot\nN/A\nN/A\n114\n28\n7\n81\nEvalPerf\n 67 (56.8%)\nBefore\nN/A\nN/A\n14\n50\n23\n31\nCoT\nN/A\nN/A\n10\n60\n23\n34\nFew-shot\nN/A\nN/A\n12\n51\n22\n39\nAixBench\n 46 (26.3%)\nBefore\n5\n13\nN/A\nN/A\nN/A\nN/A\nCoT\n1\n12\nN/A\nN/A\nN/A\nN/A\nFew-shot\n5\n14\nN/A\nN/A\nN/A\nN/A\nCopilot Chat\nHumanEval\n 142 (86.6%)\nBefore\nN/A\nN/A\n7\n45\n39\n66\nCoT\nN/A\nN/A\n20\n55\n56\n75\nFew-shot\nN/A\nN/A\n7\n30\n37\n64\nMBPP\n 783 (80.4%)\nBefore\nN/A\nN/A\n55\n374\n261\n342\nCoT\nN/A\nN/A\n48\n398\n274\n371\nFew-shot\nN/A\nN/A\n56\n293\n262\n342\nEvalPerf\n 104 (88.1%)\nBefore\nN/A\nN/A\n3\n79\n36\n59\nCoT\nN/A\nN/A\n4\n74\n38\n62\nFew-shot\nN/A\nN/A\n3\n70\n37\n62\nAixBench\n 104 (59.4%)\nBefore\n5\n55\nN/A\nN/A\nN/A\nN/A\nCoT\n5\n52\nN/A\nN/A\nN/A\nN/A\nFew-shot\n3\n46\nN/A\nN/A\nN/A\nN/A\nTable 11\u2002 (continued)\n \n1\u202f3\nPage 35 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nperformance degraded considerably, with regressions increasing for memory (57 \u2192 214) \nand CPU (268 \u2192 284). By contrast, few-shot reduced regressions more consistently across \ndynamic metrics (Execution time: 127 \u2192 55; Memory: 57 \u2192 56; CPU: 268 \u2192 225). Finally, \nin EvalPerf, the results were again mixed. CoT reduced static regressions more strongly \n(Qodana: 23 \u2192 8) than few-shot (23 \u2192 16), while dynamic metrics showed smaller differ\u00ad\nences: CoT slightly improved execution time (67 \u2192 57), and few-shot marginally reduced \nCPU regressions (48 \u2192 46), with memory remaining relatively stable across prompts. For \nCopilot, CoT is generally more effective in reducing static performance regressions, par\u00ad\nticularly in HumanEval, MBPP, and EvalPerf, while few-shot tends to yield better results in \ndynamic performance metrics, especially execution time and CPU utilization.\nSimilarly,"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_039",
    "source_id": "PerformanceAnalysis2026",
    "text": " \u2192 225). Finally, \nin EvalPerf, the results were again mixed. CoT reduced static regressions more strongly \n(Qodana: 23 \u2192 8) than few-shot (23 \u2192 16), while dynamic metrics showed smaller differ\u00ad\nences: CoT slightly improved execution time (67 \u2192 57), and few-shot marginally reduced \nCPU regressions (48 \u2192 46), with memory remaining relatively stable across prompts. For \nCopilot, CoT is generally more effective in reducing static performance regressions, par\u00ad\nticularly in HumanEval, MBPP, and EvalPerf, while few-shot tends to yield better results in \ndynamic performance metrics, especially execution time and CPU utilization.\nSimilarly, as illustrated in Table\u00a011, CodeLlama exhibited mixed effects of prompt engi\u00ad\nneering across datasets. In HumanEval, both CoT and few-shot reduced dynamic regressions \nmost effectively, particularly for execution time and memory. In MBPP, few-shot provided \nthe most notable benefits for dynamic metrics, reducing regressions in execution time, \nmemory, and CPU utilization (Execution time: 97 \u2192 63; Memory: 38 \u2192 15; CPU: 188 \u2192 \n58), while CoT had only minor effects. In EvalPerf, differences were less pronounced: few-\nshot slightly improved memory and CPU regressions (Memory: 39 \u2192 35; CPU: 42 \u2192 41), \nwhereas CoT increased execution time regressions (64 \u2192 76). In AixBench, static analysis \nshowed that CoT reduced regressions flagged by both SpotBugs (8 \u2192 3) and PMD (37 \u2192 \n29), while few-shot achieved smaller reductions for SpotBugs (8 \u2192 6) and no improve\u00ad\nment for PMD (37). These results indicate that CoT is more effective for static performance \nregressions in CodeLlama, especially in AixBench, while few-shot tends to deliver stronger \nimprovements in dynamic performance regressions, particularly in MBPP.\nFor DeepSeek-Coder, the effects of prompt engineering were also dataset- and metric-\ndependent, as summarized in Table\u00a011. In HumanEval, both CoT and few-shot reduced \ndynamic regressions, with few-shot yielding the most consistent improvements (Execution \ntime: 17 \u2192 4; Memory: 6 \u2192 4; CPU: 13 \u2192 6). In MBPP, prompt engineering again proved \neffective: CoT and few-shot both reduced execution time regressions relative to the baseline \n(186 \u2192 145 and 186 \u2192 114, respectively), while memory and CPU regressions were sub\u00ad\nstantially lower with few-shot (Memory: 89 \u2192 28; CPU: 213 \u2192 81). In EvalPerf, the trends \nwere more nuanced. CoT reduced static regressions flagged by Qodana (14 \u2192 10), but at \nthe dynamic level it increased execution time regressions (50 \u2192 60) and did not improve \nmemory usage (23 \u2192 23). Few-shot achieved only a slight reduction in memory regressions \n(23 \u2192 22), while execution time (51) and CPU regressions (39) remained above baseline, \nindicating limited benefits for runtime efficiency. In AixBench, CoT outperformed few-shot"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_040",
    "source_id": "PerformanceAnalysis2026",
    "text": " while memory and CPU regressions were sub\u00ad\nstantially lower with few-shot (Memory: 89 \u2192 28; CPU: 213 \u2192 81). In EvalPerf, the trends \nwere more nuanced. CoT reduced static regressions flagged by Qodana (14 \u2192 10), but at \nthe dynamic level it increased execution time regressions (50 \u2192 60) and did not improve \nmemory usage (23 \u2192 23). Few-shot achieved only a slight reduction in memory regressions \n(23 \u2192 22), while execution time (51) and CPU regressions (39) remained above baseline, \nindicating limited benefits for runtime efficiency. In AixBench, CoT outperformed few-shot \nin static analysis, decreasing SpotBugs regressions (5 \u2192 1) and maintaining PMD at similar \nlevels (13 \u2192 12), whereas few-shot failed to reduce regressions (SpotBugs: 5; PMD: 14).\nFor Copilot Chat, the effects of prompt engineering were mixed across datasets (Table\u00a011). \nIn HumanEval, few-shot prompts improved dynamic performance, reducing execution time \nregressions from 45 to 30 and CPU regressions from 66 to 64, while CoT increased regres\u00ad\nsions across all dynamic metrics (execution time: 45 \u2192 55; memory: 39 \u2192 56; CPU: 66 \u2192 \n75). In MBPP, few-shot yielded moderate gains by lowering execution time regressions (374 \n\u2192 293), though memory and CPU regressions remained largely unchanged; by contrast, \nCoT increased total regressions, suggesting limited robustness of this strategy. In EvalPerf, \nboth prompting strategies showed small improvements in dynamic performance. Few-shot \n1\u202f3\n   62 \n\u2003\nPage 36 of 52\nEmpirical Software Engineering           (2026) 31:62 \nslightly reduced execution time (79 \u2192 70) and memory regressions (36 \u2192 37 remained \nnearly stable), while CoT decreased execution time regressions to 74 but increased CPU \nregressions (59 \u2192 62). For AixBench, prompt engineering was more effective in static \nanalysis. Few-shot reduced SpotBugs regressions (5 \u2192 3) and PMD regressions (55 \u2192 \n46), whereas CoT achieved smaller gains (SpotBugs: 5; PMD: 55 \u2192 52). Copilot Chat \nexhibited notable sensitivity to prompt engineering: while few-shot prompts offered tan\u00ad\ngible improvements in reducing dynamic regressions in HumanEval and MBPP, and static \nregressions in AixBench, CoT often introduced additional inefficiencies, underscoring the \nimportance of selecting appropriate prompting strategies.\nFew-shot prompt engineering consistently demonstrates the most robust improvements \nacross dynamic performance metrics (execution time, memory, CPU) for Copilot, Deep\u00ad\nSeek-Coder, and to a lesser extent CodeLlama and Copilot Chat. And CoT prompts yield \nmixed results: sometimes improving memory or CPU usage but occasionally increasing \nexecution time regressions. HumanEval and MBPP benefit most from prompt strategies, \nwhile EvalPerf and AixBench show more dataset-dependent outcomes. Overall, prompt \nengineering\u2013especially few-shot\u2013proves broadly effective for reducing performance reg"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_041",
    "source_id": "PerformanceAnalysis2026",
    "text": " and static \nregressions in AixBench, CoT often introduced additional inefficiencies, underscoring the \nimportance of selecting appropriate prompting strategies.\nFew-shot prompt engineering consistently demonstrates the most robust improvements \nacross dynamic performance metrics (execution time, memory, CPU) for Copilot, Deep\u00ad\nSeek-Coder, and to a lesser extent CodeLlama and Copilot Chat. And CoT prompts yield \nmixed results: sometimes improving memory or CPU usage but occasionally increasing \nexecution time regressions. HumanEval and MBPP benefit most from prompt strategies, \nwhile EvalPerf and AixBench show more dataset-dependent outcomes. Overall, prompt \nengineering\u2013especially few-shot\u2013proves broadly effective for reducing performance regres\u00ad\nsions across models and datasets, with Copilot and DeepSeek-Coder showing the most sub\u00ad\nstantial gains.\nPrompt Engineering is Most Effective in Optimizing Execution Time and CPU Utiliza\u00ad\ntion\u2002 Prompt engineering effectively improves execution time and CPU utilization, as evi\u00ad\ndenced by reduced performance regressions for these metrics. However, the improvements \nin memory usage are less prominent. We hypothesize that the reasons for this may include \nthe complexity of memory optimization, which often involves intricate data structures and \nalgorithmic changes that are not as directly addressed by our current prompt engineering \nstrategies. Additionally, memory management is highly dependent on the runtime environ\u00ad\nment and garbage collection mechanisms, which may not be fully captured by our prompt \ndirectives. Further research is needed to develop more effective prompts for memory \noptimization.\nPrompt engineering emerges as a valuable technique for mitigating performance\nregressions inAI-generatedcode. Few-shot prompt engineeringspeci\ufb01callytargeting\nthe root causes of performance regressions facilitates the identi\ufb01cation of potential\nperformance bottlenecks and guides models to generate more ef\ufb01cient code. While\nexecutiontime andCPU utilizationbene\ufb01t greatlyfromprompt engineering,memory\nusage improvements require further research to understand and address the underly-\ning reasons.\n5\u2002 Discussion\nIn this section, we discuss five aspects of AI-generated code performance: performance \nregressions across languages and datasets, prompt engineering for performance, energy and \ncarbon footprint considerations, model-specific differences and architectural implications, \nand directions for future research on AI-generated code performance.\n1\u202f3\nPage 37 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nPerformance Regressions of AI-Generated Code Across Languages and Datasets\u2002 While \nAI-generated code can be functionally correct, it often exhibits performance regressions \ncompared to human-written code, particularly when evaluated on performance-oriented \nbenchmarks. Additionally, all models maintain relatively high correctness on HumanE\u00ad\nval and MBPP, with pass rates typically above 70%. However, when assessed on the per\u00ad\nformance-oriented EvalPerf dataset, the magnitude of performance regressions increases \nacross all models despite comparable functional accuracy, suggesting that current LLMs \nprioritize syntactic and semantic correctness over execution efficiency. In contrast, the Aix\u00ad\nBench dataset\u2013comprising Java tasks\u2013shows the lowest correctness rates (mostly between \n37%\u201365%) and a higher incidence of static performance regressions detected by PMD and \nSpotBugs. Compared with the Python"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_042",
    "source_id": "PerformanceAnalysis2026",
    "text": "ions \ncompared to human-written code, particularly when evaluated on performance-oriented \nbenchmarks. Additionally, all models maintain relatively high correctness on HumanE\u00ad\nval and MBPP, with pass rates typically above 70%. However, when assessed on the per\u00ad\nformance-oriented EvalPerf dataset, the magnitude of performance regressions increases \nacross all models despite comparable functional accuracy, suggesting that current LLMs \nprioritize syntactic and semantic correctness over execution efficiency. In contrast, the Aix\u00ad\nBench dataset\u2013comprising Java tasks\u2013shows the lowest correctness rates (mostly between \n37%\u201365%) and a higher incidence of static performance regressions detected by PMD and \nSpotBugs. Compared with the Python-based datasets (HumanEval, MBPP, and EvalPerf), \nwhere static issues are less prevalent, Java code generated by models such as CodeLlama \nand Copilot Chat contains more inefficient structures and performance-related anti-patterns. \nThese findings indicate that Java tasks are more susceptible to static-level inefficiencies, \nlikely due to the increased syntactic rigidity and verbosity of Java. The discrepancy in per\u00ad\nformance regressions between Java and Python may be attributed to differences in language \nconstructs, compiler optimizations, or the specific challenges each language presents to the \nAI model. To gain a comprehensive understanding of performance regressions of AI-gener\u00ad\nated code across languages and datasets, we extended the scope of analysis from the initial \nfocus on GitHub Copilot to include CodeLlama, DeepSeek-Coder and Copilot Chat, ensur\u00ad\ning that our conclusions reflect the broader state of code generation technologies. We can \nmore thoroughly evaluate the performance regression of different models and uncover their \ncommonalities and differences in generating code across various programming languages \nand datasets. The datasets used in this study\u2013HumanEval, MBPP, EvalPerf, and AixBench\u2013\nprovide complementary perspectives on code generation quality. While HumanEval and \nMBPP primarily assess functional correctness, EvalPerf focuses explicitly on performance-\ncritical code. The datasets are valuable for our study due to their established use in prior \nresearch and comprehensive coverage of typical programming tasks. However, they might \nnot fully capture the complete spectrum of performance-critical code. Despite these limi\u00ad\ntations, our selection of datasets provides a solid foundation for evaluating AI-generated \ncode, particularly in Java and Python, offering a meaningful perspective on the capabilities \nand limitations of current LLMs in generating efficient code. Future work should expand \nperformance-oriented datasets and incorporate more realistic, resource-intensive scenarios \nto better evaluate the computational robustness of AI-generated code.\nPrompt Engineering for the Performance of AI-Generated Code\u2002 Few-shot prompt engi\u00ad\nneering has proven effective in reducing performance regressions, but it does not entirely \neliminate them. This suggests that existing AI models may require more complex architec\u00ad\ntures to better identify, predict, and prioritize performance optimization opportunities. In \nour study, we used few-shot prompts based on the root causes of performance regressions \nto guide the code generation models in producing more efficient code and conducted base\u00ad\nline evaluations. In addition to these exemplar-based prompts, we further incorporated CoT \nprompting, a widely studied strategy for eliciting step-by-step reasoning. The comparative \nresults reveal that prompt type plays a crucial role in shaping model behavior: Co"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_043",
    "source_id": "PerformanceAnalysis2026",
    "text": " of AI-Generated Code\u2002 Few-shot prompt engi\u00ad\nneering has proven effective in reducing performance regressions, but it does not entirely \neliminate them. This suggests that existing AI models may require more complex architec\u00ad\ntures to better identify, predict, and prioritize performance optimization opportunities. In \nour study, we used few-shot prompts based on the root causes of performance regressions \nto guide the code generation models in producing more efficient code and conducted base\u00ad\nline evaluations. In addition to these exemplar-based prompts, we further incorporated CoT \nprompting, a widely studied strategy for eliciting step-by-step reasoning. The comparative \nresults reveal that prompt type plays a crucial role in shaping model behavior: CoT tends to \nimprove outcomes in static regression analyses (e.g., HumanEval, MBPP), while few-shot \nexemplar prompts more consistently enhance dynamic performance metrics such as execu\u00ad\n1\u202f3\n   62 \n\u2003\nPage 38 of 52\nEmpirical Software Engineering           (2026) 31:62 \ntion time and CPU utilization. These findings indicate that prompt design is not a one-size-\nfits-all solution but instead interacts with dataset characteristics and model families. Future \nresearch should therefore explore systematic prompt design, including controlled ablation \nstudies on exemplar count, selection strategy, and formatting style, as well as comparative \nanalyses of advanced prompting paradigms such as instruction-tuned or negative-example \nprompts\u00a0(White et\u00a0al. 2023). Such directions can provide a more principled framework for \nassessing prompt robustness and for further enhancing the performance of AI-generated \ncode.\nEnergy and Carbon Footprint of AI-Generated Code\u2002 Performance evaluation is increasingly \nextending beyond traditional runtime and memory metrics to encompass energy consump\u00ad\ntion and carbon footprint, which are critical for sustainable software engineering. Inefficient \ncode can result in unnecessary power draw and higher greenhouse gas emissions, particu\u00ad\nlarly when deployed at scale or executed repeatedly in production environments\u00a0(Vartzi\u00ad\notis 2024; Shi et\u00a0al. 2025; Sikand et\u00a0al. 2024). While our analysis focused on execution \ntime, memory usage, and CPU utilization, it provides an indirect foundation for under\u00ad\nstanding energy implications. Execution time and CPU utilization are strongly correlated \nwith energy consumption: the performance regressions we identified\u2013such as inefficient \nAPI usage, nested looping, and missed mathematical optimizations\u2013not only degrade run\u00ad\ntime efficiency but also amplify energy consumption through prolonged or computationally \nintensive execution. In this context, our findings that prompt engineering, particularly few-\nshot prompting, helps reduce execution time and CPU utilization across multiple datasets \nand models suggest that such strategies may also yield energy savings. By guiding models \ntoward more efficient algorithmic patterns, performance-aware prompting can potentially \nreduce both computational overhead and its associated energy footprint, highlighting an \nenvironmental benefit that extends beyond immediate user experience. Future work should \nincorporate explicit energy efficiency metrics using tools such as PowerAPI, PyJoules into \nperformance evaluations. Additionally, exploring prompt strategies explicitly designed for \nenergy-efficient code generation and developing benchmark datasets that stress-test energy \nefficiency would complement existing correctness- and performance-oriented evaluations.\nModel-Specific Differences and Architectural Implications\u2002 Our comparative evaluation \nac"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_044",
    "source_id": "PerformanceAnalysis2026",
    "text": " prompt engineering, particularly few-\nshot prompting, helps reduce execution time and CPU utilization across multiple datasets \nand models suggest that such strategies may also yield energy savings. By guiding models \ntoward more efficient algorithmic patterns, performance-aware prompting can potentially \nreduce both computational overhead and its associated energy footprint, highlighting an \nenvironmental benefit that extends beyond immediate user experience. Future work should \nincorporate explicit energy efficiency metrics using tools such as PowerAPI, PyJoules into \nperformance evaluations. Additionally, exploring prompt strategies explicitly designed for \nenergy-efficient code generation and developing benchmark datasets that stress-test energy \nefficiency would complement existing correctness- and performance-oriented evaluations.\nModel-Specific Differences and Architectural Implications\u2002 Our comparative evaluation \nacross GitHub Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder reveals that model \narchitecture, training objectives, and decoding strategies jointly shape the observed per\u00ad\nformance characteristics. DeepSeek-Coder demonstrates comparatively strong static code \nquality, likely owing to its extensive pre-training on large-scale and diverse programming \ncorpora emphasizing syntactic correctness and conventional implementation patterns. How\u00ad\never, its dynamic performance occasionally lags behind, suggesting that the model\u2019s optimi\u00ad\nzation objectives prioritize functional completeness and correctness over runtime efficiency. \nMoreover, its relatively constrained token budget may limit deeper reasoning within a single \ngeneration cycle, resulting in implementations that are correct but less optimized in execu\u00ad\ntion. CodeLlama exhibits a more balanced performance profile across benchmarks, benefit\u00ad\nting from its larger parameter scale and extended context window, which enhance its ability \nto process complex prompts and incorporate few-shot exemplars. Nevertheless, this flex\u00ad\nibility can lead to output variability, particularly on heterogeneous datasets such as MBPP. \nIts fine-tuning focus on code comprehension rather than aggressive optimization may also \n1\u202f3\nPage 39 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \naccount for its moderate static violations and stable but not outstanding runtime perfor\u00ad\nmance. A notable divergence is observed between the effects of few-shot and CoT prompt\u00ad\ning strategies across model families. Few-shot prompting\u2013providing concrete exemplars \nof efficient implementations\u2013consistently enhances performance for code-oriented models \nsuch as GitHub Copilot, CodeLlama, and DeepSeek-Coder, which are trained primarily \nunder completion-style objectives and thus effectively leverage pattern-based generaliza\u00ad\ntion. In contrast, CoT prompting, which encourages explicit reasoning sequences, tends \nto yield limited or even adverse effects on these models, suggesting that low-level code \nsynthesis tasks benefit less from abstract reasoning chains. Models with extended context \nwindows, such as CodeLlama, demonstrate greater capacity to exploit few-shot exemplars \nby retaining and abstracting multiple efficiency-oriented code structures simultaneously. \nConversely, models with stronger reasoning-oriented architectures, such as Copilot Chat, \nexhibit higher robustness to CoT prompting, although their improvements remain moderate \nin magnitude. This differential sensitivity underscores that prompt engineering effectiveness \nis inherently model-dependent, reflecting the degree to which each architecture aligns with \nreasoning-oriented versus pattern-imitation paradigms. The service-based models\u2013GitHub \nCopilot and Copilot Chat\u2013illustrate another dimension of architectural influence. Their pro\u00ad\nprietary nature and continuous deployment enable the integration of advanced"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_045",
    "source_id": "PerformanceAnalysis2026",
    "text": " reasoning chains. Models with extended context \nwindows, such as CodeLlama, demonstrate greater capacity to exploit few-shot exemplars \nby retaining and abstracting multiple efficiency-oriented code structures simultaneously. \nConversely, models with stronger reasoning-oriented architectures, such as Copilot Chat, \nexhibit higher robustness to CoT prompting, although their improvements remain moderate \nin magnitude. This differential sensitivity underscores that prompt engineering effectiveness \nis inherently model-dependent, reflecting the degree to which each architecture aligns with \nreasoning-oriented versus pattern-imitation paradigms. The service-based models\u2013GitHub \nCopilot and Copilot Chat\u2013illustrate another dimension of architectural influence. Their pro\u00ad\nprietary nature and continuous deployment enable the integration of advanced post-process\u00ad\ning mechanisms, broader and more recent training corpora, and reinforcement strategies \naimed at improving alignment with user intent. Copilot Chat, powered by GPT-4.1, achieves \nthe highest functional correctness and static code quality, indicating superior internal rea\u00ad\nsoning and error-detection capabilities. However, its elevated dynamic regression rate \nsuggests a tendency to favor generalizable and feature-complete solutions over highly opti\u00ad\nmized implementations\u2013an expected trade-off stemming from instruction-tuning and con\u00ad\nversational fine-tuning objectives. Overall, these model-specific observations indicate that \nperformance regressions in AI-generated code stem not solely from dataset characteristics \nbut from the interplay among architectural scale, fine-tuning orientation, decoding strategy, \nand responsiveness to prompt design. A deeper understanding of these internal mechanisms \nprovides valuable insight for future model development, emphasizing the importance of \nincorporating performance-awareness into pre-training objectives and decoding strategies \nto achieve a more balanced trade-off between functional correctness and runtime efficiency.\nFuture Research on AI-Generated Code Performance\u2002 We investigate the performance \nregressions of code generated by GitHub Copilot, Copilot Chat, CodeLlama, and Deep\u00ad\nSeek-Coder, which are representative of state-of-the-art code generation LLMs. The inclu\u00ad\nsion of multiple models broadens the scope of our findings, suggesting that the observed \nperformance regressions may be a common challenge for LLMs utilizing similar archi\u00ad\ntectures and methodologies. By employing both static and dynamic analysis techniques, \nwe have provided a comprehensive evaluation of performance regressions and their under\u00ad\nlying causes. This dual approach helps mitigate the limitations of relying solely on one \ntype of analysis, ensuring a more holistic view of code performance. Our findings point to \nseveral areas for further research and development. First, expanding performance evalua\u00ad\ntion to encompass a broader spectrum of programming languages, paradigms, and execu\u00ad\ntion environments would clarify whether observed regressions arise from language-specific \ncharacteristics or reflect more fundamental architectural constraints. Such cross-environ\u00ad\nmental analyses could further disentangle the interplay between compiler optimizations, \n1\u202f3\n   62 \n\u2003\nPage 40 of 52\nEmpirical Software Engineering           (2026) 31:62 \nruntime systems, and structural properties of generated code. Second, continued explora\u00ad\ntion of prompt engineering strategies is essential for mitigating performance inefficiencies. \nBeyond few-shot and CoT prompting, systematic investigations into exemplar selection, \nformatting consistency, and adaptive or feedback-driven prompt design\u2013potentially"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_046",
    "source_id": "PerformanceAnalysis2026",
    "text": " programming languages, paradigms, and execu\u00ad\ntion environments would clarify whether observed regressions arise from language-specific \ncharacteristics or reflect more fundamental architectural constraints. Such cross-environ\u00ad\nmental analyses could further disentangle the interplay between compiler optimizations, \n1\u202f3\n   62 \n\u2003\nPage 40 of 52\nEmpirical Software Engineering           (2026) 31:62 \nruntime systems, and structural properties of generated code. Second, continued explora\u00ad\ntion of prompt engineering strategies is essential for mitigating performance inefficiencies. \nBeyond few-shot and CoT prompting, systematic investigations into exemplar selection, \nformatting consistency, and adaptive or feedback-driven prompt design\u2013potentially guided \nby empirical profiling of performance bottlenecks\u2013may yield more robust frameworks for \nsteering models toward efficiency-aware code generation. Third, tighter integration of per\u00ad\nformance feedback into model training and inference processes holds promise for bridging \nthe persistent divide between functional correctness and runtime efficiency. Incorporating \nexecution-aware training objectives, reinforcement signals derived from profiling data, or \niterative runtime-guided refinement could enable models to internalize and optimize for \nperformance trade-offs during generation. Finally, the development of standardized, perfor\u00ad\nmance-oriented benchmarks that jointly capture static and dynamic dimensions of efficiency \nwould enable reproducible and fine-grained evaluation. Such benchmarks could support \ncausal analyses of performance regressions, allowing researchers to isolate algorithmic inef\u00ad\nficiencies from systemic overhead with greater precision. Collectively, these research direc\u00ad\ntions extend the insights of our current study\u2013spanning cross-language variability, prompt \nresponsiveness, and architectural diversity\u2013and chart a path toward next-generation, perfor\u00ad\nmance-conscious code generation systems capable of balancing correctness, readability, and \ncomputational efficiency.\n6\u2002 Threats\nIn this section, we discuss the threats to the validity of our research.\nExternal Validity\u2002 The generalizability of our findings may be affected by the choice of \ndatasets and code generation models. Our findings are based on four open-source datasets, \ni.e., HumanEval, AixBench, MBPP, and EvalPerf. While valuable, these datasets might not \nfully represent the spectrum of performance-critical code in diverse real-world applications. \nThis study focuses on Java and Python, and results might not generalize directly to other \nprogramming languages such as C++ or Rust. Performance regressions may significantly \ndiffer due to language features, compiler optimizations, and running environments. While \nGitHub Copilot, CodeLlama, DeepSeek-Coder, and Copilot Chat are representative code \ngeneration models, our findings may not apply universally to all LLMs. These LLMs dif\u00ad\nfer in training data composition, architectural design, and decoding strategies, which may \ninfluence the observed patterns of performance regressions. Additionally, our study adopts \none widely used configuration per model (e.g., CodeLlama-7B, DeepSeek-Coder-6.7B) and \nfixed decoding parameters (temperature = 0.0, top-p = 0.9) to ensure fairness and compa\u00ad\nrability across experiments. However, it is worth acknowledging that variations in model \nscale, sampling temperature, or decoding strategy can substantially alter generation diver\u00ad\nsity, determinism, and, consequently, performance efficiency"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_047",
    "source_id": "PerformanceAnalysis2026",
    "text": " our findings may not apply universally to all LLMs. These LLMs dif\u00ad\nfer in training data composition, architectural design, and decoding strategies, which may \ninfluence the observed patterns of performance regressions. Additionally, our study adopts \none widely used configuration per model (e.g., CodeLlama-7B, DeepSeek-Coder-6.7B) and \nfixed decoding parameters (temperature = 0.0, top-p = 0.9) to ensure fairness and compa\u00ad\nrability across experiments. However, it is worth acknowledging that variations in model \nscale, sampling temperature, or decoding strategy can substantially alter generation diver\u00ad\nsity, determinism, and, consequently, performance efficiency. For instance, larger models \nmay capture more nuanced optimization patterns but also incur greater resource overhead, \nwhereas higher-temperature sampling might promote algorithmic creativity at the cost of \nruntime stability. Similarly, proprietary systems such as Copilot Chat expose configurable \nparameters (e.g., model selections, completion length, contextual window) that may influ\u00ad\nence generated code quality and efficiency. Future studies could systematically explore \n1\u202f3\nPage 41 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nmodel variants, decoding strategies, and hyperparameter settings to quantify their effects on \nperformance regressions to construct a more comprehensive understanding of how architec\u00ad\ntural and configurational factors influence the performance of AI-generated code.\nInternal Validity\u2002 To detect potential performance regressions, we employ three static analy\u00ad\nsis tools (e.g., Qodana, Spotbugs) and three dynamic profilers (e.g., cProfile). These tools \nwere chosen for their established use in prior research and their robust support for the pro\u00ad\ngramming languages. While these tools provide robust coverage, several sources of bias \nmay remain. Static analyzers rely on rule-based heuristics that can generate false positives \nunrelated to actual runtime inefficiencies. Conversely, dynamic profilers depend on the rep\u00ad\nresentativeness of the test inputs and can introduce profiling overhead. To mitigate these \nthreats, all dynamic measurements were repeated ten times in a clean execution environ\u00ad\nment, with the first warm-up run discarded. We applied the Mann\u2013Whitney U test and Cliff\u2019s \n\u03b4 to evaluate the significance and magnitude of performance differences, thereby improving \nthe reliability of observed effects. To further refine our dynamic analysis, we introduced \nan additional refinement stage focusing on the critical execution path of each regression \ncase. Instead of profiling entire scripts\u2013which may include initialization, imports, or logging \noverhead\u2013we restricted profiling to the execution of the verification function to isolate the \ncore algorithmic behavior of generated code. This refinement mitigates confounding fac\u00ad\ntors arising from superficial structural variations and enables a more causal interpretation \nof performance differences. Although this approach substantially improves interpretability, \nit remains limited to single-function tasks and may not fully capture multi-module interac\u00ad\ntions. Manually classifying reasons for performance regressions in AI-generated code can \nintroduce subjective factors. To mitigate this, we employ two authors for independent code \nexamination. Disagreements are resolved with a tie-breaker to ensure consistency. We cal\u00ad\nculate Cohen\u2019s Kappa statistic of 0.87, indicating considerable agreement"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_048",
    "source_id": "PerformanceAnalysis2026",
    "text": " \noverhead\u2013we restricted profiling to the execution of the verification function to isolate the \ncore algorithmic behavior of generated code. This refinement mitigates confounding fac\u00ad\ntors arising from superficial structural variations and enables a more causal interpretation \nof performance differences. Although this approach substantially improves interpretability, \nit remains limited to single-function tasks and may not fully capture multi-module interac\u00ad\ntions. Manually classifying reasons for performance regressions in AI-generated code can \nintroduce subjective factors. To mitigate this, we employ two authors for independent code \nexamination. Disagreements are resolved with a tie-breaker to ensure consistency. We cal\u00ad\nculate Cohen\u2019s Kappa statistic of 0.87, indicating considerable agreement\u00a0(McHugh 2012). \nFurther user and case studies could strengthen this area and provide deeper insights into \nthe rationale behind these regressions. Our approach relies on specific performance metrics \n(e.g., CPU and memory usage) chosen based on the software systems\u2019 nature. While these \nare common choices, selecting appropriate metrics can require system-specific expertise. \nFuture work could incorporate more sophisticated instrumentation techniques, standardized \nfunction bodies for cross-language comparison, and finer-grained profiling mechanisms that \ncapture multi-function dependencies. Expanding these methodological refinements would \nhelp disentangle algorithmic inefficiencies from structural or environmental noise, yielding \na clearer causal understanding of performance regressions in AI-generated code.\nConstruct Validity\u2002 Dynamic analysis using profilers can be influenced by environmental \nfactors and noise. To mitigate this issue, we executed the generated code multiple times in \na clean environment and applied the Mann-Whitney U test and Cliff\u2019s \u03b4 to analyze the ten \nrepeated runs, aiming to better capture the actual performance of the code. However, some \nnoise is inherent in performance monitoring. Future studies could consider increasing rep\u00ad\netitions based on time and resource constraints. In addition, incorporating refined, function-\nlevel profiling could further reduce measurement bias by isolating algorithmic inefficiencies \nfrom structural overhead. Moreover, static analysis\u2013though scalable and widely adopted\u2013\nrelies on rule-based heuristics that may not fully capture real performance regressions. Tools \nsuch as Qodana, SpotBugs, and PMD identify code smells that do not always result in \n1\u202f3\n   62 \n\u2003\nPage 42 of 52\nEmpirical Software Engineering           (2026) 31:62 \nmeasurable slowdowns at runtime. To assess the validity of these rule-based detections, \nwe conducted a manual inspection of 50 randomly sampled static warnings across datasets \nand tools. The analysis revealed that approximately 46% of the flagged cases corresponded \nto genuine performance regressions when cross-checked via dynamic profiling. This find\u00ad\ning highlights that static analysis serves as an effective large-scale screening mechanism \nbut with limited precision due to the generality of rule definitions. Accordingly, our study \nintegrates both static and dynamic evidence to ensure a more reliable and comprehensive \nassessment of performance regressions in AI-generated code. Future research can explore \nadditional techniques or tools to uncover more complex performance problems within AI-\ngenerated code.\nConclusion Validity\u2002 In our study, potential threats arise from performance variability across \ndatasets and model architectures, as well as the stochastic nature of LLMs. Moreover, differ\u00ad\nences in runtime environments and the limited number of samples"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_049",
    "source_id": "PerformanceAnalysis2026",
    "text": "% of the flagged cases corresponded \nto genuine performance regressions when cross-checked via dynamic profiling. This find\u00ad\ning highlights that static analysis serves as an effective large-scale screening mechanism \nbut with limited precision due to the generality of rule definitions. Accordingly, our study \nintegrates both static and dynamic evidence to ensure a more reliable and comprehensive \nassessment of performance regressions in AI-generated code. Future research can explore \nadditional techniques or tools to uncover more complex performance problems within AI-\ngenerated code.\nConclusion Validity\u2002 In our study, potential threats arise from performance variability across \ndatasets and model architectures, as well as the stochastic nature of LLMs. Moreover, differ\u00ad\nences in runtime environments and the limited number of samples per dataset may influence \nthe stability of the conclusions. To mitigate these issues, we report statistical significance, \neffect sizes, and variance indicators (standard deviations and confidence intervals) to quan\u00ad\ntify both the strength and consistency of observed differences. We also employ repeated \nexecutions for each generated code sample to reduce random noise and ensure stable mea\u00ad\nsurements. Furthermore, by evaluating both closed-source (e.g., GitHub Copilot) and open-\nsource models with transparent configurations, and conducting controlled experiments \ncomparing few-shot and CoT prompting strategies, we strengthen the interpretability and \nreproducibility of our findings. Nonetheless, residual uncertainty remains due to inherent \nrandomness in LLM outputs and the limited coverage of benchmark tasks. Future research \ncould enhance conclusion validity through larger-scale replication studies and longitudinal \nanalyses across LLM versions to assess temporal consistency and causal robustness.\n7\u2002 Related Work\nIn this section, we discuss research related to this work, including AI-assisted code genera\u00ad\ntion, assessing the quality of code generation techniques, and code performance analysis.\n7.1\u2002 AI-Assisted Code Generation\nExtensive prior research has explored automated code generation. Existing techniques fall \ninto two main categories: learning-based and retrieval-based approaches. The learning-\nbased approach focuses on extracting natural language features from training data and using \nthem for code generation. It can be further subdivided into supervised learning\u00a0(Ling et\u00a0al. \n2016; Yin and Neubig 2017; Rabinovich et\u00a0al. 2017; Iyer et\u00a0al. 2018; Wei et\u00a0al. 2019; Sun \net\u00a0al. 2020) and pre-trained model approaches\u00a0(Feng et\u00a0al. 2020; Guo et\u00a0al. 2021; Ahmad \net\u00a0al. 2021; Wang et\u00a0al. 2021; Nijkamp et\u00a0al. 2023; Li et\u00a0al. 2022). Supervised learning \nmethods often employ sequence-to-sequence models, which follow an encoder-decoder \nstructure. Pre-trained models, on the other hand, leverage self-supervised training on vast \namounts of unlabeled data. Notably, the Transformer architecture is prevalent in pre-trained \nmodels for code generation. Researchers have developed specialized pre-trained models \nfor the code domain, achieving impressive results in code generation tasks. Given the vast \n1\u202f3\nPage 43 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nsize of the code generation solution space, retrieval-based approaches incorporate"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_050",
    "source_id": "PerformanceAnalysis2026",
    "text": "amp et\u00a0al. 2023; Li et\u00a0al. 2022). Supervised learning \nmethods often employ sequence-to-sequence models, which follow an encoder-decoder \nstructure. Pre-trained models, on the other hand, leverage self-supervised training on vast \namounts of unlabeled data. Notably, the Transformer architecture is prevalent in pre-trained \nmodels for code generation. Researchers have developed specialized pre-trained models \nfor the code domain, achieving impressive results in code generation tasks. Given the vast \n1\u202f3\nPage 43 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nsize of the code generation solution space, retrieval-based approaches incorporate similar \ncode retrieval to assist the decoder in generating code\u00a0(Drain et\u00a0al. 2021; Hayati et\u00a0al. 2018; \nGuo et\u00a0al. 2019; Parvez et\u00a0al. 2021; Zhou et\u00a0al. 2023). This approach effectively reduces \nthe decoding space, leading to improved quality in the generated code. Applying the NLP \ntechnologies like fine-tuning\u00a0(Luo et\u00a0al. 2024b; Wei et\u00a0al. 2024) and reinforcement learn\u00ad\ning\u00a0(Li et\u00a0al. 2024a; Chae et\u00a0al. 2024) to code generation models enhances the perfor\u00ad\nmance of generated code. Besides, the integration of AI-assisted code generation models \ninto development environments has become a prominent trend in software development. \nBeyond well-known tools like GitHub Copilot, emerging solutions such as\u00a0Cursor (2025) \nand\u00a0Claude Code (2025) are actively advancing this field. These AI programming tools not \nonly offer code generation and editing capabilities but also actively integrate with version \ncontrol systems (e.g., Git) and continuous integration (CI) tools to optimize development \nworkflows\u00a0(Claude Code 2025).\n7.2\u2002 Assessing the Quality of Code Generation Techniques\nMany studies have evaluated the quality of code generation techniques or tools, e.g., Chat\u00ad\nGPT, Copilot, CodeLlama, DeepSeek-Coder and CodeWhisperer, primarily focusing on \nwhether these tools and models produce code that fulfills its intended function. Studies \nlike \u00a0Yetistiren et\u00a0al. (2022) highlighted GitHub Copilot\u2019s ability to generate valid code with \na high success rate. \u00a0Sobania et\u00a0al. (2022) found no significant difference in correctness \nbetween Copilot and other approaches. Similarly,\u00a0Nguyen and Nadi (2022) and\u00a0Yetistiren \net\u00a0al. (2023) evaluate code correctness, efficiency, and overall quality, with\u00a0Yetistiren et\u00a0al. \n(2023) observing improvements in generated code over time. However, recent research has \nbegun to emphasize user experience and the broader impact of these tools on developer \nproductivity. \u00a0 Barke et\u00a0al. (2023) showed that while Copilot might not directly shorten \ndevelopment time, it often serves as a valuable starting point, though challenges remain \nin understanding, editing, and debugging generated code snippets.\u00a0Lertbanjongngam et\u00a0al. \n(2022) compared human-written code with AlphaCode"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_051",
    "source_id": "PerformanceAnalysis2026",
    "text": "uyen and Nadi (2022) and\u00a0Yetistiren \net\u00a0al. (2023) evaluate code correctness, efficiency, and overall quality, with\u00a0Yetistiren et\u00a0al. \n(2023) observing improvements in generated code over time. However, recent research has \nbegun to emphasize user experience and the broader impact of these tools on developer \nproductivity. \u00a0 Barke et\u00a0al. (2023) showed that while Copilot might not directly shorten \ndevelopment time, it often serves as a valuable starting point, though challenges remain \nin understanding, editing, and debugging generated code snippets.\u00a0Lertbanjongngam et\u00a0al. \n(2022) compared human-written code with AlphaCode-generated code, emphasizing the \nneed for developer review to identify performance bottlenecks.\u00a0Coignion et\u00a0al. (2024) found \nthat although LLM-generated code performs well in some cases, it is slower than 27% of \nhuman-written code on the LeetCode dataset.\u00a0Liu et\u00a0al. (2024b) found three instances of \ninefficient implementations within the HumanEval ground truth, which caused slow per\u00ad\nformance on inputs of reasonable size. The prior studies underscore the need for a more \ncomprehensive evaluation methodology that considers not only functional correctness but \nalso potential performance implications.\u00a0Hou and Ji (2024) found performance limitations \nin GPT-4-generated code, which can be partially addressed through optimization. \u00a0Garg \net\u00a0al. (2023) proposed leveraging LLMs with prompt engineering to optimize code perfor\u00ad\nmance, showing effective improvements in addressing performance regressions. Moreover, \na growing number of performance-centric evaluation benchmarks, e.g., EffiBench\u00a0(Huang \net\u00a0al. 2024b), Mercury\u00a0(Du et\u00a0al. 2024), and ECCO\u00a0(Waghjale et\u00a0al. 2024), have emerged. \nConcurrently, certain studies have explored the optimization of code performance using \nclassification and reinforcement learning approaches\u00a0(Seo et\u00a0al. 2024; He et\u00a0al. 2025). In \naddition, several recent works have proposed model-side improvements for efficiency. \nEffiLearner\u00a0(Huang et\u00a0al. 2024c) introduces a self-optimization framework that iteratively \nimproves code efficiency through execution overhead profiling, demonstrating significant \n1\u202f3\n   62 \n\u2003\nPage 44 of 52\nEmpirical Software Engineering           (2026) 31:62 \nreductions in execution time across multiple models and benchmarks. \u00a0Shypula et\u00a0al. (2024) \npresent Performance-Improving Edits, a method for learning performance-improving code \nedits from a curated dataset of over 77,000 human-made optimizations in competitive \nprogramming, enabling LLMs to suggest high-level algorithmic and API optimizations. \nACECode\u00a0(Yang et\u00a0al. 2024) employs reinforcement learning with a dual-objective reward \nsystem to simultaneously optimize code efficiency and correctness in code LLMs, achieving \nnotable improvements in both pass rates and runtime performance. PerfCodeGen\u00a0(Peng et\u00a0al. \n2025) leverages execution feedback to guide LLMs toward more performant generations, \n"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_052",
    "source_id": "PerformanceAnalysis2026",
    "text": " models and benchmarks. \u00a0Shypula et\u00a0al. (2024) \npresent Performance-Improving Edits, a method for learning performance-improving code \nedits from a curated dataset of over 77,000 human-made optimizations in competitive \nprogramming, enabling LLMs to suggest high-level algorithmic and API optimizations. \nACECode\u00a0(Yang et\u00a0al. 2024) employs reinforcement learning with a dual-objective reward \nsystem to simultaneously optimize code efficiency and correctness in code LLMs, achieving \nnotable improvements in both pass rates and runtime performance. PerfCodeGen\u00a0(Peng et\u00a0al. \n2025) leverages execution feedback to guide LLMs toward more performant generations, \nusing runtime profiling information to refine generated solutions iteratively. These works \nfocus on optimizing model training and inference through various techniques, including \nself-optimization, reinforcement learning, execution feedback, and specialized fine-tuning \nstrategies, whereas our study complements them by providing a systematic benchmarking \nand prompt-engineering perspective that evaluates performance regressions across multiple \nmodels and explores practical mitigation strategies accessible without model retraining. \nHowever, a comprehensive analysis of performance regressions in AI-generated code is still \nneeded to identify root causes and guide further improvements.\n7.3\u2002 Code Performance Analysis\nExtensive research has been conducted to analyze performance at the code level, which is \ntypically divided into two main categories: static code performance regression analysis and \ndynamic performance regression analysis. Static analysis examines code without execu\u00ad\ntion, identifying potential performance regressions through code structure and patterns, e.g., \nperformance anti-pattern\u00a0(Chen et\u00a0al. 2014; Reichelt et\u00a0al. 2019a; van Dinten et\u00a0al. 2024). \nFor example, \u00a0Gao et\u00a0al. (2024) employed abstract syntax trees to represent the structural \ninformation of code, thereby identifying and comparing optimization patterns to support \nLLMs in more effectively optimizing code. Meanwhile,\u00a0Venkatesh et\u00a0al. (2024) investigated \nthe application of LLMs in static analysis tasks, e.g., call graph analysis and type infer\u00ad\nence in Python programs. Many static analysis tools have also been proposed to analyze \ncode performance regression. For instance, Qodana\u00a0(JetBrains s.r.o 2024), developed by \nJetBrains, is a comprehensive static analysis engine that supports identifying a wide array \nof issues, including performance regressions. Other tools like Spotbugs\u00a0(SpotBugs 2024) \nand PMD\u00a0(PMD 2024) are tailored for Java, focusing on detecting bugs. Dynamic analysis \ninvolves running the code and measuring performance in a real-world environment. This \napproach provides insights into the actual runtime behavior of the code by executing unit \ntests\u00a0(Reichelt et\u00a0al. 2019b; Chen et\u00a0al. 2023) and profiling\u00a0(Yan et\u00a0al. 2012; Weng et\u00a0al. \n2023). For example,\u00a0Huang et\u00a0al. (2024a) substantially improved the efficiency of llm-based \ncode generation by integrating an adaptive optimization strategy informed by dynamic \nperformance analysis. Beyond traditional runtime and memory concerns, performance is \nincreasingly tied to energy consumption and environmental impact. Recent studies"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_053",
    "source_id": "PerformanceAnalysis2026",
    "text": " focusing on detecting bugs. Dynamic analysis \ninvolves running the code and measuring performance in a real-world environment. This \napproach provides insights into the actual runtime behavior of the code by executing unit \ntests\u00a0(Reichelt et\u00a0al. 2019b; Chen et\u00a0al. 2023) and profiling\u00a0(Yan et\u00a0al. 2012; Weng et\u00a0al. \n2023). For example,\u00a0Huang et\u00a0al. (2024a) substantially improved the efficiency of llm-based \ncode generation by integrating an adaptive optimization strategy informed by dynamic \nperformance analysis. Beyond traditional runtime and memory concerns, performance is \nincreasingly tied to energy consumption and environmental impact. Recent studies\u00a0(Vartzi\u00ad\notis 2024; Shi et\u00a0al. 2025; Sikand et\u00a0al. 2024) emphasize the importance of sustainable code \ngeneration, highlighting that inefficient code not only degrades user experience but also \nincurs unnecessary energy costs and carbon footprint. \u00a0Islam et\u00a0al. (2025) conducted a com\u00ad\nprehensive study evaluating the energy efficiency of code generated by 20 popular LLMs \nand revealed that LLM-generated code often exhibits significantly higher energy consump\u00ad\ntion compared to human-written canonical solutions. \u00a0Cursaru et\u00a0al. (2024) performed a \n1\u202f3\nPage 45 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \ncontrolled experiment specifically examining Code Llama\u2019s energy efficiency, demonstrat\u00ad\ning systematic patterns of energy waste in AI-generated code. Furthermore, recent research \nhas also begun to explore methods for optimizing the energy efficiency of LLM-generated \ncode. \u00a0Cappendijk et\u00a0al. (2025) explored the effectiveness of different prompting strategies \nfor generating energy-efficient code, finding that explicit energy-aware prompts can reduce \nenergy consumption but with trade-offs in code complexity. Tools such as PowerAPI, psutil, \nand PyJoules provide promising means to integrate energy-related metrics into future evalu\u00ad\nations of LLM-generated code. Nevertheless, a multifaceted approach encompassing static \n, dynamic, and sustainability-oriented evaluations remains essential for gaining a holistic \nunderstanding of AI-generated code quality, especially in the face of increasingly complex \ngenerative models and stringent performance requirements.\n8\u2002 Conclusion\nIn this work, we analyzed the performance regression of code generated by LLM-based \nmodels, including GitHub Copilot, Copilot chat, CodeLlama, and DeepSeek-Coder. Our \nfindings indicate that while these models are effective at generating functionally correct \ncode, their output code exhibits significant performance regressions compared to human-\nwritten solutions. Our analysis revealed that the root causes of performance regressions \nare primarily at the code level, involving common root causes such as inefficient func\u00ad\ntion calls and inefficient looping. To mitigate these performance regressions, we designed \nand implemented a few-shot prompting approach. This method leverages the identified root \ncauses of code-level performance regressions to construct prompts with concrete examples, \nguiding the models to focus more on performance optimization during code generation. \nOur experimental results demonstrated that few-shot prompting can improve the perfor\u00ad\nmance of generated code, validating its potential as a mitigation strategy. In contrast, CoT \nprompt"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_054",
    "source_id": "PerformanceAnalysis2026",
    "text": " at generating functionally correct \ncode, their output code exhibits significant performance regressions compared to human-\nwritten solutions. Our analysis revealed that the root causes of performance regressions \nare primarily at the code level, involving common root causes such as inefficient func\u00ad\ntion calls and inefficient looping. To mitigate these performance regressions, we designed \nand implemented a few-shot prompting approach. This method leverages the identified root \ncauses of code-level performance regressions to construct prompts with concrete examples, \nguiding the models to focus more on performance optimization during code generation. \nOur experimental results demonstrated that few-shot prompting can improve the perfor\u00ad\nmance of generated code, validating its potential as a mitigation strategy. In contrast, CoT \nprompting proved less effective\u2013and in some cases detrimental\u2013suggesting that reasoning-\noriented strategies do not necessarily translate into improved performance regressions. \nBeyond individual datasets, we emphasized the importance of evaluating models under \nperformance-critical conditions. Our analysis across both general-purpose and efficiency-\noriented benchmarks highlights that performance regressions persist regardless of dataset \nscope, reinforcing the need for future evaluation frameworks that integrate efficiency as \na first-class dimension of code quality. Overall, this study not only highlights the limita\u00ad\ntions of current LLMs in code generation performance but also underscores the need for \nfurther optimization of these models to meet the demands of performance-critical applica\u00ad\ntions. While prompt engineering provides a viable means of mitigating certain performance \nregressions, its improvements remain inconsistent across tasks, suggesting that performance \noptimization requires integration at deeper levels of model design\u2013particularly within train\u00ad\ning objectives, decoding strategies, and data curation pipelines. Future work should incor\u00ad\nporate performance into model adaptation and pursue unified benchmarks that jointly assess \nfunctional, dynamic, and energy-related aspects of AI-generated code.\nAcknowledgements\u2002 This work is partially supported by the National Natural Science Foundation of China \n(Grant Nos. 62302347 and U2436205) and Open Project Funding of the Key Laboratory of Intelligent Sens\u00ad\ning System and Security (Ministry of Education).\n1\u202f3\n   62 \n\u2003\nPage 46 of 52\nEmpirical Software Engineering           (2026) 31:62 \nAuthor Contributions\u2002 Shuang Li (First Author): Methodology, Data collection, Field study, Writing\u2013draft \npreparation, review and editing. Yuntao Cheng (Second Author): Data curation, Writing\u2013draft preparation. \nJinfu Chen (Corresponding Author): Idea, Writing\u2013review and editing, Writing\u2013original draft preparation \nvisualization, Investigation. Jifeng Xuan: Investigation and Methodology. Sen He: Investigation and Meth\u00ad\nodology. Weiyi Shang: Investigation and Methodology. All authors have read and agreed to the published \nversion of the manuscript.\nFunding\u2002 This research received no external funding.\nData Availability\u2002 Our data and script are open source and available at \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200bh\u200be\u200bb\u200be\u200bn\u200ba\u200b/\u200bP\u200be\u200br\u200bf\u200bo\u200br\u200bm\u200ba\u200bn\u200b\nc\u200be\u200b-\u200bA\u200bn\u200ba\u200b"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_055",
    "source_id": "PerformanceAnalysis2026",
    "text": " and Methodology. Sen He: Investigation and Meth\u00ad\nodology. Weiyi Shang: Investigation and Methodology. All authors have read and agreed to the published \nversion of the manuscript.\nFunding\u2002 This research received no external funding.\nData Availability\u2002 Our data and script are open source and available at \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200bh\u200be\u200bb\u200be\u200bn\u200ba\u200b/\u200bP\u200be\u200br\u200bf\u200bo\u200br\u200bm\u200ba\u200bn\u200b\nc\u200be\u200b-\u200bA\u200bn\u200ba\u200bl\u200by\u200bs\u200bi\u200bs\u200b-\u200bo\u200bf\u200b-\u200bA\u200bI\u200b-\u200bG\u200be\u200bn\u200be\u200br\u200ba\u200bt\u200be\u200bd\u200b-\u200bC\u200bo\u200bd\u200be.\nDeclarations\nEthical Approval\u2002 Not applicable.\nInformed Consent\u2002 Not applicable.\nConflict of Interest\u2002 The authors declare no potential conflict of interest.\nClinical Trial Number\u2002 Not applicable.\nReferences\nAhmad WU, Chakraborty S, Ray B, Chang K (2021) Unified pre-training for program understanding and \ngeneration. In: Proceedings of the 2021 Conference of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, \n2021, pages 2655\u20132668. Association for Computational Linguistics\nAustin J, Odena A, Nye M, Bosma M, Michalewski H, Dohan D, Jiang E, Cai C, Terry M, Le Q, et\u00a0al (2021) \nProgram synthesis with large language models. arXiv:2108.07732\nBarke S, James MB, Polikarpova N (2023) Grounded copilot: How programmers interact with code-generat\u00ad\ning models. Proc ACM Program Lang 7(OOPSLA1):85\u2013111\nCappendijk T, de\u00a0Reus P, Oprescu A (2025) An exploration of prompting llms to generate energy-efficient \ncode. In: 9th IEEE/ACM International Workshop on Green and Sustainable Software, GREENS@ICSE \n2025, Ottawa, ON, Canada, April 29, 2025, pages 31\u201338. IEEE\nChae H, Kwon T, Moon S, Song Y, Kang D, Ong KT, Kwak B, Bae S, Hwang S, Yeo J (2024) Coffee-gym: \nAn environment for evaluating and improving natural language feedback on erroneous code. In Y.\u00a0Al-\nOnaizan, M.\u00a0Bansal, and Y.\u00a0Chen, editors, Proceedings of the 2024 Conference on Empirical Meth\u00ad\nods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages \n22503\u201322524. Association for Computational Linguistics\nChen J, Shang W, Shihab E (2022) Perfjit: Test-level just-in-time prediction for performance regression"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_056",
    "source_id": "PerformanceAnalysis2026",
    "text": " KT, Kwak B, Bae S, Hwang S, Yeo J (2024) Coffee-gym: \nAn environment for evaluating and improving natural language feedback on erroneous code. In Y.\u00a0Al-\nOnaizan, M.\u00a0Bansal, and Y.\u00a0Chen, editors, Proceedings of the 2024 Conference on Empirical Meth\u00ad\nods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages \n22503\u201322524. Association for Computational Linguistics\nChen J, Shang W, Shihab E (2022) Perfjit: Test-level just-in-time prediction for performance regression \nintroducing commits. IEEE Trans Software Eng 48(5):1529\u20131544\nChen J, Ding Z, Tang Y, Sayagh M, Li H, Adams B, Shang W (2023) Iopv: On inconsistent option perfor\u00ad\nmance variations. In: Proceedings of the 31st ACM Joint European Software Engineering Conference \nand Symposium on the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA, \nUSA, December 3-9, 2023, pages 845\u2013857. ACM\nChen M, Tworek J, Jun H, Yuan Q, Pinto H.P dO, Kaplan J, Edwards H, Burda Y, Joseph N, Brockman G et\u00a0al \n(2021) Evaluating large language models trained on code. arXiv:2107.03374\nChen T, Shang W, Jiang ZM, Hassan AE, Nasser MN, Flora P (2014) Detecting performance anti-patterns for \napplications developed using object-relational mapping. In: 36th International Conference on Software \nEngineering, ICSE \u201914, Hyderabad, India - May 31 - June 07, 2014, pages 1001\u20131012. ACM\nClaude Code (2025) Claude code: The ai coding assistant. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200ba\u200bn\u200bt\u200bh\u200br\u200bo\u200bp\u200bi\u200bc\u200b.\u200bc\u200bo\u200bm\u200b/\u200be\u200bn\u200b/\u200bd\u200bo\u200bc\u200bs\u200b/\u200ba\u200bg\u200be\u200bn\u200bt\u200bs\u200b-\u200ba\u200bn\u200bd\u200b-\u200bt\u200bo\u200b\no\u200bl\u200bs\u200b/\u200bc\u200bl\u200ba\u200bu\u200bd\u200be\u200b-\u200bc\u200bo\u200bd\u200be\u200b/\u200bo\u200bv\u200be\u200br\u200bv\u200bi\u200be\u200bw. Accessed:2025-02-27\nCoignion T, Quinton C, Rouvoy R (2024) A performance study of llm-generated code on leetcode. In: Pro\u00ad\nceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, \npages 79\u201389\n1\u202f3\nPage 47 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nCursaru V, Duits L, Milligan J, Ural D, Sanchez BR, Stoico"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_057",
    "source_id": "PerformanceAnalysis2026",
    "text": "e\u200b-\u200bc\u200bo\u200bd\u200be\u200b/\u200bo\u200bv\u200be\u200br\u200bv\u200bi\u200be\u200bw. Accessed:2025-02-27\nCoignion T, Quinton C, Rouvoy R (2024) A performance study of llm-generated code on leetcode. In: Pro\u00ad\nceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering, \npages 79\u201389\n1\u202f3\nPage 47 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nCursaru V, Duits L, Milligan J, Ural D, Sanchez BR, Stoico V, Malavolta I (2024) A controlled experiment on \nthe energy efficiency of the source code generated by code llama. CoRR, abs/2405.03616\nCursor (2025) Cursor - The AI Code Editor. https://www.cursor.com/en. Accessed: 2025-02-27\nDing Z, Chen J, Shang W (2020) Towards the use of the readily available tests from the release pipeline as \nperformance tests: are we there yet? In: ICSE \u201920: 42nd International Conference on Software Engi\u00ad\nneering, Seoul, South Korea, 27 June - 19 July, 2020, pages 1435\u20131446. ACM\nDrain D, Hu C, Wu C, Breslav M, Sundaresan N (2021) Generating code with the help of retrieved template \nfunctions and stack overflow answers. CoRR, abs/2104.05310\nDu M, Luu AT, Ji B, Liu Q (2024) Ng S (2024) Mercury: A code efficiency benchmark for code large lan\u00ad\nguage models. In: Globersons A, Mackey L, Belgrave D, Fan A, Paquet U, Tomczak JM, Zhang C (eds) \nAdvances in Neural Information Processing Systems 38: Annual Conference on Neural Information \nProcessing Systems 2024, NeurIPS 2024. BC, Canada, December, Vancouver, pp 10\u201315\nFeng Z, Guo D, Tang D, Duan N, Feng X, Gong M, Shou L, Qin B, Liu T, Jiang D, Zhou M (2020) Codebert: \nA pre-trained model for programming and natural languages. In: Findings of the Association for Com\u00ad\nputational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of \nFindings of ACL, pages 1536\u20131547. Association for Computational Linguistics\nFu Y, Liang P, Li Z, Shahin M, Yu J, Chen J (2025) Security weaknesses of copilot-generated code in github \nprojects: An empirical study. ACM Transactions on Software Engineering and Methodology\nGao S, Gao C, Gu W, Lyu MR (2024) Search-based llms for code optimization. CoRR, abs/2408.12159\nGarg S, Moghaddam RZ, Sundaresan N (2023) Rapgen: An approach for fixing code inefficiencies in zero-\nshot."
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_058",
    "source_id": "PerformanceAnalysis2026",
    "text": "2020, volume EMNLP 2020 of \nFindings of ACL, pages 1536\u20131547. Association for Computational Linguistics\nFu Y, Liang P, Li Z, Shahin M, Yu J, Chen J (2025) Security weaknesses of copilot-generated code in github \nprojects: An empirical study. ACM Transactions on Software Engineering and Methodology\nGao S, Gao C, Gu W, Lyu MR (2024) Search-based llms for code optimization. CoRR, abs/2408.12159\nGarg S, Moghaddam RZ, Sundaresan N (2023) Rapgen: An approach for fixing code inefficiencies in zero-\nshot. arXiv:2306.17077\nGitHub Docs (2025a) Asking GitHub copilot questions in your IDE. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200be\u200bn\u200b/\u200bc\u200bo\u200bp\u200bi\u200bl\u200bo\u200bt\u200b/\u200bh\u200bo\u200b\nw\u200b-\u200bt\u200bo\u200bs\u200b/\u200bu\u200bs\u200be\u200b-\u200bc\u200bh\u200ba\u200bt\u200b/\u200bu\u200bs\u200be\u200b-\u200bc\u200bh\u200ba\u200bt\u200b-\u200bi\u200bn\u200b-\u200bi\u200bd\u200be. Accessed: 2025-09-01\nGitHub Docs (2025b) Github language support. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200be\u200bn\u200b/\u200bg\u200be\u200bt\u200b-\u200bs\u200bt\u200ba\u200br\u200bt\u200be\u200bd\u200b/\u200bl\u200be\u200ba\u200br\u200bn\u200bi\u200bn\u200bg\u200b-\u200ba\u200bb\u200bo\u200bu\u200bt\u200b-\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b\n/\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b-\u200bl\u200ba\u200bn\u200bg\u200bu\u200ba\u200bg\u200be\u200b-\u200bs\u200bu\u200bp\u200bp\u200bo\u200br\u200bt. Accessed: 2025-02-25\nGitHub, Inc. (2025) Quickstart for GitHub Copilot. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200be\u200bn\u200b/\u200bc\u200bo\u200bp\u200bi\u200bl\u200bo\u200bt\u200b/\u200bg\u200be\u200bt\u200b-\u200bs\u200bt\u200ba\u200br\u200bt\u200be\u200bd\u200b/\u200bq\u200bu\u200bi\u200bc\u200bk\u200bs\u200bt\u200ba\u200br\u200bt\u200b\n?\u200bt\u200bo\u200bo\u200bl\u200b=\u200bv\u200bs\u200bc\u200bo\u200bd\u200be. Accessed: 2025-09-01\nGuo D, Tang D, Duan N, Zhou M, Yin J (2019) Coupling"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_059",
    "source_id": "PerformanceAnalysis2026",
    "text": "\u200bs\u200b.\u200bg\u200bi\u200bt\u200bh\u200bu\u200bb\u200b.\u200bc\u200bo\u200bm\u200b/\u200be\u200bn\u200b/\u200bc\u200bo\u200bp\u200bi\u200bl\u200bo\u200bt\u200b/\u200bg\u200be\u200bt\u200b-\u200bs\u200bt\u200ba\u200br\u200bt\u200be\u200bd\u200b/\u200bq\u200bu\u200bi\u200bc\u200bk\u200bs\u200bt\u200ba\u200br\u200bt\u200b\n?\u200bt\u200bo\u200bo\u200bl\u200b=\u200bv\u200bs\u200bc\u200bo\u200bd\u200be. Accessed: 2025-09-01\nGuo D, Tang D, Duan N, Zhou M, Yin J (2019) Coupling retrieval and meta-learning for context-dependent \nsemantic parsing. In: Proceedings of the 57th Conference of the Association for Computational Lin\u00ad\nguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 855\u2013866. \nAssociation for Computational Linguistics\nGuo D, Ren S, Lu S, Feng Z, Tang D, Liu S, Zhou L, Duan N, Svyatkovskiy A, Fu S, Tufano M, Deng SK, \nClement CB, Drain D, Sundaresan N, Yin J, Jiang D, Zhou M (2021) Graphcodebert: Pre-training code \nrepresentations with data flow. In: 9th International Conference on Learning Representations, ICLR \n2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net\nGuo D, Zhu Q, Yang D, Xie Z, Dong K, Zhang W, Chen G, Bi X, Wu Y, Li YK, Luo F, Xiong Y, Liang W \n(2024) Deepseek-coder: When the large language model meets programming - the rise of code intel\u00ad\nligence. CoRR, abs/2401.14196\nHao Y, Li G, Liu Y, Miao X, Zong H, Jiang S, Liu Y, Wei H (2022) Aixbench: A code generation benchmark \ndataset. arXiv:2206.13179\nHayati SA, Olivier R, Avvaru P, Yin P, Tomasic A, Neubig G (2018) Retrieval-based neural code genera\u00ad\ntion. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, \nBrussels, Belgium, October 31 - November 4, 2018, pages 925\u2013930. Association for Computational \nLinguistics\nHe P, Wang S, Chen T-H (2025) Codepromptzip: Code-specific prompt compression for retrieval-augmented \ngeneration in coding tasks with lms. arXiv:2502.14925\nHou W, Ji Z (2024) A systematic evaluation of large language models for generating programming code. \narXiv:2403.00894\nHuang D, Zeng G, Dai J, Luo M, Weng H, Qing Y, Cui H, Guo Z, Zhang JM (2024a) Effi-code: Unleashing \ncode efficiency in language models. CoRR, abs/2410."
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_060",
    "source_id": "PerformanceAnalysis2026",
    "text": " pages 925\u2013930. Association for Computational \nLinguistics\nHe P, Wang S, Chen T-H (2025) Codepromptzip: Code-specific prompt compression for retrieval-augmented \ngeneration in coding tasks with lms. arXiv:2502.14925\nHou W, Ji Z (2024) A systematic evaluation of large language models for generating programming code. \narXiv:2403.00894\nHuang D, Zeng G, Dai J, Luo M, Weng H, Qing Y, Cui H, Guo Z, Zhang JM (2024a) Effi-code: Unleashing \ncode efficiency in language models. CoRR, abs/2410.10209\nHuang D, Qing Y, Shang W, Cui H (2024) Zhang J (2024b) Effibench: Benchmarking the efficiency of auto\u00ad\nmatically generated code. In: Globersons A, Mackey L, Belgrave D, Fan A, Paquet U, Tomczak JM, \nZhang C (eds) Advances in Neural Information Processing Systems 38: Annual Conference on Neural \nInformation Processing Systems 2024, NeurIPS 2024. BC, Canada, December, Vancouver, pp 10\u201315\nHuang D, Dai J, Weng H, Wu P, Qing Y, Cui H, Guo Z (2024) Zhang J (2024c) Effilearner: Enhancing \nefficiency of generated code via self-optimization. In: Globersons A, Mackey L, Belgrave D, Fan A, \nPaquet U, Tomczak JM, Zhang C (eds) Advances in Neural Information Processing Systems 38: Annual \nConference on Neural Information Processing Systems 2024, NeurIPS 2024. BC, Canada, December, \nVancouver, pp 10\u201315\nHuang Y, Li Y, Wu W, Zhang J, Lyu MR (2023) Do not give away my secrets: Uncovering the privacy issue \nof neural code completion tools. CoRR, abs/2309.07639\n1\u202f3\n   62 \n\u2003\nPage 48 of 52\nEmpirical Software Engineering           (2026) 31:62 \nIslam MA, Jonnala DV, Rekhi R, Pokharel P, Cilamkoti S, Imran A, Kosar T, Turkkan BO (2025) Evaluating \nthe energy-efficiency of the code generated by llms. CoRR, abs/2505.20324\nIyer S, Konstas I, Cheung A, Zettlemoyer L (2018) Mapping language to code in programmatic context. In: \nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, \nBelgium, October 31 - November 4, 2018, pages 1643\u20131652. Association for Computational Linguistics\nJangali M, Tang Y, Alexandersson N, Leitner P, Yang J, Shang W (2023) Automated generation and evalua\u00ad\ntion of JMH microbenchmark suites from unit tests. IEEE Trans Software Eng 49(4):1704\u20131725\nJetBrains s.r.o"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_061",
    "source_id": "PerformanceAnalysis2026",
    "text": "\nIyer S, Konstas I, Cheung A, Zettlemoyer L (2018) Mapping language to code in programmatic context. In: \nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, \nBelgium, October 31 - November 4, 2018, pages 1643\u20131652. Association for Computational Linguistics\nJangali M, Tang Y, Alexandersson N, Leitner P, Yang J, Shang W (2023) Automated generation and evalua\u00ad\ntion of JMH microbenchmark suites from unit tests. IEEE Trans Software Eng 49(4):1704\u20131725\nJetBrains s.r.o (2024) About Qodana. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bw\u200bw\u200bw\u200b.\u200bj\u200be\u200bt\u200bb\u200br\u200ba\u200bi\u200bn\u200bs\u200b.\u200bc\u200bo\u200bm\u200b/\u200bh\u200be\u200bl\u200bp\u200b/\u200bq\u200bo\u200bd\u200ba\u200bn\u200ba\u200b/\u200ba\u200bb\u200bo\u200bu\u200bt\u200b-\u200bq\u200bo\u200bd\u200ba\u200bn\u200ba\u200b.\u200bh\u200bt\u200bm\u200bl. Accessed: \n2024-12-30\nKhoury R, Avila AR, Brunelle J, Camara BM (2023) How secure is code generated by chatgpt? In: IEEE \nInternational Conference on Systems, Man, and Cybernetics, SMC 2023, Honolulu, Oahu, HI, USA, \nOctober 1-4, 2023, pages 2445\u20132451. IEEE\nLaaber C, Leitner P (2018) An evaluation of open-source software microbenchmark suites for continuous \nperformance assessment. In: Proceedings of the 15th International Conference on Mining Software \nRepositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018, pages 119\u2013130. ACM\nLertbanjongngam, S., Chinthanet, B., Ishio, T., Kula, R.\u00a0G., Leelaprute, P., Manaskasemsak, B., Rungsawang, \nA., and Matsumoto, K. (2022). An empirical evaluation of competitive programming AI: A case study of \nalphacode. In: 16th IEEE International Workshop on Software Clones, IWSC 2022, Limassol, Cyprus, \nOctober 2, 2022, pages 10\u201315. IEEE\nLi B, Sun Z, Huang T, Zhang H, Wan Y, Li G, Jin Z, Lyu C (2024a) Ircoco: Immediate rewards-guided deep \nreinforcement learning for code completion. Proc ACM Softw Eng 1(FSE):182\u2013203\nLi S, Cheng Y, Chen J, Xuan J, He S, Shang W (2024b) Assessing the performance of ai-generated code: A \ncase study on github copilot. In: 35th IEEE International Symposium on Software Reliability Engineer\u00ad\ning, ISSRE 2024, Tsukuba, Japan, October 28-31, 2024, pages "
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_062",
    "source_id": "PerformanceAnalysis2026",
    "text": "10\u201315. IEEE\nLi B, Sun Z, Huang T, Zhang H, Wan Y, Li G, Jin Z, Lyu C (2024a) Ircoco: Immediate rewards-guided deep \nreinforcement learning for code completion. Proc ACM Softw Eng 1(FSE):182\u2013203\nLi S, Cheng Y, Chen J, Xuan J, He S, Shang W (2024b) Assessing the performance of ai-generated code: A \ncase study on github copilot. In: 35th IEEE International Symposium on Software Reliability Engineer\u00ad\ning, ISSRE 2024, Tsukuba, Japan, October 28-31, 2024, pages 216\u2013227. IEEE\nLi Y, Choi DH, Chung J, Kushman N, Schrittwieser J, Leblond R, Eccles T, Keeling J, Gimeno F, Lago AD, \nHubert T, Choy P, de\u00a0Masson\u00a0d\u2019Autume C, Babuschkin I, Chen X, Huang P, Welbl J, Gowal S, Cherepa\u00ad\nnov A, Molloy J, Mankowitz DJ, Robson ES, Kohli P, de\u00a0Freitas N, Kavukcuoglu K, Vinyals O (2022) \nCompetition-level code generation with alphacode. CoRR, abs/2203.07814\nLing W, Blunsom P, Grefenstette E, Hermann KM, Kocisk\u00fd T, Wang F, Senior AW (2016) Latent predictor \nnetworks for code generation. In: Proceedings of the 54th Annual Meeting of the Association for Com\u00ad\nputational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The \nAssociation for Computer Linguistics\nLiu J, Xie S, Wang J, Wei Y, Ding Y, Zhang L (2024a) Evaluating language models for efficient code genera\u00ad\ntion. CoRR, abs/2408.06450\nLiu,J. Xia CS, Wang Y, Zhang L (2024b) Is your code generated by chatgpt really correct? rigorous evaluation \nof large language models for code generation. Advances in Neural Information Processing Systems, 36\nLuo W, Keung JW, Yang B, Ye H, Le Goues C, Bissyand\u00e9 TF, Tian H, Le B (2024a) When fine-tuning \nllms meets data privacy: An empirical study of federated learning in llm-based program repair. CoRR, \nabs/2412.01072\nLuo Z, Xu C, Zhao P, Sun Q, Geng X, Hu W, Tao C, Ma J, Lin Q, Jiang D (2024b) Wizardcoder: Empowering \ncode large language models with evol-instruct. In: The Twelfth International Conference on Learning \nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net\nMayer L, Heumann C, A\u00dfenmacher M (2024) Can opensource beat chatgpt? - A comparative study of large \nlanguage models for text"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_063",
    "source_id": "PerformanceAnalysis2026",
    "text": " empirical study of federated learning in llm-based program repair. CoRR, \nabs/2412.01072\nLuo Z, Xu C, Zhao P, Sun Q, Geng X, Hu W, Tao C, Ma J, Lin Q, Jiang D (2024b) Wizardcoder: Empowering \ncode large language models with evol-instruct. In: The Twelfth International Conference on Learning \nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net\nMayer L, Heumann C, A\u00dfenmacher M (2024) Can opensource beat chatgpt? - A comparative study of large \nlanguage models for text-to-code generation. CoRR, abs/2409.04164\nMcHugh ML (2012) Interrater reliability: the kappa statistic. Biochemia medica 22(3):276\u2013282\nMicrosoft (2025) Visual Studio Code - Code Editing. https://code.visualstudio.com/. Accessed: 2025-09-01\nNguyen N, Nadi S (2022) An empirical evaluation of github copilot\u2019s code suggestions. In: 19th IEEE/ACM \nInternational Conference on Mining Software Repositories, MSR 2022, Pittsburgh, PA, USA, May \n23-24, 2022, pages 1\u20135. ACM\nNijkamp E, Pang B, Hayashi H, Tu L, Wang H, Zhou Y, Savarese S, Xiong C (2023) Codegen: An open large \nlanguage model for code with multi-turn program synthesis. In: The Eleventh International Conference \non Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net\nNiu L, Mirza MS, Maradni Z, P\u00f6pper C (2023) Codexleaks: Privacy leaks from code generation language \nmodels in github copilot. In: 32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, \nCA, USA, August 9-11, 2023, pages 2133\u20132150. USENIX Association\nParvez MR, Ahmad WU, Chakraborty S, Ray B, Chang K (2021) Retrieval augmented code generation and \nsummarization. In: Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual \nEvent / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 2719\u20132734. Association for \nComputational Linguistics\n1\u202f3\nPage 49 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nPearce H, Ahmad B, Tan B, Dolan-Gavitt B, Karri R (2022) Asleep at the keyboard? assessing the security of \ngithub copilot\u2019s code contributions. In: 43rd IEEE Symposium on Security and Privacy, SP 2022, San \nFrancisco, CA, USA, May 22-26, 2022, pages 754\u2013768. IEEE\nPeng Y, Gotmare AD, Lyu MR, Xiong C, Savarese"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_064",
    "source_id": "PerformanceAnalysis2026",
    "text": "\u20132734. Association for \nComputational Linguistics\n1\u202f3\nPage 49 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nPearce H, Ahmad B, Tan B, Dolan-Gavitt B, Karri R (2022) Asleep at the keyboard? assessing the security of \ngithub copilot\u2019s code contributions. In: 43rd IEEE Symposium on Security and Privacy, SP 2022, San \nFrancisco, CA, USA, May 22-26, 2022, pages 754\u2013768. IEEE\nPeng Y, Gotmare AD, Lyu MR, Xiong C, Savarese S, Sahoo D (2025) Perfcodegen: Improving performance \nof LLM generated code with execution feedback. In: IEEE/ACM Second International Conference \non AI Foundation Models and Software Engineering, Forge@ICSE 2025, Ottawa, ON, Canada, April \n27-28, 2025, pages 1\u201313. IEEE\nPMD (2024) PMD: An extensible cross-language static code analyzer. https://pmd.github.io/. Accessed: \n2024-04-25\nPsutil Documentation Team (2025). Psutil documentation \u2014 psutil 7.1.0 documentation. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bp\u200bs\u200bu\u200bt\u200bi\u200bl\u200b.\u200br\u200be\u200ba\u200bd\u200bt\u200bh\u200b\ne\u200bd\u200bo\u200bc\u200bs\u200b.\u200bi\u200bo\u200b/\u200be\u200bn\u200b/\u200bl\u200ba\u200bt\u200be\u200bs\u200bt\u200b/\u200b. Accessed: 2025-09-01\nPython Software Foundation (2025a) psutil 7.0.0. https://pypi.org/project/psutil/. Accessed: 2025-09-01\nPython Software Foundation (2025b) The Python Profilers. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bp\u200by\u200bt\u200bh\u200bo\u200bn\u200b.\u200bo\u200br\u200bg\u200b/\u200b3\u200b/\u200bl\u200bi\u200bb\u200br\u200ba\u200br\u200by\u200b/\u200bp\u200br\u200bo\u200bf\u200bi\u200bl\u200be\u200b.\u200bh\u200bt\u200bm\u200bl. \nAccessed: 2025-09-01\nPython Software Foundation (2025c) tracemalloc \u2013 Trace memory allocations. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bp\u200by\u200bt\u200bh\u200bo\u200bn\u200b.\u200bo\u200br\u200bg\u200b/\u200b3\u200b/\u200bl\u200bi\u200bb\u200br\u200b\na\u200br\u200by\u200b/\u200bt\u200br\u200ba\u200bc\u200be\u200bm\u200ba\u200bl\u200bl\u200bo\u200bc\u200b.\u200bh\u200bt\u200bm\u200bl. Accessed: 2025-09-01\nRabinovich M, Stern M, Klein D (2017) Abstract syntax networks for code generation and semantic parsing. \nIn: Proceedings of the 55th Annual Meeting of the Association for Computational Lingu"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_065",
    "source_id": "PerformanceAnalysis2026",
    "text": "t\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bd\u200bo\u200bc\u200bs\u200b.\u200bp\u200by\u200bt\u200bh\u200bo\u200bn\u200b.\u200bo\u200br\u200bg\u200b/\u200b3\u200b/\u200bl\u200bi\u200bb\u200br\u200b\na\u200br\u200by\u200b/\u200bt\u200br\u200ba\u200bc\u200be\u200bm\u200ba\u200bl\u200bl\u200bo\u200bc\u200b.\u200bh\u200bt\u200bm\u200bl. Accessed: 2025-09-01\nRabinovich M, Stern M, Klein D (2017) Abstract syntax networks for code generation and semantic parsing. \nIn: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL \n2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1139\u20131149. Association \nfor Computational Linguistics\nReichelt DG, K\u00fchne S, Hasselbring W (2019) On the validity of performance antipatterns at code level. \nSoftwaretechnik-Trends 39(4):32\u201334\nReichelt DG, K\u00fchne S, Hasselbring W (2019b) Peass: A tool for identifying performance changes at code \nlevel. In: 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019, \nSan Diego, CA, USA, November 11-15, 2019, pages 1146\u20131149. IEEE\nRozi\u00e8re B, Gehring J, Gloeckle F, Sootla S, Gat I, Tan XE, Adi Y, Liu J, Remez T, Rapin J, Kozhevnikov A, \nEvtimov I, Bitton J, Bhatt M, Canton-Ferrer C, Grattafiori A, Xiong W, D\u00e9fossez A, Copet J, Azhar F, \nTouvron H, Martin L, Usunier N, Scialom T, Synnaeve G (2023) Code llama: Open foundation models \nfor code. CoRR, abs/2308.12950\nSeo M, Baek J, Hwang SJ (2024) Rethinking code refinement: Learning to judge code efficiency. In: Al-\nOnaizan Y, Bansal M, Chen Y (eds) Findings of the Association for Computational Linguistics: EMNLP \n2024, Miami, Florida, USA, November 12\u201316, 2024. Association for Computational Linguistics, pp \n11045\u201311056\nShi J, Yang Z, Lo D (2025) Efficient and green large language models for software engineering: Literature \nreview, vision, and the road ahead. ACM Trans. Softw. Eng. Methodol. 34(5):1\u201322\nShypula A, Madaan A, Zeng Y, Alon U, Gardner JR, Yang Y, Hashemi M, Neubig G, Ranganathan P, Bastani \nO, Yazdanbakhsh A (2024) Learning performance-improving code edits. In: The Twelfth International \nConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 202"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_066",
    "source_id": "PerformanceAnalysis2026",
    "text": "\u201311056\nShi J, Yang Z, Lo D (2025) Efficient and green large language models for software engineering: Literature \nreview, vision, and the road ahead. ACM Trans. Softw. Eng. Methodol. 34(5):1\u201322\nShypula A, Madaan A, Zeng Y, Alon U, Gardner JR, Yang Y, Hashemi M, Neubig G, Ranganathan P, Bastani \nO, Yazdanbakhsh A (2024) Learning performance-improving code edits. In: The Twelfth International \nConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.\nnet\nSikand S, Mehra R, Sharma VS, Kaulgud V, Podder S, Burden AP (2024) Do generative AI tools ensure \ngreen code? an investigative study. In: Proceedings of the 2nd International Workshop on Responsible \nAI Engineering, RAIE 2024, Lisbon, Portugal, 16 April 2024, pages 52\u201355. ACM\nSobania D, Briesch M, Rothlauf F (2022) Choose your programming copilot: a comparison of the program \nsynthesis performance of github copilot and genetic programming. In: GECCO \u201922: Genetic and Evo\u00ad\nlutionary Computation Conference, Boston, Massachusetts, USA, July 9 - 13, 2022, pages 1019\u20131027. \nACM\nSonarQube (2024) SonarQube: A code quality management platform. https://www.sonarqube.org/. Accessed: \n2024-04-28\nSpotBugs (2024) SpotBugs: Find bugs in Java Programs. https://spotbugs.github.io/. Accessed: 2024-04-25\nSun Z, Zhu Q, Xiong Y, Sun Y, Mou L, Zhang L (2020) Treegen: A tree-based transformer architecture for \ncode generation. In: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The \nThirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth \nAAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, \nUSA, February 7-12, 2020, pages 8984\u20138991. AAAI Press\nTanenbaum AS (2015) Modern Operating Systems. Pearson, Boston, fourth edition edition\nvan Dinten I, Derakhshanfar P, Panichella A, Zaidman A (2024) The slow and the furious? performance \nantipattern detection in cyber-physical systems. J Syst Softw 210:111904\nVartziotis T, Dellatolas I, Dasoulas G, Schmidt M, Schneider F, Hoffmann T, Kotsopoulos S, Keckeisen M \n(2024) Learn to code sustainably: An empirical study on green code generation. In: LLM4CODE@\nICSE, pages 30\u201337\n1\u202f3\n   62 \n\u2003\nPage 50 of 52\nEmpirical Software Engineering           (2026"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_067",
    "source_id": "PerformanceAnalysis2026",
    "text": "inten I, Derakhshanfar P, Panichella A, Zaidman A (2024) The slow and the furious? performance \nantipattern detection in cyber-physical systems. J Syst Softw 210:111904\nVartziotis T, Dellatolas I, Dasoulas G, Schmidt M, Schneider F, Hoffmann T, Kotsopoulos S, Keckeisen M \n(2024) Learn to code sustainably: An empirical study on green code generation. In: LLM4CODE@\nICSE, pages 30\u201337\n1\u202f3\n   62 \n\u2003\nPage 50 of 52\nEmpirical Software Engineering           (2026) 31:62 \nVenkatesh APS, Sabu S, Mir AM, Reis S, Bodden E (2024) The emergence of large language models in static \nanalysis: A first look through micro-benchmarks. In: D.\u00a0Lo, X.\u00a0Xia, M.\u00a0D. Penta, and X.\u00a0Hu, editors, \nProceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Soft\u00ad\nware Engineering, FORGE 2024, Lisbon, Portugal, 14 April 2024, pages 35\u201339. ACM\nVisual Studio Code Docs (2025) Github copilot in vs code settings reference. \u200bh\u200bt\u200bt\u200bp\u200bs\u200b:\u200b/\u200b/\u200bc\u200bo\u200bd\u200be\u200b.\u200bv\u200bi\u200bs\u200bu\u200ba\u200bl\u200bs\u200bt\u200bu\u200bd\u200bi\u200bo\u200b.\u200bc\u200bo\u200bm\u200b/\u200b\nd\u200bo\u200bc\u200bs\u200b/\u200bc\u200bo\u200bp\u200bi\u200bl\u200bo\u200bt\u200b/\u200br\u200be\u200bf\u200be\u200br\u200be\u200bn\u200bc\u200be\u200b/\u200bc\u200bo\u200bp\u200bi\u200bl\u200bo\u200bt\u200b-\u200bs\u200be\u200bt\u200bt\u200bi\u200bn\u200bg\u200bs. Accessed: 2025-09-01\nWaghjale S, Veerendranath V, Wang Z, Fried D (2024) ECCO: can we improve model-generated code effi\u00ad\nciency without sacrificing functional correctness? In: Y.\u00a0Al-Onaizan, M.\u00a0Bansal, and Y.\u00a0Chen, editors, \nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP \n2024, Miami, FL, USA, November 12-16, 2024, pages 15362\u201315376. Association for Computational \nLinguistics\nWang Y, Wang W, Joty SR, Hoi SCH (2021) Codet5: Identifier-aware unified pre-trained encoder-decoder \nmodels for code understanding and generation. In: Proceedings of the 2021 Conference on Empiri\u00ad\ncal Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican \nRepublic, 7-11 November, 2021, pages 8696\u20138708. Association for Computational Linguistics\nWang Z, Zhou Z, Song D, Huang Y, Chen S, Ma L, Zhang T (2025"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_068",
    "source_id": "PerformanceAnalysis2026",
    "text": "-16, 2024, pages 15362\u201315376. Association for Computational \nLinguistics\nWang Y, Wang W, Joty SR, Hoi SCH (2021) Codet5: Identifier-aware unified pre-trained encoder-decoder \nmodels for code understanding and generation. In: Proceedings of the 2021 Conference on Empiri\u00ad\ncal Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican \nRepublic, 7-11 November, 2021, pages 8696\u20138708. Association for Computational Linguistics\nWang Z, Zhou Z, Song D, Huang Y, Chen S, Ma L, Zhang T (2025) Towards understanding the characteristics \nof code generation errors made by large language models\nWei B, Li G, Xia X, Fu Z, Jin Z (2019) Code generation as a dual task of code summarization. In: \nAdvances in Neural Information Processing Systems 32: Annual Conference on Neural Information \nProcessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages \n6559\u20136569\nWei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi EH, Le QV, Zhou D (2022) Chain-of-thought \nprompting elicits reasoning in large language models. In S.\u00a0Koyejo, S.\u00a0Mohamed, A.\u00a0Agarwal, D.\u00a0Bel\u00ad\ngrave, K.\u00a0Cho, and A.\u00a0Oh, editors, Advances in Neural Information Processing Systems 35: Annual \nConference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, \nNovember 28 - December 9, 2022\nWei Y, Cassano F, Liu J, Ding Y, Jain N, Mueller Z, de Vries H, von Werra L, Guha A (2024) Zhang L (2024) \nSelfcodealign: Self-alignment for code generation. In: Globersons A, Mackey L, Belgrave D, Fan A, \nPaquet U, Tomczak JM, Zhang C (eds) Advances in Neural Information Processing Systems 38: Annual \nConference on Neural Information Processing Systems 2024, NeurIPS 2024. BC, Canada, December, \nVancouver, pp 10\u201315\nWeng L, Hu Y, Huang P, Nieh J, Yang J (2023) Effective performance issue diagnosis with value-assisted \ncost profiling. In: Proceedings of the Eighteenth European Conference on Computer Systems, EuroSys \n2023, Rome, Italy, May 8-12, 2023, pages 1\u201317. ACM\nWhite J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-Smith J, Schmidt DC (2023) A \nprompt pattern catalog to enhance prompt engineering with chatgpt. CoRR, abs/2302.11382\nYan D, Xu G, Rountev A (2012) Uncovering performance problems in java applications with reference \npropagation profiling. In M.\u00a0Glinz, G.\u00a0C."
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_069",
    "source_id": "PerformanceAnalysis2026",
    "text": "isted \ncost profiling. In: Proceedings of the Eighteenth European Conference on Computer Systems, EuroSys \n2023, Rome, Italy, May 8-12, 2023, pages 1\u201317. ACM\nWhite J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-Smith J, Schmidt DC (2023) A \nprompt pattern catalog to enhance prompt engineering with chatgpt. CoRR, abs/2302.11382\nYan D, Xu G, Rountev A (2012) Uncovering performance problems in java applications with reference \npropagation profiling. In M.\u00a0Glinz, G.\u00a0C. Murphy, and M.\u00a0Pezz\u00e8, editors, 34th International Confer\u00ad\nence on Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, pages 134\u2013144. IEEE \nComputer Society\nYang C, Kang HJ, Shi J, Lo D (2024) Acecode: A reinforcement learning framework for aligning code effi\u00ad\nciency and correctness in code language models. CoRR, abs/2412.17264\nYang Z, Zhao Z, Wang C, Shi J, Kim D, Han D, Lo D (2023) Gotcha! this model uses my code! evaluating \nmembership leakage risks in code models. CoRR, abs/2310.01166\nYetistiren B, Ozsoy I, Tuzun E (2022) Assessing the quality of github copilot\u2019s code generation. In: Proceed\u00ad\nings of the 18th International Conference on Predictive Models and Data Analytics in Software Engi\u00ad\nneering, PROMISE 2022, Singapore, Singapore, 17 November 2022, pages 62\u201371. ACM\nYetistiren B, \u00d6zsoy I, Ayerdem M, T\u00fcz\u00fcn E (2023) Evaluating the code quality of ai-assisted code \ngeneration tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. CoRR, \nabs/2304.10778\nYin P, Neubig G (2017) A syntactic neural model for general-purpose code generation. In: Proceedings of \nthe 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, \nCanada, July 30 - August 4, Volume 1: Long Papers, pages 440\u2013450. Association for Computational \nLinguistics\nZan D, Chen B, Zhang F, Lu D, Wu B, Guan B, Wang Y, Lou J-G (2022) Large language models meet \nnl2code: A survey. preprint arXiv:2212.09420\nZeng Y, Chen J, Shang W, Chen T-HP (2019) Studying the characteristics of logging practices in mobile apps: \na case study on F-Droid. Empir Softw Eng 24(6):3394\u20133434\n1\u202f3\nPage 51 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nZhang B, Du T, Tong J, Zhang X, Chow"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_070",
    "source_id": "PerformanceAnalysis2026",
    "text": " B, Zhang F, Lu D, Wu B, Guan B, Wang Y, Lou J-G (2022) Large language models meet \nnl2code: A survey. preprint arXiv:2212.09420\nZeng Y, Chen J, Shang W, Chen T-HP (2019) Studying the characteristics of logging practices in mobile apps: \na case study on F-Droid. Empir Softw Eng 24(6):3394\u20133434\n1\u202f3\nPage 51 of 52\u2003\n   62 \nEmpirical Software Engineering           (2026) 31:62 \nZhang B, Du T, Tong J, Zhang X, Chow K, Cheng S, Wang X, Yin J (2024) Seccoder: Towards generalizable \nand robust secure code generation. In: Y.\u00a0Al-Onaizan, M.\u00a0Bansal, and Y.\u00a0Chen, editors, Proceedings \nof the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, \nMiami, FL, USA, November 12-16, 2024, pages 14557\u201314571. Association for Computational \nLinguistics\nZheng Z, Ning K, Wang Y, Zhang J, Zheng D, Ye M, Chen J (2023) A survey of large language models for \ncode: Evolution, benchmarking, and future trends. arXiv:2311.10372\nZhou S, Alon U, Xu FF, Jiang Z, Neubig G (2023) Docprompting: Generating code by retrieving the docs. \nIn: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, \nMay 1-5, 2023. OpenReview.net\nPublisher's Note\u2002 Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nSpringer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a \npublishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manu\u00ad\nscript version of this article is solely governed by the terms of such publishing agreement and applicable law.\nAuthors and Affiliations\nShuang\u00a0Li1\u00a0\u00b7 Yuntao\u00a0Cheng1\u00a0\u00b7 Jinfu\u00a0Chen1,2\n\u00a0\u00b7 Jifeng\u00a0Xuan1\u00a0\u00b7 Sen\u00a0He3\u00a0\u00b7 Weiyi\u00a0Shang4\n\t\n Jinfu Chen\njinfuchen@whu.edu.cn\nShuang Li\nshuangli.cs@whu.edu.cn\nYuntao Cheng\ncytzzz@whu.edu.cn\nJifeng Xuan\njxuan@whu.edu.cn\nSen He\nsenhe@arizona.edu\nWeiyi Shang\nwshang@uwaterloo.ca\n1\t\nSchool of Computer Science, Wuhan University, Wuhan, China\n2\t\nSchool of Computer Science & Key Laboratory of Intelligent Sensing System and Security \n(Ministry of Education), Wuhan University & Hubei University, Wuhan, China\n3\t\nDepartment of Electrical and Computer Engineering, University of Arizona, Tucson, Arizona, \nUSA\n4"
  },
  {
    "chunk_id": "PerformanceAnalysis2026_chunk_071",
    "source_id": "PerformanceAnalysis2026",
    "text": "uchen@whu.edu.cn\nShuang Li\nshuangli.cs@whu.edu.cn\nYuntao Cheng\ncytzzz@whu.edu.cn\nJifeng Xuan\njxuan@whu.edu.cn\nSen He\nsenhe@arizona.edu\nWeiyi Shang\nwshang@uwaterloo.ca\n1\t\nSchool of Computer Science, Wuhan University, Wuhan, China\n2\t\nSchool of Computer Science & Key Laboratory of Intelligent Sensing System and Security \n(Ministry of Education), Wuhan University & Hubei University, Wuhan, China\n3\t\nDepartment of Electrical and Computer Engineering, University of Arizona, Tucson, Arizona, \nUSA\n4\t\nDepartment of Electrical and Computer Engineering, University of Waterloo, Waterloo, \nOntario, Canada\n1\u202f3\n   62 \n\u2003\nPage 52 of 52\n"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_001",
    "source_id": "NoviceProgramming2024",
    "text": "Citation: Zviel-Girshin, R. The Good\nand Bad of AI Tools in Novice\nProgramming Education. Educ. Sci.\n2024, 14, 1089. https://doi.org/\n10.3390/educsci14101089\nAcademic Editors: Julie Delello and\nRochell McWhorter\nReceived: 26 August 2024\nRevised: 29 September 2024\nAccepted: 30 September 2024\nPublished: 6 October 2024\nCopyright:\n\u00a9 2024 by the author.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\neducation \nsciences\nArticle\nThe Good and Bad of AI Tools in Novice Programming Education\nRina Zviel-Girshin\nThe Center for Research in Technological and Engineering Education, Faculty of Engineering,\nRuppin Academic Center, Kfar Monash 4025000, Israel; rinazg@ruppin.ac.il\nAbstract: As AI coding tools become more prevalent in programming, it is essential to understand\nhow they influence programming education. This study, conducted in a first-semester Introduction\nto Programming course, aimed to determine the positive and negative effects of these tools on\nstudents\u2019 learning experiences and their ability to develop essential programming skills. Using a\nmixed-methods approach, we collected data from 73 teams of engineering students over a 12-week\nperiod. Students completed surveys and reported on their AI tool usage. We analyzed this data\nquantitatively to identify trends in tool familiarity, usage, and student satisfaction. Additionally,\nqualitative analysis of student reports provided insights into the specific ways AI tools were used\nand their perceived benefits and drawbacks. The findings revealed a significant increase in AI tool\nfamiliarity (from 28% to 100%) and usage among students. Students\u2019 satisfaction with AI tools\nimproved over time. The most prevalent tasks for which novice programmers used AI tools included\ncreating comments (91.7%), identifying and correcting bugs (80.2%), and seeking information (68.5%),\nwhile other tasks were less common. While these tools offered benefits like assisting in learning and\nenhancing real-world relevance, they also raised concerns about cheating, over-reliance on AI tools,\nand a limited understanding of core programming concepts.\nKeywords: artificial intelligence (AI); AI in education; programing education; introduction to\nprogramming; AI coding tools\n1. Introduction\nThe integration of Artificial Intelligence (AI) and AI-driven coding tools in program-\nming education has opened new avenues for enhancing both the teaching and learning\nexperience. These tools, while offering significant benefits, can also be a double-edged\nsword. Our study raises important questions about the AI tools\u2019 long-term impact on\nstudents\u2019 understanding and proficiency. The potential of AI tools to transform education\nis undeniable. They can automate routine coding tasks, provide instant feedback, improve\ndebugging and troubleshooting, and offer personalized, student-centered learning experi-\nences. However, their use also introduces challenges, particularly regarding the balance\nbetween automation and preserving essential programming skills [1\u20135].\nAI in education is rapidly emerging as a key area of focus in educational technology,"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_002",
    "source_id": "NoviceProgramming2024",
    "text": " Artificial Intelligence (AI) and AI-driven coding tools in program-\nming education has opened new avenues for enhancing both the teaching and learning\nexperience. These tools, while offering significant benefits, can also be a double-edged\nsword. Our study raises important questions about the AI tools\u2019 long-term impact on\nstudents\u2019 understanding and proficiency. The potential of AI tools to transform education\nis undeniable. They can automate routine coding tasks, provide instant feedback, improve\ndebugging and troubleshooting, and offer personalized, student-centered learning experi-\nences. However, their use also introduces challenges, particularly regarding the balance\nbetween automation and preserving essential programming skills [1\u20135].\nAI in education is rapidly emerging as a key area of focus in educational technology, yet\nits broader pedagogical implications remain a subject of ongoing debate. The uncertainty\nabout how educators can effectively harness AI to create meaningful learning experiences\nin higher education is a major concern [3,6,7]. Educators are tasked with the crucial role of\npreparing students for a future where AI will be ubiquitous, not only in technology-driven\nindustries but across various sectors. To achieve this, educators must equip students with a\ncomprehensive skill set that includes traditional programming principles and knowledge\nin addition to the ability to leverage cutting-edge AI tools.\nTeaching students to use AI tools in programming education aligns with the current\ndemands of the high-tech industry and enhances their readiness for the workplace. By\ntraining students to effectively utilize AI tools, educators can improve their efficiency and\nproductivity, allowing them to focus on more complex and creative aspects of software\ndevelopment [8,9]. These tools can automate tedious and repetitive tasks, thereby freeing up\nEduc. Sci. 2024, 14, 1089. https://doi.org/10.3390/educsci14101089\nhttps://www.mdpi.com/journal/education\nEduc. Sci. 2024, 14, 1089\n2 of 17\ncognitive resources for higher-order problem-solving and innovation [10,11]. However, the\nintegration of AI tools in programming education must be approached with caution. Over-\nreliance on AI-driven solutions can result in a superficial understanding of programming\nconcepts, where students may become adept at using tools without fully grasping the\nunderlying logic and principles that drive software development [12,13].\nA balanced approach is essential, where AI tools are used to complement, rather than\nreplace, traditional coding practices and teaching methods [4,14]. This balance ensures that\nstudents develop a deep understanding of programming fundamentals while also learning\nto use AI to enhance their work. The primary objective of the current study is to investigate\nhow college students use these tools in a first-semester introductory programming course\nand to explore how educators can achieve this balance, ensuring that AI tools are used\nto augment learning without diminishing the importance of manual coding and critical\nthinking [15]. The study also seeks to address the potential risks associated with AI tools,\nsuch as the risk of fostering surface-level learning and the possible erosion of essential\nproblem-solving skills.\nUltimately, while AI tools offer significant potential for transforming programming\neducation, their integration must be thoughtfully managed to ensure that they contribute\npositively to the educational process. By striking the right balance, educators can harness\nthe power of AI to create more dynamic and effective learning environments that prepare\n"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_003",
    "source_id": "NoviceProgramming2024",
    "text": " of the current study is to investigate\nhow college students use these tools in a first-semester introductory programming course\nand to explore how educators can achieve this balance, ensuring that AI tools are used\nto augment learning without diminishing the importance of manual coding and critical\nthinking [15]. The study also seeks to address the potential risks associated with AI tools,\nsuch as the risk of fostering surface-level learning and the possible erosion of essential\nproblem-solving skills.\nUltimately, while AI tools offer significant potential for transforming programming\neducation, their integration must be thoughtfully managed to ensure that they contribute\npositively to the educational process. By striking the right balance, educators can harness\nthe power of AI to create more dynamic and effective learning environments that prepare\nstudents for the challenges of the future.\n1.1. Literature Review\nCode-generation tools powered by large language models can correctly and reliably\nsolve most of programming problems that are typical in introductory courses. It is therefore\nunsurprising that Finnie-Ansley et al. [10] showed that AI tools could solve programming\nassignments and exam problems that are typically given in CS1 (computer science) and\nCS2 more effectively than most students. The implications of this capability extend beyond\nmere performance; they challenge the traditional approaches to teaching introductory\nprogramming.\nLau and Guo [13], in their semi-structured interviews with introductory programming\ninstructors, revealed a range of perspectives on the integration of AI tools into education.\nWhile short-term concerns focused heavily on the potential for cheating, long-term per-\nspectives were more varied. Some educators expressed a desire to resist the use of AI tools\nin introductory programming courses, concerned that such tools might undermine the\ndevelopment of fundamental skills. On the other hand, others advocated for embracing AI,\nsuggesting that these tools could be integrated into the curriculum to enhance learning and\nbetter prepare students for a future where AI is ubiquitous in software development.\nThe debate about how to manage the influence of AI tools in education is further\ncomplicated by the issues of academic integrity and the assessment of student learning.\nBommasani et al. [6] emphasized the increasing difficulty educators face in determining\nthe extent of a student\u2019s individual contribution. The challenge of preventing ineffective\ncollaborations and detecting plagiarism has become more pronounced as AI tools become\nmore capable. This raises significant questions about the validity of traditional assess-\nment methods and the need for new approaches that can accurately measure a student\u2019s\nunderstanding and effort in an environment where AI-generated code is easily accessible.\nBecker et al. [1] argued in their position paper that AI-generated code presents both\nopportunities and challenges for students and educators alike. On the one hand, these tools\ncould serve as powerful aids, helping students overcome obstacles in their coding tasks\nand providing instant feedback that can enhance learning. On the other hand, they pose\nsignificant risks, such as the potential for students to become overly reliant on AI-generated\nsolutions, which may hinder the development of critical thinking and problem-solving\nskills that are central to programming education.\nDenny et al. [3] further argued that generative AI brings both challenges and opportu-\nnities to computing education, calling for updated pedagogical strategies that focus on new\nEduc. Sci. 2024, 14, 1089\n3 of 17\nskill sets. The ability of generative AI models to generate"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_004",
    "source_id": "NoviceProgramming2024",
    "text": " the one hand, these tools\ncould serve as powerful aids, helping students overcome obstacles in their coding tasks\nand providing instant feedback that can enhance learning. On the other hand, they pose\nsignificant risks, such as the potential for students to become overly reliant on AI-generated\nsolutions, which may hinder the development of critical thinking and problem-solving\nskills that are central to programming education.\nDenny et al. [3] further argued that generative AI brings both challenges and opportu-\nnities to computing education, calling for updated pedagogical strategies that focus on new\nEduc. Sci. 2024, 14, 1089\n3 of 17\nskill sets. The ability of generative AI models to generate solutions to problems typical of\nintroductory programming courses raises concerns about student overreliance and misuse.\nHowever, this also opens the door for reimagining the educational landscape, where the\nfocus shifts from rote learning and simple problem-solving to more complex tasks that\nrequire higher-order thinking and a deeper understanding of underlying concepts.\nIn a study conducted by Yilmaz and Yilmaz [11] on undergraduate students, it was\nfound that tools like ChatGPT could enhance students\u2019 programming self-efficacy and\nmotivation. The study suggested that ChatGPT could be a valuable tool for teaching\ncomputational thinking, providing students with a supportive environment where they\ncan explore programming concepts with instant feedback. This potential for AI tools to\npositively influence student learning underscores the need for educators to consider how\nthese tools can be integrated into the curriculum in a way that maximizes their benefits\nwhile mitigating their risks.\nSeveral studies showed that ChatGPT could provide users with explanations, exam-\nples, and guidance to help them understand complex concepts and technologies. It could be\nused to assist with debugging by analyzing data on the programming language, code struc-\nture, error messages, and code documentation, and even assist with code review [16\u201319].\nVukoji\u02c7ci\u00b4c\u2019s and Krsti\u00b4c\u2019s [20] study found that using ChatGPT as a programming\nassistant resulted in better outcomes than working without external assistance. Students\nwho used ChatGPT demonstrated improved coding proficiency, wrote higher-quality\nexplanations, and gained a deeper understanding of standard solution methods.\nZhai [21] examined ChatGPT from the perspective of educators, asserting that the AI\ntool holds significant potential for offering specific and personalized guidance to both teach-\ners and student teachers. The study found ChatGPT highly effective in automating essential\ntasks such as assessment development, grading, learning guidance, and recommending\nlearning materials. This automation can greatly reduce the time and effort required for\ncreating educational materials, enabling educators to dedicate more time to direct teaching\nand student engagement. However, to fully leverage these benefits, educators must possess\nthe necessary technological, content, and professional expertise to access and evaluate the\nquality and relevance of the materials provided by ChatGPT.\nMalinka et al. [22] studied the impact of ChatGPT on higher education, focusing\nprimarily on computer security-oriented specializations. The study demonstrated the\neffectiveness and usability of ChatGPT for handling programming assignments, completing\nexams, and writing term papers. Various levels of tool misuse were evaluated, ranging\nfrom using it as a consultant to simply copying its outputs. The benefits"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_005",
    "source_id": "NoviceProgramming2024",
    "text": " automation can greatly reduce the time and effort required for\ncreating educational materials, enabling educators to dedicate more time to direct teaching\nand student engagement. However, to fully leverage these benefits, educators must possess\nthe necessary technological, content, and professional expertise to access and evaluate the\nquality and relevance of the materials provided by ChatGPT.\nMalinka et al. [22] studied the impact of ChatGPT on higher education, focusing\nprimarily on computer security-oriented specializations. The study demonstrated the\neffectiveness and usability of ChatGPT for handling programming assignments, completing\nexams, and writing term papers. Various levels of tool misuse were evaluated, ranging\nfrom using it as a consultant to simply copying its outputs. The benefits of ChatGPT for\ncomputer science education were also highlighted.\nRudolph et al. [23] examined the relevance of ChatGPT in the context of higher\neducation, with a particular focus on its impact on assessment, learning, and teaching.\nFollowing an overview of ChatGPT\u2019s functionality and a summary of its strengths and\nlimitations, the study delved into the broader implications of this technology for the\nfuture of higher education. The potential of AI chatbots like ChatGPT to reshape learning,\nteaching, and assessment practices was explored. ChatGPT was positioned within the\ncurrent landscape of AI in education research, with its applications for students, educators,\nand educational systems discussed. The analysis highlighted both the opportunities and\nchallenges presented by AI integration in education. Practical recommendations were\nproposed for students, educators, and higher education institutions on how to effectively\nnavigate and leverage these emerging technologies.\nFinnie-Ansley et al. [10] likened the introduction of Codex into students\u2019 hands to\ngiving a power tool to an amateur\u2014a tool with the potential to either construct or destruct,\ndepending on how it is used. While the presence of AI tools in educational settings presents\nclear threats to student learning and academic integrity, it also offers fantastic opportuni-\nties to refactor existing curricula. By leveraging these tools appropriately, educators can\ndesign learning experiences that not only impart essential programming skills but also\nEduc. Sci. 2024, 14, 1089\n4 of 17\nprepare students for a future where collaboration with AI is an integral part of the software\ndevelopment process.\nTraditional teaching methods often suffer from low student participation, lack of\npersonalized instruction, and insufficient motivation [24,25]. Modern approaches in ed-\nucation enhance active learning approaches [26]. Traditional education often relies on\ntheory-based materials, but the integration of ChatGPT can foster personalized learning\nexperiences tailored to individual needs and preferences. Students can leverage ChatGPT\nto outsource certain knowledge tasks, allowing them to concentrate on \u2018hands-on\u2019 learning\nand gain practical experience in their chosen fields. Additionally, ChatGPT could also\ncreate adaptive learning environments that respond to individual learner progress and\nperformance [14,27,28].\nConstructivist theory emphasizes that students construct knowledge through their\nown experience and participation. Integrating AI tools into a constructivist framework can\nsignificantly enhance educational experiences by encouraging students to take charge of\ntheir educational journeys through real-time assistance like clarification of concepts, answer-\ning questions, and offering tailored guidance based on individual learning needs and styles.\nWith AI tools, educators can create environments that emphasize active learning. Stu-\nd"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_006",
    "source_id": "NoviceProgramming2024",
    "text": " preferences. Students can leverage ChatGPT\nto outsource certain knowledge tasks, allowing them to concentrate on \u2018hands-on\u2019 learning\nand gain practical experience in their chosen fields. Additionally, ChatGPT could also\ncreate adaptive learning environments that respond to individual learner progress and\nperformance [14,27,28].\nConstructivist theory emphasizes that students construct knowledge through their\nown experience and participation. Integrating AI tools into a constructivist framework can\nsignificantly enhance educational experiences by encouraging students to take charge of\ntheir educational journeys through real-time assistance like clarification of concepts, answer-\ning questions, and offering tailored guidance based on individual learning needs and styles.\nWith AI tools, educators can create environments that emphasize active learning. Stu-\ndents can engage in inquiry-based activities, utilizing AI as a resource for exploration and\nproblem-solving [29\u201333]. AI integration may transform educators from content providers\nto facilitators, emphasizing mentorship and the development of soft skills [4,5,23,27].\nCognitive load theory posits that learning is most effective when cognitive load is\nmanaged, helping students focus on essential concepts without being overwhelmed [34,35].\nAI tools can reduce extraneous cognitive load by automating routine tasks, allowing\nstudents to concentrate on understanding core programming principles. This can be\nparticularly useful for novice programmers who may struggle with complex concepts.\nMandai and co-authors [36], in their opinion paper, discussed the potential impacts of\nChatGPT on higher education through the lens of educational theories by John Dewey\u2019s\nReflective Thought and Action model and the revised Bloom\u2019s taxonomy. The analysis\nwas based on a review of existing literature on ChatGPT and educational theories. The\nkey points mentioned positive expectations: ChatGPT could enhance personalized learn-\ning, shift education towards more practical, hands-on learning by reducing the need for\nmemorizing information, and lead to assessment reforms. They also mentioned negative\nexpectations, like over-reliance on ChatGPT, failing to acquire necessary skills, diminished\nstudent creativity and originality, and they raised concerns about the authenticity and\naccuracy of sources.\nLo [32], in a literature review, provided an analysis of the implications of ChatGPT in\neducational contexts. The review assessed ChatGPT\u2019s capabilities across various subject\ndomains, its potential applications in education, and the challenges it poses. ChatGPT\u2019s per-\nformance varied significantly across different subjects, demonstrating outstanding results\nin critical thinking and economics but showing unsatisfactory performance in mathematics\nand medical education. Potential applications in education include serving as an assistant\nfor instructors or as a virtual tutor for students. Several challenges were identified, such\nas accuracy issues, plagiarism concerns, and the need for updated institutional policies to\naddress these issues. The conclusion emphasized that while ChatGPT holds significant\npromise for enhancing educational practices, careful consideration must be given to its\nlimitations and the ethical implications of its use.\n1.2. Aims and Research Questions\nAs we see, the central question is that of finding the right balance between the different\naspects of the use of the tools. To determine the most effective approach, we need a clear\nunderstanding of how these tools are actually used in practice, particularly: how students\nperceive them and how they apply them in real-life programming courses. Especially\ninteresting is observing students\u2019 first encounters with these tools, such as"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_007",
    "source_id": "NoviceProgramming2024",
    "text": "\nas accuracy issues, plagiarism concerns, and the need for updated institutional policies to\naddress these issues. The conclusion emphasized that while ChatGPT holds significant\npromise for enhancing educational practices, careful consideration must be given to its\nlimitations and the ethical implications of its use.\n1.2. Aims and Research Questions\nAs we see, the central question is that of finding the right balance between the different\naspects of the use of the tools. To determine the most effective approach, we need a clear\nunderstanding of how these tools are actually used in practice, particularly: how students\nperceive them and how they apply them in real-life programming courses. Especially\ninteresting is observing students\u2019 first encounters with these tools, such as during their\ninitial programming course in the first semester of their studies.\nEduc. Sci. 2024, 14, 1089\n5 of 17\nIn this study, we aim to analyze novice programmers\u2019 attitudes toward AI tools. We\nseek to understand the dynamics of AI tool usage and changes in behavior over time.\nAdditionally, we want to uncover patterns of AI usage and explore students\u2019 sentiments\nabout these tools. Essentially, our goal is to identify the \u2018Good\u2019 and the \u2018Bad\u2019 aspects of\nAI tool usage in novice programming education. To achieve this, we have formulated the\nfollowing research questions:\nRQ1. How familiar are novice programmers with AI tools during their programming\neducation?\nRQ2. Does the usage of AI tools evolve over time in an introductory programming\ncourse?\nRQ3. Are students satisfied with the results provided by AI tools, and does this\nsatisfaction improve over time?\nRQ4. What common tasks do novice programming teams use AI tools for, and how\nprevalent are these tasks among the teams?\nRQ5. What are the common benefits and concerns students have about using AI tools\nin their studies?\n2. Methodology\nThe primary aims of this study are to understand the dynamics of AI tools usage,\nuncover patterns in their application, and explore students\u2019 sentiments regarding these tools.\nData from students\u2019 reports and questionnaires on AI tool usage during the Introduction to\nProgramming course are analyzed in this paper. The course spans a period of 12 weeks,\nwith an instructional format consisting of 2 h of lectures and 4 h of lab sessions each week.\nA pragmatic mixed-methods study was undertaken to explore potential global patterns\nbased on quantitative data, enriched with qualitative data. Students completed surveys\nand reported on their AI tool usage. The analytical framework of the study is presented in\nFigure 1.\nFigure 1. Analytical framework of the study.\n2.1. Participants\nThe participants in this study were first-semester, first-year undergraduate students,\nwho were novice programmers enrolled in the Introduction to Programming course. A\ntotal of 73 teams, each comprising two students from the Faculty of Engineering at Ruppin\nAcademic Center in Israel, took part in the study. Students joined a team of their choice and\nremained with the same team until the end of the semester. Among the participants, 53%\nwere male (78) and 47% were female (68). The median age was 24, with the range between\n18 and 29 years. Only teams that submitted all required data items were included in this\nreport. The data from these teams were used for the majority of"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_008",
    "source_id": "NoviceProgramming2024",
    "text": " Participants\nThe participants in this study were first-semester, first-year undergraduate students,\nwho were novice programmers enrolled in the Introduction to Programming course. A\ntotal of 73 teams, each comprising two students from the Faculty of Engineering at Ruppin\nAcademic Center in Israel, took part in the study. Students joined a team of their choice and\nremained with the same team until the end of the semester. Among the participants, 53%\nwere male (78) and 47% were female (68). The median age was 24, with the range between\n18 and 29 years. Only teams that submitted all required data items were included in this\nreport. The data from these teams were used for the majority of the analysis in this study.\n2.2. Procedure and Data Analysis\nAll procedures performed in this study were in accordance with APA ethical guidelines\nand the ethical standards of the institutional research committee. Prior to conducting the\nresearch, the requirements of the institutional ethics committee were complied with. At\nthe beginning of the semester, before starting the experimental process, the students were\ninformed about the aims and procedures of the research. The tasks the students would\nundertake during the experimental process were explained. Each participant was instructed\nEduc. Sci. 2024, 14, 1089\n6 of 17\non how to create a unique identifier using a combination of personal but non-identifiable\ninformation. This identifier was used in all surveys and bonus questions throughout the\nsemester. Team members merged the two personal, unique identifiers together.\nBefore the course started, participants completed a pre-semester questionnaire that\nincluded questions on students\u2019 personal information and basic knowledge of AI. At the\nend of the course, participants completed a post-semester questionnaire that assessed their\nperceptions of their AI knowledge, as well as their views on the benefits and concerns\nrelated to using AI tools in their studies.\nBelieving in the importance of incorporating AI tools into classrooms from the very\nbeginning, specific assignment questions were designed where students were tasked with\nusing AI tools to generate or explain portions of code, as well as to explore or learn about\nparticular modules, concepts, or functions. Since these tasks required students to have\na basic understanding of AI and the ability to effectively interact with AI tools, several\nexamples were provided to demonstrate how these tools could be used in the context of\ntheir assignments. Although no specific AI tool was mandated, students were encouraged\nto explore different options to find what best suited their needs. As the formulation of the\nquestion of the student to the AI tool (usually called the prompt) could be important, in\naddition to these examples, prompt-writing tips were shared to help students craft more\neffective queries, enabling them to maximize the potential of the AI tools they chose to\nwork with.\nFurthermore, on a weekly basis, students worked on home assignments. A 5-point\nbonus question related to the usage of AI tools and students\u2019 feelings about them was\nadded to some assignments. A bonus question is an additional question or set of questions\nincluded in an assignment that offers students the opportunity to earn extra points beyond\nthe regular scoring criteria. To enable continuous data collection throughout the semester\nand gain insights into how teams interacted with AI tools as they progressed through\nthe course, weekly multipart bonus questions were included in each assignment for the\nteams to answer"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_009",
    "source_id": "NoviceProgramming2024",
    "text": " to these examples, prompt-writing tips were shared to help students craft more\neffective queries, enabling them to maximize the potential of the AI tools they chose to\nwork with.\nFurthermore, on a weekly basis, students worked on home assignments. A 5-point\nbonus question related to the usage of AI tools and students\u2019 feelings about them was\nadded to some assignments. A bonus question is an additional question or set of questions\nincluded in an assignment that offers students the opportunity to earn extra points beyond\nthe regular scoring criteria. To enable continuous data collection throughout the semester\nand gain insights into how teams interacted with AI tools as they progressed through\nthe course, weekly multipart bonus questions were included in each assignment for the\nteams to answer. Detailed, ongoing data was gathered, mapping the progression of AI\ntool integration into their work and providing a nuanced understanding of how their\nusage evolved.\nThe course spanned 12 weeks, with weekly assignments categorized into two groups:\nthose requiring the use of AI tools (assignments 3, 7, and 10) and those that did not\n(assignments 2, 6, 9, and 11). Bonus questions were incorporated into assignments 2, 3, 6, 7,\n9, and 11, each having a slightly different structure based on whether AI tools were used.\nA sample of the bonus question questionnaires can be found in Appendix A. No bonus\nquestions were included in the remaining weeks.\n2.3. Data Analysis\nThe impact of AI tools was measured using several indicators: familiarity with AI\ntools was assessed through pre- and post-semester questionnaires that captured students\u2019\nself-reported familiarity levels; the frequency of AI tool usage in assignments was evaluated\nwith a dichotomous (yes/no) statement, \u201cI used AI tools during this assignment\u201d, focusing\non weeks where usage was not explicitly required to identify trends in voluntary adoption;\nstudents\u2019 comfort levels with AI tools were tracked during weeks 3, 7, and 10 using a\n5-point Likert scale, allowing for the assessment of how comfort evolved as they became\nmore accustomed to the tools; satisfaction with AI tools was measured through survey\nquestions asking students to rate their satisfaction with the results provided by the tools on\na Likert scale, offering insights into their perceptions of the quality of outcomes from AI\ntool usage.\nIBM SPSS Statistics 28 software was applied for quantitative data analysis. Several\nstatistical tests were used in this study: the Wilcoxon signed ranks test for two related\nsamples, the Friedman test for several related samples, and Cohran\u2019s Q test. Cronbach\u2019s\nalpha coefficients, showing the internal consistency reliability, were computed. The values\nwere acceptable: 0.935 for feeling comfortable with usage of AI tools in the assignment on\nEduc. Sci. 2024, 14, 1089\n7 of 17\nweeks 3, 7, 10; 0.924 for using AI tools during the assignment on weeks 2, 6, 9; 0.758 for\nbeing happy with the results provided by AI tools on weeks 3, 7, 10.\nFor the qualitative data, participants\u2019 responses to the open-ended questions included\nin the bonus tasks were analyzed using content analysis. Prior to analyzing the data,\ntwo sets of categories based on prior research and literature were designed"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_010",
    "source_id": "NoviceProgramming2024",
    "text": " consistency reliability, were computed. The values\nwere acceptable: 0.935 for feeling comfortable with usage of AI tools in the assignment on\nEduc. Sci. 2024, 14, 1089\n7 of 17\nweeks 3, 7, 10; 0.924 for using AI tools during the assignment on weeks 2, 6, 9; 0.758 for\nbeing happy with the results provided by AI tools on weeks 3, 7, 10.\nFor the qualitative data, participants\u2019 responses to the open-ended questions included\nin the bonus tasks were analyzed using content analysis. Prior to analyzing the data,\ntwo sets of categories based on prior research and literature were designed. The first set\nof categories was designed for responses to the statement, \u201cI used AI tools during this\nassignment for the following tasks\u201d, and the second set was for responses to the direction,\n\u201cDescribe the benefits and concerns about using AI tools in your studies, personally\u201d.\nWe initially employed a deductive coding approach to categorize the data based on\nthese predefined categories. However, several new categories emerged during the analysis,\nwhich were subsequently added to the pre-existing codes. Thus, a hybrid coding approach\nwas adopted: the deductive coding provided a structured framework, while the inductive\ncoding allowed for the discovery of additional categories, adding depth and nuance to\nthe findings.\n3. Results\nBased on the responses received, we analyzed the results to address our research\nquestions. We did not specify the use of particular tools. The majority of students utilized\ntools designed to understand and generate human-quality text in response to a variety of\nprompts and questions. Most of these tools are user-friendly, featuring simple interfaces\nthat allow users to input prompts and receive responses easily.\n3.1. Familiarity with AI Tools\nTo answer the first research question about the familiarity of novice programmers with\nAI tools during their programming education, the level of agreement of the students to the\nfollowing statement, \u201cI feel familiar with AI tools usage\u201d, was analyzed at the beginning\nand at the end of the course. The descriptive frequency and percentage of responses can\nbe found in Table 1. The analysis of survey responses indicated that, initially, only 28% of\nthe teams reported feeling familiar with the usage of AI tools. However, by the end of the\ncourse, this familiarity had increased to 100%.\nTable 1. Frequency and percent of responses to the statement, \u201cI feel familiar with AI tools usage\u201d, at\nthe beginning and at the end of the semester (n = 73).\nResponse\nPre-Semester\nPost-Semester\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n33\n45.2\n0\n0\ndisagree (2)\n16\n21.9\n0\n0\nneutral (3)\n4\n5.5\n0\n0\nagree (4)\n13\n17.8\n39\n53.4\nstrongly agree (5)\n7\n9.6\n34\n46.6\nWilcoxon signed ranks test for two related samples was used to compare the responses\nat the beginning and at the end of the semester to the statement, \u201cI feel familiar with AI\ntools usage\u201d, measured on a Likert 1\u20135 scale. This test revealed significant differences\nbetween the"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_011",
    "source_id": "NoviceProgramming2024",
    "text": "emester\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n33\n45.2\n0\n0\ndisagree (2)\n16\n21.9\n0\n0\nneutral (3)\n4\n5.5\n0\n0\nagree (4)\n13\n17.8\n39\n53.4\nstrongly agree (5)\n7\n9.6\n34\n46.6\nWilcoxon signed ranks test for two related samples was used to compare the responses\nat the beginning and at the end of the semester to the statement, \u201cI feel familiar with AI\ntools usage\u201d, measured on a Likert 1\u20135 scale. This test revealed significant differences\nbetween the responses at the beginning and at the end: Z = \u22126.085, p < 0.001.\n3.2. Dynamics of AI Tool Integration\nTo address the second research question\u2014whether the usage of AI tools evolves over\ntime in an introductory programming course\u2014the level of agreement with the statement,\n\u201cI feel comfortable using AI tools in this assignment\u201d, was analyzed on a Likert scale from\n1 to 5 during weeks 3, 7, and 10. During these weeks, the use of AI tools was required to\nEduc. Sci. 2024, 14, 1089\n8 of 17\ncomplete the assignments. The descriptive frequencies and percentages of responses are\npresented in Table 2.\nTable 2. Frequency and percentage of responses to the statement, \u201cI feel comfortable using AI tools in\nthis assignment\u201d, during weeks 3, 7, and 10 (n = 73).\nResponse\nWeek 3\nWeek 7\nWeek 10\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n0\n0\n0\n0\n0\n0\ndisagree (2)\n0\n0\n0\n0\n0\n0\nneutral (3)\n4\n5.5\n3\n4.1\n2\n2.7\nagree (4)\n34\n46.6\n33\n45.2\n31\n42.5\nstrongly agree (5)\n35\n47.9\n37\n50.7\n40\n54.8\nThe Friedman test for several related samples was applied, comparing the responses\nto the item, \u201cI feel comfortable with usage of AI tools in this assignment\u201d, on weeks 3, 7,\nand 10. The differences in medians were significant: \u03c72 (2) = 9.8, p = 0.007.\nAdditionally, to answer the second research question, we analyzed the level of agree-\nment to the dichotomous (yes, no) statement, \u201cI used AI tools during this assignment\u201d,\non weeks 2, 6, 9 and 11. Those weeks did not require the use of AI tools. The descriptive\nfrequency and percentage of responses to the statement can be found in Table 3.\nTable 3. Frequency and percentage of responses to the statement, \u201cI used AI tools during this\nassignment\u201d, during weeks 2, 6, 9, and 11 (n = 73).\nResponse\nWeek 2\nWeek 6\nWeek 9\nWeek 11\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nFrequency"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_012",
    "source_id": "NoviceProgramming2024",
    "text": ", we analyzed the level of agree-\nment to the dichotomous (yes, no) statement, \u201cI used AI tools during this assignment\u201d,\non weeks 2, 6, 9 and 11. Those weeks did not require the use of AI tools. The descriptive\nfrequency and percentage of responses to the statement can be found in Table 3.\nTable 3. Frequency and percentage of responses to the statement, \u201cI used AI tools during this\nassignment\u201d, during weeks 2, 6, 9, and 11 (n = 73).\nResponse\nWeek 2\nWeek 6\nWeek 9\nWeek 11\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nyes\n24\n32.9\n30\n41.1\n30\n41.1\n42\n57.5\nno\n49\n67.1\n43\n58.9\n43\n58.9\n31\n42.5\nNon-parametric Cohran\u2019s Q test was used to compare the response to the item, \u201cI used\nAI tools during this assignment\u201d, on weeks 2, 6, 9, and 11 (yes, no). Cohran\u2019s Q = 34.839,\np < 0.001, meaning that there were significant differences on the response between weeks 2,\n6, 9, and 11, indicating that students used more AI tools.\n3.3. Student Satisfaction with AI Tools\nTo address the third research question, \u201cAre students satisfied with the results pro-\nvided by AI tools, and does this satisfaction improve over time?\u201d, we analyzed students\u2019\nlevel of agreement with the statement, \u201cI was happy with the results provided by AI tools\u201d,\nusing a Likert scale of 1 to 5 during weeks 3, 7, and 10. During these specific weeks, the use\nof AI tools was mandatory for completing an assignment. The descriptive frequency and\npercentage of responses can be found in Table 4.\nThe Friedman test for several related samples was applied comparing the responses to\nthe item, \u201cI was happy with the results provided by AI tools\u201d, on weeks 3, 7, and 10. The\ndifferences in medians were significant: \u03c72 (2) = 11.594, p = 0.003, meaning that the answers\nwere better. However, since students used the same language model, this result suggests\nthat they improved their prompt-engineering skills over time.\nEduc. Sci. 2024, 14, 1089\n9 of 17\nTable 4. Frequency and percentage of responses to the statement, \u201cI was happy with the results\nprovided by AI tools\u201d, during weeks 3, 7, 10 (n = 73).\nResponse\nWeek 3\nWeek 7\nWeek 10\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n0\n0\n0\n0\n0\n0\ndisagree (2)\n3\n4.1\n1\n1.4\n0\n0\nneutral (3)\n10\n13.7\n6\n8.2\n3\n4.1\nagree (4)\n31\n42.5\n34\n46.6\n35\n47.9\nstrongly agree ("
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_013",
    "source_id": "NoviceProgramming2024",
    "text": " and percentage of responses to the statement, \u201cI was happy with the results\nprovided by AI tools\u201d, during weeks 3, 7, 10 (n = 73).\nResponse\nWeek 3\nWeek 7\nWeek 10\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n0\n0\n0\n0\n0\n0\ndisagree (2)\n3\n4.1\n1\n1.4\n0\n0\nneutral (3)\n10\n13.7\n6\n8.2\n3\n4.1\nagree (4)\n31\n42.5\n34\n46.6\n35\n47.9\nstrongly agree (5)\n29\n39.7\n32\n43.8\n35\n47.9\n3.4. Common AI Tool Tasks and Prevalence\nTo address the fourth research question, \u201cWhat common tasks do novice programming\nteams use AI tools for, and how prevalent are these tasks among the teams?\u201d, we analyzed\nthe responses to the statement, \u201cI used AI tools during this assignment for the following\ntasks\u201d. Each team was allowed to provide as many responses as they wished. Given\nthat the quality of the code and its accompanying description becomes more demanding\ntoward the end of the course, we focused on the teams\u2019 responses from weeks 9 and 11. We\ncombined the responses from each team across these two weeks, and if the same category\nwas mentioned more than once, it was counted only once.\nCategories that emerged from the analysis of the responses in the qualitative stage of\nthe research and the percentage of the teams that expressed them were as follows: creation\nof comments mentioned by 67 teams (91.7%), bugs identification and correction\u201459 teams\n(80.2%), information seeking\u201450 teams (68.5%), code comparisons\u201433 teams (45.21%),\ncreating some part of the code\u201425 teams (34.2%), debugging assistance\u201425 teams (34.2%),\ncode simplification\u201414 teams (19.2%), translation of exercise from Hebrew (the original lan-\nguage) to another language (for example, Arabic or Russian)\u201412 teams (16.4%), algorithm\nselection\u201411 teams (15.1%). Translation of exercise from the original language to another\nlanguage was a category that was added as a result of the inductive coding approach.\n3.5. Benefits and Concerns of AI Tool Usage\nTo address the fifth research question about the common benefits and concerns stu-\ndents have regarding the use of AI tools in their studies, responses to the open-ended\ndirection, \u201cDescribe the benefits and concerns you personally have about using AI tools\nin your studies\u201d, during the last week of the course were analyzed. Common benefit\ncategories emerged from the analysis of the responses, and the percentage of the teams that\nexpressed them were as follows: instant help or quick answers and explanations mentioned\nby 58 teams (79.4%), assistance with repetitive tasks (like writing comments)\u201455 teams\n(75.3%), immediate response/feedback\u201442 teams (57.5%), help with identification and\ncorrection of errors\u201440 teams (54.8%), access to diverse resources/providing additional\nexamples\u201433 teams (45.21%), spelling correction\u201417 teams (23.3%), translation"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_014",
    "source_id": "NoviceProgramming2024",
    "text": "direction, \u201cDescribe the benefits and concerns you personally have about using AI tools\nin your studies\u201d, during the last week of the course were analyzed. Common benefit\ncategories emerged from the analysis of the responses, and the percentage of the teams that\nexpressed them were as follows: instant help or quick answers and explanations mentioned\nby 58 teams (79.4%), assistance with repetitive tasks (like writing comments)\u201455 teams\n(75.3%), immediate response/feedback\u201442 teams (57.5%), help with identification and\ncorrection of errors\u201440 teams (54.8%), access to diverse resources/providing additional\nexamples\u201433 teams (45.21%), spelling correction\u201417 teams (23.3%), translation from one\nlanguage to another\u201414 teams (19.2%). Once again, translation from one language to\nanother was added during the inductive coding process.\nCommon concern categories that emerged from the analysis of the responses and the\npercentage of the teams that expressed them were as follows: inaccurate or misleading\ninformation/answers mentioned by 65 teams (89.0%), over-reliance on AI tools/becoming\ntoo dependent on AI tools\u201455 teams (75.3%), identification of advanced functions and\nstructures that were not learned in class\u201433 teams (45.21%), incorrect explanations which\nappear believable to novices\u201427 teams (36.9%), low code quality\u201425 teams (34.2%), wast-\ning too much time on unsuccessful bug identification and error fixing\u201412 teams (16.4%),\ninconsistency/producing different outputs, even when given the same prompt\u201410 teams\n(13.6%). This last category was added as a result of the inductive coding approach.\nEduc. Sci. 2024, 14, 1089\n10 of 17\n4. Discussion\n4.1. General Discussion\nThis study uniquely combines quantitative and qualitative methods to examine novice\nprogrammers\u2019 interactions with AI tools. The findings illustrate a progression in usage\npatterns, suggesting that, as students became more familiar with AI tools, they were\nmore inclined to use them voluntarily, even in assignments where they were not explicitly\nrequired. Notably, we achieved an impressive 98.5% response rate for the bonus question,\ngenerating substantial and valuable information.\nWe will focus on identifying the \u2018Good\u2019 and \u2018Bad\u2019 aspects of AI tool usage in novice\nprogramming education.\nThe results of the first research question revealed that, at the beginning of the course,\nonly 28% of the teams reported feeling familiar with the use of AI tools. However, by\nthe end of the course, this familiarity had significantly increased, with all teams (100%)\nexpressing confidence in their ability to use these tools. This dramatic rise in familiarity\nunderscores the effectiveness of integrating AI tools into the curriculum, allowing students\nto gradually build their skills and comfort levels as they progressed through the course. The\ninitial hesitation gave way to a comprehensive understanding of AI tools, suggesting that\nconsistent exposure and practical application were key factors in fostering this growth. The\nanalysis of the second research question reveals a progression in students\u2019 confidence and\nusage of AI tools over time in an introductory programming course. Initially, a significant\nportion of students felt uncomfortable using AI tools, as indicated by the low agreement\nto the statement, \u201cI feel comfortable with usage of AI tools"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_015",
    "source_id": "NoviceProgramming2024",
    "text": " significantly increased, with all teams (100%)\nexpressing confidence in their ability to use these tools. This dramatic rise in familiarity\nunderscores the effectiveness of integrating AI tools into the curriculum, allowing students\nto gradually build their skills and comfort levels as they progressed through the course. The\ninitial hesitation gave way to a comprehensive understanding of AI tools, suggesting that\nconsistent exposure and practical application were key factors in fostering this growth. The\nanalysis of the second research question reveals a progression in students\u2019 confidence and\nusage of AI tools over time in an introductory programming course. Initially, a significant\nportion of students felt uncomfortable using AI tools, as indicated by the low agreement\nto the statement, \u201cI feel comfortable with usage of AI tools in this assignment\u201d, in Week\n3. However, by Weeks 7 and 10, there was a marked increase in comfort levels, with all\nstudents either agreeing or strongly agreeing with the statement, as demonstrated by the\nsignificant results of the Friedman test.\nMoreover, additional evidence for this trend comes from the results of the Cochrane\u2019s\nQ test, which analyzed AI tool usage in assignments where their use was not explicitly\nrequired (Weeks 2, 6, 9, and 11). The significant increase in the number of students using\nAI tools on their own initiative over time suggests that as students became more familiar\nand comfortable with these tools, they became more inclined to integrate them into their\nwork, even when not mandated. This indicates a positive shift in both the acceptance and\npractical application of AI tools as the course progressed.\nThese findings are consistent with the principles of constructivist theory, which em-\nphasizes that students construct knowledge through active participation and hands-on\nexperiences. The gradual increase in students\u2019 confidence and voluntary usage of AI tools\nsuggests that they were actively engaging with these technologies as they constructed\ntheir understanding of programming concepts. Furthermore, the shift in students\u2019 usage\npatterns, from initial hesitation to confident application, reflects their growing autonomy\nin learning, a key goal in a constructivist educational framework.\nThe results of the third research question, \u201cAre students satisfied with the results\nprovided by AI tools, and does this satisfaction improve over time?\u201d, reveal a significant\nincrease in student satisfaction as the course progressed. In Week 3, a substantial proportion\nof students expressed dissatisfaction with the results provided by AI tools, as reflected\nin the higher percentages of \u201cdisagree\u201d and \u201cstrongly disagree\u201d responses. However, by\nWeeks 7 and 10, the majority of students reported either \u201cagree\u201d or \u201cstrongly agree\u201d with\nthe statement, indicating a marked improvement in their satisfaction levels.\nThe significant findings from the Friedman test suggest that not only did students\u2019\nsatisfaction improve over time, but the quality of their interactions with AI tools also likely\nimproved. Since the same AI model was used throughout the course, this trend indicates\nthat students became more adept at prompt engineering, enabling them to extract better\nresults from the AI tools for any topic, not only programming. Additionally, a majority\nof students reported switching to English as their prompt language, which could also\nprovide better results. These findings open up several areas for further discussion. First,\nthe improvement in satisfaction over time underscores the importance of practice and\nEduc. Sci. 2024, 14, 1089\n11 of 17\n"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_016",
    "source_id": "NoviceProgramming2024",
    "text": " levels.\nThe significant findings from the Friedman test suggest that not only did students\u2019\nsatisfaction improve over time, but the quality of their interactions with AI tools also likely\nimproved. Since the same AI model was used throughout the course, this trend indicates\nthat students became more adept at prompt engineering, enabling them to extract better\nresults from the AI tools for any topic, not only programming. Additionally, a majority\nof students reported switching to English as their prompt language, which could also\nprovide better results. These findings open up several areas for further discussion. First,\nthe improvement in satisfaction over time underscores the importance of practice and\nEduc. Sci. 2024, 14, 1089\n11 of 17\nexperience when using AI tools. As students became more proficient with the capabilities\nand limitations of these tools, they were able to craft better prompts, leading to more\nsatisfactory outcomes. This suggests that prompt engineering is a critical skill that can be\ndeveloped over time and should be an integral part of AI education. Therefore, teaching\nstudents how to effectively interact with AI tools can enhance their learning experiences, a\npoint that has not been extensively addressed in the prior literature.\nThe analysis of the fourth research question reveals a diverse range of applications\nfor AI tools in an introductory programming course. These findings suggest that novice\nprogramming teams are leveraging AI tools primarily to optimize and enhance their\ncoding processes, particularly in areas that require repetitive or detailed work, such as\ncommenting and debugging. The varied use of AI tools across different tasks also highlights\ntheir versatility and the teams\u2019 growing confidence in incorporating these tools into various\naspects of their assignments as the course progresses.\nThe data indicates that, as the course demands increased, so did the reliance on\nAI tools for more complex tasks, reflecting a deeper integration of these tools into the\nprogramming workflow. This underscores the importance of AI tools as a valuable resource\nin programming education, particularly for tasks that can support learning and improve\ncode quality. However, it also raises questions about the potential for over-reliance and the\nneed to balance AI tool usage with the development of fundamental programming skills.\nThe analysis of the fifth research question, which explored the common benefits and\nconcerns students have regarding the use of AI tools in their studies, revealed a complex\ninterplay of advantages and challenges experienced by novice programmers. The most\nfrequently cited benefit was the provision of instant help and quick answers or explana-\ntions, mentioned by 79.4% of teams (58 teams). This was closely followed by assistance\nwith repetitive tasks, such as writing comments, reported by 75.3% of teams (55 teams).\nThis means that AI tools can minimize extraneous cognitive load by automating routine\ntasks, thereby enabling students to focus more effectively on mastering core programming\nprinciples. Immediate response and feedback from AI tools were valued by 57.5% of\nteams (42 teams), while help with identifying and correcting errors was acknowledged\nby 54.8% of teams (40 teams). Other notable benefits included access to diverse resources\nand additional examples (45.21%), spelling correction (23.3%), and translation between\nlanguages (19.2%). These findings are in line with Malinka et al. [22], who suggested\nthat ChatGPT could serve as a valuable aid in discussing challenges encountered during\nassignments"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_017",
    "source_id": "NoviceProgramming2024",
    "text": " (55 teams).\nThis means that AI tools can minimize extraneous cognitive load by automating routine\ntasks, thereby enabling students to focus more effectively on mastering core programming\nprinciples. Immediate response and feedback from AI tools were valued by 57.5% of\nteams (42 teams), while help with identifying and correcting errors was acknowledged\nby 54.8% of teams (40 teams). Other notable benefits included access to diverse resources\nand additional examples (45.21%), spelling correction (23.3%), and translation between\nlanguages (19.2%). These findings are in line with Malinka et al. [22], who suggested\nthat ChatGPT could serve as a valuable aid in discussing challenges encountered during\nassignments or speed up the learning process. These results are consistent with the findings\nof Biswas [16], Haleem et al. [17], Jalil et al. [18], and Surameery and Shakor [19], which\nindicated that ChatGPT could provide users with explanations, examples, and guidance,\nassist with debugging by analyzing data on the programming language, code structure,\nand error messages, code documentation and even assist with code review.\nThe significant role of AI tools in reducing cognitive load is evident from our findings.\nAs cognitive load theory posits, learning is optimized when extraneous cognitive load is\nminimized, allowing students to focus on essential concepts. The automation of routine\ntasks, such as debugging and commenting, by AI tools reduced the cognitive demands on\nstudents, thereby enabling them to concentrate on mastering core programming principles.\nThis is particularly beneficial for novice programmers, who often struggle with the dual\nchallenge of learning programming concepts and managing complex coding tasks. By\noffloading repetitive and time-consuming activities to AI, students could devote more cogni-\ntive resources to understanding and applying programming logic, which likely contributed\nto the observed increase in their satisfaction and comfort with AI tools over time.\nOn the other hand, several significant concerns emerged from the analysis. The\nmost prevalent issue was the provision of inaccurate or misleading information, reported\nby 89.0% of teams (65 teams). Over-reliance on AI tools and the risk of becoming too\ndependent on them was a concern for 75.3% of teams (55 teams). Additionally, 45.21%\nof teams (33 teams) expressed frustration with AI tools identifying advanced functions\nor structures that were not covered in class, potentially confusing students. Incorrect but\nEduc. Sci. 2024, 14, 1089\n12 of 17\nseemingly believable explanations were another concern, mentioned by 36.9% of teams\n(27 teams). Issues related to low code quality (34.2%), wasted time on unsuccessful bug\nidentification and error fixing (16.4%), and inconsistency in outputs, even when the same\nprompt was used (13.6%), were also highlighted. These findings, particularly the concerns\nabout inaccurate information and over-reliance on AI tools, align with challenges identified\nin Lo\u2019s literature review [32], which highlighted similar issues in the use of AI in educational\nsettings. Lo\u2019s work emphasized the potential for AI to provide misleading information and\nthe risk of students becoming overly dependent on AI tools, mirroring our observations in\nthis study.\nThe finding that nearly 90% of students perceived AI as inaccurate has significant\nimplications for the integration of"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_018",
    "source_id": "NoviceProgramming2024",
    "text": " Issues related to low code quality (34.2%), wasted time on unsuccessful bug\nidentification and error fixing (16.4%), and inconsistency in outputs, even when the same\nprompt was used (13.6%), were also highlighted. These findings, particularly the concerns\nabout inaccurate information and over-reliance on AI tools, align with challenges identified\nin Lo\u2019s literature review [32], which highlighted similar issues in the use of AI in educational\nsettings. Lo\u2019s work emphasized the potential for AI to provide misleading information and\nthe risk of students becoming overly dependent on AI tools, mirroring our observations in\nthis study.\nThe finding that nearly 90% of students perceived AI as inaccurate has significant\nimplications for the integration of AI into education. It underscores a critical challenge that\nmust be addressed to ensure AI is used effectively and responsibly in learning environments.\nRepeated inaccuracies can erode students\u2019 trust in AI tools, potentially hindering their\nwillingness to use these tools in the future, even when they could be beneficial. Additionally,\nif students rely on inaccurate information from AI tools, they may struggle to achieve\ndesired learning outcomes. This finding emphasizes the crucial role of teachers in equipping\nstudents with the skills to evaluate the quality of information generated by AI and guiding\nthem in using these tools effectively.\nA third of the students reported issues related to low code quality, which was sur-\nprising, keeping in mind that studies showed that AI tools could solve programming\nassignments and exam problems that are typically given in introduction to programming\ncourses more effectively than most students [10,37].\nThese findings underscore the dual nature of AI tools in programming education.\nWhile they offer substantial benefits, such as speeding up the learning process and provid-\ning immediate assistance, they also present challenges that can hinder learning and foster\ndependency. The balance between leveraging the strengths of AI tools and mitigating their\ndrawbacks is crucial for maximizing their effectiveness in educational settings. Educa-\ntors must be aware of these benefits and concerns to better support students in using AI\ntools effectively while also encouraging the development of independent problem-solving\nskills. Rahman and Watanobe [38] explored how ChatGPT helps students improve their\nprogramming skills. They claimed that ChatGPT could generate nearly accurate answers\nto technical questions from a wide range of topics and correct or partially correct program-\nming code based on problem descriptions, algorithm and problem names, etc. However,\nsimply acquiring answers and code from ChatGPT can be a barrier to improving learners\u2019\ncritical thinking and problem-solving skills.\n4.2. The Good\nThe positive (\u2018Good\u2019) aspects can be summarized as follows: AI tools significantly\nenhanced participants\u2019 learning experiences by assisting with information retrieval, bug\nidentification, and writing comments. Participants felt that the use of AI tools brought\nreal-world relevance to the course, fostering essential literacies and skills that are crucial\nfor understanding technology and preparing them for the future. Additionally, some\nparticipants noted that receiving feedback from AI tools when they were stuck boosted their\nself-efficacy and motivation to learn. This finding aligns with Yilmaz and Yilmaz [11], who\ndemonstrated that ChatGPT enhances student programming self-efficacy and motivation.\nSimilarly, Yin et al. [15] found that chatbot-based learning effectively increases college\nstudents\u2019 motivation in basic computer science courses.\nAnother advantage (\u2018Good\u2019)"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_019",
    "source_id": "NoviceProgramming2024",
    "text": " learning experiences by assisting with information retrieval, bug\nidentification, and writing comments. Participants felt that the use of AI tools brought\nreal-world relevance to the course, fostering essential literacies and skills that are crucial\nfor understanding technology and preparing them for the future. Additionally, some\nparticipants noted that receiving feedback from AI tools when they were stuck boosted their\nself-efficacy and motivation to learn. This finding aligns with Yilmaz and Yilmaz [11], who\ndemonstrated that ChatGPT enhances student programming self-efficacy and motivation.\nSimilarly, Yin et al. [15] found that chatbot-based learning effectively increases college\nstudents\u2019 motivation in basic computer science courses.\nAnother advantage (\u2018Good\u2019) of incorporating AI tools (beyond just programming\ncourses) is their role in shaping learners\u2019 mindsets and skills related to AI, fostering their\nunderstanding and application of these technologies. Usage of these tools will help students\nin their future endeavors to understand the principles of AI, experience AI achievements,\nand implement AI applications effectively.\nEduc. Sci. 2024, 14, 1089\n13 of 17\n4.3. The Bad\nOn the negative (\u2018Bad\u2019) side, challenges emerged, including instances of cheating,\nwhere students relied on AI tools to generate entire solutions, particularly under time\npressure towards the end of the semester or when facing difficulties with certain learning\ntopics. The efficiency of AI tools also led to the over-automation of some programming\ntasks, such as generating comments, with 92% of teams fully outsourcing this task to AI.\nAdditionally, there were occasional issues with the quality of produced code, unsuccessful\nbug identification and error fixing, and the use of incorrect or non-existent functions and\npackages. AI can generate inaccurate outputs, inaccurate code, or incorrect explanations\nwhich appear believable to novices.\nThe generation of solutions (creating some part of the code) was mentioned only\nby a third of the teams, possibly because students perceive it as a violation of academic\nintegrity. Despite this, the literature indicates that a high percentage of computer science\nstudents engage in some form of plagiarism, with some studies reporting the percentage\nbeing nearly 80% [39,40]. Sheard et al. [40] argued that when students are given tasks\nwith readily available solutions in textbooks or lecture notes, they may be tempted to take\nshortcuts, thus bypassing the intended learning experience. In contrast, Albluwi [41], in his\nsystematic review, noted that research on the relationship between pressure in computing\ncourses and plagiarism is limited and does not adequately reflect the significant impact\nthis factor has on academic dishonesty.\nThe \u2018Bad\u2019 side of AI tool usage highlighted several significant concerns. One of the\nprimary issues is the risk of students developing an excessive or even blind reliance on\nthese tools, which can lead to a superficial understanding of programming concepts and\na diminished ability to solve problems independently. This reliance also opens the door\nto potential misuse, where the ease of accessibility and inclusion of these tools can tempt\nstudents to shortcut their learning process rather than engaging deeply with the material.\nAs Chen et al. [42] mentioned, the value of a tool depends on its use, and there is the\npotential for Codex to be used in ways that limit learning, or ways that make the work\nof educators difficult. Actually, the"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_020",
    "source_id": "NoviceProgramming2024",
    "text": " dishonesty.\nThe \u2018Bad\u2019 side of AI tool usage highlighted several significant concerns. One of the\nprimary issues is the risk of students developing an excessive or even blind reliance on\nthese tools, which can lead to a superficial understanding of programming concepts and\na diminished ability to solve problems independently. This reliance also opens the door\nto potential misuse, where the ease of accessibility and inclusion of these tools can tempt\nstudents to shortcut their learning process rather than engaging deeply with the material.\nAs Chen et al. [42] mentioned, the value of a tool depends on its use, and there is the\npotential for Codex to be used in ways that limit learning, or ways that make the work\nof educators difficult. Actually, the developers of Codex mentioned one such challenge:\npossible over-reliance on Codex by novice programmers.\nFurthermore, the overuse of code generation tools was found to result in a limited\ngrasp of fundamental programming principles and concepts. This not only undermines\nstudents\u2019 ability to write and understand code on their own but also makes them more\nvulnerable to technical issues and algorithmic errors. Such errors often arise from the\nAI\u2019s misinterpretation of tasks or from poorly formulated queries, leading to incorrect or\nsuboptimal solutions.\nMoreover, instead of fostering a collaborative learning environment where students\nwork alongside AI to enhance their understanding, the AI is often doing most of the work.\nThis dynamic can erode the learning experience, as students become passive recipients\nrather than active participants in the problem-solving process. The result is a concerning\nshift away from critical engagement with programming tasks towards a more superficial\nreliance on AI-generated solutions.\n5. Conclusions\nSo, do advantage overweigh the disadvantages of these tools? Despite many diffi-\nculties and challenges, integration of AI coding tools in programming courses will bring\nbenefits to most of the students and increase the number of participants. Discovering\npatterns in the AI tool usage of novel programmers can enhance students\u2019 learning. Iden-\ntifying trends can help in tailoring future educational strategies and interventions based\non observed patterns of engagement. We agree with Finnie-Ansley and co-authors [10]\nthat, whatever we do, it is certain that the AI revolution has arrived at the door of our\nclassrooms, and we must consider how we adapt to it. Keeping it out is not an option.\nIncorporating AI tools into courses represents a new essential form of digital literacy\nin modern education. As technology becomes integral to various aspects of life and work,\nEduc. Sci. 2024, 14, 1089\n14 of 17\nproficiency in AI tools is as crucial as traditional digital skills. This new literacy involves\nnot only effectively using AI but also critically assessing its outputs and understanding its\nethical implications. By integrating AI tools into curricula, educators equip students with\nthe skills needed to navigate an increasingly automated future, fostering creativity and\ninnovation. Ultimately, teaching students how to leverage AI thoughtfully prepares them\nto thrive in evolving job markets while enhancing their problem-solving abilities.\nAs educators, we believe it is crucial to integrate AI tools into our classrooms from the\nbeginning, and to teach students how to use them responsibly. In our forward-thinking\napproach to programming education, we acknowledge the importance of equipping stu-\ndents with AI skills that will be essential for their future"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_021",
    "source_id": "NoviceProgramming2024",
    "text": " is as crucial as traditional digital skills. This new literacy involves\nnot only effectively using AI but also critically assessing its outputs and understanding its\nethical implications. By integrating AI tools into curricula, educators equip students with\nthe skills needed to navigate an increasingly automated future, fostering creativity and\ninnovation. Ultimately, teaching students how to leverage AI thoughtfully prepares them\nto thrive in evolving job markets while enhancing their problem-solving abilities.\nAs educators, we believe it is crucial to integrate AI tools into our classrooms from the\nbeginning, and to teach students how to use them responsibly. In our forward-thinking\napproach to programming education, we acknowledge the importance of equipping stu-\ndents with AI skills that will be essential for their future careers. The learning of AI, and\nlearning with AI, has the potential to significantly enhance students\u2019 critical thinking skills\nas they engage with AI applications, analyze their real-world implications, and rigorously\ntest the code generated by these tools. When faced with incorrect or suboptimal solutions,\nstudents are required to reconsider and refine their prompts, rephrase their initial queries,\nand develop a deeper understanding of what went wrong in their previous attempts. This\nprocess not only sharpens their problem-solving abilities but also fosters a more nuanced\ncomprehension of AI interactions and the logic behind coding.\nBy balancing the integration of AI, addressing ethical considerations, and fostering\nadaptability, educators can play a crucial role in preparing a workforce that is well-equipped\nto navigate the challenges and opportunities presented by AI in the programming field.\n6. Limitations\nThis study has some limitations. First, the experimental process was conducted with\n73 teams, which introduces the possibility that one team member may have been more\ndominant in the decision-making process. As a result, the outcomes and responses may\nreflect the perspectives or actions of that single dominant member, rather than representing\nthe collective input of the entire team. In addition, the sample consists of a specific group\nof students from one course; therefore, the results might not be representative of all student\npopulations. In further research, the number of participants can be increased, students\nfrom additional courses or institutions can join this study, and the wider results can be\ncompared to current ones.\nAdditionally, a limitation of this study was the lack of direct measures of critical\nthinking skills. Future research incorporating such measures would provide a more com-\nprehensive understanding of the impact of AI tools on student development. This addition\nwould allow for a more precise evaluation of how AI tools affect students\u2019 critical thinking\nabilities, rather than relying solely on indirect indicators or self-reported data.\nAlso, much of the data relies on self-reported measures, such as students\u2019 comfort lev-\nels or perceived improvements. This could introduce biases, as students might overestimate\nor underestimate their abilities or comfort with AI tools.\n7. Future Research\nIn our future research, we plan to further investigate the characteristics of effective\nand ineffective prompts, explore the impact of AI tools on individual versus team-based\nlearning, and compare different AI tools in programming education. Additionally, we\nwill examine how AI-enhanced learning affects critical thinking skills and conduct a com-\nparative study between traditional and AI-enhanced teaching methods across various\ncourses. This research aims to provide valuable insights into the effective integration of AI\nin education and its potential to improve student learning outcomes.\nFunding: This research was funded by"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_022",
    "source_id": "NoviceProgramming2024",
    "text": ", such as students\u2019 comfort lev-\nels or perceived improvements. This could introduce biases, as students might overestimate\nor underestimate their abilities or comfort with AI tools.\n7. Future Research\nIn our future research, we plan to further investigate the characteristics of effective\nand ineffective prompts, explore the impact of AI tools on individual versus team-based\nlearning, and compare different AI tools in programming education. Additionally, we\nwill examine how AI-enhanced learning affects critical thinking skills and conduct a com-\nparative study between traditional and AI-enhanced teaching methods across various\ncourses. This research aims to provide valuable insights into the effective integration of AI\nin education and its potential to improve student learning outcomes.\nFunding: This research was funded by Ruppin Academic Center grant number 33139.\nInstitutional Review Board Statement: The study was conducted in accordance with the guidelines\nof the Ruppin Academic Center, and approved by the Institutional Review Board under approval\nnumber 208, Approval Date: 20 March 2024.\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the study.\nEduc. Sci. 2024, 14, 1089\n15 of 17\nData Availability Statement: The data supporting the reported results in this study are available\non request from the corresponding author. Due to privacy and ethical restrictions, the data are not\npublicly available.\nAcknowledgments: This work was supported by the internal project of Ruppin Academic Center.\nConflicts of Interest: The author declares no conflict of interest.\nAppendix A\nThe bonus questionnaire structure:\n1.\nI feel familiar with AI tools usage (Likert scale from 1 to 5). (This question was given\nat the beginning of the course and was given again at the end of the course.)\n2.\nI feel comfortable with usage of AI tools in this assignment (Likert scale from 1 to 5).\n(This question was given only for assignments that required the use of AI tools.)\n3.\nWhich tools did you use: ____________________\n4.\nI used AI tools during this assignment (yes/no). (This question was given only for\nassignments that do not require the use of AI tools.)\n5.\nQuery language: I used only English, only Hebrew, both English and Hebrew, other\nlanguage ____\n6.\nI was happy with the results provided by AI tools (Likert scale from 1 to 5).\n7.\nI am concerned that I may not have enough time to complete the assignment without\nthe help of AI tools (Likert scale from 1 to 5).\n8.\nI used AI tools during this assignment for the following tasks ____________________\n(Note: In the analysis of this question, we did not analyze the specific tasks required\nby the assignment itself.)\n9.\nProvide a screenshot of the good prompt (a compulsory question in all assignments\nwhere students were asked to use AI tools).\n10.\nProvide a screenshot of the bad prompt (a compulsory question in all assignments\nwhere students were asked to use AI tools).\n11.\nDescribe the benefits and concerns about using AI tools in your studies, personally.\n(This question was given in the middle of the course and was given again at the end\nof the course.)\nReferences\n1.\nBecker, B.A.; Denny, P.; Finnie-Ansley, J.; Luxton-Reilly, A.; Prather, J.; Santos, E.A."
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_023",
    "source_id": "NoviceProgramming2024",
    "text": " analysis of this question, we did not analyze the specific tasks required\nby the assignment itself.)\n9.\nProvide a screenshot of the good prompt (a compulsory question in all assignments\nwhere students were asked to use AI tools).\n10.\nProvide a screenshot of the bad prompt (a compulsory question in all assignments\nwhere students were asked to use AI tools).\n11.\nDescribe the benefits and concerns about using AI tools in your studies, personally.\n(This question was given in the middle of the course and was given again at the end\nof the course.)\nReferences\n1.\nBecker, B.A.; Denny, P.; Finnie-Ansley, J.; Luxton-Reilly, A.; Prather, J.; Santos, E.A. Programming is hard-or at least it used to be:\nEducational opportunities and challenges of ai code generation. In Proceedings of the 54th ACM Technical Symposium on Computer\nScience Education V. 1; Association for Computing Machinery: New York, NY, USA, 2023; pp. 500\u2013506.\n2.\nCotton, D.R.E.; Cotton, P.A.; Shipway, J.R. Chatting and cheating: Ensuring academic integrity in the era of ChatGPT. Innov. Educ.\nTeach. Int. 2023, 61, 228\u2013239. [CrossRef]\n3.\nDenny, P.; Prather, J.; Becker, B.A.; Finnie-Ansley, J.; Hellas, A.; Leinonen, J.; Luxton-Reilly, A.; Reeves, B.N.; Santos, E.A.; Sarsa, S.\nComputing Education in the Era of Generative AI. Commun. ACM 2024, 67, 56\u201367. [CrossRef]\n4.\nFirat, M. What ChatGPT means for universities: Perceptions of scholars and students. J. Appl. Learn. Teach. 2023, 6, 57\u201363.\n5.\nTlili, A.; Shehata, B.; Adarkwah, M.A.; Bozkurt, A.; Hickey, D.T.; Huang, R.; Agyemang, B. What if the devil is my guardian angel:\nChatGPT as a case study of using chatbots in education. Smart Learn. Environ. 2023, 10, 15. [CrossRef]\n6.\nBommasani, R.; Hudson, D.A.; Adeli, E.; Altman, R.; Arora, S.; von Arx, S.; Bernstein, M.S.; Bohg, J.; Bosselut, A.; Brunskill, E.;\net al. On the opportunities and risks of foundation models. arXiv 2021, arXiv:2108.07258.\n7.\nZawacki-Richter, O.; Mar\u00edn, V.I.; Bond, M.; Gouverneur, F. Systematic review of research on artificial intelligence applications in\nhigher education\u2013where are the educators? Int. J. Educ. Technol. High. Educ. 2019, 16, 1\u201327. [CrossRef]\n8.\nKalliamvakou, E. Research: Quantifying GitHub Copilot\u2019s Impact on Developer Productivity and Happiness. GitHub Blog\n2022. Available online: https://github.blog/news-insights"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_024",
    "source_id": "NoviceProgramming2024",
    "text": " E.;\net al. On the opportunities and risks of foundation models. arXiv 2021, arXiv:2108.07258.\n7.\nZawacki-Richter, O.; Mar\u00edn, V.I.; Bond, M.; Gouverneur, F. Systematic review of research on artificial intelligence applications in\nhigher education\u2013where are the educators? Int. J. Educ. Technol. High. Educ. 2019, 16, 1\u201327. [CrossRef]\n8.\nKalliamvakou, E. Research: Quantifying GitHub Copilot\u2019s Impact on Developer Productivity and Happiness. GitHub Blog\n2022. Available online: https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-\nproductivity-and-happiness/ (accessed on 1 May 2024).\n9.\nPeng, S.; Kalliamvakou, E.; Cihon, P.; Demirer, M. The impact of ai on developer productivity: Evidence from github copilot.\narXiv 2023, arXiv:2302.06590.\n10.\nFinnie-Ansley, J.; Denny, P.; Becker, B.A.; Luxton-Reilly, A.; Prather, J. The robots are coming: Exploring the implications of\nOpenAI Codex on introductory programming. In Proceedings of the 24th Australasian Computing Education Conference, Virtual\nEvent, 14\u201318 February 2022; pp. 10\u201319.\nEduc. Sci. 2024, 14, 1089\n16 of 17\n11.\nYilmaz, R.; Yilmaz, F.G.K. The effect of generative artificial intelligence (AI)-based tool use on students\u2019 computational thinking\nskills, programming self-efficacy and motivation. Comput. Educ. Artif. Intell. 2023, 4, 100147. [CrossRef]\n12.\nBird, C.; Ford, D.; Zimmermann, T.; Forsgren, N.; Kalliamvakou, E.; Lowdermilk, T.; Gazit, I. Taking Flight with Copilot: Early\ninsights and opportunities of AI-powered pair-programming tools. Queue 2022, 20, 35\u201357. [CrossRef]\n13.\nLau, S.; Guo, P. From \u201cBan it till we understand it\u201d to \u201cResistance is futile\u201d: How university programming instructors plan to\nadapt as more students use AI code generation and explanation tools such as ChatGPT and GitHub Copilot. In Proceedings of the\n2023 ACM Conference on International Computing Education Research-Volume 1; Association for Computing Machinery: New York,\nNY, USA, 2023; pp. 106\u2013121.\n14.\nRay, P.P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future\nscope. Internet Things Cyber-Phys. Syst. 2023, 3, 121\u2013154. [CrossRef]\n15.\nYin, J.; Goh, T.T.; Yang, B.; Xiaobin, Y. Conversation technology with micro-learning: The impact of chatbot-based learning on\nstudents\u2019 learning motivation and performance. J. Educ. Comput"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_025",
    "source_id": "NoviceProgramming2024",
    "text": ". In Proceedings of the\n2023 ACM Conference on International Computing Education Research-Volume 1; Association for Computing Machinery: New York,\nNY, USA, 2023; pp. 106\u2013121.\n14.\nRay, P.P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future\nscope. Internet Things Cyber-Phys. Syst. 2023, 3, 121\u2013154. [CrossRef]\n15.\nYin, J.; Goh, T.T.; Yang, B.; Xiaobin, Y. Conversation technology with micro-learning: The impact of chatbot-based learning on\nstudents\u2019 learning motivation and performance. J. Educ. Comput. Res. 2021, 59, 154\u2013177. [CrossRef]\n16.\nBiswas, S. Role of ChatGPT in Computer Programming. Mesopotamian J. Comput. Sci. 2023, 2023, 9\u201315. [CrossRef] [PubMed]\n17.\nHaleem, A.; Javaid, M.; Singh, R.P. An era of ChatGPT as a significant futuristic support tool: A study on features, abilities, and\nchallenges. BenchCouncil Trans. Benchmarks Stand. Eval. 2022, 2, 100089. [CrossRef]\n18.\nJalil, S.; Rafi, S.; LaToza, T.D.; Moran, K.; Lam, W. Chatgpt and software testing education: Promises & perils. In Proceedings of\nthe 2023 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW), Dublin, Ireland,\n16\u201320 April 2023; pp. 4130\u20134137.\n19.\nSurameery, N.M.S.; Shakor, M.Y. Use chat gpt to solve programming bugs. Int. J. Inf. Technol. Comput. Eng. 2023, 3, 17\u201322.\n[CrossRef]\n20.\nVukoji\u02c7ci\u00b4c, M.; Krsti\u00b4c, J. ChatGPT in programming education: ChatGPT as a programming assistant. InspirED Teach. Voice 2023,\n2023, 7\u201313.\n21.\nZhai, X. ChatGPT for next generation science learning. XRDS Crossroads ACM Mag. Stud. 2023, 29, 42\u201346. [CrossRef]\n22.\nMalinka, K.; Peres\u00edni, M.; Firc, A.; Hujn\u00e1k, O.; Janus, F. On the educational impact of chatgpt: Is artificial intelligence ready to\nobtain a university degree? In Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1;\nAssociation for Computing Machinery: New York, NY, USA, 2023; pp. 47\u201353.\n23.\nRudolph, J.; Tan, S.; Tan, S. ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? J. Appl. Learn.\nTeach. 2023, 6, 342\u2013363.\n24.\nCovill, A.E. College students\u2019 perceptions of the traditional lecture method. Coll. Stud. J. 2011,"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_026",
    "source_id": "NoviceProgramming2024",
    "text": " On the educational impact of chatgpt: Is artificial intelligence ready to\nobtain a university degree? In Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1;\nAssociation for Computing Machinery: New York, NY, USA, 2023; pp. 47\u201353.\n23.\nRudolph, J.; Tan, S.; Tan, S. ChatGPT: Bullshit spewer or the end of traditional assessments in higher education? J. Appl. Learn.\nTeach. 2023, 6, 342\u2013363.\n24.\nCovill, A.E. College students\u2019 perceptions of the traditional lecture method. Coll. Stud. J. 2011, 45, 92\u2013102.\n25.\nYue, S. The Evolution of Pedagogical Theory: From Traditional to Modern Approaches and Their Impact on Student Engagement\nand Success. J. Educ. Educ. Res. 2024, 7, 226\u2013230. [CrossRef]\n26.\nStukalenko, N.M.; Zhakhina, B.B.; Kukubaeva, A.K.; Smagulova, N.K.; Kazhibaeva, G.K. Studying innovation technologies in\nmodern education. Int. J. Environ. Sci. Educ. 2016, 11, 7297\u20137308.\n27.\nBaidoo-Anu, D.; Ansah, L.O. Education in the era of generative artificial intelligence (AI): Understanding the potential benefits of\nChatGPT in promoting teaching and learning. J. AI 2023, 7, 52\u201362. [CrossRef]\n28.\nPardos, Z.A.; Bhandari, S. Learning gain differences between ChatGPT and human tutor generated algebra hints. arXiv 2023,\narXiv:2302.06871.\n29.\nChen, R.; Zhao, H. ChatGPT in Creative Writing Courses in Chinese Universities: Application and Research. In Proceedings of\nthe 2024 12th International Conference on Information and Education Technology (ICIET), Yamaguchi, Japan, 18\u201320 March 2024;\npp. 243\u2013247.\n30.\nFischer, R.; Luczak-Roesch, M.; Karl, J.A. What does chatgpt return about human values? exploring value bias in chatgpt using a\ndescriptive value theory. arXiv 2023, arXiv:2304.03612.\n31.\nHuang, Z.; Mao, Y.; Zhang, J. The Influence of Artificial Intelligence Technology on College Students\u2019 Learning Effectiveness from\nthe Perspective of Constructivism\u2014Taking ChatGPT as an Example. J. Educ. Humanit. Soc. Sci. 2024, 30, 40\u201346. [CrossRef]\n32.\nLo, C.K. What is the impact of ChatGPT on education? A rapid review of the literature. Educ. Sci. 2023, 13, 410. [CrossRef]\n33.\nMishra, P.; Warr, M.; Islam, R. TPACK in the age of ChatGPT and Generative AI. J. Digit. Learn. Teach. Educ. 2023, 39, 235\u2013251.\n[CrossRef]\n34"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_027",
    "source_id": "NoviceProgramming2024",
    "text": " College Students\u2019 Learning Effectiveness from\nthe Perspective of Constructivism\u2014Taking ChatGPT as an Example. J. Educ. Humanit. Soc. Sci. 2024, 30, 40\u201346. [CrossRef]\n32.\nLo, C.K. What is the impact of ChatGPT on education? A rapid review of the literature. Educ. Sci. 2023, 13, 410. [CrossRef]\n33.\nMishra, P.; Warr, M.; Islam, R. TPACK in the age of ChatGPT and Generative AI. J. Digit. Learn. Teach. Educ. 2023, 39, 235\u2013251.\n[CrossRef]\n34.\nKo\u00b4c-Januchta, M.M.; Sch\u00f6nborn, K.J.; Roehrig, C.; Chaudhri, V.K.; Tibell, L.A.; Heller, H.C. \u201cConnecting concepts helps put main\nideas together\u201d: Cognitive load and usability in learning biology with an AI-enriched textbook. Int. J. Educ. Technol. High. Educ.\n2022, 19, 11. [CrossRef]\n35.\nSandoval-Medina, C.; Ar\u00e9valo-Mercado, C.A.; Mu\u00f1oz-Andrade, E.L.; Mu\u00f1oz-Arteaga, J. Self-Explanation Effect of Cognitive\nLoad Theory in Teaching Basic Programming. J. Inf. Syst. Educ. 2024, 35, 303\u2013312. [CrossRef]\n36.\nMandai, K.; Tan, M.J.H.; Padhi, S.; Pang, K.T. A Cross-Era Discourse on ChatGPT\u2019s Influence in Higher Education through the\nLens of John Dewey and Benjamin Bloom. Educ. Sci. 2024, 14, 614. [CrossRef]\n37.\nKuhail, M.A.; Mathew, S.S.; Khalil, A.; Berengueres, J.; Shah, S.J.H. \u201cWill I be replaced?\u201d Assessing ChatGPT\u2019s effect on software\ndevelopment and programmer perceptions of AI tools. Sci. Comput. Program. 2024, 235, 103111. [CrossRef]\n38.\nRahman, M.M.; Watanobe, Y. ChatGPT for education and research: Opportunities, threats, and strategies. Appl. Sci. 2023, 13, 5783.\n[CrossRef]\nEduc. Sci. 2024, 14, 1089\n17 of 17\n39.\nDick, M.; Sheard, J.; Bareiss, C.; Carter, J.; Joyce, D.; Harding, T.; Laxer, C. Addressing student cheating: Definitions and solutions.\nACM SigCSE Bull. 2002, 35, 172\u2013184. [CrossRef]\n40.\nSheard, J.; Simon Butler, M.; Falkner, K.; Morgan, M.; Weerasinghe, A. Strategies for maintaining academic integrity in first-year\ncomputing courses. In Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education,\nBologna, Italy, 3\u20135 July 2017; pp. 244\u2013249.\n41.\nAlbluwi, I. Plagiar"
  },
  {
    "chunk_id": "NoviceProgramming2024_chunk_028",
    "source_id": "NoviceProgramming2024",
    "text": ", J.; Bareiss, C.; Carter, J.; Joyce, D.; Harding, T.; Laxer, C. Addressing student cheating: Definitions and solutions.\nACM SigCSE Bull. 2002, 35, 172\u2013184. [CrossRef]\n40.\nSheard, J.; Simon Butler, M.; Falkner, K.; Morgan, M.; Weerasinghe, A. Strategies for maintaining academic integrity in first-year\ncomputing courses. In Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education,\nBologna, Italy, 3\u20135 July 2017; pp. 244\u2013249.\n41.\nAlbluwi, I. Plagiarism in programming assessments: A systematic review. ACM Trans. Comput. Educ. (TOCE) 2019, 20, 1\u201328.\n[CrossRef]\n42.\nChen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto, H.P.; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al.\nEvaluating large language models trained on code. arXiv 2021, arXiv:2107.03374.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.\n"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_001",
    "source_id": "CopilotExperiment2023",
    "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1\u2217Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n\u2217To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArti\ufb01cial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in \ufb01elds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\n\ufb01rms, including changes in employment, skills, and \ufb01rm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI\u2019s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Over\ufb02ow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signi\ufb01cant: the treated group completed the task 55.8% faster (95% con\ufb01dence interval:\n21-89%). Developers with less programming"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_002",
    "source_id": "CopilotExperiment2023",
    "text": " Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Over\ufb02ow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signi\ufb01cant: the treated group completed the task 55.8% faster (95% con\ufb01dence interval:\n21-89%). Developers with less programming experience, older programmers, and those who\nprogram more hours per day bene\ufb01ted the most. These heterogeneous effects point towards\npromise for AI-pair programmers in support of expanding access to careers in software devel-\nopment.\nThe paper proceeds as follows. We \ufb01rst describe the design of the controlled trial and\nprovide summary statistics. We then present the results. We conclude by a discussion on im-\nplications of the study for productivity research on AI-powered tools, its limitations, and future\nresearch directions on the broader economic impacts of AI-driven productivity.\n2\nStudy Design\nWe conducted a controlled experiment to measure the productivity impact of using GitHub\nCopilot in programming tasks. The experiment began on May 15, 2022 and ended on June 20,\n2022, right before GitHub Copilot became generally available. We recruited 95 professional\nprogrammers through Upwork, a freelancing platform. Participation in the experiment was\nadvertised on Upwork as a job posting, looking to recruit freelancer developers. Figures 1 and\n2 show (respectively) the job posting and the contract that was sent to participants to sign, in\naccordance with Upwork\u2019s policies. Once participants signed the contract, they were randomly\nsplit into control and treatment groups.\nFigure 3 shows the instructions sent to each group through email. The treated group was\ninstructed to watch a 1-minute video introducing them to GitHub Copilot. In addition to the\ninstructions, they also received an automated email with installation instructions for GitHub\nCopilot once granted access to the tool. We verify from telemetry after the experiment that all\nparticipants from the treated group have con\ufb01gured GitHub Copilot and accepted recommenda-\ntions other than \ufb01ve who did not \ufb01nish the sign up and thus started the experiment without the\nGitHub Copilot. Both treated and control groups were instructed to complete an entry survey to\nprovide demographic information such as age, gender, location, and educational background.\nBefore we began recruitment, we received approval for the study from the Microsoft Research\nEthics Review Board.\nParticipants were instructed to write an HTTP server in JavaScript\u2014the treatment group\nwould use GitHub Copilot to complete the task, while the control group would not. Besides the\nuse of GitHub Copilot in the treated group, participants were unconstrained in their software\ndevelopment \u2014they could use any sources of information as they normally do, such as internet\nsearch and Stack Over\ufb02ow.\n3\nWe calculated two metrics as a measure of performance for each group: task success and\ntask completion time. Task success was measured as the percentage of participants in a group\nthat adequately completed"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_003",
    "source_id": "CopilotExperiment2023",
    "text": " as age, gender, location, and educational background.\nBefore we began recruitment, we received approval for the study from the Microsoft Research\nEthics Review Board.\nParticipants were instructed to write an HTTP server in JavaScript\u2014the treatment group\nwould use GitHub Copilot to complete the task, while the control group would not. Besides the\nuse of GitHub Copilot in the treated group, participants were unconstrained in their software\ndevelopment \u2014they could use any sources of information as they normally do, such as internet\nsearch and Stack Over\ufb02ow.\n3\nWe calculated two metrics as a measure of performance for each group: task success and\ntask completion time. Task success was measured as the percentage of participants in a group\nthat adequately completed the task. Task completion time was measured as the time from start\nto end of the task. Using a standardized task provides us with precise measures of performance\nas it is dif\ufb01cult to measure productivity of software developers.\nTo administer the task, we used GitHub Classroom, a platform for teachers to issue and\ngrade coding assignments. In this way, we accurately measured the timing and completion for\neach participant. The instructions gave participants a link to a particular GitHub Classroom\ninstance with a single assignment referencing a template repository. When joining the assign-\nment, participants received a personal copy of the template repository, with the task description\n(shown in Figure 4) and a skeleton codebase for participants to build upon. The creation date\nand time of that personal copy created a timestamp. Each participant\u2019s repository was private\nto them and visible to the researchers conducting the experiment\u2014but not to other participants.\nWe included a test suite in the repository, comprising twelve checks for submission correct-\nness. If a submission passes, all twelve tests we counted are successfully completed. Partici-\npants could see the tests but were unable to alter them.\nWhen participants committed and pushed their changes to GitHub, GitHub Classroom ran\nthe test suite on their submission and reported the number of passing tests. Participants could\npush as often as they pleased, automatically logging a timestamp each time. The time elapsed\nbetween the timestamp of repository creation and the timestamp of the \ufb01rst commit to success-\nfully pass all 12 tests was counted as the participant\u2019s task completion time.\nThe full history of test suite runs is visible on each repository, enabling researchers to ob-\nserve partial results for participants that did not fully complete the task. The participants\u2019 \ufb01nal\ncompensation is calculated based on their time to completion and the scale we had previously\nshared with them (shown in Figure 1).\n4\nAfter participants had completed the task, we sent them the link to an exit survey. We asked\nthe treatment group how helpful they found GitHub Copilot as they worked on the task, as well\nas asked them to estimate how much faster they completed the task compared to how long this\ntask would have taken them without using GitHub Copilot. We also asked the control group to\nestimate the size of the speed gain they would have experienced if they used GitHub Copilot,\nafter showing them a 1-minute demo video.\nResults\nA total of 166 offers were sent during the experiment, and 95 were accepted. The 95 developers\nwere randomly assigned into control and treated groups, with 45 in the treated group and 50 in\ncontrol. Thirty-\ufb01ve developers from both the treated and control groups completed the task and"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_004",
    "source_id": "CopilotExperiment2023",
    "text": "the treatment group how helpful they found GitHub Copilot as they worked on the task, as well\nas asked them to estimate how much faster they completed the task compared to how long this\ntask would have taken them without using GitHub Copilot. We also asked the control group to\nestimate the size of the speed gain they would have experienced if they used GitHub Copilot,\nafter showing them a 1-minute demo video.\nResults\nA total of 166 offers were sent during the experiment, and 95 were accepted. The 95 developers\nwere randomly assigned into control and treated groups, with 45 in the treated group and 50 in\ncontrol. Thirty-\ufb01ve developers from both the treated and control groups completed the task and\nsurvey. Figure 5 presents the summary statistics of these participants.\nMost of the participants are in the age group of 25-34 and come from India and Pakistan.\nThis group of participants is also characterized by relatively lower income (median yearly in-\ncome between $10,000-$19,000) compared to US standards but high education level (the ma-\njority have a 4-year degree and above). The group has an average coding experience of 6 years\nand, on average, reported spending 9 hours on coding in a working day.\nFigure 6 plots the distribution between time to completion between treated and control\ngroups. Conditioning on completing the task, the average completion time from the treated\ngroup is 71.17 minutes and 160.89 minutes for the control group. This represents a 55.8% re-\nduction in completion time. The p-value for the t-test is 0.0017, and a 95% con\ufb01dence interval\nfor the improvement is between [21%, 89%]. There are four outliers with time to completion\nabove 300 min. All outliers are in the control group, however our results remain robust if these\noutliers are dropped. This result suggests that Copilot increases average productivity signif-\nicantly in our experiment population. We also \ufb01nd that the treated group\u2019s success rate is 7\n5\nTable 1: Heterogeneous Treatment Effects\nEstimates\nSE\nt-Stat\np-Value\n(Intercept)\n78.01\n67.84\n1.15\n0.2552\nProgramming experience (years)\n8.23\n4.36\n1.90\n0.0629\nHours of programming per day\n-11.70\n4.74\n-2.47\n0.0168\nAge: 25-44\n-74.55\n33.52\n-2.22\n0.0303\nUnemployed\n-35.98\n36.33\n-0.99\n0.3263\nIncome less than $20,000\n0.64\n27.47\n0.02\n0.9814\nNo college\n-36.57\n32.89\n-1.11\n0.2711\nLanguage Preference: Java\n-11.77\n33.16\n-0.35\n0.7240\nLanguage Preference: Python\n22.90\n42.19\n0.54\n0.5895\nNote: This table presents the heterogeneous treatment effects. The results suggest developer with less program-\nming experience are more likely to bene\ufb01t from"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_005",
    "source_id": "CopilotExperiment2023",
    "text": "\n0.0303\nUnemployed\n-35.98\n36.33\n-0.99\n0.3263\nIncome less than $20,000\n0.64\n27.47\n0.02\n0.9814\nNo college\n-36.57\n32.89\n-1.11\n0.2711\nLanguage Preference: Java\n-11.77\n33.16\n-0.35\n0.7240\nLanguage Preference: Python\n22.90\n42.19\n0.54\n0.5895\nNote: This table presents the heterogeneous treatment effects. The results suggest developer with less program-\nming experience are more likely to bene\ufb01t from Copilot, similarly for developers with more daily programming\nhours and in the age group above 25.\npercentage points higher than the control group, but the estimate is not statistically signi\ufb01cant,\nwith a 95% con\ufb01dence interval of [-0.11, 0.25].\nWe then investigate whether this effect is heterogeneous across different dimensions includ-\ning experience, employment status, income, education and software language preference. We\nassume the treatment effect is a linear function of the covariates of interest. We apply Horvitz-\nThomson transformation in [Athey and Imbens, 2015] (see also [Banerjee and Du\ufb02o, 2003] and\n[Carneiro et al., 2011])) and then regress the transformed outcome of interest on observables.\nThe estimates in Table 1 report coef\ufb01cients from this regression. The results show that less ex-\nperienced developers (years of professional coding), developers with heavy coding load (hours\nof coding per day), and older developers (developers aged between 25 and 44) bene\ufb01t more\nfrom Copilot.\nWe conducted an exit survey with two questions to learn about the experience of subjects.\nFirst, we asked them to estimate how much productivity gain or loss (in percentage term) Copi-\nlot provided to them for completing the task. While the control group was not exposed to Copi-\n6\nlot during the task, they were given the tutorial video before answering this question so that\nthey are aware of the features of Copilot. Figure 7 presents the distribution of the self-reported\nproductivity gain estimates from the control and treated groups. On average, participants in\nboth treated and control groups estimated a 35% increase in productivity, which is an underes-\ntimation compared with the 55.8% increase in their revealed productivity.\nIn the second question, participants were asked the highest monthly price at which they\nwould be interested in getting noti\ufb01ed about the release of GitHub Copilot. The intention is to\nlearn about developers\u2019 willingness to pay for Copilot as the answer to this question provides\nan upper bound for the developers\u2019 willingness to pay. Figure 8 presents the distribution of the\nirrelevant price separated for the control and treated groups. The average irrelevant price for the\ntreated group is $27.25, and the average irrelevant price for the control group is $16.91, both per\nmonth. The difference is statistically signi\ufb01cant at the 95% level. This result provides indirect\nevidence that treated group bene\ufb01ted from Copilot during their"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_006",
    "source_id": "CopilotExperiment2023",
    "text": " highest monthly price at which they\nwould be interested in getting noti\ufb01ed about the release of GitHub Copilot. The intention is to\nlearn about developers\u2019 willingness to pay for Copilot as the answer to this question provides\nan upper bound for the developers\u2019 willingness to pay. Figure 8 presents the distribution of the\nirrelevant price separated for the control and treated groups. The average irrelevant price for the\ntreated group is $27.25, and the average irrelevant price for the control group is $16.91, both per\nmonth. The difference is statistically signi\ufb01cant at the 95% level. This result provides indirect\nevidence that treated group bene\ufb01ted from Copilot during their task as their willingness to pay\nis signi\ufb01cantly higher than the control group.\nDiscussion\nThis paper presents evidence on the productivity effects of generative AI tools in software de-\nvelopment. To the best of our knowledge, it is the \ufb01rst controlled experiment to measure the\nproductivity of AI tools in professional software development. Our results suggest that Copilot\nhas statistically and practically signi\ufb01cant impact on productivity: the treated group that has\naccess to GitHub Copilot was able to complete the task 55.8% faster than the control group.\nFurther investigations into the productivity impacts of AI-powered tools in software devel-\nopment are warranted. This study examines a standardized programming task in an experiment\nto obtain a precise measure of productivity, instead of a task where developers collaborate on\nlarge projects in professional proprietary and/or open-source settings. Productivity bene\ufb01ts may\n7\nvary across speci\ufb01c tasks and programming languages, so more research is needed to understand\nhow our results generalizes to other tasks. Finally, this study does not examine the effects of\nAI on code quality. AI assistance can increase code quality if it suggests code better than the\nprogrammer writes, or it can reduce quality if the programmer pays less attention to code. The\ncode quality can have performance and security considerations that can change the real-world\nimpact of AI.\nThe heterogeneous effects identi\ufb01ed in this study warrant close attention. Our results sug-\ngest that less experienced programmers bene\ufb01t more from Copilot. If this result persists in\nfurther studies, the productivity bene\ufb01ts for novice programmers and programmers of older\nage point to important possibilities for skill initiatives that support job transitions into software\ndevelopment.\nThe economic impacts of these models also warrant further research [Manning et al., 2022],\nwith particular attention on their implications for labor market. In 2021, over 4.6 million people\nin the United States worked in computer and mathematical occupations,1 a Bureau of Labor\nStatistics category that includes computer programmers, data scientists, and statisticians. These\nworkers earned $464.8 billion or roughly 2% of US GDP. If the results of this study were to be\nextrapolated to the population level, a 55.8% increase in productivity would imply a signi\ufb01cant\namount of cost savings in the economy and have a notable impact on GDP growth. It is, as of\nyet, unclear how such gains would be distributed and how job tasks would change to incorporate\nAI-powered developer tools. It is important to consider such impacts and to begin research on\nthese implications at the outset ["
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_007",
    "source_id": "CopilotExperiment2023",
    "text": "\nin the United States worked in computer and mathematical occupations,1 a Bureau of Labor\nStatistics category that includes computer programmers, data scientists, and statisticians. These\nworkers earned $464.8 billion or roughly 2% of US GDP. If the results of this study were to be\nextrapolated to the population level, a 55.8% increase in productivity would imply a signi\ufb01cant\namount of cost savings in the economy and have a notable impact on GDP growth. It is, as of\nyet, unclear how such gains would be distributed and how job tasks would change to incorporate\nAI-powered developer tools. It is important to consider such impacts and to begin research on\nthese implications at the outset [Klinova and Korinek, 2021].\n1https://www.bls.gov/oes/current/oes150000.htm\n8\nReferences\n[Agrawal et al., 2019] Agrawal, A., Gans, J. S., and Goldfarb, A. (2019). Arti\ufb01cial intelli-\ngence: the ambiguous labor market impact of automating prediction. Journal of Economic\nPerspectives, 33(2):31\u201350.\n[Athey and Imbens, 2015] Athey, S. and Imbens, G. W. (2015). Machine learning methods for\nestimating heterogeneous causal effects. stat, 1050(5):1\u201326.\n[Banerjee and Du\ufb02o, 2003] Banerjee, A. V. and Du\ufb02o, E. (2003). Inequality and growth: What\ncan the data say? Journal of Economic Growth, 8:267\u2013299.\n[Barke et al., 2022] Barke, S., James, M. B., and Polikarpova, N. (2022). Grounded copilot:\nHow programmers interact with code-generating models. arXiv preprint arXiv:2206.15000.\n[Carneiro et al., 2011] Carneiro, P., Heckman, J. J., and Vytlacil, E. J. (2011).\nEstimating\nmarginal returns to education. American Economic Review, 101(6):2754\u201381.\n[Chen et al., 2021] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J.,\nEdwards, H., Burda, Y., Joseph, N., Brockman, G., et al. (2021). Evaluating large language\nmodels trained on code. arXiv preprint arXiv:2107.03374.\n[Finnie-Ansley et al., 2022] Finnie-Ansley, J., Denny, P., Becker, B. A., Luxton-Reilly, A., and\nPrather, J. (2022). The robots are coming: Exploring the implications of openai codex on\nintroductory programming. In Australasian Computing Education Conference, pages 10\u201319.\n[Klinova and Korinek, 2021] Klinova, K. and Korinek, A. (2021). Ai and shared prosperity. In\n"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_008",
    "source_id": "CopilotExperiment2023",
    "text": " al. (2021). Evaluating large language\nmodels trained on code. arXiv preprint arXiv:2107.03374.\n[Finnie-Ansley et al., 2022] Finnie-Ansley, J., Denny, P., Becker, B. A., Luxton-Reilly, A., and\nPrather, J. (2022). The robots are coming: Exploring the implications of openai codex on\nintroductory programming. In Australasian Computing Education Conference, pages 10\u201319.\n[Klinova and Korinek, 2021] Klinova, K. and Korinek, A. (2021). Ai and shared prosperity. In\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 645\u2013651.\n[Manning et al., 2022] Manning, S., Mishkin, P., Had\ufb01eld, G., Eloundou, T., and Eisner, E.\n(2022). A research agenda for assessing the economic impacts of code generation models.\n9\n[Mozannar et al., 2022] Mozannar, H., Bansal, G., Fourney, A., and Horvitz, E. (2022). Read-\ning between the lines: Modeling user behavior and costs in ai-assisted programming. arXiv\npreprint arXiv:2210.14306.\n[Nguyen and Nadi, 2022] Nguyen, N. and Nadi, S. (2022). An empirical evaluation of github\ncopilot\u2019s code suggestions. In Proceedings of the 19th International Conference on Mining\nSoftware Repositories, pages 1\u20135.\n[Raj and Seamans, 2018] Raj, M. and Seamans, R. (2018). Arti\ufb01cial intelligence, labor, pro-\nductivity, and the need for \ufb01rm-level data. In The economics of arti\ufb01cial intelligence: An\nagenda, pages 553\u2013565. University of Chicago Press.\n[Sandoval et al., 2022] Sandoval, G., Pearce, H., Nys, T., Karri, R., Dolan-Gavitt, B., and Garg,\nS. (2022). Security implications of large language model code assistants: A user study. arXiv\npreprint arXiv:2208.09727.\n[Vaithilingam et al., 2022] Vaithilingam, P., Zhang, T., and Glassman, E. L. (2022). Expec-\ntation vs. experience: Evaluating the usability of code generation tools powered by large\nlanguage models.\n[Zhang et al., 2022] Zhang, D., Maslej, N., Brynjolfsson, E., Etchemendy, J., Lyons, T.,\nManyika, J., Ngo, H., Niebles, J. C., Sellitto, M., Sakhaee, E., et al. (2022). The ai index\n2022 annual report. arXiv preprint arXiv:2205.03468.\n[Ziegler et al., 2022] Ziegler, A., Kalliamvakou, E., Li, X. A.,"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_009",
    "source_id": "CopilotExperiment2023",
    "text": " Expec-\ntation vs. experience: Evaluating the usability of code generation tools powered by large\nlanguage models.\n[Zhang et al., 2022] Zhang, D., Maslej, N., Brynjolfsson, E., Etchemendy, J., Lyons, T.,\nManyika, J., Ngo, H., Niebles, J. C., Sellitto, M., Sakhaee, E., et al. (2022). The ai index\n2022 annual report. arXiv preprint arXiv:2205.03468.\n[Ziegler et al., 2022] Ziegler, A., Kalliamvakou, E., Li, X. A., Rice, A., Rifkin, D., Simister,\nS., Sittampalam, G., and Aftandilian, E. (2022). Productivity assessment of neural code\ncompletion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, pages 21\u201329.\n10\nFigure 1: Upwork job posting\nNote: Job posting on Upwork starting May 25th 2022. The posting includes the task description, skill requirements\nand budget information.\n11\nFigure 2: Upwork contract\nNote: The contract sent to participants through Upwork. Upon accepting the contract, participants were random-\nized into control and treatment groups and given instructions for the task.\n12\nFigure 3: Instruction email to participants\nNote: Email instructions sent to participants in the treatment (top) and control (bottom) groups.\n13\nFigure 4: Participants\u2019 view of the task description\nNote: The task description participants saw in the index.js \ufb01le in the repository that was automatically created for\nthem by GitHub Classroom.\n14\n15\nFigure 5: Summary statistics of the experiment participants\nFrom left to right on each row see the following distributions: Participant age; Number of different languages used\nin the last 2 years; Level of education; Employment status; Geographical location; Yearly income; Programming\nexperience; Time spent coding daily.\n16\nFigure 6: Time to task completion\nNote: Distribution of time to task completion between treated (blue) and control (orange) groups\n17\nFigure 7: Self-estimated productivity gain\nNote: This graph shows the distribution of the estimated productivity improvement when using Copilot. Blue\nrepresents the estimation from the treated group and orange represents the estimation from the control group.\n18\nFigure 8: Distributing of irrelevant price\nNote: This graph shows the distribution of the irrelevant price between the treated (blue) and control (orange)\ngroups.\n19\n"
  },
  {
    "chunk_id": "CopilotExperiment2023_chunk_010",
    "source_id": "CopilotExperiment2023",
    "text": " (orange)\ngroups.\n19\n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_001",
    "source_id": "OpenSourceImpact2024",
    "text": "The Impact of Generative AI on Collaborative Open-Source Software \nDevelopment: Evidence from GitHub Copilot \n \n        Fangchen Song                                 Ashish Agarwal                                Wen Wen \nUniversity of Texas at Austin      University of Texas at Austin           University of Texas at Austin \n        2110 Speedway                                 2110 Speedway                                2110 Speedway \n        Austin, Texas                                    Austin, Texas                                   Austin, Texas \n        United States                                     United States                                   United States \n             78712                                                78712                                               78712 \nfangchen.song@mccombs.utexas.edu   ashish.agarwal@mccombs.utexas.edu    wen.wen@mccombs.utexas.edu \n \nAbstract \nGenerative artificial intelligence (AI) enables automated content production, including coding in software \ndevelopment, which can significantly influence developer participation and performance. To explore its \nimpact on collaborative open-source software (OSS) development, we investigate the role of GitHub \nCopilot, a generative AI pair programmer, in OSS development where multiple distributed developers \nvoluntarily collaborate. Using GitHub's proprietary Copilot usage data, combined with public OSS \nrepository data obtained from GitHub, we find that Copilot use increases project-level code contributions \nby 5.9%. This gain is driven by a 2.1% increase in individual code contributions and a 3.4% rise in \ndeveloper coding participation. However, these benefits come at a cost as coordination time for code \nintegration increases by 8% due to more code discussions enabled by AI pair programmers. This reveals an \nimportant tradeoff: While AI expands who can contribute and how much they contribute, it slows \ncoordination in collective development efforts. Despite this tension, the combined effect of these two \ncompeting forces remains positive, indicating a net gain in overall project-level productivity from using AI \npair programmers. Interestingly, we also find the effects differ across developer roles. Peripheral developers \nshow relatively smaller gains in project-level code contributions and face a higher increase in coordination \ntime than core developers, likely due to the difference in their project familiarity. In summary, our study \nunderscores the dual role of AI pair programmers in affecting project-level code contributions and \ncoordination time in OSS development. Our findings on the differential effects between core and peripheral \ndevelopers also provide important implications for the structure of OSS communities in the long run. \n \nKeywords: Generative AI, AI Pair Programmer, Open-source Software Development, Project-level Code \nContributions, Coordination Time, Core Developers, Peripheral Developers \n \n \n \n \n 1 \n1. Introduction \nThe continuous advancements in generative artificial intelligence (AI) are transforming content production \nacross a wide range of domains. Cutting-edge generative AI tools can not only automate mundane tasks but \nalso enhance original content. In the context of software development, generative AI-powered pair \nprogrammers (i.e., AI pair programmers) like GitHub Copilot, Amazon Q Developer, Google Gemini, and \nChat GPT can swiftly generate code based on developers\u2019 prompts, parameters, and descriptions (Dakhel \net al. 2023). By reducing common coding errors and repetitive coding needs, these AI pair programmers \nhold great potential to shape the software development process. A growing body of literature has \ninvestigated various impacts of generative AI on well-defined and discrete tasks on individuals, such as \nwriting and customer service tasks (Noy and Zhang 2023, Brynjolf"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_002",
    "source_id": "OpenSourceImpact2024",
    "text": " AI tools can not only automate mundane tasks but \nalso enhance original content. In the context of software development, generative AI-powered pair \nprogrammers (i.e., AI pair programmers) like GitHub Copilot, Amazon Q Developer, Google Gemini, and \nChat GPT can swiftly generate code based on developers\u2019 prompts, parameters, and descriptions (Dakhel \net al. 2023). By reducing common coding errors and repetitive coding needs, these AI pair programmers \nhold great potential to shape the software development process. A growing body of literature has \ninvestigated various impacts of generative AI on well-defined and discrete tasks on individuals, such as \nwriting and customer service tasks (Noy and Zhang 2023, Brynjolfsson et al. 2025). However, there is \nlimited understanding on the role of generative AI in complex tasks that involve team collaboration and \nrequire rich contextual knowledge, such as software development. \nAs a popular form of software development, open-source software (OSS) development relies on \nthe voluntary participation of geographically dispersed developers without formal hierarchies or \nstandardized workflows (Levine and Prietula 2014, Lindberg et al. 2016). Prior work shows that generative \nAI can enhance individual developer productivity (Peng et al. 2023, Cui et al. 2024). However, it remains \nunclear how such improvements at the individual level translate into project-level code contributions, which \nare a function of both individual contribution intensity and the extent of developer participation. While AI \npair programmers accelerate individual coding tasks, they may also affect developers\u2019 decisions to \nparticipate. Research has documented generative AI has led to a significant reduction in participation in \nonline discussions on platforms like Stack Overflow (Burtch et al. 2024). However, given the distinctions \nbetween OSS development and those information exchange communities, it is not clear how generative AI \naffects participation in OSS settings. On one hand, developers may be encouraged to participate because \ngenerating code becomes easier. On the other hand, this may reduce their incentive to contribute to OSS as \na means of skill development or reputation building. \n 2 \nBeyond project-level code contributions, effective coordination is critical to the success of OSS \ndevelopment (Koushik and Mookerjee 1995, Reagans et al. 2016, Mawdsley et al. 2022). However, \ncoordination is often informal and decentralized in OSS settings, shaped by fluid team composition and \nvoluntary participation. In our study, we focus on one key aspect of coordination efficiency, namely, the \ncoordination time needed to integrate individual code contributions into the codebase of an OSS project \n(Howison and Crowston 2014, Lindberg et al. 2016, Medappa and Srivastava 2019, Shaikh and Vaast 2023). \nShorter coordination time for code integration enables faster software development. Despite its importance, \nit remains unclear how AI pair programmers affect coordination time in OSS projects. Since AI pair \nprogrammers can make it easier to generate and interpret code, developers may be able to allocate more \neffort to integration, potentially reducing coordination time. However, if developers also participate more \nin the review process, this could increase the amount of communication required, thereby raising \ncoordination time"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_003",
    "source_id": "OpenSourceImpact2024",
    "text": "ination time needed to integrate individual code contributions into the codebase of an OSS project \n(Howison and Crowston 2014, Lindberg et al. 2016, Medappa and Srivastava 2019, Shaikh and Vaast 2023). \nShorter coordination time for code integration enables faster software development. Despite its importance, \nit remains unclear how AI pair programmers affect coordination time in OSS projects. Since AI pair \nprogrammers can make it easier to generate and interpret code, developers may be able to allocate more \neffort to integration, potentially reducing coordination time. However, if developers also participate more \nin the review process, this could increase the amount of communication required, thereby raising \ncoordination time. Moreover, given the inherent tradeoff between contribution volume and the coordination \neffort required to manage them (Bakos and Brynjolfsson 1993), it is not yet clear how AI pair programmers \nshape the overall project-level productivity of OSS development.  \nMoreover, OSS development typically involves two types of developers: core developers and \nperipheral developers (Setia et al. 2012). Core developers design the overall architecture, write code, and \nmaintain control over the codebase, whereas peripheral developers contribute on an irregular basis and \nfocus on fixing bugs and adding incremental features (Setia et al. 2012, Gousios et al. 2014, Medappa and \nSrivastava 2019). Thus, one important distinction between core and peripheral developers lies in their level \nof contextual knowledge about a focal project, or project familiarity, which refers to understanding of the \nproject's architecture, design, and norms. While existing literature has focused on the heterogenous impacts \nof generative AI based on workers\u2019 skill levels (e.g., Brynjolfsson et al. 2025), the conclusions cannot be \neasily generalized to understand how generative AI affects core versus peripheral developers, due to the \nimportant theoretical distinctions between skills needed to complete individual tasks and project familiarity \nrequired for complex software development.  \n 3 \nMotivated by these observations, in this study we aim to answer the following research questions. \nFirst, how do AI pair programmers affect project-level code contributions in OSS development? Second, \nhow do AI pair programmers affect coordination time for code integration in OSS development? Third, \nhow do AI pair programmers affect code contributions and coordination time for core versus peripheral \ndevelopers differently? Answers to questions could provide important implications on how to leverage \ngenerative AI tools to support collaborative and distributed software development in the OSS community.  \nWe argue that AI pair programmers can increase project-level code contributions in OSS \ndevelopment, because AI pair programmers not only boost individual code contributions but also encourage \ngreater developer participation. However, because AI pair programmers also encourage more participation \nin discussions on OSS projects, they could lead to longer coordination time for code integration. Moreover, \nwe argue that because of peripheral developers\u2019 lower project familiarity and the limited ability of AI pair \nprogrammers to learn the project\u2019s full context, the increase in project-level code contributions from \nperipheral developers may be smaller than that from core developers. At the same time, code contributed \nby peripheral developers may require more coordination time to integrate than code from core developers. \nOverall, the productivity gains from AI pair programmers may be smaller for peripheral developers than \nfor core developers. \nTo empirically"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_004",
    "source_id": "OpenSourceImpact2024",
    "text": ", because AI pair programmers not only boost individual code contributions but also encourage \ngreater developer participation. However, because AI pair programmers also encourage more participation \nin discussions on OSS projects, they could lead to longer coordination time for code integration. Moreover, \nwe argue that because of peripheral developers\u2019 lower project familiarity and the limited ability of AI pair \nprogrammers to learn the project\u2019s full context, the increase in project-level code contributions from \nperipheral developers may be smaller than that from core developers. At the same time, code contributed \nby peripheral developers may require more coordination time to integrate than code from core developers. \nOverall, the productivity gains from AI pair programmers may be smaller for peripheral developers than \nfor core developers. \nTo empirically test these hypotheses, we examine how the AI pair programmer, GitHub Copilot, \ninfluences project-level code contributions and coordination time of OSS projects on GitHub, as well as \nhow its effect varies among core versus peripheral developers. GitHub is one of the largest code-hosting \nrepositories based on the Git version control system (Dabbish et al. 2012). In this setting, a repository is a \nfundamental unit that typically contains the source code and resource files for a software project, along with \ninformation related to the project\u2019s evolution history, high-level features, and developer details (Zhang et \nal. 2017). Such repositories are often used to investigate collaborative development practices (Dabbish et \nal. 2012). Our unit of analysis is at the repository-month-level, with the sample period from January 2021 \nto December 2022. To examine our research questions, we use a combination of publicly available data on \nGitHub repositories and proprietary data on Copilot use provided by GitHub organization. Our treatment \n 4 \ngroup consists of repositories where Copilot was both supported by local coding environments and used by \ndevelopers to code. Thus, the post-treatment period includes the months during which Copilot was \nsupported and used in a focal repository, and the pre-treatment period includes all other months. The control \ngroup includes repositories where Copilot was not used throughout the sample period. We estimate our \nmodel using the Generalized Synthetic Control Method (GSCM) and validate the results through alternative \nmatching techniques and a comprehensive set of robustness checks. \nOur empirical results show that the adoption of GitHub Copilot is associated with a 5.9% increase \nin the number of project-level code contributions but also an 8% increase in coordination time for code \nintegration. These findings indicate a tradeoff between contribution gains and coordination time in the OSS \ndevelopment following the adoption of Copilot. Further analysis of the underlying mechanism suggests that \nthe observed increase in project-level code contributions is accompanied by a significant increase in both \nindividual code contributions and developer participation. At the same time, the increase in coordination \ntime is driven by a higher volume of discussions surrounding code contributions, a broader set of developers \nparticipating in these discussions, and greater discussion intensity per developer. Importantly, the combined \neffect of these two competing forces still yields an overall positive effect on the project-level productivity, \nmeasured by the total code contributions with timely integration into the codebase. \nFurthermore, we find that compared to core developers, AI pair programmers lead to a smaller \nincrease in project-level code contributions made by peripheral developers; following the adoption of AI \npair programmers"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_005",
    "source_id": "OpenSourceImpact2024",
    "text": " underlying mechanism suggests that \nthe observed increase in project-level code contributions is accompanied by a significant increase in both \nindividual code contributions and developer participation. At the same time, the increase in coordination \ntime is driven by a higher volume of discussions surrounding code contributions, a broader set of developers \nparticipating in these discussions, and greater discussion intensity per developer. Importantly, the combined \neffect of these two competing forces still yields an overall positive effect on the project-level productivity, \nmeasured by the total code contributions with timely integration into the codebase. \nFurthermore, we find that compared to core developers, AI pair programmers lead to a smaller \nincrease in project-level code contributions made by peripheral developers; following the adoption of AI \npair programmers, there is also a larger increase in coordination time for integrating code contributed by \nperipheral developers. These results are consistent with our hypothesis that due to the different levels of \nproject familiarity held by core versus peripheral developers and the limitations of generative AI tools, \nperipheral developers may realize less productivity gain from AI pair programmers than core developers. \nOur study provides several contributions to the literature. First, it contributes to the literature on \ngenerative AI in software development (Imai 2022, Barke et al. 2023, Peng et al. 2023, Cui et al. 2024). \nWhile prior research has shown that generative AI improves individual developer productivity (e.g., Peng \net al. 2023; Cui et al. 2024), less is known about its impact on voluntary participation in collaborative \n 5 \nsoftware development. Existing studies suggest that generative AI might reduce voluntary participation in \nQ&A communities by substituting for information exchange (Xu et al. 2023, Burtch et al. 2024). However, \nOSS communities are fundamentally different in that they involve not only information sharing but also \ncomplex problem solving and team collaboration. To our knowledge, we are among the first to show \ngenerative AI encourages more participation in OSS development, including both coding and non-coding \nparticipation (i.e., code discussions). \nSecond, our study contributes to the literature on generative AI in team-based collaboration (Li et \nal. 2024, Dell'Acqua et al. 2025). While prior work has examined the impact of generative AI within \ntraditional teams characterized by fixed size and formal coordination processes for performing a common \ntask, we extend this work to open collaborative environments, where team composition is fluid, \nparticipation is voluntary, and individuals perform distinct tasks that must be integrated. Our study is among \nthe first to uncover some unexpected impacts of generative AI tools\u2014because these AI tools encourage \ndevelopers\u2019 participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_006",
    "source_id": "OpenSourceImpact2024",
    "text": " of generative AI tools\u2014because these AI tools encourage \ndevelopers\u2019 participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023, Cui et al. 2024). Our results draw a sharp contrast with \nthese findings, as we demonstrate that peripheral developers obtain less productivity gain from AI pair \nprogrammers than core developers in OSS settings, potentially because the former do not possess important \nand necessary contextual knowledge about an OSS project to effectively use the AI tools. \n2. Literature Review  \n2.1 Generative AI in Software Development \nA growing body of literature has started to examine the impact of generative AI on software development. \nMost studies have focused on the implications of generative AI for individual productivity on specific tasks. \n 6 \nFor example, Imai (2022) finds that GitHub Copilot produces more lines of code than a human pair \nprogrammer when completing a task in Python. Peng et al. (2023) find that GitHub Copilot enables \nindividual developers to implement an HTTP server 55.8% faster than those not using the tool. Hoffmann \net al. (2024) show that GitHub Copilot causes individual developers to shift focus towards coding tasks and \naway from project management.  \nDespite these insights, research on how generative AI influences project-level outcomes for \ncomplex tasks involving multiple developers remains limited. Yeverechyahu et al. (2024) investigate the \ninnovation capabilities of generative AI, particularly its role in extrapolative versus interpolative thinking, \nand compare its influence on innovation in Python versus R. Different from the existing literature and built \nupon the OSS literature, our hypotheses are motivated by the unique characteristics of OSS, namely, the \nsoftware development process in an open and collaborative environment. Because participation is often \nvoluntary and coordination does not follow formal centralized processes in this environment, it remains \nunclear how generative AI influences open participation in both coding and non-coding activities, as well \nas team coordination, all of which could have important implications for project-level software \ndevelopment productivity. \nIn addition, existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al."
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_007",
    "source_id": "OpenSourceImpact2024",
    "text": " existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al. 2012). \nTherefore, the prediction from prior studies on how individuals with different levels of skills benefit from \ngenerative AI provides limited insights on how it influences core and peripheral developers within OSS \ncommunities. We seek to bridge this important gap in the literature by investigating the differential impact \nof generative AI on core versus peripheral developers. \n 7 \n2.2 Generative AI and Voluntary Participation \nVoluntary participation is essential to online communities. Prior research suggests that individuals \ncontribute to online communities not only to exchange information, but also to derive social benefits (Zhang \nand Zhu 2011). With generative AI tools capable of producing content autonomously, there have been \ngrowing concerns that AI tools may crowd out human contributions. For example, studies on platforms \nsuch as Stack Overflow find that generative AI reduces participation by replacing straightforward \ninformation exchange (Xu et al. 2023, Burtch et al. 2024). Similarly, research on freelancer platforms shows \na decline in job postings for simple coding tasks (Demirci et al. 2025).  \nHowever, these online communities differ from collaborative environments such as OSS \ndevelopment, so prior studies on the implications of AI for participation in online communities may not \nhold in the OSS setting. More specifically, beyond information exchange, developers participate in open-\nsource projects to solve complex problems, with the goals of improving skills, enhancing software quality, \nand building professional reputation (Shah 2006, Kononenko et al. 2018). On the one hand, generative AI \nmay reduce the need for deep coding expertise, potentially diminishing developers\u2019 incentives to contribute \nif they no longer view OSS as a valuable avenue for skill development or reputation building. On the other \nhand, by helping developers overcome technical challenges more efficiently, generative AI could lower \nbarriers to contribution and support the broader goals of improving code quality and sustaining \ncollaborative progress. As a result, it is unclear ex ante the overall impact of generative AI on participation \nin OSS development.  \n2.3 Generative AI in Team Coordination \nWhile team collaboration can improve problem-solving and lead to higher-quality outcomes by leveraging \ndiverse perspectives, it also introduces coordination challenges that can hinder team-level productivity and \nperformance (Janardhanan et al. 2020, Mattarelli et al. 2022). In software development, collaboration is a \ncore activity as developers jointly design and implement software development. The extent to which \ndevelopers can effectively coordinate their activities has a direct impact on software team performance \n(Koushik and Mookerjee 1995, Reagans et al"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_008",
    "source_id": "OpenSourceImpact2024",
    "text": ". As a result, it is unclear ex ante the overall impact of generative AI on participation \nin OSS development.  \n2.3 Generative AI in Team Coordination \nWhile team collaboration can improve problem-solving and lead to higher-quality outcomes by leveraging \ndiverse perspectives, it also introduces coordination challenges that can hinder team-level productivity and \nperformance (Janardhanan et al. 2020, Mattarelli et al. 2022). In software development, collaboration is a \ncore activity as developers jointly design and implement software development. The extent to which \ndevelopers can effectively coordinate their activities has a direct impact on software team performance \n(Koushik and Mookerjee 1995, Reagans et al. 2016, Mawdsley et al. 2022). In the OSS setting, team \n 8 \nmembers are often globally distributed, participation is voluntary, and team composition is fluid. Without \ncentralized task allocation or hierarchical oversight, developers must self-organize and manage \ninterdependent tasks (Lindberg et al. 2024). This fluid and decentralized structure makes efficient \ncoordination challenging in the OSS development environment. \nAlthough some research has examined coordination in open-source projects (Medappa and \nSrivastava 2019), little is known about how emerging technologies, such as generative AI, shape \ncoordination in open collaboration. Li et al. (2024) demonstrate that the use of generative AI does not \nenhance coordination within teams, largely due to the increased volume and diversity of ideas it introduces, \nwhich can hinder convergence. Conversely, Dell\u2019Acqua et al. (2025) find that generative AI facilitates \ncoordination during idea generation in cross-functional teams by supporting alignment across disciplinary \nboundaries. In both studies, team size is held constant, and coordination is operationalized as the collective \ndevelopment of a single, shared solution. As such, the observed coordination primarily reflects convergence \ntoward a common outcome, rather than the integration of heterogeneous, individual contributions. \nMoreover, these studies do not examine how differences in participants\u2019 familiarity with the task may \ninfluence coordination performance. To the best of our knowledge, we are among the first to examine the \nimpact of generative AI on team coordination in an open collaboration setting, where team size and \ncomposition are fluid and individuals are performing distinct coding tasks in a self-organizing manner. We \nfocus specifically on coordination time for code integration, a key indicator of coordination efficiency that \nis critical for rapid iteration and timely delivery of software.  \n3. Theoretical Motivation \n3.1 AI Pair Programmers and Project-level Code Contributions  \nOpen-source software (OSS) development is characterized by a fully decentralized and open environment, \nwhere a group of voluntary software developers work together to develop and refine code, subsequently \nmaking it accessible to fellow developers and the broader community (Levine and Prietula 2014).  \nAI pair programmers, guided by explicit prompts from developers, offer immediate code \nsuggestions and corrections. These tools support software developers by fixing bugs, proposing \n 9 \nenhancements, and facilitating the transfer of coding knowledge across various domains. 1  AI pair \nprogrammers use extensive databases and advanced machine learning algorithms to provide suggestions \nthat follow best practices, reducing search time. This helps lower the time developers spend writing code \nand waiting for peer"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_009",
    "source_id": "OpenSourceImpact2024",
    "text": "  \nOpen-source software (OSS) development is characterized by a fully decentralized and open environment, \nwhere a group of voluntary software developers work together to develop and refine code, subsequently \nmaking it accessible to fellow developers and the broader community (Levine and Prietula 2014).  \nAI pair programmers, guided by explicit prompts from developers, offer immediate code \nsuggestions and corrections. These tools support software developers by fixing bugs, proposing \n 9 \nenhancements, and facilitating the transfer of coding knowledge across various domains. 1  AI pair \nprogrammers use extensive databases and advanced machine learning algorithms to provide suggestions \nthat follow best practices, reducing search time. This helps lower the time developers spend writing code \nand waiting for peer support, thereby accelerating the code production. Recent research has documented \nthe beneficial effects of AI pair programmers on individual developers\u2019 productivity (Imai 2022).  \nBesides the positive effect on individual productivity as documented in the prior literature, we argue \nthat AI pair programmers can also lead to more participation in the development of a focal OSS project (i.e., \nmore OSS code contributors), due to the following reasons. According to the expectancy value models, \ndevelopers evaluate both the expected outcomes and the value of participation when deciding whether to \ncontribute to a project (Atkinson 1957, Hertel et al. 2003, Setia et al. 2012). Given their limited time, \ndevelopers would select projects that are both meaningful and rewarding, balancing the costs of \nparticipation against the potential benefits (Wen et al. 2013). Traditionally, developers needed specialized \nknowledge to effectively contribute to OSS projects, creating significant entry barriers. AI pair \nprogrammers help reduce these barriers by shortening the learning curve and lowering the time and effort \nneeded to contribute. This allows developers to contribute more easily to different projects. By lowering \nentry barriers and reducing the cost of participation, AI pair programmers could encourage more developers \nto participate in a focal OSS project. \nTaken together, because AI pair programmers not only enable greater intensity of individual code \ncontributions but also encourage more developers to participate in the project, we expect AI pair \nprogrammers to result in an increase in total code contributions to the project. \nHypothesis 1: AI pair programmers lead to an increase in project-level code contributions in OSS \ndevelopment. \n3.2 AI Pair Programmers and Coordination Time \n \n1 The GitHub website discusses the use cases of Copilot: https://github.com/features/copilot. https://github.blog/2022-\n09-14-8-things-you-didnt-know-you-could-do-with-github-copilot/.  \n 10 \nUnlike traditional software teams, OSS projects involve distributed developers and operate without formal \nhierarchies or centralized oversight, making coordination more essential for maintaining coherence and \nprogress. As OSS teams grow in size and diversity, the complexity of integrating code contributions \nincreases (Weber 2006, Feri et al. 2010, Yu et al. 2015, Roels and Corbett 2024). Differences in \ndevelopment styles, coding conventions, design philosophies and architectural decisions can exacerbate \nintegration challenges and, if not adequately managed, may undermine team effectiveness (Ancona and \nCaldwell 1992, Janardhanan et al. 2020, Mattarelli et al."
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_010",
    "source_id": "OpenSourceImpact2024",
    "text": "  \n 10 \nUnlike traditional software teams, OSS projects involve distributed developers and operate without formal \nhierarchies or centralized oversight, making coordination more essential for maintaining coherence and \nprogress. As OSS teams grow in size and diversity, the complexity of integrating code contributions \nincreases (Weber 2006, Feri et al. 2010, Yu et al. 2015, Roels and Corbett 2024). Differences in \ndevelopment styles, coding conventions, design philosophies and architectural decisions can exacerbate \nintegration challenges and, if not adequately managed, may undermine team effectiveness (Ancona and \nCaldwell 1992, Janardhanan et al. 2020, Mattarelli et al. 2022). While many open-source projects adopt \nmodular designs and parallel development strategies to reduce interdependencies (Howison and Crowston \n2014), evidence suggests that significant code and developer interdependencies remain (Lindberg et al. \n2016). These interdependencies underscore the need for effective coordination to align efforts, resolve \nconflicts, and ensure that individual contributions integrate effectively into the existing codebase. \nTherefore, besides making code contributions to the codebase, developers also spend a significant \namount of time and efforts managing project-level dependencies and ensuring cohesive and aligned actions \namong team members. Coordination theory highlights communication as a central mechanism (Malone and \nCrowston 1994, Pikkarainen et al. 2008), as communication enables teams to align objectives, share \nprogress updates, resolve interdependencies, and establish a common ground (Okhuysen and Bechky 2009). \nThis is especially critical in interdependent task environments, where coordination requires continuous \nadjustments and information exchange (Srikanth and Puranam 2014, Oliveira and Lumineau 2017, Im and \nAhuja 2023). In OSS settings, online discussions serve as a primary channel for communication (Roberts \net al. 2006), helping developers clarify project goals, receive feedback on contributions, and resolve \nintegration conflicts (Harbring 2006). Motivated by these observations, in this study, we focus on \ncoordination time for code integration, which is often spent on discussing code and resolving conflicts and \ndifferent ideas among developers (Howison and Crowston 2014, Lindberg et al. 2016, Medappa and \nSrivastava 2019, Shaikh and Vaast 2023). \nBesides encouraging coding activities as discussed above, AI pair programmers may also \nencourage more participation in discussions on the focal project, which could in turn lead to longer \n 11 \ncoordination time for code integration, for reasons as follows. AI pair programmers can assist developers \nin understanding algorithms and exploring alternative solutions (Barke et al. 2023). This improved code \ncomprehensibility increases individual developers\u2019 capacity to contribute comments and participate in \ndiscussions. It also lowers the barrier to participation for those who might otherwise be discouraged by \nunfamiliar or intricate code. In addition, by automating repetitive coding tasks, AI pair programmers enable \ndevelopers to have more time to provide feedback and participate in discussions about implementation and \ndesign decisions. As more developers participate in code discussions with each developer contributing more \nthoughts and ideas, it could potentially become more difficult to align perspectives and reach consensus. \nAs"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_011",
    "source_id": "OpenSourceImpact2024",
    "text": " \ncoordination time for code integration, for reasons as follows. AI pair programmers can assist developers \nin understanding algorithms and exploring alternative solutions (Barke et al. 2023). This improved code \ncomprehensibility increases individual developers\u2019 capacity to contribute comments and participate in \ndiscussions. It also lowers the barrier to participation for those who might otherwise be discouraged by \nunfamiliar or intricate code. In addition, by automating repetitive coding tasks, AI pair programmers enable \ndevelopers to have more time to provide feedback and participate in discussions about implementation and \ndesign decisions. As more developers participate in code discussions with each developer contributing more \nthoughts and ideas, it could potentially become more difficult to align perspectives and reach consensus. \nAs a result, given the same amount of code contributions to an OSS project, AI pair programmers could \npotentially lead to longer coordination time for the code to be integrated into the codebase of the project. \nHypothesis 2: AI pair programmers lead to an increase in coordination time for integrating code \ncontributions in OSS development. \n3.3 Core and Peripheral Developers \nThe OSS development process involves two categories of developers: core and peripheral (Setia et al. 2012). \nCore developers, who often include the projects\u2019 administrators and key maintainers, are responsible for \ndefining the overarching objectives and ensuring the delivery of the final code product. As shown in Figure \n1, software development usually begins with core developers designing the primary codebase and hosting \nit on a platform for open collaboration (Singh and Phelps 2013). With the write access and full control over \nthe projects, core developers can submit code and either directly integrate it into the primary codebase or \nhave it reviewed by other core developers for integrity.  \nOn the other hand, peripheral developers introduce additional perspectives shaped by different \ndomains, use cases, and user needs. Peripheral developers typically participate on a more limited basis and \noften contribute across multiple projects (Howison and Crowston 2014, Krishnamurthy et al. 2016). They \nmainly focus on enhancing the existing codebase such as fixing bugs and adding new features (Setia et al. \n2012). Meanwhile, without the write access to codebases, peripheral developers\u2019 code submissions need to \n 12 \nbe reviewed by core developers, who decide whether to accept the code changes, request additional \nmodifications, or reject them (Gousios et al. 2014, Medappa and Srivastava 2019). \nBecause of the different roles and responsibilities assumed by core versus peripheral developers, a \nkey distinction between core developers and peripheral developers lies in their level of contextual \nknowledge about the OSS project, which refers to understanding of a software project on aspects such as \nits overall design, architecture, coding norms and practices (labeled as \u201cproject familiarity\u201d for simplicity). \nIn particular, due to their intensive involvement with the project, core developers typically possess higher \nfamiliarity with the project\u2019s architecture, decision history, and development norms. On the other hand, \nbecause peripheral developers often participate on an irregular basis and complete specific tasks such as \nfixing bugs and adding patches, they tend to have more limited familiarity with the project.  \nThis difference in project familiarity becomes particularly relevant when considering the use of AI \npair programmers. While AI pair programmers can facilitate code generation, they may not be able to"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_012",
    "source_id": "OpenSourceImpact2024",
    "text": " \nknowledge about the OSS project, which refers to understanding of a software project on aspects such as \nits overall design, architecture, coding norms and practices (labeled as \u201cproject familiarity\u201d for simplicity). \nIn particular, due to their intensive involvement with the project, core developers typically possess higher \nfamiliarity with the project\u2019s architecture, decision history, and development norms. On the other hand, \nbecause peripheral developers often participate on an irregular basis and complete specific tasks such as \nfixing bugs and adding patches, they tend to have more limited familiarity with the project.  \nThis difference in project familiarity becomes particularly relevant when considering the use of AI \npair programmers. While AI pair programmers can facilitate code generation, they may not be able to \nautomatically fully incorporate a software project's specific context, such as broader architectural structure, \ninterdependencies, performance considerations, into the code they generated (Feldman et al. 2023, Liguori \net al. 2024, Pi\u00f1eiro-Mart\u00edn et al. 2025). Human developers remain essential for interpreting high-level \ndesign intent, validating architectural coherence, and ensuring that code generated by the AI tools is \ncompatible with the overall OSS project vision. In this context, the developer\u2019s familiarity with the project \nbecomes a critical resource in maximizing the effective use of AI pair programmers. \nGiven the difference in project familiarity between core developers and peripheral developers and \nthe limitations of AI pair programmers, we expect these two types of developers may realize different levels \nof productivity gain from AI pair programmers. In the following sub-sections, we first discuss how the \nimpact of AI pair programmers on project-level code contributions may vary between those from core and \nperipheral developers, followed by discussions on how the impact on coordination time for code integration \ncould differ for code submitted by these two types of developers. \n3.3.1. AI Pair Programmers and Code Contributions for Core vs. Peripheral Developers \n 13 \nAs noted above, core developers play a central role in defining the project\u2019s architecture and guiding its \nlong-term development (Crowston et al. 2006). Given their familiarity with the project\u2019s structure, coding \nconventions, and historical evolution, core developers may be able to craft more effective prompts and \ncritically evaluate AI-generated code within the context of project requirements. Moreover, due to the \nproductivity gain enabled by AI pair programmers, core developers may be more willing to contribute; they \nmay also be more motivated to reallocate their time and efforts towards more coding activities from project \nmanagement activities (Hoffmann et al. 2024). As a result, the individual code contributions made by a \ncore developer to a focal project could be significantly boosted by AI pair programmers.  \nIf AI pair programmers can effectively reduce the time and effort required for core developers to \ncode, the cost of participating in coding activities could also be lowered. Accordingly, a focal OSS project \nmay attract more core developers aided by these AI tools to participate. Thus, we expect AI pair \nprogrammers could lead to a significant increase in total project-level code contributions made by core \ndevelopers. In contrast, because peripheral developers tend to have limited contextual knowledge about a \nfocal OSS project, they may not be able to realize as much productivity gain from AI pair programmers as \ncore developers, for reasons as follows. First, they may struggle to frame contextually appropriate prompts"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_013",
    "source_id": "OpenSourceImpact2024",
    "text": "core developer to a focal project could be significantly boosted by AI pair programmers.  \nIf AI pair programmers can effectively reduce the time and effort required for core developers to \ncode, the cost of participating in coding activities could also be lowered. Accordingly, a focal OSS project \nmay attract more core developers aided by these AI tools to participate. Thus, we expect AI pair \nprogrammers could lead to a significant increase in total project-level code contributions made by core \ndevelopers. In contrast, because peripheral developers tend to have limited contextual knowledge about a \nfocal OSS project, they may not be able to realize as much productivity gain from AI pair programmers as \ncore developers, for reasons as follows. First, they may struggle to frame contextually appropriate prompts \nto generate code. Since AI-generated code is highly dependent on the quality and specificity of the prompts \n(Feldman et al. 2023, Liguori et al. 2024, Pi\u00f1eiro-Mart\u00edn et al. 2025), peripheral developers may generate \nless relevant or misaligned code. Second, because peripheral developers tend to lack comprehensive \nknowledge of its architectural decisions, development history, and coding norm, they may not recognize \nwhen AI outputs violate project-specific conventions. As a result, their code generated through AI tools is \nmore likely to require revision, delay integration, or be rejected (Zhang et al. 2022).  \nTherefore, while AI pair programmers could benefit peripheral developers by facilitating their code \ngeneration and reducing their cost of participation in an OSS project, the benefit could be constrained by \ntheir limited ability in leveraging the AI tools due to their lack of familiarity with a focal project. This may \nnot only lead to a smaller increase in individual code contributions but also a smaller increase in their \nincentive to participate, compared to core developers. Overall, we expect AI pair programmers lead to a \n 14 \nsmaller increase in total project-level code contributions made by peripheral developers than those from \ncore developers.   \nHypothesis 3a: AI pair programmers lead to a smaller increase in project-level code contributions made \nby peripheral developers compared to core developers.  \n3.3.2. AI Pair Programmers and Coordination Time for Core vs. Peripheral Developers \nWe have argued above that AI pair programmers may encourage developers to participate in code \ndiscussions more, which could lead to longer coordination time for code integration. We further expect that \nthe extent of such an increase in coordination time could also depend on the code being submitted. Because \nof the difference in project familiarity between peripheral developers and core developers, we expect the \nincrease in coordination time could be even bigger for code contributions made by peripheral developers \nthan those made by core developers, for reasons as follows. \nWhen AI pair programmers assist peripheral developers, the limited contextual knowledge held by \nperipheral developers suggests that they may generate code that is semantically unclear or misaligned with \nthe project, as generative AI relies on rick contextual information to perform effectively (Feldman et al. \n2023, Liguori et al. 2024, Pi\u00f1eiro-Mart\u00edn et al. 2025). This is likely because the AI tools generate code \nbased on generalized training data rather than project-specific patterns, and peripheral developers often fail \nto provide complete context in their prompts. Although AI pair programmers can help peripheral developers \nproduce code quickly, the output may potentially be inconsistent"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_014",
    "source_id": "OpenSourceImpact2024",
    "text": " those made by core developers, for reasons as follows. \nWhen AI pair programmers assist peripheral developers, the limited contextual knowledge held by \nperipheral developers suggests that they may generate code that is semantically unclear or misaligned with \nthe project, as generative AI relies on rick contextual information to perform effectively (Feldman et al. \n2023, Liguori et al. 2024, Pi\u00f1eiro-Mart\u00edn et al. 2025). This is likely because the AI tools generate code \nbased on generalized training data rather than project-specific patterns, and peripheral developers often fail \nto provide complete context in their prompts. Although AI pair programmers can help peripheral developers \nproduce code quickly, the output may potentially be inconsistent with the implicit design principles, create \nintegration challenges, or require substantial clarification. These issues could lead to many comments from \nother developers to request the focal peripheral developer to clarify design intent, align code with norms, \nand resolve ambiguities (Gousios et al. 2014). Consequently, their code is more likely to prompt extended \ndiscussions, leading to relatively long coordination time. \nIn contrast, core developers\u2019 deep project familiarity allows them to better leverage AI pair \nprogrammers to generate code that can be integrated well into the existing system. As a result, for core \ndevelopers, their code contributions facilitated by AI pair programmers may require fewer revisions or \n 15 \nclarifications. This suggests less coordination time would be needed for integrating their code contributions \ninto the codebase. \nHypothesis 3b: AI pair programmers lead to a larger increase in coordination time for integrating code \ncontributed by peripheral developers compared to the code contributed by core developers. \n4. Data \nWe use GitHub data for our empirical analysis where a repository serves as the basic unit for collaborative \nsoftware development (Dabbish et al. 2012). We investigate how the introduction of AI pair programmer \nGitHub Copilot impacts project-level code contributions and coordination time for GitHub repositories. \nCopilot's introduction occurred in stages, commencing with limited availability in June 2021 and a formal \npublic launch in June 20222. During the one-year period between June 2021 and June 2022, Copilot \nunderwent a technical preview phase. Based on this timeline, we collect panel data of GitHub repositories \nfrom GitHub Archive Dataset, spanning a two-year timeframe, from January 2021 to December 20223.  \nTo ensure the generalizability of our analysis, we follow established procedures in the literature \n(Kalliamvakou et al. 2014, AlMarzouq et al. 2020) to identify active OSS repositories during our panel \nperiod. We select repositories with non-zero size, at least one specified programming language and license, \na description, and no mirror or personal store designation. To exclude ghost or abandoned projects, we \nrequire at least one code submission every six months from 2021 to 2022 and at least one additional activity, \nsuch as a release or creation, each year. As we are interested in evaluating project-level code contributions \nand coordination time involving multiple developers, we focus on repositories with at least three developers \ncontributing each month. Additionally, as discussed in greater detail below, we use IDE4 information to \n \n2 GitHub launched the technical preview of Copilot in June 2021: https://github.blog/2021"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_015",
    "source_id": "OpenSourceImpact2024",
    "text": " during our panel \nperiod. We select repositories with non-zero size, at least one specified programming language and license, \na description, and no mirror or personal store designation. To exclude ghost or abandoned projects, we \nrequire at least one code submission every six months from 2021 to 2022 and at least one additional activity, \nsuch as a release or creation, each year. As we are interested in evaluating project-level code contributions \nand coordination time involving multiple developers, we focus on repositories with at least three developers \ncontributing each month. Additionally, as discussed in greater detail below, we use IDE4 information to \n \n2 GitHub launched the technical preview of Copilot in June 2021: https://github.blog/2021-06-29-introducing-github-\ncopilot-ai-pair-programmer/. It then announced the formal launch and public availability of Copilot in June 2022: \nhttps://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/. \n3 We choose this endpoint to ensure our results are not influenced by the rise in popularity of Chat GPT, which began \nin early 2023.  \n4 IDE stands for integrated development environment, which is a software application that provides local environments \nfor coding, testing, and debugging. \n 16 \nidentify repositories in the treatment group versus those in the control groups, so we further restrict our \nsample to those that disclose IDE information. These criteria result in a sample of 9,244 repositories. \nTo identify repositories where Copilot was used by developers (i.e., the treatment group), we \ncollaborated with GitHub organization, which provided proprietary aggregated Copilot usage data at the \nrepository level. This dataset indicates the monthly proportion of developers contributing code to a given \nrepository who also used Copilot.5 Since Copilot requires a compatible IDE, we also consider IDE usage. \nDuring our analysis period, only a limited number of IDEs supported Copilot: Visual Studio Code, the \nJetBrains suite of IDEs, Neovim, and Visual Studio.6 To determine IDE usage, we examine the webpages \nof the repositories in our sample to gather information on the IDE usage by contributing developers. We \ncategorize repositories with developers who used Copilot and one of the supported IDEs as our treatment \ngroup, and those using unsupported IDEs and not using Copilot as our control group. In total, our sample \nincludes 5,687 repositories in the treatment group and 3,557 repositories in the control group. We designate \nthe first month when Copilot was supported by IDEs and used by a non-zero proportion of developers as \nthe treatment start time for each repository. The months before this time are defined as the pre-treatment \nperiod and the months after this time as the post-treatment period for each repository, covering the two-\nyear span from 2021 to 2022. \nTo further improve comparability, we restrict the sample to repository-month observations with at \nleast one code contribution. This focus allows us to estimate Copilot\u2019s effect on collaborative development \nin actively maintained projects. Moreover, because the analysis of coordination time requires at least one \ncode contribution to compute acceptance time, this criterion allows us to use the same sample to investigate \nH1 and H2. The final sample includes 7,637 repositories, with 4"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_016",
    "source_id": "OpenSourceImpact2024",
    "text": " of developers as \nthe treatment start time for each repository. The months before this time are defined as the pre-treatment \nperiod and the months after this time as the post-treatment period for each repository, covering the two-\nyear span from 2021 to 2022. \nTo further improve comparability, we restrict the sample to repository-month observations with at \nleast one code contribution. This focus allows us to estimate Copilot\u2019s effect on collaborative development \nin actively maintained projects. Moreover, because the analysis of coordination time requires at least one \ncode contribution to compute acceptance time, this criterion allows us to use the same sample to investigate \nH1 and H2. The final sample includes 7,637 repositories, with 4,491 in the treatment group and 3,146 in \nthe control group. Table 1 provides descriptions and summary statistics for repository-month-level variables \nused in the main analysis. We check the robustness of our results by removing the restriction on the number \n \n5 We do not know the identity of individual Copilot users because of privacy concerns. Although such Copilot usage \nis measured at the GitHub platform level, developers are likely to integrate Copilot into their workflows extensively \nand thus would use it for all repositories where it is feasible (Maranguni\u0107 and Grani\u0107 2015).  \n6 GitHub lists the supported IDEs for Copilot: https://github.com/features/copilot. \n 17 \nof code contribution. Additionally, we test the robustness using a larger sample that includes repositories \nwith as few as two developers.  \n[insert Table 1 here] \n5. Empirical Analyses \n5.1 Measures \nTo test H1 and H2, the objective in our empirical analyses is to determine how Copilot influences project-\nlevel code contributions and coordination time for code integration. To measure project-level code \ncontributions, we use the total number of pull requests (PRs) submitted to each repository that were \neventually merged7. PRs represent code changes submitted by developers and need further evaluation by \ncore developers. This evaluation results in either approval, leading to merged PRs, or rejection, leading to \nclosed but unmerged PRs. Thus, merged PRs reflect successful code changes that have been eventually \nincorporated into the development of repositories and are commonly used in the literature to assess \nmeaningful contributions by developers (Gousios et al. 2014, Tsay et al. 2014, Kononenko et al. 2018).  \nTo evaluate coordination time for code integration, we follow prior work in both economics \n(Simcoe 2012) and software engineering (Espinosa et al. 2007, Yu et al. 2015, El Mezouar et al. 2019) by \nmeasuring the duration between the submission and acceptance of each PR. Because a repository may \nreceive many PRs in a month, we then compute the average duration (i.e., average time) to merge a PR for \na repository in a month.  \n5.2 Model and Estimation \nIn our setup, repositories adopt Copilot at different times after it is available. The variation in treatment \ntiming and the larger number of treated repositories compared to untreated ones complicate the application \nof traditional matching techniques in this context. Given these complexities, we adopt the Generalized \nSynthetic Control Method (G"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_017",
    "source_id": "OpenSourceImpact2024",
    "text": " al. 2007, Yu et al. 2015, El Mezouar et al. 2019) by \nmeasuring the duration between the submission and acceptance of each PR. Because a repository may \nreceive many PRs in a month, we then compute the average duration (i.e., average time) to merge a PR for \na repository in a month.  \n5.2 Model and Estimation \nIn our setup, repositories adopt Copilot at different times after it is available. The variation in treatment \ntiming and the larger number of treated repositories compared to untreated ones complicate the application \nof traditional matching techniques in this context. Given these complexities, we adopt the Generalized \nSynthetic Control Method (GSCM) (Xu 2017), which allows for improved estimation of treatment effects \nby generating weighted synthetic controls that closely resemble the treated units prior to treatment. This \n \n7 More specifically, we consider all PRs initiated in a month that get merged anytime between that month and 12/2023 \nas code contributions for that month and call these as merged pull requests. \n 18 \napproach combines the idea of a synthetic control (Abadie et al. 2010) with interactive fixed effects (Bai \n2009). This allows us to address multiple treated units with staggered treatment times while accounting for \ntime-varying unobservables. The GSCM shares core assumptions with the standard synthetic control \nmethod, effectively managing unobserved time-variant confounders by giving more weight to control units \nthat mirror the pre-treatment trends of the treatment group (Xu 2017). This approach offers improved pre-\ntreatment control and more credible parallel trends relative to traditional difference-in-differences (DID) \nand matching methods and has been shown to exhibit lower bias (Xu 2017, Wang et al. 2021).  \nWe use the following linear factor model (Bai 2009, Xu 2017) with the unit of analysis at the \nrepository-month level:  \n\ud835\udc4c!\" = d!\" \ud835\udc37!\" + \ud835\udc4b!\"\n# b + l!\n#\ud835\udc53\"  + e!\"                                   (1) \nwhere \ud835\udc4c!\" represents the outcome variable, which is either the project-level code contributions measured by \ntotal number of merged PRs of repository \ud835\udc56 in month \ud835\udc61, or coordination time for code integration measured \nby the average time taken to merge a PR of repository \ud835\udc56 in month \ud835\udc61. We take log transformation to reduce \nthe skewness of both variables. \ud835\udc37!\" is the treatment indicator which equals one if repository \ud835\udc56 has been \ndeveloped with Copilot by month \ud835\udc61 and zero otherwise. The parameter of primary interest is d!\", which \nsignifies the dynamic impact of Copilot on log number of merged PRs and log average time taken to merge \na PR. d!\" is heterogeneous treatment effect and its subscripts \ud835\udc56 and \ud835\udc61 indicate that the estimates vary across \nrepositories and months.  \nIn the above equation, \ud835\udc53\" = [\ud835\udc53$\", \u2026 , \ud835\udc53%\"]# is an (\ud835\udc5f\u00d7 1) vector of unobserved common factors \nassociated with factor loadings l!\n# = [\ud835\udf06!$, \u2026 , \ud835\udf06!%"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_018",
    "source_id": "OpenSourceImpact2024",
    "text": " if repository \ud835\udc56 has been \ndeveloped with Copilot by month \ud835\udc61 and zero otherwise. The parameter of primary interest is d!\", which \nsignifies the dynamic impact of Copilot on log number of merged PRs and log average time taken to merge \na PR. d!\" is heterogeneous treatment effect and its subscripts \ud835\udc56 and \ud835\udc61 indicate that the estimates vary across \nrepositories and months.  \nIn the above equation, \ud835\udc53\" = [\ud835\udc53$\", \u2026 , \ud835\udc53%\"]# is an (\ud835\udc5f\u00d7 1) vector of unobserved common factors \nassociated with factor loadings l!\n# = [\ud835\udf06!$, \u2026 , \ud835\udf06!%] and e!\" is the error term with a mean of zero. The factor \ncomponent l!\n#\ud835\udc53\" can be expressed as l!\n#\ud835\udc53\" = l!$\ud835\udc53$\" + l!&\ud835\udc53&\" + \u22ef+ l!%\ud835\udc53%\".  The factor component nests a \nrange of unobserved heterogeneities including additive unit and time fixed effects and unit-specific linear \nand quadratic time trends. Note that a two-way fixed effects specification is a special case of the factor \ncomponent, where \ud835\udc5f= 2 , \ud835\udc53$\" = 1 , and l!& = 1 , so that l!\n#\ud835\udc53\" = l!$ + \ud835\udc53&\" . Here, l!$  represents the \n 19 \nrepository fixed effects, while \ud835\udc53&\" is the month fixed effects. We specify the two-way fixed effects to \naccount for heterogeneity across repositories and time, while considering other unobserved latent factors.  \nWe estimate the optimal number of latent factors using a cross-validation procedure (Xu 2017). \nBriefly, this involves first estimating the parameters of model using the control group data only and \nemploying a cross-validation procedure to determine the number of latent factors r. Next, the optimal \nnumber of factor loadings for each treated unit is estimated by minimizing the mean squared errors of the \npredicted treated outcomes in the pre-treatment periods. We provide a detailed explanation of the model \nand the estimation including the determination of the optimal latent factors in Online Appendix A. After \naccounting for time and repository fixed effects, the optimal number of unobserved factors determined by \nthe cross-validation technique is zero. This suggests that the fixed effects setting has effectively accounted \nfor any unobserved time-varying characteristics (Xu 2017).   \nThe GSCM estimator predicts the counterfactuals for treated units in the post-treatment periods \nusing the parameter estimates obtained in the previous two steps. The causal effect of the treatment is \ncalculated as the average treatment effect on the treated (ATT), based on the differences between the \nobserved outcome of a treated unit \ud835\udc4c!\"(1) and its constructed counterfactual \ud835\udc4c6!\"(0): \n\ud835\udc34\ud835\udc47\ud835\udc47\" = \n$\n|\ud835\udcaf| \u2211\n;\ud835\udc4c!\"(1) \u2212 \ud835\udc4c6!\"(0) = = \n$\n|\ud835\udcaf| \u2211\nd!\"\n!\u2208\ud835\udcaf\n!\u2208\ud835\udcaf\n    (2) \nwhere"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_019",
    "source_id": "OpenSourceImpact2024",
    "text": " for treated units in the post-treatment periods \nusing the parameter estimates obtained in the previous two steps. The causal effect of the treatment is \ncalculated as the average treatment effect on the treated (ATT), based on the differences between the \nobserved outcome of a treated unit \ud835\udc4c!\"(1) and its constructed counterfactual \ud835\udc4c6!\"(0): \n\ud835\udc34\ud835\udc47\ud835\udc47\" = \n$\n|\ud835\udcaf| \u2211\n;\ud835\udc4c!\"(1) \u2212 \ud835\udc4c6!\"(0) = = \n$\n|\ud835\udcaf| \u2211\nd!\"\n!\u2208\ud835\udcaf\n!\u2208\ud835\udcaf\n    (2) \nwhere \ud835\udcaf denotes the set of treated units and |\ud835\udcaf| represents the number of units in \ud835\udcaf.  \nIdentification: Repository fixed effects account for static unobservable differences across \nrepositories such as the characteristics of the projects or the developers that can influence code contribution \nto these repositories and the required coordination time. Additionally, time fixed effects account for the \ncommon time trends that could influence the outcomes. There may be dynamic unobservables which could \ninfluence the outcomes. GSCM synthesizes a weighted control unit that closely mirrors the data pattern of \nthe log number of merged PRs and the log average time taken to merge a single PR during the pre-treatment \nperiod for the treated unit. The outcome variable of this synthetic control unit during the post-treatment \nperiod serves as the counterfactual prediction for the treated unit. By modeling the trend of the outcome \n 20 \nvariables, the GSCM can naturally accommodate the influence of unobservable confounders that evolve \nover time. Furthermore, it allows each treatment unit to have a different treatment period and can efficiently \nconstruct synthetic control units from a relatively small control sample. \nTo assess the validity of results, we conduct equivalence tests to examine the presence of any pre-\ntreatment trends. The results of these tests are reported in Online Appendix B and suggest that there are no \nsignificant pre-treatment differences in the outcomes between treated and corresponding synthetic control \nrepositories. Additionally, we perform placebo tests and visualize the time-varying treatment effects across \npre-treatment and post-treatment periods to ensure the robustness of our findings. Detailed results of these \nanalyses are presented in Online Appendix C. We also validate our results using different samples, matching \ntechnique, and alternative estimation as described in the following sections. \n6. Results \n6.1 Main Results \nTable 2 presents the estimated overall effects of Copilot adoption on project-level code contributions and \ncoordination time for code integration. We find that, following Copilot adoption, the number of merged \nPRs of active repositories increased by 5.9%, while the average time required to merge a single PR \nincreased by 8%. These results indicate that, following Copilot adoption, project-level code contributions \nsignificantly increased, but coordination time for code integration also became longer. \nThe increase in project-level code contributions may be driven by two key mechanisms. First, \nCopilot might increase individual code contributions by providing useful code suggestions and auto-\ncompletions, thereby streamlining the coding processes and accelerating code submissions. Second, Copilot \nmay reduce the participation barriers and costs for developers, enabling more developers to contribute to \nrepositories and thus leading to"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_020",
    "source_id": "OpenSourceImpact2024",
    "text": " integration. We find that, following Copilot adoption, the number of merged \nPRs of active repositories increased by 5.9%, while the average time required to merge a single PR \nincreased by 8%. These results indicate that, following Copilot adoption, project-level code contributions \nsignificantly increased, but coordination time for code integration also became longer. \nThe increase in project-level code contributions may be driven by two key mechanisms. First, \nCopilot might increase individual code contributions by providing useful code suggestions and auto-\ncompletions, thereby streamlining the coding processes and accelerating code submissions. Second, Copilot \nmay reduce the participation barriers and costs for developers, enabling more developers to contribute to \nrepositories and thus leading to an increase in project-level code contributions.  \nMeanwhile, the observed increase in coordination time could be attributable to increased code \ndiscussions. As discussed earlier, generative AI tools such as Copilot may improve code comprehension, \nenabling developers to propose and discuss more suggestions. Additionally, these AI tools may free up time \n 21 \nfor developers to participate more actively in code discussions, leading to longer coordination time. We \nexplore these potential drivers in detailed mechanism analyses in Section 6.3. \n[insert Table 2 here] \n6.2 Robustness Checks \nTo further investigate the robustness of our findings, we employ a range of empirical methods to examine \nthe relationship between Copilot use and the project-level code contributions and coordination time for \ncode integration. A summary of these checks is provided in Table D.1 of Online Appendix D. \nWithin-IDE Analysis: In our baseline identification, we use IDEs that supported Copilot during \nour sample period as part of the criteria to identify repositories in the treatment group and consider \nrepositories developed with IDEs that did not support Copilot for the control group. To mitigate concerns \nregarding potential unobservable differences in IDE usage, we conduct the analysis by only focusing on \nrepositories using Copilot-supported IDEs and further restricting the treatment versus control group \ncomparison within the same Copilot-supported IDE. This approach addresses three key sources of potential \nbias. First, it eliminates concerns about differences between repositories using supported versus \nunsupported IDEs, the latter of which may be associated with lower popularity. Second, it controls for \nvariation across different supported IDEs, which may attract distinct types of projects or developer teams. \nThird, it accounts for the possibility that developers who choose IDEs compatible with Copilot may differ \nfrom those who do not. By focusing on comparing repositories developed within the same Copilot-\nsupported IDE, we ensure that both treatment and control groups share the same development environment \nand that contributing developers are likely to have similar preferences and characteristics. Therefore, the \nonly variation across groups lies in the timing of Copilot adoption.  \nSpecifically, our first step is to identify the sample of repositories using Copilot-supported IDEs. \nThen, in order to identify treatment and control groups, we use repositories that adopted Copilot in the first \nhalf of the period from July 2021 to December 2022 when Copilot was available, i.e., July 2021 - March \n2022, as the treatment group. Accordingly, we conduct this analysis using only the period from January \n2021 to March 2022, so repositories that adopted Copilot after March 2022 can be considered as the control \n "
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_021",
    "source_id": "OpenSourceImpact2024",
    "text": " contributing developers are likely to have similar preferences and characteristics. Therefore, the \nonly variation across groups lies in the timing of Copilot adoption.  \nSpecifically, our first step is to identify the sample of repositories using Copilot-supported IDEs. \nThen, in order to identify treatment and control groups, we use repositories that adopted Copilot in the first \nhalf of the period from July 2021 to December 2022 when Copilot was available, i.e., July 2021 - March \n2022, as the treatment group. Accordingly, we conduct this analysis using only the period from January \n2021 to March 2022, so repositories that adopted Copilot after March 2022 can be considered as the control \n 22 \ngroup. We choose March 2022 as the endpoint so that there are enough repositories with Copilot usage \nduring this time window (i.e., a total of nine months from July 2021 to March 2022). Our second step is to \nmatch treatment and control repositories within the same IDE. After matching, we use the pooled sample \nto run the GSCM model. This analysis includes 3,215 repositories in the treatment group and 1,247 \nrepositories in the control group, a total of 4,462 repositories. The treatment turn-on time for each repository \nin the treatment group is decided in the same manner as in our baseline analysis above. The results based \non estimation using GSCM in column (1) of Tables 3 and 4 show that Copilot increased the number of \nmerged PRs and the average time taken to merge a single PR, which are consistent with our main findings. \nRefined Sample: As developers work on multiple repositories, it is possible that some may be \nworking on both treatment and control repositories. Approximately 3.5% of the selected repositories in our \nmain analysis have developers involved in both groups. Although these developers might not be using \nCopilot for the control repositories due to IDE restrictions, there is potential for knowledge transfer, which \ncould bias our results. To eliminate this bias, we refine our sample to exclusively assess the impact of \nCopilot on repositories where no developer is involved in both groups. Corresponding results are shown in \ncolumn (2) of Tables 3 and 4, indicating that Copilot increased the number of merged PRs and the average \ntime taken to merge a single PR, which are qualitatively similar to our main results. \nThere may be spillover effects wherein repositories not using Copilot could still benefit if their \ndevelopers have prior experience with Copilot from other projects. In such cases, our estimates would \nrepresent a lower bound on the true impact of Copilot on code contributions and coordination time for code \nintegration. The observed increase in code contributions would likely be greater if developers in the control \nrepositories had no prior exposure to Copilot. Similarly, the increase in coordination time would be more \npronounced when compared to repositories unaffected by such spillover effects. \nOutlier Removal: To rule out the possibility that extreme values are disproportionately influencing \nthe results, we conduct an analysis that excludes repositories exhibiting unusually high or low values in \nterms of the number of merged PRs and the average time taken to merge a single PR. Such extreme \nobservations may arise due to idiosyncratic project dynamics. By removing these outliers, we ensure that \n 23"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_022",
    "source_id": "OpenSourceImpact2024",
    "text": " bound on the true impact of Copilot on code contributions and coordination time for code \nintegration. The observed increase in code contributions would likely be greater if developers in the control \nrepositories had no prior exposure to Copilot. Similarly, the increase in coordination time would be more \npronounced when compared to repositories unaffected by such spillover effects. \nOutlier Removal: To rule out the possibility that extreme values are disproportionately influencing \nthe results, we conduct an analysis that excludes repositories exhibiting unusually high or low values in \nterms of the number of merged PRs and the average time taken to merge a single PR. Such extreme \nobservations may arise due to idiosyncratic project dynamics. By removing these outliers, we ensure that \n 23 \nour results reflect generalizable trends rather than being driven by a few atypical cases. The results, reported \nin column (3) of Tables 3 and 4, remain consistent with our main findings.  \nNon-AI Topic Projects: Another potential concern is that the observed effects may be driven by \nproject-specific enthusiasm associated with certain topics, particularly those related to artificial intelligence. \nAI-related repositories may attract different groups of developers compared to repositories focused on other \ntopics, which could confound the estimated effect of Copilot adoption. To address this concern, we conduct \na robustness check by excluding repositories associated with AI-related topics and re-estimate the models \nusing the remaining sample. This approach allows us to test whether the impact of Copilot is generalizable \nacross a broader range of repository topics and not limited to those inherently aligned with AI development. \nAs shown in column (4) of Tables 3 and 4, the results remain consistent with our main findings.  \nPSM and DID: We further validate our results using an alternative matching and estimation \napproach. Specifically, we use the difference-in-differences (DID) estimation combined with the propensity \nscore matching (PSM). In this analysis, we use June 2021, the first availability date of Copilot, as the single \ntreatment turn-on time for all repositories classified into the treatment group in the baseline analysis. Hence, \nthis approach may alleviate concerns about non-random treatment turn-on time for Copilot usage in the \ntreatment group in the main analysis. However, we acknowledge that because developers may sign up \ngradually after the initial availability date, this approach may consider the period when Copilot was actually \nnot used as the post-treatment period, leading to an underestimation of the positive effect.  \nWe calculate each repository\u2019s propensity score, defined as the probability of adopting Copilot, \nusing a logit regression model. To construct the covariates for matching, we calculate the monthly values \nof several repository characteristics over the pre-treatment period (January to June 2021), average them \nacross the six-month window, and then apply a log transformation. Specifically, we use the log number of \nmerged PRs, the log average time taken to merge a single PR, the log number of unique developers whose \nPRs have been merged, the log number of push events, the log number of release events, the log number of \nopened issues, and the log number of closed issues.  \n 24 \nNext, we employ the nearest-neighbor matching algorithm without replacement and set a caliper of \n0.05 to match each treated repository with a control unit. Following"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_023",
    "source_id": "OpenSourceImpact2024",
    "text": "ates for matching, we calculate the monthly values \nof several repository characteristics over the pre-treatment period (January to June 2021), average them \nacross the six-month window, and then apply a log transformation. Specifically, we use the log number of \nmerged PRs, the log average time taken to merge a single PR, the log number of unique developers whose \nPRs have been merged, the log number of push events, the log number of release events, the log number of \nopened issues, and the log number of closed issues.  \n 24 \nNext, we employ the nearest-neighbor matching algorithm without replacement and set a caliper of \n0.05 to match each treated repository with a control unit. Following this matching process between \nrepositories where Copilot was used and those that Copilot was not used, we arrive at a dataset comprising \n2,122 repositories, consisting of 1,061 treatments and 1,061 controls. We report the details of the balance \nchecks of our matched sample in Online Appendix E. Overall, there is no statistically significant differences \nbetween the treatment and control groups across these pre-treatment covariates after matching. \nBased on the matched sample, we analyze the impact of Copilot on the number of merged PRs and \nthe average time taken to merge a single PR using the following regression specification: \n\ud835\udc4c!\" = \ud835\udefd* + \ud835\udefd$\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61! \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc61\" + \ud835\udefc! + \ud835\udf07\" + \ud835\udf00!\"     (3) \nwhere the dependent variable \ud835\udc4c!\" represents the log number of merged PRs and the log average time taken \nto merge a PR of repository \ud835\udc56 in month \ud835\udc61. \ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61! is a binary indicator that is denoted with \u201c1\u201d for \nrepository \ud835\udc56 where Copilot was used and \u201c0\u201d otherwise. \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc61\" is a binary indicator that is set to \u201c1\u201d in \nmonths after June 2021 and \u201c0\u201d otherwise. To control for time-invariant heterogeneity across repositories \nand time trends, we include repository-level fixed-effects \ud835\udefc! and month-level fixed-effects \ud835\udf07\".  \nThe main coe\ufb00icient of interest is \ud835\udefd$, as it indicates the change in the number of merged PRs or the \nchange in the average time taken to merge a PR of repositories before and after the availability of Copilot, \nrelative to the changes of repositories where Copilot was not used at all throughout the sample period. \nAdditionally, we confirm that the parallel trend assumption holds for the DID model (details of the parallel \npre-trend test conducted using the relative time model are provided in Online Appendix F). We report the \nresults in column (5) of Tables 3 and 4. The results show that Copilot significantly increased the number of \nmerged PRs and the average time taken to merge a single PR, which are qualitatively consistent with our \nfindings from the main analysis. \n[insert Table 3 here] \n[insert Table 4 here] \n 25 \nAlternative Measures: In the main"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_024",
    "source_id": "OpenSourceImpact2024",
    "text": " the availability of Copilot, \nrelative to the changes of repositories where Copilot was not used at all throughout the sample period. \nAdditionally, we confirm that the parallel trend assumption holds for the DID model (details of the parallel \npre-trend test conducted using the relative time model are provided in Online Appendix F). We report the \nresults in column (5) of Tables 3 and 4. The results show that Copilot significantly increased the number of \nmerged PRs and the average time taken to merge a single PR, which are qualitatively consistent with our \nfindings from the main analysis. \n[insert Table 3 here] \n[insert Table 4 here] \n 25 \nAlternative Measures: In the main analysis, we measure project-level code contributions using \nmerged PRs and coordination time using hours between PR submission and acceptance. To ensure our \nresults are not driven by specific measures, we examine alternative measures, including submitted PRs and \ncommits for project-level code contributions, and time intervals in days and minutes for coordination time. \nResults in Tables G.1 and G.2 of Online Appendix G remain consistent, indicating our findings are robust \nto outcome variable definitions. \nEffect of Workload: One important alternative explanation for the observed increase in \ncoordination time for merging a PR following Copilot adoption is that there were simply more PRs that \nneeded to be reviewed, which could lead to some delay. To address this concern, we include the log number \nof cumulative submitted PRs till the focal month as a control variable to account for potential increases in \noverall workload when analyzing coordination time. The results, reported in Tables G.3 of Online Appendix \nG, are qualitatively similar to our main results for coordination time. \nAlternative Estimation:  We also validate our results using a two-way fixed effects model applied \nto the full sample. The findings, presented in Table G.4 of Online Appendix G, are qualitatively consistent \nwith our main analysis. \nExpanded Sample: Because our main sample includes repositories with three or more developers, \nwe also examine the robustness of our results using an expanded sample that includes repositories with two \ndevelopers. Results are reported in Table G.5 of Online Appendix G and are qualitatively similar to our \nmain analysis. \nAdditional Alternative Sample: In our main analysis, we restrict the sample to repository-month \nobservations in which at least one PR was eventually merged. To assess the robustness of our findings, we \nreplicate the analysis by including all repository-month observations, regardless of whether any PRs were \nmerged. As shown in Table G.6 of Online Appendix G, the results remain consistent with those from our \nmain analysis. However, it is important to note that we cannot extend this analysis to coordination time as \nmeasuring coordination time requires data on PR acceptance time, which is only available when PR is \nmerged.  \n 26 \n6.3 Mechanism Analyses \n6.3.1 Project-level Code Contributions \nIn our hypothesis development for H1, we argue that AI pair programmers could influence project-level \ncode contributions by increasing individual code contributions and encouraging more developers to \nparticipate in coding. To shed light on these potential mechanisms, we conduct the following analyses.  \nIndividual Code Contributions: The increase in project-level code contributions could be driven \nby a higher level of individual code contributions, as AI pair programmers support"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_025",
    "source_id": "OpenSourceImpact2024",
    "text": " those from our \nmain analysis. However, it is important to note that we cannot extend this analysis to coordination time as \nmeasuring coordination time requires data on PR acceptance time, which is only available when PR is \nmerged.  \n 26 \n6.3 Mechanism Analyses \n6.3.1 Project-level Code Contributions \nIn our hypothesis development for H1, we argue that AI pair programmers could influence project-level \ncode contributions by increasing individual code contributions and encouraging more developers to \nparticipate in coding. To shed light on these potential mechanisms, we conduct the following analyses.  \nIndividual Code Contributions: The increase in project-level code contributions could be driven \nby a higher level of individual code contributions, as AI pair programmers support software development \nactivities of individual developers. To explore this possibility, we replicate our main analysis using the \naverage number of merged PRs per developer as the dependent variable (Subramaniam et al. 2009, \nMedappa and Srivastava 2019). As shown in column (1) of Table 5, Copilot use is associated with a 2.1% \nincrease in average number of merged PRs per developer in active repositories. At the same time, as shown \nin column (1) of Table H.1 of Online Appendix H, the volume of individual code contributions is positively \ncorrelated with project-level code contributions. These results collectively support our argument that AI \npair programmers could lead to an increase in project-level code contributions by increasing individual \ncode contributions. \nDeveloper Coding Participation: If AI pair programmers encourage more developers to contribute \nto repositories, this could also increase project-level code contributions. To test this potential mechanism, \nwe measure developer\u2019s coding participation by counting the number of unique developers whose PRs have \nbeen merged into the primary codebase (Subramaniam et al. 2009, Medappa and Srivastava 2019). We \nconduct the analysis using the same GSCM model as discussed above with developer\u2019s coding participation \nas the dependent variable. The results, shown in column (2) of Table 5, indicate that Copilot increased the \nnumber of developers whose PRs have been merged by 3.4%. Meanwhile, as shown in column (2) of Table \nH.1 of Online Appendix H, developer coding participation is positively correlated with project-level code \ncontributions. These results are consistent with our argument that AI pair programmers could increase \ndeveloper coding participation by reducing the participation costs developers face when deciding whether \n 27 \nto contribute (Atkinson 1957, Hertel et al. 2003, Setia et al. 2012, Wen et al. 2013). As a result, project-\nlevel code contributions increase. \n[insert Table 5 here] \n6.3.2 Coordination Time \nIn our hypothesis development for H2, we suggest that AI pair programmers may promote more \nparticipation in discussions on the focal OSS project, which could lead to longer coordination time for code \nintegration. To probe this potential mechanism, we analyze how AI pair programmers influence code \ndiscussions for each merged PR. Moreover, because more code discussions for each merged PR could be \ndriven by more participation by developers into code discussion, higher code discussion intensity per \ndeveloper, or both, we further examine how AI pair programmers influence each of these factors. \nCode"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_026",
    "source_id": "OpenSourceImpact2024",
    "text": ", Wen et al. 2013). As a result, project-\nlevel code contributions increase. \n[insert Table 5 here] \n6.3.2 Coordination Time \nIn our hypothesis development for H2, we suggest that AI pair programmers may promote more \nparticipation in discussions on the focal OSS project, which could lead to longer coordination time for code \nintegration. To probe this potential mechanism, we analyze how AI pair programmers influence code \ndiscussions for each merged PR. Moreover, because more code discussions for each merged PR could be \ndriven by more participation by developers into code discussion, higher code discussion intensity per \ndeveloper, or both, we further examine how AI pair programmers influence each of these factors. \nCode Discussion Volume: To provide some direct evidence on this mechanism, we re-estimate our \nbaseline model using code discussion volume as the outcome variable. We measure code discussion volume \nusing the average number of total comments per merged PR (Cataldo et al. 2006, Oh and Jeon 2007, \nGousios et al. 2014). As shown in column (1) of Table 6, Copilot adoption is associated with a 6.5% increase \nin code discussion volume per merged PR. Moreover, as shown in column (1) of Table H.2 in Online \nAppendix H, we observe a positive correlation between code discussion volume and coordination time. \nThese findings support our argument that AI pair programmers may promote more code discussions, which \nin turn leads to longer coordination time. \nCode Discussion Participation: Increased participation by developers in code discussion can \nintroduce more perspectives, thereby extending the time required to reach consensus (Weber 2006, Feri et \nal. 2010, Yu et al. 2015, Roels and Corbett 2024). To explore whether Copilot impacts code discussion \nparticipation, we measure code discussion participation using the average number of unique developers \nwho left comments per merged PR (Oh and Jeon 2007). Then we employ the same GSCM model using \ncode discussion participation as the dependent variable.  Column (2) of Table 6 presents the results, showing \na significant 1.7% increase in code discussion participation. Meanwhile, column (2) of Table H.2 in Online \nAppendix H suggests a positive correlation between code discussion participation and coordination time.   \n 28 \nThese results suggest that AI pair programmers could encourage broader participation in code discussions, \nwhich leads to longer coordination time. \nCode Discussion Intensity:  AI pair programmers may also affect the intensity of code discussion \nmade by each developer on a merged PR, which could contribute to more code discussion volume and \ncoordination time for code integration. To test this, we measure code discussion intensity using the average \nnumber of comments per developer per merged PR (Oh and Jeon 2007) and use it as the dependent variable \nbased on the same GSCM model as our baseline model. As shown in column (3) of Table 6, repositories \nusing Copilot show a 5.9% increase in code discussion intensity per developer. Column (3) of Table H.2 in \nOnline Appendix H also shows a positive correlation between code discussion intensity and coordination \ntime. These results indicate that AI pair programmers could encourage each individual developer to \nparticipate more in code discussion"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_027",
    "source_id": "OpenSourceImpact2024",
    "text": ", which could contribute to more code discussion volume and \ncoordination time for code integration. To test this, we measure code discussion intensity using the average \nnumber of comments per developer per merged PR (Oh and Jeon 2007) and use it as the dependent variable \nbased on the same GSCM model as our baseline model. As shown in column (3) of Table 6, repositories \nusing Copilot show a 5.9% increase in code discussion intensity per developer. Column (3) of Table H.2 in \nOnline Appendix H also shows a positive correlation between code discussion intensity and coordination \ntime. These results indicate that AI pair programmers could encourage each individual developer to \nparticipate more in code discussion, resulting in longer coordination time. \n[insert Table 6 here] \nWe further examine how Copilot affects the content of code discussions among developers. \nSpecifically, we conduct a supplementary text analysis of topic diversity using Latent Dirichlet Allocation \n(LDA) to extract discussion topics from 5,404,735 merged PR comments of our selected repositories in the \nmain analysis from 2021 to 2022. Details are provided in Online Appendix I. As shown in Table I.1, Copilot \nincreased the diversity of code discussion topics per merged PR, suggesting that Copilot also expanded the \nrange of topics being discussed, further increasing coordination time. \nAlternative Explanation: One alternative explanation for the increased coordination time is that \ncore developers, who are responsible for reviewing and merging the code, may be allocating less effort to \nthese activities after adopting Copilot, as they become more engaged in contributing code themselves. To \ninvestigate this possibility, we examine three measures of core developer review activity on merged pull \nrequests (PRs): (1) the total number of reviews finished by core developers, which reflects the overall \nvolume of review work; (2) the number of unique core developers who reviewed code submissions for \nmerged PRs, which indicates the breadth of participation; and (3) the average number of reviews per core \ndeveloper, which captures review productivity of each core developer. As reported in Table 7, we find that \n 29 \nCopilot adoption is associated with an increase in all three measures. Although Copilot leads to greater \ncode contribution volume, it also appears to enhance review productivity and encourage broader \nparticipation in review among core developers. These findings suggest that, rather than reducing their \nreview activity, developers are completing more reviews per month following Copilot adoption. Therefore, \nthe increased coordination time for code integration does not seem to be driven by less effort spent on \nreviewing the code.  \n[insert Table 7 here] \n6.4 Combined Effects of Project-level Code Contributions and Coordination Time \nThe results in the previous sections indicate that AI pair programmers may increase both project-level code \ncontributions and coordination time for code integration. Thus, the combined effects on project-level \nproductivity, defined as timely merge of code contributions into the codebase of a project, remain unclear \nex ante. To provide some empirical evidence on this, we consider the number of PRs that were merged \nwithin a particular time window as a measure of project-level productivity. For robustness, we use three \ntime windows based on the distribution of merge times prior to Copilot\u2019s introduction: (1) the 25th percentile \n(1 day), (2"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_028",
    "source_id": "OpenSourceImpact2024",
    "text": "7 here] \n6.4 Combined Effects of Project-level Code Contributions and Coordination Time \nThe results in the previous sections indicate that AI pair programmers may increase both project-level code \ncontributions and coordination time for code integration. Thus, the combined effects on project-level \nproductivity, defined as timely merge of code contributions into the codebase of a project, remain unclear \nex ante. To provide some empirical evidence on this, we consider the number of PRs that were merged \nwithin a particular time window as a measure of project-level productivity. For robustness, we use three \ntime windows based on the distribution of merge times prior to Copilot\u2019s introduction: (1) the 25th percentile \n(1 day), (2) the median (3 days), and (3) the 75th percentile (10 days). Then, we calculate the number of \nPRs merged within each of the three time windows.  \nThe results, presented in Table 8, indicate that Copilot adoption is associated with a 3.5% increase \nin PRs merged within one day, a 4.1% increase in PRs merged within three days, and a 5.1% increase in \nPRs merged within ten days. As reported in Table H.3 of Online Appendix H, we further confirm a positive \ncorrelation between project-level code contributions and project-level productivity, and a negative \ncorrelation between coordination time and project-level productivity. These findings suggest that, despite \nthe increased coordination time, the effect of Copilot on overall project-level productivity is positive.  \n[insert Table 8 here] \nImpact on Code Quality: While the use of Copilot increased productivity at the project level, it is \nequally important to understand its impact on code quality. On one hand, Copilot may reduce errors and \nimprove the quality of code written by individual developers. On the other hand, it could potentially lower \n 30 \noverall quality if it encourages increased participation from less-skilled developers. Thus, ex-ante, the effect \nof Copilot on quality is theoretically ambiguous.  \nTo examine whether and how Copilot affects code quality, we include four additional dependent \nvariables related to issue reports. First, we measure the total number of issues reported in each repository. \nSecond, since issues can vary in nature and severity, we focus specifically on bug-related issues, which are \nmore severe and widely used in the software engineering literature as a proxy for quality (Vasilescu et al. \n2015). Third, we construct two normalized measures: the average number of total issues per merged PR \nand the average number of total bug-related issues per merged PR. These normalized metrics allow us to \nassess quality relative to the volume of contributions. As reported in Table 9, we find that although Copilot \nadoption is associated with an increase in the total number of issues and bugs, the normalized measures \nremained statistically unchanged. These findings suggest that the increased issues and bug reports are due \nto a higher volume of code contributions, rather than a reduction in the quality of individual contributions.8 \n[insert Table 9 here] \n6.5 Core and Peripheral Developers \nOur H3a and H3b highlight some differential effects of AI pair programmers between peripheral and core \ndevelopers. To assess the effect of AI pair programmers on the relative project-level code contributions \nmade by peripheral developers"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_029",
    "source_id": "OpenSourceImpact2024",
    "text": ". These normalized metrics allow us to \nassess quality relative to the volume of contributions. As reported in Table 9, we find that although Copilot \nadoption is associated with an increase in the total number of issues and bugs, the normalized measures \nremained statistically unchanged. These findings suggest that the increased issues and bug reports are due \nto a higher volume of code contributions, rather than a reduction in the quality of individual contributions.8 \n[insert Table 9 here] \n6.5 Core and Peripheral Developers \nOur H3a and H3b highlight some differential effects of AI pair programmers between peripheral and core \ndevelopers. To assess the effect of AI pair programmers on the relative project-level code contributions \nmade by peripheral developers compared to core developers (H3a), we compute the proportion of merged \nPRs from peripheral developers to the total number of merged PRs in each repository.9 A negative treatment \neffect estimate on this outcome variable indicates that after using Copilot, the proportion of code generated \nby peripheral developers among all code merged in a repository became lower, i.e. the adoption of AI pair \nprogrammers led to a relative decline in contribution share from peripheral developers.  \n \n8 We also examine whether Copilot\u2019s effects differ by project complexity and find no significant differences between \nsimple and complex projects. Detailed discussions of the results are in Online Appendix J. \n9 Note that there are only two types of developers in a repository: core or peripheral developers. We use this measure \ninstead of the ratio of peripheral developers\u2019 merged PRs to core developers\u2019 merged PRs because the latter measure \ndoes not allow us to incorporate cases where core developers had zero merged PR in a repository in a month.  \n 31 \nTo assess the relative change in coordination time for integrating code contributed by peripheral \ndevelopers compared to the code from core developers (H3b), we calculate the ratio of the average time to \nmerge peripheral developers\u2019 PRs to the overall average code-merging time in each repository. A positive \nestimate on this outcome would indicate that PRs from peripheral developers faced relatively longer \ncoordination time after Copilot adoption.  \nWe next examine the difference in productivity gain from AI pair programmers between peripheral \ndevelopers and core developers. Specifically, we compute the proportion of PRs from peripheral developers \nthat were merged within one, three, or ten days, and use each of these as the dependent variable. Negative \nestimates based on these outcome variables suggest that AI pair programmers led to less productivity gain \nfor peripheral developers than for core developers. \nAs shown in column (1) in Table 10, the proportion of merged PRs from peripheral developers \nsignificantly declined by 0.019 or 3.7%10. Additionally, as shown in column (2), peripheral developers \nexperienced an increase of 0.085 or 5.4%11 in their relative average merge time. Collectively, these findings \nsuggest that Copilot led to relatively fewer project-level code contributions from peripheral developers and \nlonger coordination time for them. The results, as well as the ones in columns (3) to (5) that use the \nproportion of timely merging of peripheral developers\u2019 code as the dependent variables, suggest peripheral \ndevelopers realized less productivity gain from Copilot than core developers. \n[insert Table 10 here] \nTable 11 next reports the results on the mechanisms through which Copilot"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_030",
    "source_id": "OpenSourceImpact2024",
    "text": "0.019 or 3.7%10. Additionally, as shown in column (2), peripheral developers \nexperienced an increase of 0.085 or 5.4%11 in their relative average merge time. Collectively, these findings \nsuggest that Copilot led to relatively fewer project-level code contributions from peripheral developers and \nlonger coordination time for them. The results, as well as the ones in columns (3) to (5) that use the \nproportion of timely merging of peripheral developers\u2019 code as the dependent variables, suggest peripheral \ndevelopers realized less productivity gain from Copilot than core developers. \n[insert Table 10 here] \nTable 11 next reports the results on the mechanisms through which Copilot shapes project-level \ncode contributions and coordination time for peripheral versus core developers differently. We find that \nfollowing the adoption of Copilot, peripheral developers had relatively lower individual code contributions, \nas shown in column (1), and relatively lower coding participation, as shown in column (2). These could \n \n10 The mean proportion of merged PRs by peripheral developers to the total number of merged PRs, as reported in \nTable 1, is 0.52.  This proportion decreased by 0.019, representing a 3.7% decrease. \n11 The mean ratio of the average time taken to merge a PR submitted by peripheral developers to the average time \ntaken to merge a PR submitted by all types of developers, as reported in Table 1, is 1.56. This ratio increases by 0.085, \nrepresenting an 5.4% increase. \n 32 \npotentially explain the effect of Copilot on reducing the proportion of project-level code contributions made \nby peripheral developers, supporting our arguments for H3a.  \nAt the same time, as shown in columns (3) to (5) of Table 11, peripheral developers\u2019 code \ncontributions received relatively higher code discussion volume, more developers participating in \ndiscussing their code, and greater code discussion intensity per developer on their code. These results could \ncollectively explain why peripheral developers experienced a relatively longer coordination time for \nmerging their code following the adoption of Copilot and are consistent with our arguments for H3b.  \n[insert Table 11 here] \nIt is worth noting that the observed differential effects in Tables 10 and 11 are either due to lower \nincreases or reductions in activities of peripheral developers. To provide further support to H3a (which \nsuggests the project-level code contributions from both peripheral and core developers could increase and \nthe former may have a smaller increase than the latter), we construct a range of metrics to capture absolute \nchanges in project-level code contributions for both core and peripheral developers. Similarly, to provide \nfurther support to H3b (which suggests the coordination time for both peripheral and core developers could \nincrease, and the former may have a larger increase than the latter), we also use a range of metrics to capture \nabsolute changes in coordination time for both core and peripheral developers. The detailed results are \nreported in the Online Appendix K. Note that as GSCM generates different synthetic controls for these \nmeasures across samples, we cannot directly compare the magnitudes across tables. However, these \nfindings corroborate the results in Tables 10 and 11 and suggest that observed lower relative share of \nproject-level code contributions and greater"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_031",
    "source_id": "OpenSourceImpact2024",
    "text": " capture absolute \nchanges in project-level code contributions for both core and peripheral developers. Similarly, to provide \nfurther support to H3b (which suggests the coordination time for both peripheral and core developers could \nincrease, and the former may have a larger increase than the latter), we also use a range of metrics to capture \nabsolute changes in coordination time for both core and peripheral developers. The detailed results are \nreported in the Online Appendix K. Note that as GSCM generates different synthetic controls for these \nmeasures across samples, we cannot directly compare the magnitudes across tables. However, these \nfindings corroborate the results in Tables 10 and 11 and suggest that observed lower relative share of \nproject-level code contributions and greater coordination time for peripheral developers are not driven by \nabsolute declines in their activity levels following Copilot adoption. Rather, they reflect differential returns \nto Copilot shaped by developer roles. \nIn our hypothesis development, we argue that one key distinction between peripheral developers \nand core developers is that the former may lack the level of project familiarity needed to overcome AI pair \nprogrammers\u2019 limitations and use them effectively. This could be one potential reason why AI pair \nprogrammers benefit peripheral developers less than core developers, as hypothesized in H3a and H3b. To \n 33 \nprovide some preliminary evidence on this, in Online Appendix L, we show that peripheral developers \nindeed have less project familiarity compared to core developers, measured by their tenure with the project. \nWe also show project familiarity seemed to influence the benefits from using Copilot. \nAlternative Explanations: The heterogeneous effects of Copilot on peripheral and core developers \nmay result from other differences between peripheral and core developers, such as programming language \nexpertise or Copilot usage, instead of project familiarity. We conduct additional analyses to examine these \nalternative explanations and report the detailed results in Online Appendix M. Overall, there are no \nsignificant differences among core and peripheral developers in terms of their programming language \nexpertise. Furthermore, the Copilot usage seems even higher among peripheral developers as compared to \ncore developers. These results suggest that following Copilot adoption, the lower project-level code \ncontributions and longer coordination time of peripheral developers as compared to core developers are not \nlikely to be driven by lower programming skills or lower Copilot usage by peripheral developers. \nAnother possible explanation is that the heterogenous effects of Copilot on core and peripheral \ndevelopers could be driven by changes in task allocations between the two groups following the adoption \nof Copilot. However, prior OSS literature highlights the self-organizing nature of these communities \n(Lindberg et al. 2024). In the absence of a formal hierarchical structure, developers are generally not \nassigned with tasks. Instead, they voluntarily select areas in which they wish to contribute. Given that open-\nsource projects often adopt modular architectures and support parallel development (Howison and \nCrowston 2014, Medappa and Srivastava 2019), core and peripheral developers may contribute \nsimultaneously to similar features, effectively working on the same tasks. As such, task reallocation from \nperipheral to core developers may not be a likely explanation for the observed pattern. \n7. Discussion and Conclusions \nThe rise of AI pair programmers has garnered increasing attention due to their potential to transform \nsoftware development. In this study, we explore the effects of AI pair programmers within the context of \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_032",
    "source_id": "OpenSourceImpact2024",
    "text": " structure, developers are generally not \nassigned with tasks. Instead, they voluntarily select areas in which they wish to contribute. Given that open-\nsource projects often adopt modular architectures and support parallel development (Howison and \nCrowston 2014, Medappa and Srivastava 2019), core and peripheral developers may contribute \nsimultaneously to similar features, effectively working on the same tasks. As such, task reallocation from \nperipheral to core developers may not be a likely explanation for the observed pattern. \n7. Discussion and Conclusions \nThe rise of AI pair programmers has garnered increasing attention due to their potential to transform \nsoftware development. In this study, we explore the effects of AI pair programmers within the context of \ncollaborative OSS development, with a focus on their impact on project-level code contributions and \ncoordination time for code integration. Using repository-level proprietary data from GitHub organization, \n 34 \nwe evaluate the impact of GitHub Copilot, an AI pair programmer, on the development outcomes of open-\nsource repositories, along with its underlying mechanisms.  \nOur findings indicate that Copilot adoption significantly increases project-level code contribution \nalongside longer coordination time, highlighting a tradeoff between facilitated code generation and \nheightened coordination cost in collaborative software development. Mechanism analyses reveal that the \nincreased project-level code contributions are driven by more developers in participating in coding and \ngreater individual code contributions. In contrast, the increased coordination time seems driven by greater \ncode discussions, including higher code discussion volumes, more developers participating in code \ndiscussions, and greater discussion intensity per developer. We extend prior research on generative AI's \nproductivity benefits for individual developers (e.g., Peng et al. 2023; Cui et al. 2024) by examining its \nimpact on collaborative, voluntary participation in open-source software (OSS) projects. We also contribute \nto the literature on the role of generative AI in team collaboration. Unlike earlier work focused on traditional \nteams with fixed roles and structured coordination (Li et al. 2024, Dell'Acqua et al. 2025), we explore fluid, \nvolunteer-driven OSS environments. We reveal that AI tools, by encouraging engagement in discussions, \nmay increase coordination time due to the need to align diverse perspectives. \nWe also examine the effects of Copilot on core and peripheral developers within the OSS \ncommunity, who differ in their levels of familiarity with the focal projects. Our analysis shows that, \nfollowing Copilot adoption, peripheral developers experienced a relatively smaller increase in project-level \ncode contributions but a larger increase in coordination time, when compared to core developers. In addition, \nwe find that peripheral developers contribute a smaller proportion of timely integrated code, suggesting that \nthe productivity gain from AI pair programmers could be less for peripheral developers than for core \ndevelopers. Prior studies (e.g., Dell\u2019Acqua et al. 2023; Peng et al. 2023) show that less-skilled users often \nbenefit more from AI tools. In contrast, we find that in OSS settings where developers handle complex \nsoftware development projects, core developers gain more from AI pair programming than peripheral ones, \nlikely due to their greater contextual knowledge of the project.  \n7.1 Implications \n 35 \nOur paper has several implications for both practice and the broader OSS communities. First, our findings \nsuggest that organizations should invest in AI pair"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_033",
    "source_id": "OpenSourceImpact2024",
    "text": " contribute a smaller proportion of timely integrated code, suggesting that \nthe productivity gain from AI pair programmers could be less for peripheral developers than for core \ndevelopers. Prior studies (e.g., Dell\u2019Acqua et al. 2023; Peng et al. 2023) show that less-skilled users often \nbenefit more from AI tools. In contrast, we find that in OSS settings where developers handle complex \nsoftware development projects, core developers gain more from AI pair programming than peripheral ones, \nlikely due to their greater contextual knowledge of the project.  \n7.1 Implications \n 35 \nOur paper has several implications for both practice and the broader OSS communities. First, our findings \nsuggest that organizations should invest in AI pair programmers, as they increase project level code \ncontributions by increasing individual code contributions and developer participation. However, these \nbenefits come with increased coordination time. Organizations should adopt complementary strategies to \nmanage integration challenges, such as aligning coding practices, clarifying project goals, and improving \ncommunication workflows, to fully capitalize on the benefits of AI-assisted software development.  \nSecond, our results raise concerns about potential shifts in team structure within OSS communities. \nWhile AI pair programmers bring benefits, the gains favor core developers, who possess greater familiarity \nwith project structure and context, which underscores the importance of integrating human intelligence with \nthe automation and augmentation capabilities of AI pair programmers. In contrast, peripheral developers \nface relatively longer coordination time that reduces their overall project productivity. Over time, this \nimbalance could lead to more concentrated development teams, where core developers dominate and \nperipheral contributions decline. Such a trend risks undermining the open-source ethos of diversity of \nperspectives, potentially making open collaboration more closed.  \nIn addition, AI pair programmers introduce new challenges to traditional norms of code ownership. \nHistorically, contributors retain copyright over their code while licensing it under open-source terms that \nallow others to freely use, modify, and distribute the work. However, AI-generated code blurs the \nboundaries of code ownership, as these tools are trained on public repositories without attributing credit to \nthe original contributors. Because it is often unclear whether a piece of code was written by a human or \ngenerated by AI, the transparency of contributions may diminish. Over time, this lack of attribution and \nclarity may erode trust in the origin and quality of code within OSS communities. \n7.2 Limitations and Future Work \nOur study has several limitations. First, due to privacy constraints, we lack individual-level Copilot use \ndata. Future research could leverage more granular AI usage data to validate our findings and explore role \ntransitions in the OSS development and potential spillover effects enabled by AI pair programmers. \n 36 \nSecond, our work represents the first step in examining how generative AI affects core and \nperipheral developers differently. We acknowledge that beyond different levels of project familiarity, \ndevelopers may differ in motivation, ability, and preference for the type of work they undertake. Due to the \nobservational nature of our data, we are not able to measure these metrics. Future work could explore these \nnuanced behavioral dynamics, potentially through surveys, interviews, or mixed-method approaches.  \nThird, although our study emphasizes the quantitative tradeoff between project-level code \ncontributions and coordination time, it does not capture the subjective aspects of working with AI tools. \nFuture research could examine how developers perceive and"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_034",
    "source_id": "OpenSourceImpact2024",
    "text": "ver effects enabled by AI pair programmers. \n 36 \nSecond, our work represents the first step in examining how generative AI affects core and \nperipheral developers differently. We acknowledge that beyond different levels of project familiarity, \ndevelopers may differ in motivation, ability, and preference for the type of work they undertake. Due to the \nobservational nature of our data, we are not able to measure these metrics. Future work could explore these \nnuanced behavioral dynamics, potentially through surveys, interviews, or mixed-method approaches.  \nThird, although our study emphasizes the quantitative tradeoff between project-level code \ncontributions and coordination time, it does not capture the subjective aspects of working with AI tools. \nFuture research could examine how developers perceive and adapt to AI pair programmers, including their \nsatisfaction, trust, and perceived control over their contributions. \nFinally, the potential risks associated with AI pair programmers warrant further investigation. One \nimportant concern is the possibility of skill erosion. Developers may become overly reliant on AI-generated \nsuggestions, potentially diminishing their coding proficiency over time. In addition, AI systems trained on \nlarge-scale public repositories may perpetuate existing coding biases, potentially reinforcing suboptimal \npractices. Furthermore, AI-generated code may inadvertently introduce security vulnerabilities, especially \nif it lacks the contextual awareness required for sensitive or security-critical components. Future studies \nshould propose frameworks to ensure safe and ethical AI usage in software development. \nReferences \nAbadie A, Diamond A, Hainmueller J (2010) Synthetic control methods for comparative case studies: \nEstimating the effect of California\u2019s tobacco control program. Journal of the American statistical \nAssociation 105(490):493-505. \nAlMarzouq M, AlZaidan A, AlDallal J (2020) Mining GitHub for research and education: challenges and \nopportunities. International Journal of Web Information Systems 16(4):451-473. \nAncona DG, Caldwell DF (1992) Demography and design: Predictors of new product team performance. \nOrganization science 3(3):321-341. \nAtkinson JW (1957) Motivational determinants of risk-taking behavior. Psychological review 64(6p1):359. \nBai J (2009) Panel data models with interactive fixed effects. Econometrica 77(4):1229-1279. \nBakos JY, Brynjolfsson E (1993) Information technology, incentives, and the optimal number of suppliers. \nJournal of Management Information Systems 10(2):37-53. \nBarke S, James MB, Polikarpova N (2023) Grounded copilot: How programmers interact with code-generating \nmodels. Proceedings of the ACM on Programming Languages 7(OOPSLA1):85-111. \nBrynjolfsson E, Li D, Raymond L (2025) Generative AI at work. The Quarterly Journal of Economics:qjae044. \nBurtch G, Lee D, Chen Z (2024) The consequences of generative AI for online knowledge communities. \nScientific Reports 14(1):10413. \n 37 \nCataldo M, Wagstrom PA, Herbsleb JD, Carley KM (2006) Identification of coordination requirements: \nImplications for the design of collaboration and awareness"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_035",
    "source_id": "OpenSourceImpact2024",
    "text": "2023) Grounded copilot: How programmers interact with code-generating \nmodels. Proceedings of the ACM on Programming Languages 7(OOPSLA1):85-111. \nBrynjolfsson E, Li D, Raymond L (2025) Generative AI at work. The Quarterly Journal of Economics:qjae044. \nBurtch G, Lee D, Chen Z (2024) The consequences of generative AI for online knowledge communities. \nScientific Reports 14(1):10413. \n 37 \nCataldo M, Wagstrom PA, Herbsleb JD, Carley KM (2006) Identification of coordination requirements: \nImplications for the design of collaboration and awareness tools. Proceedings of the 2006 20th anniversary \nconference on Computer supported cooperative work, 353-362. \nCrowston K, Wei K, Li Q, Howison J (2006) Core and periphery in free/libre and open source software team \ncommunications. Proceedings of the 39th annual Hawaii international conference on system sciences \n(HICSS'06) (IEEE), 118a-118a. \nCui ZK, Demirer M, Jaffe S, Musolff L, Peng S, Salz T (2024) The effects of generative ai on high skilled work: \nEvidence from three field experiments with software developers. Available at SSRN 4945566. \nDabbish L, Stuart C, Tsay J, Herbsleb J (2012) Social coding in GitHub: transparency and collaboration in an \nopen software repository. Proceedings of the ACM 2012 conference on computer supported cooperative \nwork, 1277-1286. \nDakhel AM, Majdinasab V, Nikanjam A, Khomh F, Desmarais MC, Jiang ZMJ (2023) Github copilot ai pair \nprogrammer: Asset or liability? Journal of Systems and Software 203:111734. \nDell'Acqua F, Kogut B, Perkowski P (2023) Super mario meets ai: Experimental effects of automation and \nskills on team performance and coordination. Review of Economics and Statistics:1-47. \nDell'Acqua F, Ayoubi C, Lifshitz-Assaf H, Sadun R, Mollick ER, Mollick L, Han Y, Goldman J, Nair H, Taub \nS (2025) The Cybernetic Teammate: A Field Experiment on Generative AI Reshaping Teamwork and \nExpertise. \nDemirci O, Hannane J, Zhu X (2025) Who is AI replacing? The impact of generative AI on online freelancing \nplatforms. Management Science. \nEl Mezouar M, Zhang F, Zou Y (2019) An empirical study on the teams structures in social coding using GitHub \nprojects. Empirical Software Engineering 24(6):3790-3823. \nEspinosa JA, Slaughter SA, Kraut RE, Herbsleb JD (2007) Familiarity, complexity, and team performance in \ngeographically distributed software development. Organization science 18(4):613-630. \nFeldman P, Foulds JR, Pan S (2023) Trapping LLM hallucinations using"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_036",
    "source_id": "OpenSourceImpact2024",
    "text": " X (2025) Who is AI replacing? The impact of generative AI on online freelancing \nplatforms. Management Science. \nEl Mezouar M, Zhang F, Zou Y (2019) An empirical study on the teams structures in social coding using GitHub \nprojects. Empirical Software Engineering 24(6):3790-3823. \nEspinosa JA, Slaughter SA, Kraut RE, Herbsleb JD (2007) Familiarity, complexity, and team performance in \ngeographically distributed software development. Organization science 18(4):613-630. \nFeldman P, Foulds JR, Pan S (2023) Trapping LLM hallucinations using tagged context prompts. arXiv preprint \narXiv:2306.06085. \nFeri F, Irlenbusch B, Sutter M (2010) Efficiency gains from team-based coordination\u2014large-scale experimental \nevidence. American Economic Review 100(4):1892-1912. \nGousios G, Pinzger M, Deursen Av (2014) An exploratory study of the pull-based software development model. \nProceedings of the 36th international conference on software engineering, 345-355. \nHarbring C (2006) The effect of communication in incentive systems\u2014an experimental study. Managerial and \nDecision Economics 27(5):333-353. \nHertel G, Niedner S, Herrmann S (2003) Motivation of software developers in Open Source projects: an \nInternet-based survey of contributors to the Linux kernel. Research policy 32(7):1159-1177. \nHoffmann M, Boysel S, Nagle F, Peng S, Xu K (2024) Generative AI and the Nature of Work. Report. \nHowison J, Crowston K (2014) Collaboration through open superposition: A theory of the open source way. \nMis Quarterly 38(1):29-50. \nIm GP, Ahuja M (2023) The Interdependence of Coordination and Cooperation in Information Technology \nOutsourcing. Information Systems Research 34(4):1791-1806. \nImai S (2022) Is github copilot a substitute for human pair-programming? an empirical study. Proceedings of \nthe ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, 319-\n321. \nJanardhanan NS, Lewis K, Reger RK, Stevens CK (2020) Getting to know you: motivating cross-understanding \nfor improved team and individual performance. Organization Science 31(1):103-118. \nKalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D (2014) The promises and perils of \nmining github. Proceedings of the 11th working conference on mining software repositories, 92-101. \nKononenko O, Rose T, Baysal O, Godfrey M, Theisen D, De Water B (2018) Studying pull request merges: A \ncase study of shopify's active merchant. Proceedings of the 40th international conference on software \nengineering: software engineering in practice, 124-133. \nKoushik MV, Mooker"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_037",
    "source_id": "OpenSourceImpact2024",
    "text": ". Organization Science 31(1):103-118. \nKalliamvakou E, Gousios G, Blincoe K, Singer L, German DM, Damian D (2014) The promises and perils of \nmining github. Proceedings of the 11th working conference on mining software repositories, 92-101. \nKononenko O, Rose T, Baysal O, Godfrey M, Theisen D, De Water B (2018) Studying pull request merges: A \ncase study of shopify's active merchant. Proceedings of the 40th international conference on software \nengineering: software engineering in practice, 124-133. \nKoushik MV, Mookerjee VS (1995) Modeling coordination in software construction: An analytical approach. \nInformation Systems Research 6(3):220-254. \n 38 \nKrishnamurthy R, Jacob V, Radhakrishnan S, Dogan K (2016) Peripheral developer participation in open source \nprojects: an empirical analysis. ACM Transactions on Management Information Systems (TMIS) 6(4):1-31. \nLevine SS, Prietula MJ (2014) Open collaboration for innovation: Principles and performance. Organization \nScience 25(5):1414-1433. \nLi N, Zhou H, Mikel-Hong K (2024) Generative AI Enhances Team Performance and Reduces Need for \nTraditional Teams. arXiv preprint arXiv:2405.17924. \nLiguori P, Improta C, Natella R, Cukic B, Cotroneo D (2024) Enhancing AI-based Generation of Software \nExploits with Contextual Information. 2024 IEEE 35th International Symposium on Software Reliability \nEngineering (ISSRE) (IEEE), 180-191. \nLindberg A, Berente N, Gaskin J, Lyytinen K (2016) Coordinating interdependencies in online communities: \nA study of an open source software project. Information Systems Research 27(4):751-772. \nLindberg A, Schecter A, Berente N, Hennel P, Lyytinen K (2024) THE ENTRAINMENT OF TASK \nALLOCATION AND RELEASE CYCLES IN OPEN SOURCE SOFTWARE DEVELOPMENT. MIS \nQuarterly 48(1). \nMalone TW, Crowston K (1994) The interdisciplinary study of coordination. ACM Computing Surveys (CSUR) \n26(1):87-119. \nMaranguni\u0107 N, Grani\u0107 A (2015) Technology acceptance model: a literature review from 1986 to 2013. \nUniversal access in the information society 14:81-95. \nMattarelli E, Bertolotti F, Prencipe A, Gupta A (2022) The effect of role-based product representations on \nindividual and team coordination practices: A field study of a globally distributed new product development \nteam. Organization Science 33(4):1423-1451. \nMawdsley JK, Meyer-Doyle P, Chatain O (2022) Client-related factors and collaboration between human assets. \nOrganization Science 33(2):518-540. \nMedappa PK, Srivastava"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_038",
    "source_id": "OpenSourceImpact2024",
    "text": "\u0107 A (2015) Technology acceptance model: a literature review from 1986 to 2013. \nUniversal access in the information society 14:81-95. \nMattarelli E, Bertolotti F, Prencipe A, Gupta A (2022) The effect of role-based product representations on \nindividual and team coordination practices: A field study of a globally distributed new product development \nteam. Organization Science 33(4):1423-1451. \nMawdsley JK, Meyer-Doyle P, Chatain O (2022) Client-related factors and collaboration between human assets. \nOrganization Science 33(2):518-540. \nMedappa PK, Srivastava SC (2019) Does superposition influence the success of FLOSS projects? An \nexamination of open-source software development by organizations and individuals. Information Systems \nResearch 30(3):764-786. \nNoy S, Zhang W (2023) Experimental evidence on the productivity effects of generative artificial intelligence. \nScience 381(6654):187-192. \nOh W, Jeon S (2007) Membership herding and network stability in the open source community: The Ising \nperspective. Management science 53(7):1086-1101. \nOkhuysen GA, Bechky BA (2009) 10 coordination in organizations: An integrative perspective. Academy of \nManagement annals 3(1):463-502. \nOliveira N, Lumineau F (2017) How coordination trajectories influence the performance of interorganizational \nproject networks. Organization Science 28(6):1029-1060. \nPeng S, Kalliamvakou E, Cihon P, Demirer M (2023) The impact of ai on developer productivity: Evidence \nfrom github copilot. arXiv preprint arXiv:2302.06590. \nPikkarainen M, Haikara J, Salo O, Abrahamsson P, Still J (2008) The impact of agile practices on \ncommunication in software development. Empirical Software Engineering 13:303-337. \nPi\u00f1eiro-Mart\u00edn A, Santos-Criado F-J, Garc\u00eda-Mateo C, Doc\u00edo-Fern\u00e1ndez L, L\u00f3pez-P\u00e9rez MdC (2025) Context \nIs King: Large Language Models\u2019 Interpretability in Divergent Knowledge Scenarios. Applied Sciences \n15(3):1192. \nReagans R, Miron-Spektor E, Argote L (2016) Knowledge utilization, coordination, and team performance. \nOrganization Science 27(5):1108-1124. \nRoberts JA, Hann I-H, Slaughter SA (2006) Understanding the motivations, participation, and performance of \nopen source software developers: A longitudinal study of the Apache projects. Management science \n52(7):984-999. \nRoels G, Corbett CJ (2024) Too many meetings? Scheduling rules for team coordination. Management Science \n70(12):8647-8667. \nSetia P, Rajagopalan B, Sambamurthy V, Calantone R (2012) How peripheral developers contribute to open-\nsource software development. Information Systems Research 23(1):144"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_039",
    "source_id": "OpenSourceImpact2024",
    "text": " and team performance. \nOrganization Science 27(5):1108-1124. \nRoberts JA, Hann I-H, Slaughter SA (2006) Understanding the motivations, participation, and performance of \nopen source software developers: A longitudinal study of the Apache projects. Management science \n52(7):984-999. \nRoels G, Corbett CJ (2024) Too many meetings? Scheduling rules for team coordination. Management Science \n70(12):8647-8667. \nSetia P, Rajagopalan B, Sambamurthy V, Calantone R (2012) How peripheral developers contribute to open-\nsource software development. Information Systems Research 23(1):144-163. \n 39 \nShah SK (2006) Motivation, governance, and the viability of hybrid forms in open source software development. \nManagement science 52(7):1000-1014. \nShaikh M, Vaast E (2023) Algorithmic interactions in open source work. Information Systems Research \n34(2):744-765. \nSimcoe T (2012) Standard setting committees: Consensus governance for shared technology platforms. \nAmerican Economic Review 102(1):305-336. \nSingh PV, Phelps C (2013) Networks, social influence, and the choice among competing innovations: Insights \nfrom open source software licenses. Information Systems Research 24(3):539-560. \nSrikanth K, Puranam P (2014) The firm as a coordination system: Evidence from software services offshoring. \nOrganization Science 25(4):1253-1271. \nSubramaniam C, Sen R, Nelson ML (2009) Determinants of open source software project success: A \nlongitudinal study. Decision Support Systems 46(2):576-585. \nTsay J, Dabbish L, Herbsleb J (2014) Influence of social and technical factors for evaluating contribution in \nGitHub. Proceedings of the 36th international conference on Software engineering, 356-366. \nVasilescu B, Yu Y, Wang H, Devanbu P, Filkov V (2015) Quality and productivity outcomes relating to \ncontinuous integration in GitHub. Proceedings of the 2015 10th joint meeting on foundations of software \nengineering, 805-816. \nWang Y, Ramachandran V, Liu Sheng OR (2021) Do fit opinions matter? The impact of fit context on online \nproduct returns. Information Systems Research 32(1):268-289. \nWeber RA (2006) Managing growth to achieve efficient coordination in large groups. American Economic \nReview 96(1):114-126. \nWen W, Forman C, Graham SJ (2013) Research note\u2014The impact of intellectual property rights enforcement \non open source software project success. Information Systems Research 24(4):1131-1146. \nXu B, Nguyen T-D, Le-Cong T, Hoang T, Liu J, Kim K, Gong C, Niu C, Wang C, Le B (2023) Are we ready \nto embrace generative ai for software q&a? 2023 38th IEEE/ACM International Conference on Automated \nSoftware Engineering (ASE)"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_040",
    "source_id": "OpenSourceImpact2024",
    "text": "ber RA (2006) Managing growth to achieve efficient coordination in large groups. American Economic \nReview 96(1):114-126. \nWen W, Forman C, Graham SJ (2013) Research note\u2014The impact of intellectual property rights enforcement \non open source software project success. Information Systems Research 24(4):1131-1146. \nXu B, Nguyen T-D, Le-Cong T, Hoang T, Liu J, Kim K, Gong C, Niu C, Wang C, Le B (2023) Are we ready \nto embrace generative ai for software q&a? 2023 38th IEEE/ACM International Conference on Automated \nSoftware Engineering (ASE) (IEEE), 1713-1717. \nXu Y (2017) Generalized synthetic control method: Causal inference with interactive fixed effects models. \nPolitical Analysis 25(1):57-76. \nYeverechyahu, D., Mayya, R., & Oestreicher-Singer, G. (2024). The impact of large language models on open-\nsource innovation: Evidence from GitHub Copilot. arXiv preprint arXiv:2409.08379. \nYu Y, Wang H, Filkov V, Devanbu P, Vasilescu B (2015) Wait for it: Determinants of pull request evaluation \nlatency on github. 2015 IEEE/ACM 12th working conference on mining software repositories (IEEE), 367-\n371. \nZhang X, Zhu F (2011) Group size and incentives to contribute: A natural experiment at Chinese Wikipedia. \nAmerican Economic Review 101(4):1601-1615. \nZhang X, Yu Y, Gousios G, Rastogi A (2022) Pull request decisions explained: An empirical overview. IEEE \nTransactions on Software Engineering 49(2):849-871. \nZhang Y, Lo D, Kochhar PS, Xia X, Li Q, Sun J (2017) Detecting similar repositories on GitHub. 2017 IEEE \n24th International Conference on Software Analysis, Evolution and Reengineering (SANER) (IEEE), 13-\n23. \n \nFigures and Tables: \n \nFigure 1: Open-Source Software Development Process  \n 40 \nTable 1: Definitions and Summary Statistics, Repository-Month-Level \nVariables \nDescription \nObs \nMean \nStd.  \nMin \nMax \nMerged_PR \nThe number of code contributions (i.e., merged PRs). \n139,329 \n23.65 \n70.59 \n1 \n3956 \nMerge_time \nThe average time taken (in hours) to accept a code submission. \n139,329 \n355.63 \n956.50 \n0 \n23810 \nMergedPR_per_dev \nThe average number of code contributions per developer. \n139,329 \n3.19 \n6.63 \n1 \n613 \nNum_devs_with_mergedPR \nThe number of distinct developers with code contributions. \n139,329 \n6.30 \n13.77 \n1 \n575 \nComments_per_mergedPR \nThe average number of comments per code contribution. \n139,329 \n220.44 \n646.57 \n0 \n17301 \nComments_per_dev_per_mergedPR \nThe average number of comments per developer per code contribution."
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_041",
    "source_id": "OpenSourceImpact2024",
    "text": "Merge_time \nThe average time taken (in hours) to accept a code submission. \n139,329 \n355.63 \n956.50 \n0 \n23810 \nMergedPR_per_dev \nThe average number of code contributions per developer. \n139,329 \n3.19 \n6.63 \n1 \n613 \nNum_devs_with_mergedPR \nThe number of distinct developers with code contributions. \n139,329 \n6.30 \n13.77 \n1 \n575 \nComments_per_mergedPR \nThe average number of comments per code contribution. \n139,329 \n220.44 \n646.57 \n0 \n17301 \nComments_per_dev_per_mergedPR \nThe average number of comments per developer per code contribution. \n139,329 \n118.19 \n340.95 \n0 \n9757 \nNum_devs_with_comments_per_mergedPR \nThe average number of developers who provided comments per code \ncontribution. \n139,329 \n1 \n0.95 \n0 \n7 \nReviews \nThe number of total reviews of core developers. \n139,329 \n24.7 \n85.09 \n1 \n4104 \nReviews_per_core \nThe average number of reviews per core developer. \n139,329 \n3.58 \n7.67 \n1 \n810 \nNum_core_with_reviews \nThe number of core developers who reviewed code submissions. \n139,329 \n3.01 \n7.92 \n1 \n186 \nMergedPR_1D \nThe number of code submissions accepted within one day. \n139,329 \n14.98 \n47.95 \n0  \n2569 \nMergedPR_3D \nThe number of code submissions accepted within three days. \n139,329 \n17.56 \n55.08 \n0 \n2953 \nMergedPR_10D \nThe number of code submissions accepted within ten days. \n139,329 \n20.77 \n63.37 \n0 \n3431 \nTotal_issues \nThe number of total issues. \n139,329 \n35.36 \n114.59 \n0 \n3741 \nTotal_bugs \nThe number of total bug-related issues. \n139,329 \n3.84 \n18.51 \n0 \n1049 \nTotal_issues_per_PR \nThe average number of total issues per code contribution. \n139,329 \n1.42 \n2.36 \n0 \n201 \nTotal_bugs_per_PR \nThe average number of total bug-related issues per code contribution. \n139,329 \n0.14 \n0.59 \n0 \n57 \nPeri_merged_PR_share \nThe share of code contributions from peripheral developers.  \n77,888 \n0.52 \n0.36 \n0.0005 \n1 \nPeri_merge_time_ratio \nThe ratio of the average time taken (in hours) to accept a code submission \nfrom peripheral developers to the average time taken to accept a code \nsubmission from all developers. \n77,888 \n1.56 \n2.56 \n0 \n254.1 \nPeri_mergedPR_1D_share \nThe share of code submissions from peripheral developers and accepted \nwithin one day. \n66,005 \n0.41 \n0.38 \n0 \n1 \nPeri_mergedPR_3D_share \nThe share of code submissions from peripheral developers and accepted \nwithin three days. \n68,419 \n0.43 \n0.37 \n0"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_042",
    "source_id": "OpenSourceImpact2024",
    "text": "0.36 \n0.0005 \n1 \nPeri_merge_time_ratio \nThe ratio of the average time taken (in hours) to accept a code submission \nfrom peripheral developers to the average time taken to accept a code \nsubmission from all developers. \n77,888 \n1.56 \n2.56 \n0 \n254.1 \nPeri_mergedPR_1D_share \nThe share of code submissions from peripheral developers and accepted \nwithin one day. \n66,005 \n0.41 \n0.38 \n0 \n1 \nPeri_mergedPR_3D_share \nThe share of code submissions from peripheral developers and accepted \nwithin three days. \n68,419 \n0.43 \n0.37 \n0 \n1 \nPeri_mergedPR_10D_share \nThe share of code submissions from peripheral developers and accepted \nwithin ten days. \n71,447 \n0.46 \n0.37 \n0 \n1 \nPeri_mergedPR_per_dev_ratio \nThe ratio of the average code contributions per peripheral developer to the \naverage code contributions per developer of both types (peripheral and core). \n77,888 \n0.76 \n0.36 \n0.008 \n7.5 \nPeri_dev_with_mergedPR_share \nThe share of peripheral developers with code contributions to all developers \nwith code contributions. \n77,888 \n0.63 \n0.29 \n0.006 \n1 \nPeri_comment_per_mergedPR_ratio \nThe ratio of average number of comments per peripheral developers\u2019 merged \nPR to average number of comments per merged PR. \n77,888 \n0.55 \n0.60 \n0 \n13.09 \nPeri_comment_dev_per_mergedPR_ratio \nThe ratio of average number of developers who provided comments per \nperipheral developers\u2019 merged PR to average number of developers who \nprovided comments per merged PR. \n77,888 \n0.55 \n0.52 \n0 \n2.86 \nPeri_comment_per_dev_per_mergedPR_ratio \nThe ratio of average number of comments per developer per peripheral \ndevelopers\u2019 merged PR to the average number of comments per developer per \nmerged PR. \n77,888 \n0.24 \n0.46 \n0 \n8.62 \n 41 \nTable 2: The Impact of Copilot on Project-Level Code Contributions and Coordination Time \n \n(1) \n(2) \n \nProject-level Code Contributions \nCoordination Time \n \nLog (Merged_PR) \nLog (Merge_time) \nATT of Copilot \n0.059*** \n0.080*** \n \n(0.007) \n(0.021) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \nTable 3: The Impact of Copilot on Project-Level Code Contributions (Robustness Checks) \n \n(1) \n(2) \n(3) \n(4) \n(5) \n \nGSCM  \nWithin-IDE  \nAnalysis \nGSCM  \nRefined"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_043",
    "source_id": "OpenSourceImpact2024",
    "text": " \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \nTable 3: The Impact of Copilot on Project-Level Code Contributions (Robustness Checks) \n \n(1) \n(2) \n(3) \n(4) \n(5) \n \nGSCM  \nWithin-IDE  \nAnalysis \nGSCM  \nRefined  \nSample \nGSCM  \nOutlier \nRemoval \nGSCM  \nNon-AI \nTopic \nPSM and DID  \nSingle Treatment \nTurn-on Time \n \nLog \n(Merged_PR) \nLog \n(Merged_PR) \nLog \n(Merged_PR) \nLog \n(Merged_PR) \nLog \n(Merged_PR) \nATT of Copilot \n0.031*** \n0.062*** \n0.060*** \n0.057*** \n0.172*** \n \n(0.012) \n(0.007) \n(0.008) \n(0.008) \n(0.027) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,462 \n7,369 \n7,557 \n6,668 \n2,122 \nObservations \n60,659 \n135,731 \n138,123 \n121,366 \n42,251 \nNotes: Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of the dependent variables \nused in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \nTable 4: The Impact of Copilot on Project-Level Coordination Time (Robustness Checks)  \n \n(1) \n(2) \n(3) \n(4) \n(5) \n \nGSCM  \nWithin-IDE  \nAnalysis \nGSCM  \nRefined  \nSample \nGSCM  \nOutlier \nRemoval \nGSCM  \nNon-AI \nTopic \nPSM and DID  \nSingle Treatment \nTurn-on Time \n \nLog (Merge_time) \nLog (Merge_time) \nLog (Merge_time) \nLog (Merge_time) \nLog (Merge_time) \nATT of Copilot \n0.055** \n0.080*** \n0.074*** \n0.088*** \n0.148*** \n \n(0.027) \n(0.022) \n(0.022) \n(0.021) \n(0.043) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,462 \n7,369 \n7,557 \n6,668 \n2,122 \nObservations \n60,659 \n135,731 \n138,123 \n121,366 \n42,251 \nNotes: Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of the dependent variables \nused in this table. *** p<"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_044",
    "source_id": "OpenSourceImpact2024",
    "text": "*** \n0.088*** \n0.148*** \n \n(0.027) \n(0.022) \n(0.022) \n(0.021) \n(0.043) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,462 \n7,369 \n7,557 \n6,668 \n2,122 \nObservations \n60,659 \n135,731 \n138,123 \n121,366 \n42,251 \nNotes: Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of the dependent variables \nused in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n \n \n \n 42 \nTable 5: The Impact of Copilot on Project-Level Code Contributions (Mechanism) \n \n(1) \n(2) \n \nIndividual Code Contributions \nDeveloper Coding Participation \n \nLog (MergedPR_per_dev) \nLog (Num_devs_with_mergedPR) \nATT of Copilot \n0.021*** \n0.034*** \n \n(0.005) \n(0.004) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable 6: The Impact of Copilot on Project-Level Coordination Time (Mechanism) \n \n(1) \n(2) \n(3) \n \nDiscussion Volume  \nDeveloper Discussion Participation  \nIndividual Discussion Intensity  \n \nLog (Comments \n_per_mergedPR) \nLog (Num_devs_with_comments \n_per_mergedPR) \nLog (Comments_per_dev \n_per_mergedPR) \nATT of Copilot \n0.065*** \n0.017*** \n0.059*** \n \n(0.017) \n(0.005) \n(0.014) \nRepository FE \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \nTable 7: The Impact of Copilot on Core Developers\u2019 Review Activities \n \n(1) \nTotal Reviews \n(2) \nIndividual Review Productivity \n(3) \nDeveloper Review Participation \n \nLog (Reviews) \nLog (Reviews_per_core) \nLog (Num_core_with_reviews) \nATT of Copilot \n0.059*** \n0.016** \n0.036*** \n \n(0.007) \n(0.007) \n(0.003) \nRepository FE \nYes \nYes \nYes \nMonth FE"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_045",
    "source_id": "OpenSourceImpact2024",
    "text": " refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \nTable 7: The Impact of Copilot on Core Developers\u2019 Review Activities \n \n(1) \nTotal Reviews \n(2) \nIndividual Review Productivity \n(3) \nDeveloper Review Participation \n \nLog (Reviews) \nLog (Reviews_per_core) \nLog (Num_core_with_reviews) \nATT of Copilot \n0.059*** \n0.016** \n0.036*** \n \n(0.007) \n(0.007) \n(0.003) \nRepository FE \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable 8: The Impact of Copilot on Project Productivity \n \n(1) \nTimely Merged PRs (1 Day)  \n(2) \nTimely Merged PRs (3 Days) \n(3) \nTimely Merged PRs (10 Days) \n \nLog (MergedPR_1D) \nLog (MergedPR_3D) \nLog (MergedPR_10D) \nATT of Copilot \n0.035*** \n0.041*** \n0.051*** \n \n(0.009) \n(0.009) \n(0.008) \nRepository FE \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n 43 \n \nTable 9: The Impact of Copilot on Project-Level Code Quality \n \n(1) \n(2) \n(3) \n(4) \n \nLog (Total_issues) \nLog (Total_bugs) \nLog (Total_issues_per_PR) \nLog (Total_bugs_per_PR) \nATT of Copilot \n0.070*** \n0.052* \n0.006 \n0.002 \n \n(0.007) \n(0.030) \n(0.004) \n(0.002) \nRepository FE \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_046",
    "source_id": "OpenSourceImpact2024",
    "text": " \nLog (Total_bugs_per_PR) \nATT of Copilot \n0.070*** \n0.052* \n0.006 \n0.002 \n \n(0.007) \n(0.030) \n(0.004) \n(0.002) \nRepository FE \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 \nfor detailed definitions of the dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable 10: Heterogeneous Effect of Copilot on Core versus Peripheral Developers \n \n(1) \nProject-level  \nCode Contributions \n(2) \nCoordination \nTime \n(3) \nTimely Merged \nPRs (1 Day) \n(4) \nTimely Merged \nPRs (3 Days) \n(5) \nTimely Merged \nPRs (10 Days) \n \nPeri_merged_ \nPR_share \nPeri_merge_\ntime_ratio \nPeri_mergedPR\n_1D_share \nPeri_mergedPR \n_3D_share \nPeri_mergedPR \n_10D_share \nATT of Copilot \n-0.019*** \n0.085** \n-0.021*** \n-0.024*** \n-0.020*** \n \n(0.004) \n(0.036) \n(0.006) \n(0.006) \n(0.005) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,154 \n4,154 \n3,350 \n3,528 \n3,729 \nObservations \n77,888 \n77,888 \n66,005 \n68,419 \n71,447 \nNotes: This table is based on a smaller sample (i.e., a total of 4,154 repositories) than the main sample because this analysis requires at least one \nmerged PR from peripheral developers (in order to compute merge time). The sample becomes smaller in columns (3) to (5) due to the exclusion \nof observations with zero PR merged in that particular time window, which would lead to a value of zero for the denominator. All estimations are \nbased on the GSCM method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of \nthe dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable 11: Heterogeneous Effect of Copilot on Core versus Peripheral Developers (Mechanism) \n \n(1) \nIndividual Code \nContributions \n(2) \nDeveloper Coding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual \nDiscussion Intensity \n \nPeri_mergedPR\n_per_dev_ratio \nPeri_dev_with \n_mergedPR_share \nPeri_comment_\nper_mergedPR \n_ratio"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_047",
    "source_id": "OpenSourceImpact2024",
    "text": "M method. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of \nthe dependent variables used in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable 11: Heterogeneous Effect of Copilot on Core versus Peripheral Developers (Mechanism) \n \n(1) \nIndividual Code \nContributions \n(2) \nDeveloper Coding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual \nDiscussion Intensity \n \nPeri_mergedPR\n_per_dev_ratio \nPeri_dev_with \n_mergedPR_share \nPeri_comment_\nper_mergedPR \n_ratio \nPeri_comment_ \ndev_per_mergedPR \n_ratio \nPeri_comment_ \nper_dev_per_ \nmergedPR_ratio \nATT of Copilot \n-0.008* \n-0.017*** \n0.029*** \n0.026*** \n0.013** \n \n(0.004) \n(0.003) \n(0.008) \n(0.006) \n(0.006) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,154 \n4,154 \n4,154 \n4,154 \n4,154 \nObservations \n77,888 \n77,888 \n77,888 \n77,888 \n77,888 \nNotes: This table is based on the same analysis sample as the one used for Table 10 to maintain consistency. All estimations are based on the GSCM \nmethod. Robust standard errors clustered at repository level in parentheses. Please refer to Table 1 for detailed definitions of the dependent variables \nused in this table. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n1 \n \nONLINE APPENDIX \n \nAppendix A: Details of the Main Estimation \nOur main causal inference uses the generalized synthetic control method (Xu 2017). Below, we provide a \nbrief overview of its estimation process and relate it to our study. \nModel: The GSCM combines the Interactive Fixed Effects (IFE) model (Bai 2009), which \nconsiders latent factors with effects that can vary across time and units, with the Synthetic Control (SC) \nmethod (Abadie et al. 2015), which creates synthetic control units to act as counterfactuals for the treated \nunits. This combination enables GSCM to (1) relax the assumption of pre-treatment parallel trends, and (2) \nconsider potential unobserved factors that vary over time at the unit level. Furthermore, GSCM offers \nseveral advantages over other estimation methods. Unlike the SC method, which is limited to a single \ntreated unit, GSCM is designed for multiple treated units, eliminating the need to construct synthetic control \nunits for each treatment unit one by one. Additionally, unlike the typical difference-in-differences model \ncombined with a specific matching technique, which is most effective for a single treatment turn-on time \nand a large control group, GSCM allows each treatment unit to have a different treatment period and can \nefficiently construct synthetic control units from a relatively small control sample. These advantages make \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_048",
    "source_id": "OpenSourceImpact2024",
    "text": " parallel trends, and (2) \nconsider potential unobserved factors that vary over time at the unit level. Furthermore, GSCM offers \nseveral advantages over other estimation methods. Unlike the SC method, which is limited to a single \ntreated unit, GSCM is designed for multiple treated units, eliminating the need to construct synthetic control \nunits for each treatment unit one by one. Additionally, unlike the typical difference-in-differences model \ncombined with a specific matching technique, which is most effective for a single treatment turn-on time \nand a large control group, GSCM allows each treatment unit to have a different treatment period and can \nefficiently construct synthetic control units from a relatively small control sample. These advantages make \nGSCM particularly well-suited to our data, as repositories adopted Copilot at different points in time and \nwe have relatively fewer repositories in the control sample than in the treatment sample. \nFirst, the GSCM adopts a linear IFE framework to model the latent factors. In our study, the \noutcome \ud835\udc4c!\", the log number of merged pull requests (PR) and the log average time taken to merge a single \nPR of repository \ud835\udc56 in month \ud835\udc61, can be expressed as: \n\ud835\udc4c!\" = d!\" \ud835\udc37!\" + \ud835\udc4b!\"\n# b + l!\n#\ud835\udc53\"  + e!\"                (1) \nwhere \ud835\udc37!\" is the treatment indicator which equals one if repository \ud835\udc56 has been developed with Copilot by \nmonth \ud835\udc61 and zero otherwise; the parameter of primary interest is d!\", which signifies the dynamic impact of \nCopilot on the log number of merged PRs and the log average time taken to merge a single PR. d!\" is the \nheterogeneous treatment effect and its subscripts \ud835\udc56 and \ud835\udc61 indicate that the estimates vary across units and \n2 \n \ntime. Let \ud835\udc5f be the number of latent factors, then \ud835\udc53\" represents the (\ud835\udc5f\u00d7 1) vector of unobserved common \nfactors, and l!\n#\n is the (1 \u00d7 \ud835\udc5f) vector of unknown factor loadings. The factor component l!\n#\ud835\udc53\"  can be \nexpressed as l!\n#\ud835\udc53\" = l!$\ud835\udc53$\" + l!%\ud835\udc53%\" + \u22ef+ l!&\ud835\udc53&\". A two-way fixed effects specification is a special case \nof the factor component, where \ud835\udc5f= 2, \ud835\udc53$\" = 1, and l!% = 1, so that l!\n#\ud835\udc53\" = l!$ + \ud835\udc53%\". Here, l!$ represents \nthe repository fixed effects, while \ud835\udc53%\" is the month fixed effects. We specify the two-way fixed effects to \ncontrol for heterogeneity across repositories and time, while considering other unobserved latent factors. In \ngeneral, l!\n#\ud835\udc53\" is estimated by an iterative factor analysis of the residuals from the model. If optimal number \nof unobserved latent factors determined by the cross-validation technique is zero, it means that the fixed \neffects setting has effectively accounted for any unobserved time-varying characteristics (Xu 2017). Lastly, \ne!\" is the error term with a mean of zero"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_049",
    "source_id": "OpenSourceImpact2024",
    "text": "\ud835\udc53\" = l!$ + \ud835\udc53%\". Here, l!$ represents \nthe repository fixed effects, while \ud835\udc53%\" is the month fixed effects. We specify the two-way fixed effects to \ncontrol for heterogeneity across repositories and time, while considering other unobserved latent factors. In \ngeneral, l!\n#\ud835\udc53\" is estimated by an iterative factor analysis of the residuals from the model. If optimal number \nof unobserved latent factors determined by the cross-validation technique is zero, it means that the fixed \neffects setting has effectively accounted for any unobserved time-varying characteristics (Xu 2017). Lastly, \ne!\" is the error term with a mean of zero.  \nEstimation of Latent Factors: An essential task when utilizing the IFE framework is to identify \nthe number of latent factors, denoted as \ud835\udc5f. Xu (2017) suggests a predictive analytics method for this purpose. \nFor clarity, we divide the total number of repositories into two groups: \ud835\udc47\ud835\udc5f for treated repositories and \ud835\udc36\ud835\udc5c \nfor control repositories. The latent-factor selection algorithm first applies Equation (1) to data from control \nunits only, covering both pre-treatment and post-treatment time periods, using the following equation: \n\ud835\udc4c!\" = \ud835\udc4b!\"\n# b + l!\n#\ud835\udc53\"  + e!\" , \u2200 \ud835\udc56\u2208 \ud835\udc36\ud835\udc5c               (2) \nSince the control repositories never received treatment during the data-collection period, d!\" \ud835\udc37!\" is \nexcluded from Equation (2). The value of \ud835\udc5f can vary within a range of candidate values specified by the \nresearchers. For each specified \ud835\udc5f, the algorithm runs Equation (2) and derives the estimates b6 and \n\ud835\udc53\"7. Subsequently, a cross-validation procedure is carried out for all treated repositories. Specifically, let \ud835\udc60 \nbe the index of a pre-treatment period, ranging from one (the first pre-treatment period) to \ud835\udc61) (the last pre-\ntreatment period). Equation (3) is then applied to each pre-treatment period, starting with \ud835\udc60= 1, for all \nrepositories in the treatment group by leaving one period out and using the remaining periods to estimate \nthe factor loadings.  \nl6!,*+ = ( \ud835\udc53:*+\n)! \ud835\udc53:*+\n) )*$ \ud835\udc53:*+\n)!(\ud835\udc4c!,*+\n)\n\u2212\ud835\udc4b!,*+\n)! b6), \ud835\udc56\u2208 \ud835\udc47\ud835\udc5f, \ud835\udc60= 1, \u2026 , \ud835\udc61)        (3) \n3 \n \nIn Equation (3),  \ud835\udc53:*+\n)  and b6  are the estimates obtained from Equation (2). The superscript 0 \nindicates periods before the introduction of the treatment, and the subscript \u2212\ud835\udc60 denotes all periods except \nfor \ud835\udc60. Based on these estimates, the algorithm predicts the outcome for treated unit \ud835\udc56 in period \ud835\udc60 using \n\ud835\udc4c6!+(0) = \ud835\udc4b!+\n# b6 + l!,*+\n#\n\ud835\udc53:+ and records the out-of-sample error \ud835\udc52!+ = \ud835\udc4c!+("
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_050",
    "source_id": "OpenSourceImpact2024",
    "text": "\ufffd\ufffd, \ud835\udc60= 1, \u2026 , \ud835\udc61)        (3) \n3 \n \nIn Equation (3),  \ud835\udc53:*+\n)  and b6  are the estimates obtained from Equation (2). The superscript 0 \nindicates periods before the introduction of the treatment, and the subscript \u2212\ud835\udc60 denotes all periods except \nfor \ud835\udc60. Based on these estimates, the algorithm predicts the outcome for treated unit \ud835\udc56 in period \ud835\udc60 using \n\ud835\udc4c6!+(0) = \ud835\udc4b!+\n# b6 + l!,*+\n#\n\ud835\udc53:+ and records the out-of-sample error \ud835\udc52!+ = \ud835\udc4c!+(0) \u2212 \ud835\udc4c6!+(0) for all  \ud835\udc56\u2208 \ud835\udc47\ud835\udc5f. In the \nfinal step, the algorithm calculates the Mean Squared Error (MSE) of the prediction, summed over all pre-\ntreatment periods \ud835\udc60, for each candidate value of \ud835\udc5f. The value of \ud835\udc5f that minimizes this prediction error is \nselected, and the corresponding set of latent factors will be used in the causal inference process. \nEstimation of Average Treatment Effects: The GSCM algorithm uses the estimated function to \npredict the counterfactuals for treated units, denoted as  \ud835\udc4c6!\"(0), representing the outcome of treated \nrepositories had they not received treatment in the post-treatment period. The causal effect of the treatment \nis quantified as the Average Treatment effect on the Treated unit (ATT), calculated mathematically as: \n\ud835\udc34\ud835\udc47\ud835\udc47\" = \n$\n|\ud835\udcaf| \u2211\nB\ud835\udc4c!\"(1) \u2212 \ud835\udc4c6!\"(0) C\n!\u2208\ud835\udcaf\n for \ud835\udc61> \ud835\udc61) \nwhere \ud835\udcaf denotes the set of treated units and |\ud835\udcaf| represents the number of units in \ud835\udcaf. \ud835\udc4c!\"(1) is the \nobserved outcome for treated repository \ud835\udc56 at time \ud835\udc61. The essence of GSCM lies in the fact that if the \npredicted outcomes for the treated repositories during the pre-treatment periods are accurate, the algorithm \ncan produce a valid counterfactual for each treated unit during the post-treatment periods. Finally, the \nGSCM uses bootstrapping to estimate confidence intervals and standard errors (Xu 2017). \nUnlike typical econometric modeling, the latent-factor selection algorithm in GSCM prioritizes \nmodels with the lowest out-of-sample prediction error to accurately predict counterfactuals, rather than \nthose with the best fit based on information criterion. As a result, models with more latent factors may not \nbe selected, even if they account for more confounding factors. Moreover, the GSCM without any latent \nfactor still yields a more valid estimate than a difference-in-differences (DID) model, except under two \nconditions: (1) the treatment is randomly assigned, satisfying the parallel trend assumption; and (2) no \ntreatment heterogeneity exists. If either of these conditions is not met, the DID estimate is likely to be \ninvalid. \n \n4 \n \nAppendix B: Equivalence Test of Pre-trend in GSCM \nWe apply"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_051",
    "source_id": "OpenSourceImpact2024",
    "text": " prediction error to accurately predict counterfactuals, rather than \nthose with the best fit based on information criterion. As a result, models with more latent factors may not \nbe selected, even if they account for more confounding factors. Moreover, the GSCM without any latent \nfactor still yields a more valid estimate than a difference-in-differences (DID) model, except under two \nconditions: (1) the treatment is randomly assigned, satisfying the parallel trend assumption; and (2) no \ntreatment heterogeneity exists. If either of these conditions is not met, the DID estimate is likely to be \ninvalid. \n \n4 \n \nAppendix B: Equivalence Test of Pre-trend in GSCM \nWe apply the equivalence test to examine the presence of a pre-treatment trend in GSCM (Pan and Qiu \n2022, Egami and Yamauchi 2023, Wang et al. 2023). This is a standard way to test the performance of \nGSCM (Liu et al. 2024), as the equivalence test better incorporates substantive considerations of what \nconstitutes good balance on covariates and placebo outcomes compared to traditional tests (Hartman and \nHidalgo 2018). Specifically, we use the two-one-sided \ud835\udc61 (TOST) test. The test includes an equivalence \nrange within which differences are deemed inconsequential (Hartman and Hidalgo 2018, Lakens et al. \n2018). The test is considered passed if the average prediction error for any pre-treatment period is within \nthe equivalence range (Liu et al. 2024).  \nThe result of the equivalence test for the main outcome variables, including project-level code \ncontributions, coordination time and project productivity which combines the effect of these two competing \nforces, are shown in Figures B.1, through B.5. We observe that the average prediction error with 90% \nconfidence intervals (the gray dotted line) is within the equivalence range (the red dotted line). Thus, we \ncan reject the null of inequivalence (equivalence test p-value is 0) and conclude that there exists no pre-\ntreatment trend (Hartman and Hidalgo 2018, Pan and Qiu 2022, Egami and Yamauchi 2023, Wang et al. \n2023, Liu et al. 2024). This indicates that a sufficient set of confounders have been controlled to address \nthe endogeneity concerns and that GSCM provides a good control group.  \n \nFigure B.1: Pre-trend test (TOST) of logged merged_PR \n5 \n \n \n \nFigure B.2: Pre-trend test (TOST) of logged merge_time \n \n \nFigure B.3: Pre-trend test (TOST) of logged merged_PR_1D \n \n \n6 \n \n \nFigure B.4: Pre-trend test (TOST) of logged merged_PR_3D \n \n \n \nFigure B.5: Pre-trend test (TOST) of logged merged_PR_10D \n \n \n \n7 \n \nAppendix C: Placebo Test in GSCM \nTo further validate our causal identification strategy based on GSCM, we conduct placebo tests by \nartificially shifting the actual Copilot adoption time earlier. Specifically, we move the observed Copilot \nadoption"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_052",
    "source_id": "OpenSourceImpact2024",
    "text": " merged_PR \n5 \n \n \n \nFigure B.2: Pre-trend test (TOST) of logged merge_time \n \n \nFigure B.3: Pre-trend test (TOST) of logged merged_PR_1D \n \n \n6 \n \n \nFigure B.4: Pre-trend test (TOST) of logged merged_PR_3D \n \n \n \nFigure B.5: Pre-trend test (TOST) of logged merged_PR_10D \n \n \n \n7 \n \nAppendix C: Placebo Test in GSCM \nTo further validate our causal identification strategy based on GSCM, we conduct placebo tests by \nartificially shifting the actual Copilot adoption time earlier. Specifically, we move the observed Copilot \nadoption date three months earlier than its true occurrence and re-estimate the GSCM model under these \nplacebo scenarios. These falsification tests are essential for verifying the strict exogeneity assumption, as \nthey enable us to detect potential violations or model misspecifications. If our model specification is valid \nand the observed effects genuinely result from Copilot adoption, these placebo tests should yield no \nsignificant effects.  \nTo statistically evaluate the results from our placebo analyses, we apply the two-one-sided t (TOST) \nequivalence test, which assesses whether the estimated placebo effects are practically equivalent to zero \n(Liu et al. 2024). Similar to pre-trend assessments, the TOST approach involves specifying an equivalence \ninterval around zero and testing the null hypothesis that there is a meaningful placebo effect.  \nThe results from these tests for the main outcome variables, including project-level code \ncontributions, coordination time and project productivity, are shown in Figures C.1, through C.5. We \nobserve that the placebo effect estimates during the artificial pre-treatment period are at or below zero, \nwhile those from the artificial post-treatment period are substantially above zero. The equivalence tests \nindicate that the placebo effects remain within the equivalence range defined around zero (equivalence test \np-value is 0). Thus, we can reject the null hypothesis of inequivalence, thereby supporting the validity and \nrobustness of our causal identification strategy.  \nIt is worth noting that there is a drop at the onset of treatment (i.e., time = 0) in contribution-related \nmeasures, such as the number of merged PRs and the number of timely merged PRs. This drop likely \nreflects an initial adjustment period as developers adapt to the AI pair programmer or integrate it into their \nworkflow. Importantly, the equivalence holds in the pre-treatment window, supporting the credibility of the \nsynthetic control construction. \n8 \n \n \nFigure C.1 Placebo Equivalence Test of logged merged_PR  \n \n \nFigure C.2: Placebo Equivalence Test of logged merge_time  \n \n9 \n \n \nFigure C.3: Placebo Equivalence Test of logged merged_PR_1D \n \n \n \nFigure C.4: Placebo Equivalence Test of logged merged_PR_3D \n \n10 \n \n \nFigure C.5: Placebo Equivalence Test of logged merged_PR_10D \n \n \n \n11 \n \nAppendix D: Summary of Empirical Methods \nTable D.1: Overview of Empirical Methods \nEmpirical methods \nDescription \nKey takeaways \nGeneralized Synthetic Control \nMethod in the Main Analysis \nWe use the GSCM to construct a \nweighted control unit that matches \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_053",
    "source_id": "OpenSourceImpact2024",
    "text": "1 Placebo Equivalence Test of logged merged_PR  \n \n \nFigure C.2: Placebo Equivalence Test of logged merge_time  \n \n9 \n \n \nFigure C.3: Placebo Equivalence Test of logged merged_PR_1D \n \n \n \nFigure C.4: Placebo Equivalence Test of logged merged_PR_3D \n \n10 \n \n \nFigure C.5: Placebo Equivalence Test of logged merged_PR_10D \n \n \n \n11 \n \nAppendix D: Summary of Empirical Methods \nTable D.1: Overview of Empirical Methods \nEmpirical methods \nDescription \nKey takeaways \nGeneralized Synthetic Control \nMethod in the Main Analysis \nWe use the GSCM to construct a \nweighted control unit that matches \nthe outcome variable of the treated \nunit during the pre-treatment period. \nBecause the GSCM models the trend of the outcome \nvariable, our results are robust to the effects of \nunobservable confounders that change over time. \nGeneralized Synthetic Control \nMethod in a Within-IDE Analysis  \nWe focus on repositories using \nCopilot-supported IDEs and further \nrestrict the treatment versus control \ngroup comparison within the same \nCopilot-supported IDE. \nOur results remain robust in this within-IDE analysis, \nwhich addresses concerns regarding unobserved \ndifferences between repositories using supported versus \nunsupported IDEs, as well as potential differences \nacross different IDEs that support Copilot. \nGeneralized Synthetic Control \nMethod with Refined Sample \nWe exclude repositories associated \nwith developers who participate in \nboth treatment and control \nrepositories. \nOur results are robust with this refined sample and \naddress the concern that the knowledge transfer of \ndevelopers might bias the results. \nGeneralized Synthetic Control \nMethod and Outlier Removal \nWe exclude repositories with extreme \nvalues in project-level outcomes to \ntest robustness against the remaining \nsample. \nOur results are consistent after outlier removal, \nindicating results are not driven by extreme cases. \nGeneralized Synthetic Control \nMethod and non-AI Topic Projects \nWe remove repositories focused on \nAI topics to test whether the observed \neffects are driven by AI-intensive \nprojects. \nOur results remain stable, suggesting that the observed \neffects are generalizable beyond AI-focused \nrepositories. \nPropensity Score Matching and \nDifference-in-Differences \nWe use the PSM to construct a \ncomparable control group with a \nsingle treatment turn-on time and \nanalyze the effect using the DID \nestimation. \nOur results are consistent after addressing the concerns \nabout non-random treatment turn-on time for Copilot \nusage in the baseline analysis. \nGeneralized Synthetic Control \nMethod and Alternative Measures \nWe use alternative measures to test if \nthe observed effects are sensitive to \nthe choice of dependent variables.  \nOur results show consistency across alternative outcome \nvariables, ruling out bias from specific definitions of \noutcomes. \nGeneralized Synthetic Control \nMethod and Workload Effect  \nWe add the cumulative submitted \ncode as the control in coordination \ntime analysis. \nThe finding that Copilot increases coordination time \nremains robust after controlling for submitted code, \nsuggesting that the increase in coordination time is not \nsolely driven by higher levels of code submission. \nTwo-Way Fixed Effects with Full \nSample \nWe apply two-way fixed effects to \ntest the direct effect of Copilot on \nproject-level code contributions and \ncoordination time. \nOur results are consistent with"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_054",
    "source_id": "OpenSourceImpact2024",
    "text": " if \nthe observed effects are sensitive to \nthe choice of dependent variables.  \nOur results show consistency across alternative outcome \nvariables, ruling out bias from specific definitions of \noutcomes. \nGeneralized Synthetic Control \nMethod and Workload Effect  \nWe add the cumulative submitted \ncode as the control in coordination \ntime analysis. \nThe finding that Copilot increases coordination time \nremains robust after controlling for submitted code, \nsuggesting that the increase in coordination time is not \nsolely driven by higher levels of code submission. \nTwo-Way Fixed Effects with Full \nSample \nWe apply two-way fixed effects to \ntest the direct effect of Copilot on \nproject-level code contributions and \ncoordination time. \nOur results are consistent with TWFE and the full \nsample, suggesting that our results are robust to \nalternative estimation strategies. \nGeneralized Synthetic Control \nMethod with Expanded Sample \nWe validate our results using an \nexpanded sample that includes small \nrepositories with only two \ndevelopers. \nOur results are consistent with the expanded sample, \nsuggesting that the observed patterns are not specific to \nactive repositories with large developer teams. \nGeneralized Synthetic Control \nMethod and Additional Validation \nWe include months without any \nmerging activity in the analysis. \nThe results are consistent, suggesting that our results are \nrobust to different sample selection. \nEquivalence Pre-trend Tests and \nPlacebo Tests \nWe conduct equivalence tests to \nevaluate the performance of GSCM \nand eliminate concerns of pre-trend \nand model misspecification. \nOur results support the validity of the GSCM \nspecification, confirming no significant pre-trend effects \nand indicating that observed effects are attributable to \nCopilot adoption. \nRelative Time Model (RTM) \nWe use the RTM to verify whether \nthe parallel trend assumption holds in \nthe analysis of PSM and DID. \nOur results pass the parallel pre-trend test, indicating \nthat we have constructed a comparable control group in \nthe analysis of PSM and DID. \n \n \n12 \n \nAppendix E: Balance Checks in PSM \nTo construct a matched sample, we employ a one-to-one nearest neighbor matching approach without \nreplacement, pairing each treated repository that adopted Copilot with a single control repository that did \nnot use Copilot. Matching without replacement ensures that each control repository is used only once, \npreventing any single control project from disproportionately influencing the results. While this may reduce \nthe total number of matches, it improves the integrity of the comparison by limiting reuse of controls. \nMatching is performed based on a set of repository characteristics measured during the six-month pre-\ntreatment period (January to June 2021). For each characteristic, we calculate the monthly values, average \nthem across the six-month window, and then apply a log transformation. These variables include the number \nof merged PRs, the average time to merge a PR, the number of distinct developers with merged PRs, and \nthe number of push events, release events, opened issues, and closed issues.  \nTable E.1 reports the balance check results for the matched sample, indicating that there are no \nstatistically significant differences between the treatment and control groups across these pre-treatment \ncovariates after matching. \nTable E.1: Balance Check of Matched Sample \nVariable \nTreatment Mean \nControl Mean \nDifference \nP-value \nLog (Merged_PR) \n2.10"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_055",
    "source_id": "OpenSourceImpact2024",
    "text": " characteristic, we calculate the monthly values, average \nthem across the six-month window, and then apply a log transformation. These variables include the number \nof merged PRs, the average time to merge a PR, the number of distinct developers with merged PRs, and \nthe number of push events, release events, opened issues, and closed issues.  \nTable E.1 reports the balance check results for the matched sample, indicating that there are no \nstatistically significant differences between the treatment and control groups across these pre-treatment \ncovariates after matching. \nTable E.1: Balance Check of Matched Sample \nVariable \nTreatment Mean \nControl Mean \nDifference \nP-value \nLog (Merged_PR) \n2.10 \n2.05 \n0.05 \n0.20 \nLog (Merge_time) \n4.12 \n4.09 \n0.03 \n0.60 \nLog (Num_devs_with_mergedPR) \n1.32 \n1.29 \n0.03 \n0.13 \nLog (Push) \n2.96 \n2.91 \n0.05 \n0.33 \nLog (Release) \n0.28 \n0.27 \n0.01 \n0.79 \nLog (Open_issue) \n1.13 \n1.11 \n0.02 \n0.68 \nLog (Closed_issue) \n1.05 \n1.04 \n0.01 \n0.79 \n \n \n \n13 \n \nAppendix F: Relative Time Model (RTM) \nWe employ the RTM to examine whether there is any pre-treatment trend in the analysis of PSM and DID, \ni.e., whether the outcome variables of repositories in the treatment group are different from that in the \ncontrol group even before the treatment (i.e., the availability of Copilot) happens. The RTM has been used \nin the economics and information systems literature to validate the pre-treatment parallel trend assumption \n(Lu et al. 2019, Alyakoob and Rahman 2022). More specifically, we use the following RTM (Autor 2003). \n\ud835\udc4c!\" = \ud835\udefd)+ H \ud835\udefd$,/ I\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61! \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc61\"*/M +\n0\n/1$\n H \ud835\udefd%,/ I\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61! \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc61\"2/M +\n3\n/1)\n\ud835\udefc! + \ud835\udf07\" + \ud835\udf00!\"  \nIn this model, the sum on the right-hand side represents \ud835\udc5e lead indicators (anticipatory effects) and  \ud835\udc5d lagged \nindicators (posttreatment effects) where \ud835\udc4c!\" denotes the log number of merged PRs or the log average time \ntaken to merge a single PR for repository \ud835\udc56 in month \ud835\udc61. For our analysis of PSM and DID, we set \ud835\udc5e= 4 and \n\ud835\udc5d= 18. When the pre-treatment trend exists, the lead indicators would be able to predict changes in \ndependent variables, the log number of merged PRs and the log average time taken to merge a PR. \nConversely, if"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_056",
    "source_id": "OpenSourceImpact2024",
    "text": "\ufffd!\"  \nIn this model, the sum on the right-hand side represents \ud835\udc5e lead indicators (anticipatory effects) and  \ud835\udc5d lagged \nindicators (posttreatment effects) where \ud835\udc4c!\" denotes the log number of merged PRs or the log average time \ntaken to merge a single PR for repository \ud835\udc56 in month \ud835\udc61. For our analysis of PSM and DID, we set \ud835\udc5e= 4 and \n\ud835\udc5d= 18. When the pre-treatment trend exists, the lead indicators would be able to predict changes in \ndependent variables, the log number of merged PRs and the log average time taken to merge a PR. \nConversely, if the pre-treatment trend is unlikely to be a concern, the lead indicators would not predict \ndependent variables (Autor 2003, Angrist and Pischke 2009). The estimation results of the RTM are shown \nin Table F.1. We observe that the coefficients for the four lead indicators are statistically insignificant, \nindicating that there does not exist a pre-treatment trend.  \n \n \n \n \n \n \n \n14 \n \n \nTable F.1: Pre-trend Test for PSM and DID Analysis \n \n(1) \n(2) \n \nPSM and DID \nPSM and DID \n \nLog (Merged_PR) \nLog (Merge_time) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5f\ud835\udc524\ud835\udc5a \n-0.033 \n(0.034) \n0.037 \n(0.088) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5f\ud835\udc523\ud835\udc5a \n-0.020 \n(0.038) \n0.094 \n(0.090) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5f\ud835\udc522\ud835\udc5a \n-0.050 \n(0.041) \n0.089 \n(0.094) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5f\ud835\udc521\ud835\udc5a \n-0.031 \n(0.041) \n0.100 \n(0.094) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc610\ud835\udc5a \n0.036 \n(0.044) \n0.165* \n(0.098) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc611\ud835\udc5a \n0.068 \n(0.046) \n0.100 \n(0.101) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc612\ud835\udc5a \n0.097** \n(0.047) \n0.090 \n(0.100) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_057",
    "source_id": "OpenSourceImpact2024",
    "text": "036 \n(0.044) \n0.165* \n(0.098) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc611\ud835\udc5a \n0.068 \n(0.046) \n0.100 \n(0.101) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc612\ud835\udc5a \n0.097** \n(0.047) \n0.090 \n(0.100) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc613\ud835\udc5a \n0.042 \n(0.046) \n0.237** \n(0.116) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc614\ud835\udc5a \n0.159*** \n(0.047) \n0.325*** \n(0.100) \n\ud835\udc36\ud835\udc5c\ud835\udc5d\ud835\udc56\ud835\udc59\ud835\udc5c\ud835\udc61\" \u00d7 \ud835\udc43\ud835\udc5c\ud835\udc60\ud835\udc615\ud835\udc5a+ \n0.175*** \n(0.042) \n0.257*** \n(0.075) \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n15 \n \nAppendix G: Additional Robustness Checks \n \nTable G.1: The Impact of Copilot on Project-Level Code Contributions \n(Alternative Measures of Code Contributions) \n \n(1) \n(2) \n \nGSCM  \nMain Sample  \nGSCM  \nMain Sample  \n \nLog (Submitted PRs) \nLog (Commits) \nATT of Copilot \n0.075*** \n0.107*** \n \n(0.009) \n(0.014) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \nTable G.2: The Impact of Copilot on Project-Level Coordination Time \n(Alternative Measures of Coordination Time) \n \n(1) \n(2) \n \nGSCM  \nMain Sample \nGSCM  \nMain Sample \n \nLog (Merge time in days) \nLog (Merge time in minutes) \nATT of Copilot \n0.049*** \n0.101*** \n \n(0.016) \n(0.032) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n*"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_058",
    "source_id": "OpenSourceImpact2024",
    "text": " Measures of Coordination Time) \n \n(1) \n(2) \n \nGSCM  \nMain Sample \nGSCM  \nMain Sample \n \nLog (Merge time in days) \nLog (Merge time in minutes) \nATT of Copilot \n0.049*** \n0.101*** \n \n(0.016) \n(0.032) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \nTable G.3: The Impact of Copilot on Project-Level Coordination Time with Code Submission  \n \n(1) \n(2) \n(3) \n(4) \n(5) \n(6) \n \nGSCM  \nMain \nAnalysis \nGSCM  \nWithin-IDE  \nAnalysis \nGSCM  \nRefined  \nSample \nGSCM  \nOutlier \nRemoval \nGSCM  \nNon-AI \nTopic \nPSM and DID  \nSingle Treatment \nTurn-on Time \n \nLog \n(Merge_time) \nLog \n(Merge_time) \nLog \n(Merge_time) \nLog \n(Merge_time) \nLog \n(Merge_time) \nLog \n(Merge_time) \nATT of Copilot \n0.079*** \n0.055** \n0.077*** \n0.072*** \n0.087*** \n0.147*** \n \n(0.023) \n(0.028) \n(0.021) \n(0.023) \n(0.021) \n(0.044) \nLog (Open_PR) \n0.031 \n(0.029) \n0.041 \n(0.041) \n0.054* \n(0.031) \n0.021 \n(0.032) \n0.028 \n(0.032) \n0.057 \n(0.037) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n4,462 \n7,369 \n7,557 \n6,668 \n2,122 \nObservations \n139,329 \n60,659 \n135,731 \n138,123 \n121,366 \n42,251 \nNotes: Open_PR refers to the total number of cumulative submitted PRs till the focal month and is added to the regressions to control for the effect \nof workload. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n \n16 \n \n \n \nTable G.4: The Impact of Copilot on Project-Level Code Contributions and Coordination Time  \n(Two-way Fixed Effects with Full Sample) \n \n(1) \n(2) \n \nTWFE \nProject-level Code Contributions \nTWFE \nCoordination Time \n \nLog (Merged_PR) \nLog (Merge_time) \nATT of Copilot \n0.029*** \n0.036** \n \n(0.008) \n(0.018) \nRepository FE \nYes \nYes \nMonth FE"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_059",
    "source_id": "OpenSourceImpact2024",
    "text": "ions to control for the effect \nof workload. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n \n16 \n \n \n \nTable G.4: The Impact of Copilot on Project-Level Code Contributions and Coordination Time  \n(Two-way Fixed Effects with Full Sample) \n \n(1) \n(2) \n \nTWFE \nProject-level Code Contributions \nTWFE \nCoordination Time \n \nLog (Merged_PR) \nLog (Merge_time) \nATT of Copilot \n0.029*** \n0.036** \n \n(0.008) \n(0.018) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n9,244 \n9,244 \nObservations \n140,084 \n140,084 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable G.5: The Impact of Copilot on Project-Level Code Contributions and Coordination Time \n(Expanded Sample that Includes Repositories with Two Developers) \n \n(1) \n(2) \n \nGSCM Expanded Sample \nProject-level Code Contributions \nGSCM Expanded Sample \nCoordination Time \n \nLog (Merged_PR) \nLog (Merge_time) \nATT of Copilot \n0.013** \n0.083*** \n \n(0.006) \n(0.015) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n12,512 \n12,512 \nObservations \n240,433 \n240,433 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \nTable G.6: The Impact of Copilot on Project-Level Code Contributions  \n(Expanded Sample that Includes Repository-month Observations with Zero Merged PR) \n \n(1) \n(2) \n(3) \n \nProject-level Code Contributions \nIndividual Code Contributions \nDeveloper Coding Participation \n \nLog  \n(Merged_PR) \nLog  \n(MergedPR_per_dev) \nLog \n(Num_devs_with_mergedPR) \nATT of Copilot \n0.066*** \n0.058*** \n0.050*** \n \n(0.006) \n(0.005) \n(0.004) \nRepository FE \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \n# of repositories \n8,965 \n8,965 \n8,965 \nObservations \n206,897 \n206,897 \n206,897 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n17 \n \nAppendix H: Additional Evidence on the Mechanisms for H1 and H2 \nTable H.1: How Individual Code Contributions and Developer Coding Participation are Correlated with \nProject-Level Code Contributions  \n \n(1) \n(2) \n \nLog (Merged_PR) \nLog (Merged_PR) \nLog (MergedPR_per_dev) \n1.173*** \n("
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_060",
    "source_id": "OpenSourceImpact2024",
    "text": "965 \n8,965 \nObservations \n206,897 \n206,897 \n206,897 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n17 \n \nAppendix H: Additional Evidence on the Mechanisms for H1 and H2 \nTable H.1: How Individual Code Contributions and Developer Coding Participation are Correlated with \nProject-Level Code Contributions  \n \n(1) \n(2) \n \nLog (Merged_PR) \nLog (Merged_PR) \nLog (MergedPR_per_dev) \n1.173*** \n(0.006) \n \nLog (Num_devs_with_mergedPR) \n \n1.213*** \n(0.006) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable H.2: How Code Discussion is Correlated with Project-Level Coordination Time  \n \n(1) \n(2) \n(3) \n \nLog (Merge_time) \nLog (Merge_time) \nLog (Merge_time) \nLog (Comment_per_mergedPR) \n0.059*** \n(0.004) \n \n \n  \n \nLog (Num_devs_with_comments_per_mergedPR) \n \n0.259*** \n(0.016) \n \n  \n \nLog (Comments_per_dev_per_mergedPR) \n \n \n0.063*** \n(0.004) \n  \n \nRepository FE \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \nTable H.3: How Project-Level Code Contributions and Coordination Time are Correlated with Project-\nLevel Productivity \n \n(1) \n(2) \n(3) \n(4) \n(5) \n(6) \n \nLog \n(MergedPR\n_1D) \nLog \n(MergedPR\n_1D) \nLog \n(MergedPR\n_3D) \nLog \n(MergedPR\n_3D) \nLog \n(MergedPR\n_10D) \nLog \n(MergedPR\n_10D) \nLog (Merge_time) \n-0.152*** \n(0.002) \n \n-0.144*** \n(0.002) \n \n-0.119*** \n(0.002) \n \nLog (Merged_PR) \n \n1.027*** \n(0.003) \n \n1.058*** \n(0.002) \n \n1.069*** \n(0.002) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \n7,637 \n7,637 \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_061",
    "source_id": "OpenSourceImpact2024",
    "text": "D) \nLog \n(MergedPR\n_10D) \nLog (Merge_time) \n-0.152*** \n(0.002) \n \n-0.144*** \n(0.002) \n \n-0.119*** \n(0.002) \n \nLog (Merged_PR) \n \n1.027*** \n(0.003) \n \n1.058*** \n(0.002) \n \n1.069*** \n(0.002) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \n139,329 \n139,329 \n139,329 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n \n18 \n \nAppendix I: Text Analysis of Comment Content \n \nPrior research in team coordination emphasizes that the content of discussions also plays a role in shaping \ncoordination time, as greater diversity in discussion topics can increase the complexity and difficulty of \ncoordination (Hansen et al. 2018). Building on this insight, we argue that the same logic may apply to \nsoftware development, where developer code discussions serve as a primary mechanism for coordination. \nIn this context, a broader range of topics discussed may similarly introduce additional coordination \nchallenges.  \nTo examine the effect of GitHub Copilot on the diversity of code discussion topics within merged \nPRs, we apply a Latent Dirichlet Allocation (LDA) topic modeling approach. Specifically, we measure both \nthe number of topics discussed per merged PR and the entropy of topic distribution. We adopt the LDA \nmodel for three key reasons. First, LDA addresses the data sparsity challenges inherent in traditional \napproaches such as TF-IDF. Second, the topics and associated keywords generated by LDA are human-\ninterpretable, in contrast to the latent dimensions produced by models like doc2vec. Third, LDA has been \nwidely validated and adopted in prior literature across various research contexts, including scientific \npublications (Griffiths and Steyvers 2004, Wang and Blei 2011), social media content (Ramage et al. 2010, \nWeng et al. 2010, Lee et al. 2016, Shin et al. 2020), business descriptions (Shi et al. 2016), and mobile App \ndescription (Lee et al. 2020). The key advantage of using LDA lies in its ability to identify latent themes \nfrom the data by grouping semantically related keywords into coherent topics, rather than treating each \nword as an independent feature. We exclude comments that are not in English or are too short to convey \nmeaningful information. After this step, we estimate the topic model using a corpus of 5,404,735 merged \nPR comments collected from selected repositories between 2021 and 2022. This modeling approach \nprovides a multinomial distribution over the inferred topics for each merged PR, enabling us to assess both \nthe number of distinct topics present and the evenness of their distribution.  \nA"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_062",
    "source_id": "OpenSourceImpact2024",
    "text": " and mobile App \ndescription (Lee et al. 2020). The key advantage of using LDA lies in its ability to identify latent themes \nfrom the data by grouping semantically related keywords into coherent topics, rather than treating each \nword as an independent feature. We exclude comments that are not in English or are too short to convey \nmeaningful information. After this step, we estimate the topic model using a corpus of 5,404,735 merged \nPR comments collected from selected repositories between 2021 and 2022. This modeling approach \nprovides a multinomial distribution over the inferred topics for each merged PR, enabling us to assess both \nthe number of distinct topics present and the evenness of their distribution.  \nA critical hyperparameter in LDA modeling is the number of topics to extract. Several methods \nhave been proposed to determine the optimal number, including metrics such as perplexity, topic coherence, \nand topic diversity. Given our objective of categorizing advice-related content into distinct yet coherent \n19 \n \nthemes, we rely on both coherence and diversity scores to guide model selection. Based on these criteria, \nwe identify 10 as the optimal number of topics, balancing topic distinctiveness and coherence.  \nThe results, presented in Table I.1, indicate that following the adoption of Copilot use, there was \nan increase in both the number of code discussion topics and the entropy score. This suggests that Copilot \nintroduces diverse code discussion content, leading to increased coordination time. \nTable I.1: The Impact of Copilot on Project-Level Code Discussion Content \n \n(1) \n(2) \n \nEntropy_score \nTopics_count \nATT of Copilot \n0.017** \n0.029** \n \n(0.007) \n(0.014) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n4,009 \n4,009 \nObservations \n78,636 \n78,636 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n20 \n \nAppendix J: The Impact of Copilot and Project Complexity \n \nIn OSS communities, complex projects typically contain more features and code modules. Prior work \nsuggests that developers are more likely to contribute to complex projects, as they offer greater status and \nvisibility benefits (Chengalur-Smith et al. 2010, Hann et al. 2013, Medappa and Srivastava 2019). However, \nthese projects also tend to impose greater demands for knowledge sharing, communication, and mutual \nunderstanding (Williams 2011, Zimmermann et al. 2018). While generative AI tools enhance developers\u2019 \ncapacity to produce code, they do not reduce the inherent complexity of a project. There are two plausible, \nyet opposing, mechanisms through which project complexity might influence the observed effects of \nCopilot.  \nOn one hand, complex projects may attract more contributors due to their higher status potential, \nwhich can lead to greater project-level code contributions. However, the increased number of contributors \nalso brings a wider range of perspectives and development styles, making it more difficult to reach \nconsensus and resolve conflicting views, thereby increasing coordination time. On the other hand, these \nprojects may present"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_063",
    "source_id": "OpenSourceImpact2024",
    "text": " sharing, communication, and mutual \nunderstanding (Williams 2011, Zimmermann et al. 2018). While generative AI tools enhance developers\u2019 \ncapacity to produce code, they do not reduce the inherent complexity of a project. There are two plausible, \nyet opposing, mechanisms through which project complexity might influence the observed effects of \nCopilot.  \nOn one hand, complex projects may attract more contributors due to their higher status potential, \nwhich can lead to greater project-level code contributions. However, the increased number of contributors \nalso brings a wider range of perspectives and development styles, making it more difficult to reach \nconsensus and resolve conflicting views, thereby increasing coordination time. On the other hand, these \nprojects may present higher entry barriers, which could limit participation in both coding and non-coding \nactivities (i.e., code discussion) and reduce coordination time. In either case, it is important to determine \nwhether the effects we observe are simply artifacts of project complexity rather than the result of Copilot \nadoption. To address this concern, we conduct the analysis by splitting the sample into complex versus \nsimple projects based on the number of lines of code in each repository prior to Copilot adoption. We use \na median split, classifying repositories above the median as complex and those below the median as simple.  \nThe results in Tables J.1 and J.2 show that both simple and complex projects exhibit similar patterns. \nCopilot adoption is associated with increased project-level code contributions and longer coordination time \nin both cases. To further examine this, we conduct PSM and DID analyses using an interaction term for \nproject complexity. We define a binary variable \u201cComplexity\u201d that equals 1 if a project's lines of code \nexceed the median value before the Copilot use and 0 otherwise. As shown in Table J.3, the interaction \nterm is not statistically significant, indicating no difference in Copilot\u2019s impact between simple and \ncomplex projects. \n21 \n \nTable J.1: The Impact of Copilot on Simple Projects \n \n(1) \nProject-level Code \nContributions \n(2) \nCoordination \nTime \n(3) \nTimely Project \nOutputs (1 Day) \n(4) \nTimely Project \nOutputs (3 Days) \n(5) \nTimely Project \nOutputs (10 Days) \n \nLog  \n(Merged_PR) \nLog \n(Merge_time) \nLog  \n(MergedPR_1D) \nLog  \n(MergedPR_3D) \nLog  \n(MergedPR_10D) \nATT of Copilot \n0.046*** \n0.082** \n0.025** \n0.029*** \n0.039*** \n \n(0.011) \n(0.032) \n(0.012) \n(0.011) \n(0.011) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n3,591 \n3,591 \n3,591 \n3,591 \n3,591 \nObservations \n54,404 \n54,404 \n54,404 \n54,404 \n54,404 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n*"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_064",
    "source_id": "OpenSourceImpact2024",
    "text": " \n0.039*** \n \n(0.011) \n(0.032) \n(0.012) \n(0.011) \n(0.011) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n3,591 \n3,591 \n3,591 \n3,591 \n3,591 \nObservations \n54,404 \n54,404 \n54,404 \n54,404 \n54,404 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \nTable J.2: The Impact of Copilot on Complex Projects \n \n(1) \nProject-level Code \nContributions \n(2) \nCoordination \nTime \n(3) \nTimely Project \nOutputs (1 Day) \n(4) \nTimely Project \nOutputs (3 Days) \n(5) \nTimely Project \nOutputs (10 Days) \n \nLog  \n(Merged_PR) \nLog \n(Merge_time) \nLog  \n(MergedPR_1D) \nLog  \n(MergedPR_3D) \nLog  \n(MergedPR_10D) \nATT of Copilot \n0.086*** \n0.068** \n0.065*** \n0.071*** \n0.079*** \n \n(0.012) \n(0.029) \n(0.014) \n(0.013) \n(0.014) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,046 \n4,046 \n4,046 \n4,046 \n4,046 \nObservations \n78,706 \n78,706 \n78,706 \n78,706 \n78,706 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \nTable J.3: The Impact of Copilot on Project Complexity (PSM and DID) \n \n(1) \nProject-level Code Contributions \n(2) \nCoordination Time \n \nLog  \n(Merged_PR) \nLog \n(Merge_time) \nCopilot \n0.142*** \n0.150*** \n \n(0.034) \n(0.057) \nCopilot*Complexity \n0.059 \n(0.039) \n-0.003 \n(0.063) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n2,122 \n2,122 \nObservations \n42,251 \n42,251 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n22 \n \nAppendix K: Additional Analyses on the Differential Effects of Copilot among Core and Peripheral \nDevelopers \nWe construct 20 metrics to capture absolute changes in project-level outcomes for both core and peripheral \ndevelopers. These metrics span four key dimensions: project-level code contributions, coordination time, \nproject-level productivity, and"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_065",
    "source_id": "OpenSourceImpact2024",
    "text": "-0.003 \n(0.063) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n2,122 \n2,122 \nObservations \n42,251 \n42,251 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n22 \n \nAppendix K: Additional Analyses on the Differential Effects of Copilot among Core and Peripheral \nDevelopers \nWe construct 20 metrics to capture absolute changes in project-level outcomes for both core and peripheral \ndevelopers. These metrics span four key dimensions: project-level code contributions, coordination time, \nproject-level productivity, and the underlying mechanisms including individual code contributions, \ndeveloper coding participation, code discussion volume, code discussion participation, and code discussion \nintensity per developer.  \nSpecifically, for each repository-month, we compute: (1) the number of merged PRs submitted by \ncore and peripheral developers; (2) the average time taken to merge a single PR submitted by each group; \n(3) the number of PRs submitted by core and peripheral developers that were merged within one, three, and \nten days; (4) the average number of merged PRs per core and peripheral developer; (5) the number of \ndistinct core and peripheral developers submitting PRs which were eventually merged; (6) the average \nnumber of comments per merged PR submitted by each group; (7) the average number of unique developers \nparticipating in code discussions per merged PR submitted by each group; and (8) the average number of \ncomments per developer per merged PR submitted by each group.   \nWe report the results for core developers\u2019 project-level code contributions, coordination time, and \nproject-level productivity in Table K.1 and for peripheral developers in Table K.2. Overall, the findings \nindicate that both core and peripheral developers benefited from the adoption of Copilot, though the \nmagnitude and nature of these benefits varied. For core developers, Copilot led to a 6.1% increase in \nproject-level code contributions, a 6.7% increase in coordination time, and increases in timely integrated \ncode contributions within one, three, and ten days by 4.4%, 4.8%, and 5.8%, respectively. In contrast, \nperipheral developers experienced a 5.4% increase in project-level code contributions and a 5.3% increase \nin coordination time. Their timely integrated code contributions increased by 2.3%, 2.4%, and 3.6% within \none, three, and ten days, respectively.  \nThe results for the underlying mechanisms, such as individual code contributions, developer coding \nparticipation, and code discussion-related metrics, are presented in Table K.3 for core developers and Table \n23 \n \nK.4 for peripheral developers. As shown in Table K.3, mechanism analyses further reveal that core \ndevelopers experienced a 5.6% increase in individual code contributions, a 5.5% increase in developer \ncoding participation, and increases of 4.1%, 1.6%, and 3.6% in code discussion volume, code discussion \nparticipation, and code discussion intensity per developer, respectively.  \nFor peripheral developers, as indicated in Table K.4, they experienced"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_066",
    "source_id": "OpenSourceImpact2024",
    "text": " and ten days, respectively.  \nThe results for the underlying mechanisms, such as individual code contributions, developer coding \nparticipation, and code discussion-related metrics, are presented in Table K.3 for core developers and Table \n23 \n \nK.4 for peripheral developers. As shown in Table K.3, mechanism analyses further reveal that core \ndevelopers experienced a 5.6% increase in individual code contributions, a 5.5% increase in developer \ncoding participation, and increases of 4.1%, 1.6%, and 3.6% in code discussion volume, code discussion \nparticipation, and code discussion intensity per developer, respectively.  \nFor peripheral developers, as indicated in Table K.4, they experienced a 2.2% increase in individual \ncode contributions, a 2.8% increase in developer coding participation, and increases of 12.6%, 2.3%, and \n11.4% in code discussion volume, code discussion participation, and code discussion intensity per \ndeveloper, respectively. \nTable K.1: The Effect of Copilot on Core Developers (Absolute Changes) \n \n(1) \nProject-level Code \nContributions \n(2) \nCoordination \nTime \n(3) \nTimely Merged \nPRs (1 Day) \n(4) \nTimely Merged \nPRs (3 Days) \n(5) \nTimely Merged  \nPRs (10 Days) \n \nLog (Core_ \nmerged_PR) \nLog (Core_ \nmerge_time) \nLog (Core_ \nmergedPR_1D) \nLog (Core_ \nmergedPR_3D) \nLog (Core_ \nmergedPR_10D) \nATT of Copilot \n0.061*** \n0.067*** \n0.044*** \n0.048*** \n0.058*** \n \n(0.008) \n(0.023) \n(0.011) \n(0.010) \n(0.010) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n6,423 \n6,423 \n6,423 \n6,423 \n6,423 \nObservations \n118,853 \n118,853 \n118,853 \n118,853 \n118,853 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \nTable K.2: The Effect of Copilot on Peripheral Developers (Absolute Changes) \n \n(1) \nProject-level Code \nContributions \n(2) \nCoordination \nTime \n(3) \nTimely Merged \nPRs (1 Day) \n(4) \nTimely Merged \nPRs (3 Days) \n(5) \nTimely Merged \nPRs (10 Days) \n \nLog (Peri_ \nmerged_PR) \nLog (Peri_ \nmerge_time) \nLog (Peri_ \nmergedPR_1D) \nLog (Peri_ \nmergedPR_3D) \nLog (Peri_ \nmergedPR_10D) \nATT of Copilot \n0.054*** \n0.053* \n0.023** \n"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_067",
    "source_id": "OpenSourceImpact2024",
    "text": " Changes) \n \n(1) \nProject-level Code \nContributions \n(2) \nCoordination \nTime \n(3) \nTimely Merged \nPRs (1 Day) \n(4) \nTimely Merged \nPRs (3 Days) \n(5) \nTimely Merged \nPRs (10 Days) \n \nLog (Peri_ \nmerged_PR) \nLog (Peri_ \nmerge_time) \nLog (Peri_ \nmergedPR_1D) \nLog (Peri_ \nmergedPR_3D) \nLog (Peri_ \nmergedPR_10D) \nATT of Copilot \n0.054*** \n0.053* \n0.023** \n0.024** \n0.036*** \n \n(0.008) \n(0.028) \n(0.010) \n(0.012) \n(0.010) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,154 \n4,154 \n4,154 \n4,154 \n4,154 \nObservations \n77,888 \n77,888 \n77,888 \n77,888 \n77,888 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n \n \n \n24 \n \n \nTable K.3: The Effect of Copilot on Core Developers (Absolute Changes, Mechanism Analyses) \n \n(1) \nIndividual Code \nContributions \n(2) \nCoding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual Discussion \nIntensity \n \nLog \n(MergedPR_per\n_core) \nLog \n(Num_core_with\n_mergedPR) \nLog  \n(Comment_per\n_mergedPR) \nLog  \n(Num_dev_with_ \ncomments_per_mergedPR) \nLog  \n(Comments \n_per_dev_per_mergedPR) \nATT of Copilot \n0.056*** \n0.055*** \n0.041** \n0.016*** \n0.036** \n \n(0.011) \n(0.007) \n(0.019) \n(0.005) \n(0.017) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n6,423 \n6,423 \n6,423 \n6,423 \n6,423 \nObservations \n118,853 \n118,853 \n118,853 \n118,853 \n118,853 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \nTable K.4: The Effect of Copilot on Peripheral Developers (Absolute Changes, Mechanism Analyses) \n \n(1) \nIndividual Code \nContributions \n(2) \nCoding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual Discussion \nIntensity \n \nLog \n(MergedPR_per\n_per"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_068",
    "source_id": "OpenSourceImpact2024",
    "text": "423 \nObservations \n118,853 \n118,853 \n118,853 \n118,853 \n118,853 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \nTable K.4: The Effect of Copilot on Peripheral Developers (Absolute Changes, Mechanism Analyses) \n \n(1) \nIndividual Code \nContributions \n(2) \nCoding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual Discussion \nIntensity \n \nLog \n(MergedPR_per\n_peri) \nLog \n(Num_peri_with\n_mergedPR) \nLog  \n(Comment_per_\nmergedPR) \nLog  \n(Num_dev_with_ \ncomments_per_mergedPR) \nLog  \n(Comments \n_per_dev_per_mergedPR) \nATT of Copilot \n0.022*** \n0.028*** \n0.126*** \n0.023*** \n0.114*** \n \n(0.006) \n(0.005) \n(0.033) \n(0.007) \n(0.026) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n4,154 \n4,154 \n4,154 \n4,154 \n4,154 \nObservations \n77,888 \n77,888 \n77,888 \n77,888 \n77,888 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n25 \n \nAppendix L: Project Familiarity \nWe argue that one potential reason that explains the difference in how core and peripheral developers \nbenefit from Copilot could lie in their different levels of project familiarity. In the OSS community, core \ndevelopers are more embedded in focal projects and maintain sustained involvement (Setia et al. 2012), \nwhile peripheral developers contribute more sporadically (Howison and Crowston 2014, Krishnamurthy et \nal. 2016).  \nTo explore this potential explanation, we first compare the level of project familiarity for core \nversus peripheral developers. Based on the sample of repositories with continuous activity from 2018 to \n2021, we measure a developer\u2019s project familiarity based on their activity during this period related to a \nfocal repository. More specifically, a developer\u2019s project familiarity with a focal repository is measured by \nthe developer\u2019s tenure with the repository, i.e., the number of months a developer was active in the \nrepository before June 2021 (i.e., the introduction of Copilot to GitHub). As reported in Table L.1, the t-\ntest results show that on average, core developers have significantly longer tenure than peripheral \ndevelopers, consistent with our argument that core developers have greater project familiarity. \nTable L.1: Comparison between Core and Peripheral Developers on their Project Familiarity, \nMeasured by Project Tenure \nVariable \n \nCore \nDevelopers \nPeripheral \nDevelopers \nMean \ndiff \np-value \nAvg_tenure \n \n6.98 \n1."
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_069",
    "source_id": "OpenSourceImpact2024",
    "text": ", a developer\u2019s project familiarity with a focal repository is measured by \nthe developer\u2019s tenure with the repository, i.e., the number of months a developer was active in the \nrepository before June 2021 (i.e., the introduction of Copilot to GitHub). As reported in Table L.1, the t-\ntest results show that on average, core developers have significantly longer tenure than peripheral \ndevelopers, consistent with our argument that core developers have greater project familiarity. \nTable L.1: Comparison between Core and Peripheral Developers on their Project Familiarity, \nMeasured by Project Tenure \nVariable \n \nCore \nDevelopers \nPeripheral \nDevelopers \nMean \ndiff \np-value \nAvg_tenure \n \n6.98 \n1.64 \n5.34 \n0.00 \nNotes: The table is based on the same sample as the one used in our main analyses in the paper. \n \nWe next explore whether the differential effects of Copilot on project-level code contributions and \ncoordination time among core versus peripheral developers can be partly explained by the different levels \nof project familiarity. To do so, our main idea is to examine whether Copilot had differential effects on \nvarious outcome variables for developers with different levels of familiarity. Developers are classified into \nhigh versus low project familiarity based on whether their tenure exceeds the median value of tenure for \nthat repository before the adoption of Copilot. \nUsing the same empirical approach as Section 6.5, we compute 1) the proportion of project-level \ncode contributions made by developers with high project familiarity to all contributions, labeled as Ratio \n26 \n \nof merged PR, and 2) the ratio of coordination time for merging code contributed by high-familiarity \ndevelopers to the average coordination time, labeled as Ratio of merge time. Using each of these two \nvariables as the dependent variable, the results are shown in Tables L.2. Consistent with our arguments, \nCopilot led to an increased proportion of code contributions from high-familiarity developers; it also led to \na relative decrease in coordination time for merging code contributed by high-familiarity developers \ncompared to the average coordination time. \nTable L.3 reports the results related to the mechanism analyses. It indicates Copilot led to higher \nindividual code contributions (column [1]) and developer coding participation (column [2]) by high-\nfamiliarity developers. Meanwhile, as shown in columns (3) to (5), following the adoption of Copilot, there \nwas a relative decline in code discussion volume, discussion participation, and discussion intensity on code \ncontributed by high-familiarity developers, compared to the average code discussion level. Collectively, \nthese results support our argument that the differential effects of Copilot on project-level code contributions \nand coordination time among core versus peripheral developers might be partly explained by their different \nlevels of project familiarity.  \nTable L.2: Heterogeneous Effect of Copilot on Developers with High vs. Low Project Familiarity \n \n(1) \n(2) \n \nProject-level Code Contributions \nCoordination Time \n \nRatio of merged PR  \nRatio of merge time \nATT of Copilot \n0.024*** \n-0.052*** \n \n(0.004) \n(0.009) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n6,171 \n6,171 \nObservations \n87,048 \n87"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_070",
    "source_id": "OpenSourceImpact2024",
    "text": " our argument that the differential effects of Copilot on project-level code contributions \nand coordination time among core versus peripheral developers might be partly explained by their different \nlevels of project familiarity.  \nTable L.2: Heterogeneous Effect of Copilot on Developers with High vs. Low Project Familiarity \n \n(1) \n(2) \n \nProject-level Code Contributions \nCoordination Time \n \nRatio of merged PR  \nRatio of merge time \nATT of Copilot \n0.024*** \n-0.052*** \n \n(0.004) \n(0.009) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n6,171 \n6,171 \nObservations \n87,048 \n87,048 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n \n \n \n \n \n \n \n \n \n27 \n \nTable L.3: Heterogeneous Effect of Copilot on Developers with High vs. Low Project Familiarity \n(Mechanism) \n \n(1) \nIndividual Code \nContributions \n(2) \nDeveloper Coding \nParticipation \n(3) \nDiscussion \nVolume \n(4) \nDiscussion  \nParticipation \n(5) \nIndividual \nDiscussion Intensity \n \nmergedPR_per_\ndev_ratio \ndev_with \n_mergedPR_share \ncomment_per\n_mergedPR \n_ratio \ncomment_ \ndev_per_mergedPR \n_ratio \ncomment_ \nper_dev_per_ \nmergedPR_ratio \nATT of Copilot \n0.174*** \n0.007* \n-0.106*** \n-0.099*** \n-0.103*** \n \n(0.008) \n(0.004) \n(0.005) \n(0.005) \n(0.005) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \n# of repositories \n6,171 \n6,171 \n6,171 \n6,171 \n6,171 \nObservations \n87,048 \n87,048 \n87,048 \n87,048 \n87,048 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n28 \n \nAppendix M: Alternative Explanations for the Differential Impacts between Core and Peripheral \nDevelopers \n \nTo address the concern that the heterogeneous effects of Copilot on core versus peripheral developers may \nbe driven by differences such as programming language expertise or Copilot usage, we conduct the \nfollowing additional analyses.  \nFirst, it is worth noting that the theoretical distinction between core and peripheral developers does \nnot necessarily reflect differences in programming language expertise. As suggested by the literature, \nperipheral developers include core developers from other projects and end users (Setia et al. 2012), which \nindicates that peripheral developers may also possess substantial expertise in the relevant programming \nlanguages. Core developers may be new to a domain and rely on peripheral contributors with specialized \nknowledge to support project development. Alternatively, core developers may be domain experts, while \nperipheral developers participate in the project to learn and gain experience.  \nGiven"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_071",
    "source_id": "OpenSourceImpact2024",
    "text": " versus peripheral developers may \nbe driven by differences such as programming language expertise or Copilot usage, we conduct the \nfollowing additional analyses.  \nFirst, it is worth noting that the theoretical distinction between core and peripheral developers does \nnot necessarily reflect differences in programming language expertise. As suggested by the literature, \nperipheral developers include core developers from other projects and end users (Setia et al. 2012), which \nindicates that peripheral developers may also possess substantial expertise in the relevant programming \nlanguages. Core developers may be new to a domain and rely on peripheral contributors with specialized \nknowledge to support project development. Alternatively, core developers may be domain experts, while \nperipheral developers participate in the project to learn and gain experience.  \nGiven the plausibility of both scenarios, we compare the fraction of expert developers in core \ndevelopers against the fraction of expert developers in peripheral developers. Specifically, we classify \ndevelopers as programming language experts if they used that language in at least 75% of their prior projects. \nWe then compare each developer\u2019s expertise to the primary language used in the focal project to determine \nwhether they met the expert criteria.  \nAs reported in the first row in Table M.1, the t-test result shows no significant difference between \ncore and peripheral developers in terms of the percentage of expert developers. This suggests that \ndifferences in language expertise may not be the primary reason for the observed heterogeneity between \ncore and peripheral contributors. \nSecond, we consider the possibility that core developers benefit more simply because they are more \nlikely to use Copilot or use it more intensively. To explore this possibility, we leverage the proprietary data \nprovided by GitHub organization that indicates for each repository, the percentage of Copilot users among \ncore developers and the percentage of Copilot users among peripheral developers. Due to privacy \n29 \n \nconstraints, we do not have access to individual-level Copilot usage data for specific core or peripheral \ndevelopers.  \nAs reported in the second row in Table M.1, interestingly, we find that peripheral developers had \na significantly higher Copilot adoption rate than core developers. Despite this, our analysis in Section 6.5 \nof the main paper shows that core developers experienced relatively greater project-level code contributions \nand lower coordination time from Copilot usage. This pattern further supports our argument that the \nobserved differences in outcomes are not driven by differential Copilot adoption rates but are more \nplausibly attributed to differences in project familiarity. \nTable M.1: Comparison between Core and Peripheral Developers \nVariable \nCore Developers \nPeripheral Developers \nMean diff \np-value \n \n% of expert developers \n0.68 \n0.67 \n0.01 \n0.15 \n% of Copilot users \n0.20 \n0.30 \n-0.10 \n0.00 \nNotes: The table is based on the same sample as the one used in our main analyses in the paper. \n \n \n \n30 \n \nReferences \n \nAbadie A, Diamond A, Hainmueller J (2015) Comparative politics and the synthetic control method. American \nJournal of Political Science 59(2):495-510. \nAlyakoob M, Rahman MS (2022) Shared prosperity (or lack thereof) in the sharing economy. Information \nSystems Research 33(2):638-658. \nAngrist JD, Pischke J-S (2009) Mostly harmless econometrics:"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_072",
    "source_id": "OpenSourceImpact2024",
    "text": " \n% of Copilot users \n0.20 \n0.30 \n-0.10 \n0.00 \nNotes: The table is based on the same sample as the one used in our main analyses in the paper. \n \n \n \n30 \n \nReferences \n \nAbadie A, Diamond A, Hainmueller J (2015) Comparative politics and the synthetic control method. American \nJournal of Political Science 59(2):495-510. \nAlyakoob M, Rahman MS (2022) Shared prosperity (or lack thereof) in the sharing economy. Information \nSystems Research 33(2):638-658. \nAngrist JD, Pischke J-S (2009) Mostly harmless econometrics: An empiricist's companion (Princeton university \npress). \nAutor DH (2003) Outsourcing at will: The contribution of unjust dismissal doctrine to the growth of \nemployment outsourcing. Journal of labor economics 21(1):1-42. \nBai J (2009) Panel data models with interactive fixed effects. Econometrica 77(4):1229-1279. \nChengalur-Smith I, Sidorova A, Daniel SL (2010) Sustainability of free/libre open source projects: A \nlongitudinal study. Journal of the Association for Information Systems 11(11):5. \nEgami N, Yamauchi S (2023) Using multiple pretreatment periods to improve difference-in-differences and \nstaggered adoption designs. Political Analysis 31(2):195-212. \nGriffiths TL, Steyvers M (2004) Finding scientific topics. Proceedings of the National academy of Sciences \n101(suppl_1):5228-5235. \nHann I-H, Roberts JA, Slaughter SA (2013) All are not equal: An examination of the economic returns to \ndifferent forms of participation in open source software communities. Information Systems Research \n24(3):520-538. \nHansen S, McMahon M, Prat A (2018) Transparency and deliberation within the FOMC: A computational \nlinguistics approach. The Quarterly Journal of Economics 133(2):801-870. \nHartman E, Hidalgo FD (2018) An equivalence approach to balance and placebo tests. American Journal of \nPolitical Science 62(4):1000-1013. \nHowison J, Crowston K (2014) Collaboration through open superposition: A theory of the open source way. \nMis Quarterly 38(1):29-50. \nKrishnamurthy R, Jacob V, Radhakrishnan S, Dogan K (2016) Peripheral developer participation in open source \nprojects: an empirical analysis. ACM Transactions on Management Information Systems (TMIS) 6(4):1-31. \nLakens D, Scheel AM, Isager PM (2018) Equivalence testing for psychological research: A tutorial. Advances \nin methods and practices in psychological science 1(2):259-269. \nLee GM, Qiu L, Whinston AB (2016) A friend like me: Modeling network formation in a location-based social \nnetwork. Journal of Management Information Systems 33(4):1008-1033. \nLee GM, He S, Lee J, Wh"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_073",
    "source_id": "OpenSourceImpact2024",
    "text": " V, Radhakrishnan S, Dogan K (2016) Peripheral developer participation in open source \nprojects: an empirical analysis. ACM Transactions on Management Information Systems (TMIS) 6(4):1-31. \nLakens D, Scheel AM, Isager PM (2018) Equivalence testing for psychological research: A tutorial. Advances \nin methods and practices in psychological science 1(2):259-269. \nLee GM, Qiu L, Whinston AB (2016) A friend like me: Modeling network formation in a location-based social \nnetwork. Journal of Management Information Systems 33(4):1008-1033. \nLee GM, He S, Lee J, Whinston AB (2020) Matching mobile applications for cross-promotion. Information \nSystems Research 31(3):865-891. \nLiu L, Wang Y, Xu Y (2024) A practical guide to counterfactual estimators for causal inference with time-\nseries cross-sectional data. American Journal of Political Science 68(1):160-176. \nLu Y, Gupta A, Ketter W, Van Heck E (2019) Information transparency in business-to-business auction markets: \nThe role of winner identity disclosure. Management Science 65(9):4261-4279. \nMedappa PK, Srivastava SC (2019) Does superposition influence the success of FLOSS projects? An \nexamination of open-source software development by organizations and individuals. Information Systems \nResearch 30(3):764-786. \nPan Y, Qiu L (2022) How ride-sharing is shaping public transit system: A counterfactual estimator approach. \nProduction and Operations Management 31(3):906-927. \nRamage D, Dumais S, Liebling D (2010) Characterizing microblogs with topic models. Proceedings of the \ninternational AAAI conference on web and social media, 130-137. \nSetia P, Rajagopalan B, Sambamurthy V, Calantone R (2012) How peripheral developers contribute to open-\nsource software development. Information Systems Research 23(1):144-163. \nShi Z, Lee GM, Whinston AB (2016) Toward a better measure of business proximity. MIS quarterly \n40(4):1035-1056. \n31 \n \nShin D, He S, Lee GM, Whinston AB, Cetintas S, Lee K-C (2020) Enhancing social media analysis with visual \ndata analytics: A deep learning approach (SSRN Amsterdam, The Netherlands). \nWang C, Blei DM (2011) Collaborative topic modeling for recommending scientific articles. Proceedings of \nthe 17th ACM SIGKDD international conference on Knowledge discovery and data mining, 448-456. \nWang Q, Qiu L, Xu W (2023) Informal Payments and Doctor Engagement in an Online Health Community: \nAn Empirical Investigation Using Generalized Synthetic Control. Information Systems Research. \nWeng J, Lim E-P, Jiang J, He Q (2010) Twitterrank: finding topic-sensitive influential twitterers. Proceedings \nof the third ACM international conference on Web search and data mining, 261-270. \nWilliams C (2011) Client\u2013vendor knowledge transfer in IS offshore outsourcing"
  },
  {
    "chunk_id": "OpenSourceImpact2024_chunk_074",
    "source_id": "OpenSourceImpact2024",
    "text": "ang C, Blei DM (2011) Collaborative topic modeling for recommending scientific articles. Proceedings of \nthe 17th ACM SIGKDD international conference on Knowledge discovery and data mining, 448-456. \nWang Q, Qiu L, Xu W (2023) Informal Payments and Doctor Engagement in an Online Health Community: \nAn Empirical Investigation Using Generalized Synthetic Control. Information Systems Research. \nWeng J, Lim E-P, Jiang J, He Q (2010) Twitterrank: finding topic-sensitive influential twitterers. Proceedings \nof the third ACM international conference on Web search and data mining, 261-270. \nWilliams C (2011) Client\u2013vendor knowledge transfer in IS offshore outsourcing: insights from a survey of \nIndian software engineers. Information Systems Journal 21(4):335-356. \nXu Y (2017) Generalized synthetic control method: Causal inference with interactive fixed effects models. \nPolitical Analysis 25(1):57-76. \nZimmermann A, Oshri I, Lioliou E, Gerbasi A (2018) Sourcing in or out: Implications for social capital and \nknowledge sharing. The Journal of Strategic Information Systems 27(1):82-100. \n \n \n"
  }
]