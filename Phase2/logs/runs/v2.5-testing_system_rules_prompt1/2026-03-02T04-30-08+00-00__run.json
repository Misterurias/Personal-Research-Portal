{
  "timestamp": "2026-03-02T04:30:08+00:00",
  "query": "Do AI coding assistants increase long-term maintenance burden?",
  "retrieval_query": "Can AI coding assistants lead to increased long-term maintenance burdens?",
  "query_rewritten": true,
  "rewritten_query": "Can AI coding assistants lead to increased long-term maintenance burdens?",
  "top_k": 6,
  "answer": "Based on the provided evidence chunks, I can only answer questions that are directly addressed in these chunks. I will not provide any answers that go beyond what is presented here.\n\nIf you have specific questions or would like me to summarize the information contained within the evidence chunks, please let me know and I'll be happy to assist you!",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.8490461707115173,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_027",
      "text": " the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced contrib-\nutors face higher review workloads and a concomitant decline in their own development activity. These\nresults highlight a redistribution of effort within OSS communities that has received limited attention in\nprior work.\nFrom a technical debt perspective, our findings suggest that AI-assisted programming alters the intertem-\nporal trade-off between short-term development speed and long-term maintainability. The shortcuts enabled\nby AI tools may accelerate the output while introducing code that is difficult to integrate, extend, or refactor.\nThe widespread use of AI-assisted pair programming - and, in extreme cases, “vibe coding” - can inject a\nlarger volume of difficult-to-maintain code (Pimenova et al. 2025, Fawzy et al. 2025) into software projects,\naccelerating the accumulation of technical debt. Such contributions create latent liabilities for projects, as\nmaintainers must later invest substantial effort to review, revise, or rewrite code to meet repository stan-\ndards. In this sense, AI does not merely increase the volume of contributions; it changes the composition of\nincoming code in ways that intensify technical debt accumulation.\nA key contribution of our study is to operationalize technical debt at its point of entry. We conceptual-\nize extensive PR rework as realized technical debt: the additional modification, coordination, and revision\n16 https://github.blog/news-insights/octoverse/octoverse-2024/\n24\neffort required to bring submitted code up to acceptable standards. This measure complements prior work\nthat captures technical debt through architectural metrics, defect accumulation, or long-run performance\noutcomes. By focusing on PR-level dynamics, we provide a micro-level view of how technical debt emerges\nin real time and how it is managed through ongoing maintenance effort.\nOur findings also raise concerns about the learning implications of AI-assisted development. With AI\nproviding rapid solutions, peripheral contributors may engage less deeply with underlying programming\nprinciples and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation"
    },
    {
      "rank": 2,
      "distance_l2": 0.8605034351348877,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_013",
      "text": " we cannot conclude validity of other tools leveraged AI in different forms or \narchitectures. The 60-minute tasks did not allow us to explore the long-term ramifications of maintenance. \nThe sample of practitioners was drawn from Western technology companies and there are implications for \ngeneralizability among practice globally based on our sample. Additionally, the static analysis tools used \nas part of the scanning for a security vulnerability had important limitations, as discussed in several works, \nof being too discriminatory (false positives) and failing to positively identify classes of vulnerabilities that \ncan only be found using a dynamic analysis. Therefore, even with some of the limitations addressed by \nsome manual validation, we evaluated only the immediate introduction of a vulnerability and not the \nprocesses by which a vulnerability is subsequently identified and remediated once a code sample is placed \ninto production software, when investigating a security vulnerability. \n5.5 Future Research Directions \nNumerous important questions springing from our observations will require investigation. Longitudinal \nstudies depicting the development of AI-assisted codebases over extended periods of time will provide \nimportant insights into maintenance costs and the buildup of technical debt. Cross-comparative studies of \ndifferent AI tool implementations will also help clarify whether observed effects are associated with a \nspecific tool, or are more generalizable. Equally pressing, interventions also merit investigation, which \ncould include making AI models security-aware, humans-in-the-loop generation that performs automated \nsecurity-checks, enhanced developer training, or techniques of prompt engineering that pro-actively \nreduce the actual introduction of vulnerabilities. Interactions between AI tooling and developers' own \nskills should also be in steady inquiry, especially whether junior developers using AI tools are developing \nproblem-solving skills equivalent to developers going through training in the traditional sense. The \nanswers to such questions would be noteworthy for educational practice and hiring. Finally, we could \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n10\n \nconduct research that examines adaptive AI assistance that provides support at each stage based on \ndevelopers' competencies in order to move optimal learning trajectories forward, and that also assesses \nproductivity gains in relationship to skills development. \n \n6. Conclusion \nIn this study, we provide an in-depth data-driven investigation of AI-assisted code generation technologies \nthat demonstrate considerable productivity increases (i.e., an average of 31.4%) and, concerning, a total \nincrease in vulnerabilities of 23.7% and a total increase in critical severity of 89%. We also demonstrate \nthat some dimensions of code quality are improved with AI-assisted code generation tools (i.e., \nmaintainability, cyclomatic complexity) but caution is warranted with operational risks, to code itself (i.e. \nextra code duplication) and security vulnerabilities. We also examined differences across programming \nlanguages, and in particular, we found that while using AI-assisted code generation technologies is \nconstructive in Python, it warrants heightened caution around codex in C++ (to name only one). Finally, \nwhile we examined experience differences, we found that junior developers require support to prevent \nexcessive dependency on AI and senior developers could receive the maximum"
    },
    {
      "rank": 3,
      "distance_l2": 0.9056246280670166,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_004",
      "text": "1, Ramasubbu and Kemerer 2021). These maintenance tasks are generally less intrinsically driven and\ntypically assigned to contributors who possess both technical expertise and trust of the community to ensure\nthe quality and reliability of the codebase (Medappa et al. 2023, Eghbal 2020). Thus, we seek to answer the\nquestion: How does AI-assisted programming impact the development and maintenance activities and the\ntechnical debt of the OSS projects?\nIn this study, we examine whether technical debt and maintenance efforts of OSS projects changed after\nthe introduction of GitHub Copilot through increased code review and rework effort on PRs. To empirically\ntest this, we exploit the release of GitHub Copilot as a technical preview in June 2021, which included lim-\nited programming language endorsement. We focus on OSS projects owned by Microsoft, as the company\nhad exclusive access to OpenAI’s GPT-3, the model powering GitHub Copilot during its technical preview,\ndue to its investment in OpenAI and its prior acquisition of GitHub.4,5 The individual users in our dataset\nare contributors to Microsoft-owned OSS projects. We estimate the effect of Copilot at both the project\nand contributor levels using a Difference-in-Differences (DiD) design. Treatment and control groups were\ndefined based on the primary programming language: those using Copilot-endorsed languages formed the\ntreatment group, while non-endorsed language users served as the control (Yeverechyahu et al. 2024). For\nboth project and contributor levels, we collected data on programming activities and aggregated them at the\nmonthly level.\nWe examine the changes in code productivity after Copilot by three measures: lines of code added,\ncommits 6 and PRs submitted to the project. To capture the secondary effects of AI adoption on technical\ndebt and maintenance effortsof OSS communities, we focused on two complementary outcomes. First, we\n4 https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-\nmodel/\n5 https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-\nengineering-experience-using-generative-ai\n6 A commit is the fundamental unit of change on GitHub. Similar to saving a file that’s been edited, a commit records changes to\none or more files on GitHub - https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-\ncommits/about-commits\n4\nmeasured technical debt at the project level using PR rework, which reflects the extent to which initially\nsubmitted contributions require modification before being integrated into the codebase (Ramasubbu and\nKemerer 2016, 2021). Higher levels of rework indicate greater reliance on expedient or insufficiently inte-\ngrated solutions. Second, we measured maintenance effort at the individual level using PR reviews, which\ncapture the time and effort required from contributors to evaluate, correct, and integrate submitted code\n(Medappa et al. 2023).\nBased on our analysis of a large-scale panel dataset from GitHub, we find that while AI adoption leads to\nproductivity gains, they also increase maintenance-related activities"
    },
    {
      "rank": 4,
      "distance_l2": 0.9065772294998169,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_002",
      "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n∗Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n†Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
    },
    {
      "rank": 5,
      "distance_l2": 0.9106784462928772,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_009",
      "text": " Ramasubbu and Kemerer 2021). Both approaches have value: the former\nmetric-based techniques are scalable and actionable for tooling, while the later process indicators capture\nthe economic and human costs that managers and maintainers ultimately face.\nEmpirical studies across industry and OSS ecosystems consistently find that technical debt impairs long-\nrun productivity and increases maintenance burdens (Paramitha and Massacci 2023). Work in empirical\nsoftware engineering shows that higher measured debt correlates with increased bug rates, longer defect\nresolution times, and reduced velocity for feature delivery (Ramasubbu and Kemerer 2016). Rinta-Kahila\net al. (2023) further demonstrate that organizations can become “trapped” in technical debt due to coor-\ndination failures, organizational inertia, and escalating switching costs. The introduction of GenAI tools\nsuch as GitHub Copilot represents a departure from these assumptions (Yeverechyahu et al. 2024). Unlike\nprior productivity-enhancing tools, GenAI dramatically lowers the marginal cost of producing code and\nreduces skill barriers to contribution (Peng et al. 2023). While prior studies document productivity gains\nfrom AI-assisted coding, they provide limited insight into how these gains translate into technical debt and\nmaintenance related challenges.\nFrom a technical debt perspective, GenAI may accelerate debt accumulation by increasing code volume\nwithout proportionate improvements in quality, the integration with the software, or architectural coher-\nence (Pimenova et al. 2025). By lowering the cost of producing code, AI-assisted programming encourages\nrapid iteration and experimentation, but can shift attention away from longer-term concerns such as main-\ntainability, readability, and alignment with existing system design (Barrett et al. 2023, Schreiber and Tippe\n2025). As a result, defects, inconsistencies, and design shortcuts may be introduced more quickly than\nthey can be identified and resolved. Over time, this imbalance can compound, transforming short-term pro-\nductivity gains into persistent maintenance obligations that must be absorbed by experienced developers\n(Eghbal 2020). While GenAI promises to enhance development speed and broaden participation, it may\nsimultaneously intensify the very technical debt that constrains system reliability, scalability, and long-run\nperformance (Ramasubbu and Kemerer 2021). If this logic is valid, we expect that GenAI may lead to a\ngreater accumulation of technical debt in OSS projects.\n8\nTable 1\nSelected Literature on Technical Debt and Software Maintenance\nStudy\nContext\nMethod\nMeasurement\nKey Findings\nBanker et al.\n(2021)\nCustomer\nrelationship\nmanagement\n(CRM)\nsystems in 26 firms\nEconometric analysis\nPercentage of customized codes in the\nCRM system that do not adhere to vendor-\nprescribed standard\nHigher technical debt is associated\nwith\nlower\nfirm\nperformance\nand\nreduced operational efficiency over\ntime.\nRamasubbu\n&\nKemerer\n(2021)\nOutsourced Commercial\nOff-The-Shelf\n(COT)\nenterprise systems\nEconometric analysis\nViolations of the design and programming\nstandards established by the vendor of the\nCOTS enterprise system\nActive remediation policies reduce\nlong-term\nmaintenance\ncosts,\nbut\nexcessive deferral leads to escalating\ntechnical debt.\nRamas"
    },
    {
      "rank": 6,
      "distance_l2": 0.9125134944915771,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_006",
      "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers’ effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation’s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that “general housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors’ time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there’s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and “vibe coding” practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed–quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt1"
}