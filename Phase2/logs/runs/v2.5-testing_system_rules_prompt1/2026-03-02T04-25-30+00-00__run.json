{
  "timestamp": "2026-03-02T04:25:30+00:00",
  "query": "What quality metrics are used to evaluate AI-generated code?",
  "retrieval_query": "Here is a rewritten search query based on the user's question:\n\n\"quality metrics for evaluating AI-generated code\"\n\nThis query preserves the key entity (\"AI-generated code\") and metric (\"quality metrics\"), while condensing the question into a single sentence.",
  "query_rewritten": true,
  "rewritten_query": "Here is a rewritten search query based on the user's question:\n\n\"quality metrics for evaluating AI-generated code\"\n\nThis query preserves the key entity (\"AI-generated code\") and metric (\"quality metrics\"), while condensing the question into a single sentence.",
  "top_k": 6,
  "answer": "According to the provided evidence chunks, here are the answers:\n\n**Code quality:**\n* GitHub Copilot's code generation quality was assessed in [46] and found to be of varying quality.\n* An empirical study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT found that the use of AI-assisted code generation tools leads to a 31.4% increase in average developer productivity, but also introduces 23.7% more security vulnerabilities [47].\n\n**Security:**\n* The same empirical study found that the use of AI-assisted code generation tools increases cyclomatic complexity by 14.6%, while maintainability went up 18.2%.\n* CodeGeeX, a pre-trained model for code generation, was benchmarked on HumanEval-X and found to have multilingual capabilities [48].\n\n**Productivity:**\n* A study on neural code completion found that it can improve developer productivity [49].\n* The same empirical study mentioned earlier found that the use of AI-assisted code generation tools leads to a 31.4% increase in average developer productivity.\n\nPlease note that these answers are based solely on the provided evidence chunks and may not represent the entire scope of research or findings in the field.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.9477849006652832,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 2,
      "distance_l2": 0.9528985023498535,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_003",
      "text": " the quality of the code generated by the code generation tools?\n1 code.visualstudio.com/docs/editor/intellisense\n2 jetbrains.com\nTitle Suppressed Due to Excessive Length\n3\nRQ1.1 How valid are the code generation tools’ code suggestions?\nRQ1.2 How correct are code generation tools’ code suggestions?\nRQ1.3 How secure are code generation tools’ code suggestions?\nRQ1.4 How reliable are code generation tools’ code suggestions?\nRQ1.5 How maintainable are code generation tools’ code suggestions?\nRQ2 What is the impact of using the docstrings on the generated code quality?\nRQ3 What is the impact of using meaningful function names on the generated code\nquality?\nRQ4 How did the code generation tools evolve over time?\nWe believe our study will enable users to more effectively leverage AI-assisted code\ngenerators for generating accurate, valid, reliable, maintainable, and secure results.\nIn addition, tool developers can benefit from our findings to identify and enhance\nthe strengths and address the weaknesses of their tools in real-world situations. The\ncomparative aspect of our study provides valuable insights into the performance of each\ncode generation tool relative to its competitors.\nThe structure of our study is as follows: In Section 2, we provide some background\ninformation about the code generation tools we evaluate. In Section 3, we provide a\ndetailed explanation of the research questions we have determined by elaborating on our\nexperimental setup. Our results are presented in Section 4, and they are discussed in\nSection 5. The threats that influence the validity of our study are addressed in Section\n6. In Section 7, we discuss related work. Finally, in Section 8, we conclude our study.\n2 Background\n2.1 GitHub Copilot\nGitHub Copilot3 is a code generation tool that utilizes a variety of technologies, including\na compatible IDE, and the OpenAI Codex Model4. GitHub announced GitHub Copilot\nfor technical preview in the Visual Studio Code development environment on June 29,\n2021 (Friedman, 2021). GitHub declared on June 21, 2022, that Copilot was out of\nthe technical preview phase and is now accessible as a subscription-based service for\nindividual developers (Dohmke, 2022). It currently has subscription plans for individuals\nand businesses. GitHub Copilot can be installed and used as an extension to Visual\nStudio Code, Neovim, IDEs developed by JetBrains5, and GitHub Codespaces6. The\nunderlying service continuously takes user code samples and sends the snippets to the\nunderlying OpenAI Codex Model. GitHub Copilot generates the code and presents the\nresults of the OpenAI Codex Model by adjusting the generated code to the current\nworkspace of the programmer (Ernst and Bavota, 2022).\nThe Codex model relies on Generative Pre-trained Transformer (GPT) models the\ncompany previously invented for text generation. The public code available on GitHub\nwas used during the fine-tuning of the model to implement the code recognition and\ngeneration capabilities.\n3 copilot.github.com\n4 openai.com/blog/openai-codex\n5 plugins.jetbrains.com/plugin/17718-github-copilot\n6 github.com/features/codespaces\n4\nBurak Yetiştiren et al.\n"
    },
    {
      "rank": 3,
      "distance_l2": 0.9618691205978394,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_004",
      "text": " al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming languages used [21]. Nguyen and colleagues put forward a new set of metrics for \nmeasuring AI-generated code and reported that, although AI-powered software is very good at producing \nsyntactically correct code, it often strays from the best design patterns [22]. \nAmong the critical issues raised security implications have been the foremost. Static analysis studies \nshowed that AI systems might learn from their training data and thus recreate the same weaknesses as in \nthe case of existing vulnerabilities [23]. Pearce and co-workers pointed out the presence of a vulnerable \npattern that was quite common in GitHub Copilot's code which included SQL injection, cross-site \nscripting, and use of insecure cryptographic methods [24]. \nNevertheless, there are still some limitations that are inherent to such a growing research area. The \nmajority of research works have been limited to the evaluation of just a few tools and or small domains \nmaking the possibility of drawing general conclusions quite difficult. Only a couple of research works \nhave dealt with long-term effects of refactoring, skilled programmers, and maintenance costs. Moreover, \nthe interaction between the experience level of the developer and the effectiveness of the AI tool is still to \nbe fully explored [25]. Through a thorough and multi-language study involving developers of different \nexperience levels working on varied software engineering tasks, we fill these gaps. \n \n3. Methodology \n3.1 Research Design \nThis research utilized a between-subjects experimental design to assess the influences of AI-assisted code \ngeneration tools on software development outcomes. The independent variable was the presence of AI-\nassisted coding tools (experimental group vs. control group), while the dependent variables were the \nquality metrics, security vulnerability counts, and productivity measures of the coded produced. The \nexperiment was set up in such a way that it reduced the possibility of confounding variables while keeping \nthe ecological validity through the use of realistic programming tasks that are typical of professional \nsoftware development scenarios. Figure 1 shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2"
    },
    {
      "rank": 4,
      "distance_l2": 0.9880502223968506,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 5,
      "distance_l2": 1.006664752960205,
      "source_id": "AIReview2025",
      "chunk_id": "AIReview2025_chunk_001",
      "text": "236 \nA Review of Research on AI-Assisted Code Generation and AI-\nDriven Code Review \nYuzhi Wang \nBeijing University of Technology, Beijing, China \nshenrenaisite@yeah.net \nAbstract. With the significant breakthroughs of deep learning technologies such as large language \nmodels (LLMs) in the field of code analysis, AI has evolved from an auxiliary tool to a key technology \nthat deeply participates in code optimization and resolving performance issues. As modern software \nsystem architectures become increasingly complex, the requirements for their performance have \nalso become more stringent. During the coding stage, developers find it difficult to effectively identify \nand resolve potential performance issues using traditional methods. This review focuses on the \napplication of artificial intelligence in two key areas: AI-assisted intelligent code generation and AI-\npovered code review. The review systematically analyzed the application of LLMs in software \ndevelopment, revealing a situation where efficiency gains coexist with quality challenges. In terms \nof code generation, models such as Code Llama and Copilot have significantly accelerated the \ndevelopment process. In the field of code review, AI can effectively handle code standards and low-\nseverity defects. However, in the future, this field still needs to address the issues of the reliability \nand security of the code generated by LLMs, as well as the insufficient explainability of the results of \nautomated performance analysis. The future research focus in this field lies in addressing issues \nsuch as the lack of interpretability and insufficient domain knowledge of LLMs. It is necessary to \nprioritize enhancing the reliability of AI recommendations and promoting the transformation of AI \nfrom an auxiliary tool to an intelligent Agent with self-repair capabilities, in order to achieve a truly \nefficient and secure human-machine collaboration paradigm. This article systematically reviews the \nrelevant progress, aiming to promote the transformation of software engineering from an artificial-\ndriven model to an AI-enhanced automated paradigm. It provides theoretical references for ensuring \nthe quality of backend code, improving product delivery speed, and enhancing system reliability. \nKeywords: AI; LLM; Code Generation; Code Review. \n1. Introduction \nRecently, the growing complexity of modern software applications has driven an increased \nemphasis on high-quality, maintainable source code, thereby heightening the difficulty for developers \nto write efficient and error-free code [1-3]. Therefore, there is an urgent need for a smarter approach \nto empover developers. The emergence of Artificial Intelligence has fundamentally transformed \nconventional approaches to code optimization and refactoring by introducing a new dimension of \nautomation [2]. A striking example is LLMs, which have exhibited remarkable potential in \nprogramming tasks [4], especially in code review [5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of"
    },
    {
      "rank": 6,
      "distance_l2": 1.0206949710845947,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_028",
      "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI ’22 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1–7. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot’s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE ’22), November 07, 2022. ACM, Singapore, Singapore, 62–71. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n’23), August 06, 2023. ACM, Long Beach, CA, USA, 5673–5684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21–29. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt1"
}