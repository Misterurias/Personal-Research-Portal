{
  "timestamp": "2026-03-02T04:21:12+00:00",
  "query": "What empirical evidence measures productivity gains from AI coding tools?",
  "retrieval_query": "Productivity gains from AI coding tools: Which studies or research papers measure the impact of AI-powered coding tools on programmer productivity between specific years?",
  "query_rewritten": true,
  "rewritten_query": "Productivity gains from AI coding tools: Which studies or research papers measure the impact of AI-powered coding tools on programmer productivity between specific years?",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.63587486743927,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_015",
      "text": " Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n11\n \n6. Sarkar, A., Ross, N. A., Anantharaman, V., et al. (2024). What is it like to program with artificial \nintelligence? Proceedings of the ACM Conference on Computer-Supported Cooperative Work, 156-\n189. \n7. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI \nassistants? ACM Conference on Computer and Communications Security, 2785-2799. \n8. Asare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub Copilot a substitute for human pair-\nprogramming? An empirical study. ACM Transactions on Software Engineering, 49(2), 1-34. \n9. Dakhel, A. M., Majdinasab, V., Nikanjam, A., et al. (2023). GitHub Copilot AI pair programmer: \nAsset or liability? Journal of Systems and Software, 203, 111734. \n10. Allamanis, M., Brockschmidt, M., & Khademi, M. (2018). Learning to represent programs with \ngraphs. International Conference on Learning Representations, 1-16. \n11. Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and \nnatural languages. Findings of the Association for Computational Linguistics: EMNLP 2020, 1536-\n1547. \n12. Guo, D., Ren, S., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems,"
    },
    {
      "rank": 2,
      "distance_l2": 0.6555407047271729,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_004",
      "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60–62]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: “acceleration mode” in which the tool aided\ndevelopers when they knew what to do next, and “exploration mode” to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM’s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as “WCA@IBM,” was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
    },
    {
      "rank": 3,
      "distance_l2": 0.674193799495697,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    },
    {
      "rank": 4,
      "distance_l2": 0.6826839447021484,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 5,
      "distance_l2": 0.6846526861190796,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_006",
      "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers’ effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation’s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that “general housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors’ time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there’s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and “vibe coding” practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed–quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
    },
    {
      "rank": 6,
      "distance_l2": 0.684795618057251,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_006",
      "text": " of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023, Cui et al. 2024). Our results draw a sharp contrast with \nthese findings, as we demonstrate that peripheral developers obtain less productivity gain from AI pair \nprogrammers than core developers in OSS settings, potentially because the former do not possess important \nand necessary contextual knowledge about an OSS project to effectively use the AI tools. \n2. Literature Review  \n2.1 Generative AI in Software Development \nA growing body of literature has started to examine the impact of generative AI on software development. \nMost studies have focused on the implications of generative AI for individual productivity on specific tasks. \n 6 \nFor example, Imai (2022) finds that GitHub Copilot produces more lines of code than a human pair \nprogrammer when completing a task in Python. Peng et al. (2023) find that GitHub Copilot enables \nindividual developers to implement an HTTP server 55.8% faster than those not using the tool. Hoffmann \net al. (2024) show that GitHub Copilot causes individual developers to shift focus towards coding tasks and \naway from project management.  \nDespite these insights, research on how generative AI influences project-level outcomes for \ncomplex tasks involving multiple developers remains limited. Yeverechyahu et al. (2024) investigate the \ninnovation capabilities of generative AI, particularly its role in extrapolative versus interpolative thinking, \nand compare its influence on innovation in Python versus R. Different from the existing literature and built \nupon the OSS literature, our hypotheses are motivated by the unique characteristics of OSS, namely, the \nsoftware development process in an open and collaborative environment. Because participation is often \nvoluntary and coordination does not follow formal centralized processes in this environment, it remains \nunclear how generative AI influences open participation in both coding and non-coding activities, as well \nas team coordination, all of which could have important implications for project-level software \ndevelopment productivity. \nIn addition, existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al."
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt1"
}