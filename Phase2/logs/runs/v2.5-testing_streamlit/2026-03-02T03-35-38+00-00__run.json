{
  "timestamp": "2026-03-02T03:35:38+00:00",
  "query": "Does GitHub Copilot increase developer productivity?",
  "retrieval_query": "Is there a correlation between using GitHub Copilot and improved developer efficiency?",
  "query_rewritten": true,
  "rewritten_query": "Is there a correlation between using GitHub Copilot and improved developer efficiency?",
  "top_k": 6,
  "answer": "Based on the provided evidence chunks, here are some answers:\n\n1. What is the average time it takes for developers who use GitHub Copilot to complete a programming task?\n\nAccording to the study \"CopilotExperiment2023_chunk_006\", the treated group that has access to GitHub Copilot was able to complete the task 55.8% faster than the control group.\n\n2. How do developers' willingness to pay for GitHub Copilot compare between the control and treated groups?\n\nThe average irrelevant price for the treated group is $27.25, and the average irrelevant price for the control group is $16.91 per month, both of which are statistically significant at the 95% level.\n\n3. What are some potential economic impacts of using GitHub Copilot in software development?\n\nIf the results of this study were to be extrapolated to the population level, a 55.8% increase in productivity would imply a significant amount of cost savings in the economy and have a notable impact on GDP growth.\n\n4. How do developers' willingness to pay for GitHub Copilot relate to their experience with using the tool?\n\nAccording to the study \"CopilotExperiment2023_chunk_006\", less experienced programmers benefit more from using GitHub Copilot, which could imply that they are more willing to pay for it as well.\n\n5. What is the quality of code generated by GitHub Copilot compared to human-written code?\n\nThe study \"CodeQualityComparison2023_chunk_043\" found that there was no significant difference in code understandability between the programming languages used in the study, but there were some differences in code correctness.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5617713332176208,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_006",
      "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers’ effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation’s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that “general housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors’ time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there’s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and “vibe coding” practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed–quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
    },
    {
      "rank": 2,
      "distance_l2": 0.6252670884132385,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 3,
      "distance_l2": 0.6392360329627991,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_004",
      "text": "the treatment group how helpful they found GitHub Copilot as they worked on the task, as well\nas asked them to estimate how much faster they completed the task compared to how long this\ntask would have taken them without using GitHub Copilot. We also asked the control group to\nestimate the size of the speed gain they would have experienced if they used GitHub Copilot,\nafter showing them a 1-minute demo video.\nResults\nA total of 166 offers were sent during the experiment, and 95 were accepted. The 95 developers\nwere randomly assigned into control and treated groups, with 45 in the treated group and 50 in\ncontrol. Thirty-ﬁve developers from both the treated and control groups completed the task and\nsurvey. Figure 5 presents the summary statistics of these participants.\nMost of the participants are in the age group of 25-34 and come from India and Pakistan.\nThis group of participants is also characterized by relatively lower income (median yearly in-\ncome between $10,000-$19,000) compared to US standards but high education level (the ma-\njority have a 4-year degree and above). The group has an average coding experience of 6 years\nand, on average, reported spending 9 hours on coding in a working day.\nFigure 6 plots the distribution between time to completion between treated and control\ngroups. Conditioning on completing the task, the average completion time from the treated\ngroup is 71.17 minutes and 160.89 minutes for the control group. This represents a 55.8% re-\nduction in completion time. The p-value for the t-test is 0.0017, and a 95% conﬁdence interval\nfor the improvement is between [21%, 89%]. There are four outliers with time to completion\nabove 300 min. All outliers are in the control group, however our results remain robust if these\noutliers are dropped. This result suggests that Copilot increases average productivity signif-\nicantly in our experiment population. We also ﬁnd that the treated group’s success rate is 7\n5\nTable 1: Heterogeneous Treatment Effects\nEstimates\nSE\nt-Stat\np-Value\n(Intercept)\n78.01\n67.84\n1.15\n0.2552\nProgramming experience (years)\n8.23\n4.36\n1.90\n0.0629\nHours of programming per day\n-11.70\n4.74\n-2.47\n0.0168\nAge: 25-44\n-74.55\n33.52\n-2.22\n0.0303\nUnemployed\n-35.98\n36.33\n-0.99\n0.3263\nIncome less than $20,000\n0.64\n27.47\n0.02\n0.9814\nNo college\n-36.57\n32.89\n-1.11\n0.2711\nLanguage Preference: Java\n-11.77\n33.16\n-0.35\n0.7240\nLanguage Preference: Python\n22.90\n42.19\n0.54\n0.5895\nNote: This table presents the heterogeneous treatment effects. The results suggest developer with less program-\nming experience are more likely to beneﬁt from"
    },
    {
      "rank": 4,
      "distance_l2": 0.6483734250068665,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_002",
      "text": " team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming tasks 55.8% faster (Peng\net al. 2023). Such productivity benefits lead to promises of faster time-to-market and increased revenue\nfor organizations developing software applications. Considering these shifts, major tech organizations have\nstarted to increasingly rely on AI in their projects - “more than a quarter of all new code at Google is gen-\nerated by AI, then reviewed and accepted by engineers,\" reported Google CEO Sundar Pichai in January,\n2025.1 Moreover, Microsoft CTO Kevin Scott expects that 95% of all code will be AI-generated by 2030.2\nWhile these productivity gains are promising, they also raise important questions about the quality and\nmaintainability of AI-generated code. Because AI tools can lower the skill barrier for writing code (Dakhel\net al. 2023), AI tools enable broader participation but may also encourage developers to rely on gener-\nated solutions without fully understanding the underlying design rationale and potential integration issues\n(Barrett et al. 2023). Such reliance increases the likelihood of quick fixes that favor short-term function-\nality over long-term maintainability (Barrett et al. 2023). Extant literature characterizes “quick and dirty”\nsoftware customizations made without a complete understanding of their future implications as technical\ndebt, as they undermine system reliability and impose long-term maintenance obligations (Kruchten et al.\n2012, Brown et al. 2010, Banker et al. 2021). As a result, project maintainers must devote additional effort\nto understanding, reviewing, and reworking AI-generated code before it can be safely integrated. In our\ncontext, we contend that the growing reliance on AI-assisted development may accelerate the accumulation\nof technical debt, as design shortcuts taken to expedite system deployment become embedded in software\nsystems (Ramasubbu and Kemerer 2016, 2021).\nThe technical debt and maintenance challenges that AI poses are expected to be especially pronounced\nin distributed software development teams, such as in Open Source Software (OSS) communities. In these\ncommunities, contributors from around the world collaborate, often voluntarily, to develop and maintain\nsoftware that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making\nit freely or cheaply available to the public (Eghbal 2020, Nagle 2019). Despite the voluntary nature of\nwork in these communities, OSS constitutes critical digital infrastructure for modern society, with estimates\nsuggesting that the total cost of reproducing this software would amount to $8.8 trillion (Hoffmann et al.\n2024).3 Given"
    },
    {
      "rank": 5,
      "distance_l2": 0.6607691049575806,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_043",
      "text": " of GitHub Copilot by conducting a within-\nsubjects user study Vaithilingam et al. (2022). It was found that GitHub Copilot did\nnot cause a significant improvement in terms of speed and success rate. However, it\nwas stated that most participants preferred to use Copilot in daily programming tasks\nsince it saved the effort for the basic tasks.\nNguyen and Nadi (2022) evaluated GitHub Copilot using 33 different LeetCode\nquestions and four different programming languages (Python, Java, JavaScript, and C).\nTheir evaluation includes code correctness and code understandability for the generated\ncode. They evaluated code correctness by measuring the ratio of passed tests for each\nquestion, which is a similar approach to our study. Code understandability was measured\nby two different metrics, which are cognitive and cyclomatic complexity. In terms of\ncode correctness, Java had the highest (57%) and JavaScript had the lowest (27%) score.\n42\nBurak Yetiştiren et al.\nFor code understandability, they determined that there was no statistical significance\nbetween the programming languages.\nMastropaolo et al. (2023) presented an empirical study that focuses on the effect of\nsemantic-preserving changes in the natural language on the generated code function of\nGitHub Copilot. For this purpose, Mastropaolo et al. (2023) provided 892 non-trivial\nJava method descriptions to GitHub Copilot. Firstly, they used the original descriptions\nof methods and asked GitHub Copilot to generate them. Secondly, they paraphrased\ndescriptions manually. Thirdly, they paraphrased descriptions using automated para-\nphrasing tools. After GitHub Copilot generated all of the methods according to their\ndescriptions, they found that in 46% of cases, semantically equivalent but different\nmethod descriptions resulted in different code recommendations. Moreover, they ob-\nserved that some code recommendations were correct with only one of the semantically\nequivalent descriptions as input.\nChatGPT is the other code generator that we have chosen for our study. Since\nChatGPT is released recently, there are only a few studies similar to our work. These\nstudies are in the following.\nIn order to analyze the bug fixing performance of ChatGPT, Sobania et al. (2023)\nevaluated ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compared\nthese results with CoCoNut, Codex, and standard APR approaches. They found that\nChatGPT had a similar performance to Codex and its performance was much better\nthan standard APR approaches. When Sobania et al. (2023) used the dialogue option\nof ChatGPT and gave ChatGPT more information about the bug, they found that\nChatGPT gave an overall success rate of 77.5%. Then, they concluded that although\nChatGPT had an outstanding performance, it required mental cost to verify ChatGPT\nanswers.\nThe possible integration of ChatGPT into a well-known software testing curriculum\nis covered in another research. Jalil et al. (2023) requested that ChatGPT respond to\ntypical software testing questions. They discovered that ChatGPT could offer correct or\npartly correct responses in 44% of the cases and correct or partially correct explanations\nof answers in 57% of the cases. As a result, they noticed that"
    },
    {
      "rank": 6,
      "distance_l2": 0.6864115595817566,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_006",
      "text": " highest monthly price at which they\nwould be interested in getting notiﬁed about the release of GitHub Copilot. The intention is to\nlearn about developers’ willingness to pay for Copilot as the answer to this question provides\nan upper bound for the developers’ willingness to pay. Figure 8 presents the distribution of the\nirrelevant price separated for the control and treated groups. The average irrelevant price for the\ntreated group is $27.25, and the average irrelevant price for the control group is $16.91, both per\nmonth. The difference is statistically signiﬁcant at the 95% level. This result provides indirect\nevidence that treated group beneﬁted from Copilot during their task as their willingness to pay\nis signiﬁcantly higher than the control group.\nDiscussion\nThis paper presents evidence on the productivity effects of generative AI tools in software de-\nvelopment. To the best of our knowledge, it is the ﬁrst controlled experiment to measure the\nproductivity of AI tools in professional software development. Our results suggest that Copilot\nhas statistically and practically signiﬁcant impact on productivity: the treated group that has\naccess to GitHub Copilot was able to complete the task 55.8% faster than the control group.\nFurther investigations into the productivity impacts of AI-powered tools in software devel-\nopment are warranted. This study examines a standardized programming task in an experiment\nto obtain a precise measure of productivity, instead of a task where developers collaborate on\nlarge projects in professional proprietary and/or open-source settings. Productivity beneﬁts may\n7\nvary across speciﬁc tasks and programming languages, so more research is needed to understand\nhow our results generalizes to other tasks. Finally, this study does not examine the effects of\nAI on code quality. AI assistance can increase code quality if it suggests code better than the\nprogrammer writes, or it can reduce quality if the programmer pays less attention to code. The\ncode quality can have performance and security considerations that can change the real-world\nimpact of AI.\nThe heterogeneous effects identiﬁed in this study warrant close attention. Our results sug-\ngest that less experienced programmers beneﬁt more from Copilot. If this result persists in\nfurther studies, the productivity beneﬁts for novice programmers and programmers of older\nage point to important possibilities for skill initiatives that support job transitions into software\ndevelopment.\nThe economic impacts of these models also warrant further research [Manning et al., 2022],\nwith particular attention on their implications for labor market. In 2021, over 4.6 million people\nin the United States worked in computer and mathematical occupations,1 a Bureau of Labor\nStatistics category that includes computer programmers, data scientists, and statisticians. These\nworkers earned $464.8 billion or roughly 2% of US GDP. If the results of this study were to be\nextrapolated to the population level, a 55.8% increase in productivity would imply a signiﬁcant\namount of cost savings in the economy and have a notable impact on GDP growth. It is, as of\nyet, unclear how such gains would be distributed and how job tasks would change to incorporate\nAI-powered developer tools. It is important to consider such impacts and to begin research on\nthese implications at the outset ["
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}