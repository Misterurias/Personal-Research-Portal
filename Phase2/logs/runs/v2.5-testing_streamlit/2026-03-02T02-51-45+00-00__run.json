{
  "timestamp": "2026-03-02T02:51:45+00:00",
  "query": "How do AI coding tools affect perceived productivity compared to objective performance metrics?",
  "retrieval_query": "Impact of AI coding tools on perceived productivity versus objective performance metrics in software development.",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding tools on perceived productivity versus objective performance metrics in software development.",
  "top_k": 7,
  "answer": "According to the provided evidence chunks, here is an answer that only uses the provided information:\n\nThe research assistant can conclude that the use of AI-assisted code generation tools, such as GitHub Copilot, has a significant impact on developer productivity. The study found that developers who used these tools experienced a 31.4% increase in average developer productivity.\n\nAdditionally, the study also found that the reported benefits of receiving AI suggestions while coding span the full range of typically investigated aspects of productivity, including task time, product quality, cognitive load, enjoyment, and learning.\n\nIt's worth noting that the driving factor for these improvements appears to be not correctness as such, but whether the suggestions are useful as a starting point for further development.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.4878152012825012,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_004",
      "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60–62]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: “acceleration mode” in which the tool aided\ndevelopers when they knew what to do next, and “exploration mode” to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM’s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as “WCA@IBM,” was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
    },
    {
      "rank": 2,
      "distance_l2": 0.611393392086029,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
      "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
    },
    {
      "rank": 3,
      "distance_l2": 0.612892746925354,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_004",
      "text": " from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators… \n(1) … efficiency \n(2) … accuracy \n(3) … the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is one of several GenAIs that can assist developers by \ngenerating, completing and modifying programming code \nbased on the context of the codebase and natural language \nprompts. \nIt \nis \nintegrated \nin \nsoftware \ndevelopment \nenvironments (IDEs) like VS Code and can function as an “AI \npair programmer” for developers.  \nIn recent years, multiple studies have been conducted to \nevaluate Copilot’s output quality and developers’ productivity \ngains through AI usage. Based on benchmark tasks, Dakhel et \nal. [32] found that Copilot can generate solutions for nearly all \ngiven tasks, but its outputs are less often correct than those of \nhumans, making it a potential asset for experienced \ndevelopers but a liability for novices who may not detect non-\noptimal suggestions. So far, there is no consensus on how to \nbest measure software developers’ productivity, but time \nsavings, acceptance rate of Copilot suggestions, and successful \ncompletion of predefined tasks are frequently considered \nmetrics to evaluate developer productivity in the context of \nAI-assisted programming. Empirical studies using these \nmetrics report mixed results: Bakal et al. [1]  found that \ndevelopers using Copilot accepted 33% of suggestions and \nhad a 20% increase in lines of code; Peng et al. [37] observed \nthat programmers completed an HTTP server task 55.8% \nfaster with Copilot, with the largest productivity gains among \nless experienced programmers; while Vaithilingam et al. [45] \nfound no significant improvement in task time or success, \nthough participants still valued Copilot as a useful starting \npoint for daily programming tasks. In this study, we consider \ntask duration, the number of Copilot’s suggestions, and \nsuccessful \ntask \ncompletion \nas \nproductivity \nmetrics. \nAdditionally, we examine the impact of Copilot interaction \ntypes, \ndevelopers’ \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers’ Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers’ \nexperiences during work [6, 17, 36]. However, only a few \n"
    },
    {
      "rank": 4,
      "distance_l2": 0.620110273361206,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_002",
      "text": "higher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways that are only \npartially reflected by measures such \nas completion times for standardized \ntasks.23,a Alternatively, we can leverage \nthe developers themselves as expert \nassessors of their own productivity. \nThis meshes well with current think­\ning in software engineering research \nsuggesting \nmeasuring \nproductiv­\nity on multiple dimensions and using \nself-reported data.6 Thus, we focus on \nstudying perceived productivity.\nHere, we investigate whether usage \nmeasurements of developer interac­\ntions with GitHub Copilot can predict \nperceived productivity as reported \nby developers. We analyze ​2,631​ sur­\na\t Nevertheless, such completion times are \ngreatly reduced in many settings, often by \nmore than half.16\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    55\nILLUSTRATION BY JUSTIN METZ\nresearch\nFigure 1. GitHub Copilot’s code completion funnel.\nFigure 2. Demographic composition of survey respondents.\nThink of \nthe language \nyou have used \nthe most with \nOurTool. \nHow proﬁcient\nare you in \nthat language?\nBeginner\nIntermediate\nAdvanced\nStudent/Learning\n0–2 Years Prof. Experience\n3–5 Years Prof. Experience\n6–10 Years Prof. Experience\n11–15 Years Prof. Experience\n16+ Years Prof. Experience\nStudent\nProfessional\nHobbyist\nConsultant/Freelancer\nResearcher\nOther\nPython\nJavaScript\nTypeScript\nJava\nRuby\nGo\nC#\nRust\nHTML\nOther\nWhich best\ndescribes your\nprogramming\nexperience?\nWhich of \nthe following \nbest describes \nwhat you do?\nWhat\nprogramming\nlanguages\ndo you usually \nuse? Choose\nup to three\nfrom the list.\n0%\n25%\n50%\n75%\n100%\nAverage number of events per survey user active hour\n50\n170\nmostly unchanged\nCompletion\nunchanged\n40\n30\n20\n24\n6.6\ncompletion\nopportunity\ncompletion\nshown\ncompletion\naccepted\nafter 30\nseconds\nafter 2\nminutes\nafter 5\nminutes\nafter 10\nminutes\n5.6\n4.3\n5.3\n5.1\n5\n3.1\n3.4\n3.8\n10\n0\nvey responses from developers using \nGitHub Copilot and match their re­\nsponses to measurements collected \nfrom the IDE. We consider acceptance \ncounts and more detailed measures \nof contribution, such as the amount"
    },
    {
      "rank": 5,
      "distance_l2": 0.6273490786552429,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 6,
      "distance_l2": 0.6308873295783997,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 7,
      "distance_l2": 0.6339712738990784,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_015",
      "text": " Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n11\n \n6. Sarkar, A., Ross, N. A., Anantharaman, V., et al. (2024). What is it like to program with artificial \nintelligence? Proceedings of the ACM Conference on Computer-Supported Cooperative Work, 156-\n189. \n7. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI \nassistants? ACM Conference on Computer and Communications Security, 2785-2799. \n8. Asare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub Copilot a substitute for human pair-\nprogramming? An empirical study. ACM Transactions on Software Engineering, 49(2), 1-34. \n9. Dakhel, A. M., Majdinasab, V., Nikanjam, A., et al. (2023). GitHub Copilot AI pair programmer: \nAsset or liability? Journal of Systems and Software, 203, 111734. \n10. Allamanis, M., Brockschmidt, M., & Khademi, M. (2018). Learning to represent programs with \ngraphs. International Conference on Learning Representations, 1-16. \n11. Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and \nnatural languages. Findings of the Association for Computational Linguistics: EMNLP 2020, 1536-\n1547. \n12. Guo, D., Ren, S., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems,"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2-trust-tightened-qrewrite"
}