{
  "timestamp": "2026-03-02T03:00:10+00:00",
  "query": "What does the corpus say about the 47th president of the United States",
  "retrieval_query": "\"Information retrieval for statements related to the 47th President of the United States\"",
  "query_rewritten": true,
  "rewritten_query": "\"Information retrieval for statements related to the 47th President of the United States\"",
  "top_k": 10,
  "answer": "I can only answer based on the provided evidence chunks. Please provide the specific chunk you would like me to address, and I will respond accordingly.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 1.3219655752182007,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_062",
      "text": " and mobile App \ndescription (Lee et al. 2020). The key advantage of using LDA lies in its ability to identify latent themes \nfrom the data by grouping semantically related keywords into coherent topics, rather than treating each \nword as an independent feature. We exclude comments that are not in English or are too short to convey \nmeaningful information. After this step, we estimate the topic model using a corpus of 5,404,735 merged \nPR comments collected from selected repositories between 2021 and 2022. This modeling approach \nprovides a multinomial distribution over the inferred topics for each merged PR, enabling us to assess both \nthe number of distinct topics present and the evenness of their distribution.  \nA critical hyperparameter in LDA modeling is the number of topics to extract. Several methods \nhave been proposed to determine the optimal number, including metrics such as perplexity, topic coherence, \nand topic diversity. Given our objective of categorizing advice-related content into distinct yet coherent \n19 \n \nthemes, we rely on both coherence and diversity scores to guide model selection. Based on these criteria, \nwe identify 10 as the optimal number of topics, balancing topic distinctiveness and coherence.  \nThe results, presented in Table I.1, indicate that following the adoption of Copilot use, there was \nan increase in both the number of code discussion topics and the entropy score. This suggests that Copilot \nintroduces diverse code discussion content, leading to increased coordination time. \nTable I.1: The Impact of Copilot on Project-Level Code Discussion Content \n \n(1) \n(2) \n \nEntropy_score \nTopics_count \nATT of Copilot \n0.017** \n0.029** \n \n(0.007) \n(0.014) \nRepository FE \nYes \nYes \nMonth FE \nYes \nYes \n# of repositories \n4,009 \n4,009 \nObservations \n78,636 \n78,636 \nNotes: All estimations are based on the GSCM method. Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, \n* p<0.1. \n \n \n \n20 \n \nAppendix J: The Impact of Copilot and Project Complexity \n \nIn OSS communities, complex projects typically contain more features and code modules. Prior work \nsuggests that developers are more likely to contribute to complex projects, as they offer greater status and \nvisibility benefits (Chengalur-Smith et al. 2010, Hann et al. 2013, Medappa and Srivastava 2019). However, \nthese projects also tend to impose greater demands for knowledge sharing, communication, and mutual \nunderstanding (Williams 2011, Zimmermann et al. 2018). While generative AI tools enhance developers’ \ncapacity to produce code, they do not reduce the inherent complexity of a project. There are two plausible, \nyet opposing, mechanisms through which project complexity might influence the observed effects of \nCopilot.  \nOn one hand, complex projects may attract more contributors due to their higher status potential, \nwhich can lead to greater project-level code contributions. However, the increased number of contributors \nalso brings a wider range of perspectives and development styles, making it more difficult to reach \nconsensus and resolve conflicting views, thereby increasing coordination time. On the other hand, these \nprojects may present"
    },
    {
      "rank": 2,
      "distance_l2": 1.3961453437805176,
      "source_id": "CopilotRobustness2023",
      "chunk_id": "CopilotRobustness2023_chunk_007",
      "text": " we split the 892 methods together with their\noriginal description into four sets and assigned each of\nthem to one author. Each author was in charge of writing a\nsemantically equivalent but different description of the method\nby looking at its code and original description. This resulted\nin a dataset (available in [6]) in which, for each subject\nmethod, we have its original and paraphrased description. In\nthe end, for each original sentence, we had between one and\nthree paraphrases: paraphrasedPEGASUS, paraphrasedTP, and\nparaphrasedmanual. While paraphrasedmanual is available for\nall the methods, paraphrasedPEGASUS and paraphrasedTP are\nnot. Indeed, we exclude the cases in which each of such tools\nfailed to generate paraphrases (1 and 100, respectively) and the\nones that were not considered as semantically equivalent in our\nmanual check (based on the results of RQ0). The maximum\nnumber of semantically equivalent paraphrases is 2,575 (up to\n891 with PEGASUS, up to 792 with TP, and 892 manually).\nThe paraphrases, as well as the original description, have\nbeen used as input to Copilot, simulating developers asking it\nto synthesize the same Java method by using different natural\nlanguage descriptions. At the time of our study, Copilot does not\nprovide open APIs to access its services. The only way to use\nit is through a plugin for one of the supported IDEs. Manually\ninvoking Copilot for the thousands of times needed (up to 6,934,\nas we will explain later) was clearly not an option. For this\nreason, we developed a toolchain able to automatically invoke\nCopilot on the subject instances: We exploit the AppleScript\nlanguage to automate this task on a MacBook Pro, simulating\nthe developer’s interaction with Visual Studio Code (vscode).\nFor each method mi in our dataset, we created up to four\ndifferent versions of the Java ﬁle containing it (one for each\nof the experimented descriptions). In all such versions, we\n(i) emptied mi’s body, just leaving the opening and closing\ncurly bracket delimiting it; and (ii) removed the Doc Comment,\nreplacing it with one of the four code descriptions we prepared.\nStarting from these ﬁles, the automation script we imple-\nmented (available in our replication package [6]) performs the\nfollowing steps on each ﬁle Fi.\nFirst, it opens Fi in vscode and moves the cursor within the\ncurly brackets of the method mi of interest. Then, it presses\n“return” to invoke Copilot, waiting up to 20 seconds for its\nrecommendation. Finally, it stores the received recommendation,\nthat could possibly be empty (i.e., no recommendation received).\nTo better understand this process, the top part of Fig. 1 depicts\nhow the invocation of Copilot is performed. The gray box\nrepresents the whole Java ﬁle (i.e., the context used by Copilot\nfor the prediction). The emptied method (i.e.,getEmbeddings)\nis framed with a black border, with the cursor indicating the\nposition in which Copilot is invoked. The green comment on\ntop of the method represents one of the descriptions we"
    },
    {
      "rank": 3,
      "distance_l2": 1.4111524820327759,
      "source_id": "CopilotRobustness2023",
      "chunk_id": "CopilotRobustness2023_chunk_009",
      "text": " we preliminarily assess how far the\nparaphrased descriptions are from the original ones (i.e., the\npercentage of changed words) by computing the normalized\ntoken-level Levenshtein distance [31] (NTLev) between the\noriginal (do) and any paraphrased description (dp):\nNTLev(do, dp) =\nTLev(do, dp)\nmax({|do|, |dp|})\nwith TLev representing the token-level Levenshtein distance\nbetween the two descriptions.\nWhile the original Levenshtein distance works at character-\nlevel, it can be easily generalized at token-level (each unique\ntoken is represented as a speciﬁc character). In this case, a token\nis a word in the text. The normalized token-level Levenshtein\ndistance provides an indication of the percentage of words\nthat must be changed in the original description to obtain a\nparaphrased one.\nThen, we analyze the percentage of methods for which the\nparaphrased descriptions result in a different method prediction\nas compared to the original one. When they are different, we\nalso assess how far the methods obtained by using a given\nparaphrased description is from the method recommended\nwhen providing the original description as input. Also in this\ncase we use the token-level Levenshtein distance as metric. The\nlatter is computed with the same formula previously reported\nfor the natural text descriptions; in this case, however, the\ntokens are not the words but the Java syntactic tokens. Thus,\nNTLev indicates in this case the percentage of code tokens\nthat must be changed to convert the method obtained through\nthe original description into the one recommended with one\nof the paraphrases.\nFinally, we study the “quality” of the recommendations\nobtained using the different descriptions both in the Full\ncontext and Non-full context scenarios. Given the sets of\nmethods generated from the original description and each of the\nparaphrasing approach considered, we present the percentages\nof methods for which Copilot: (i) synthesized a method passing\nall the related test cases (PASS); (ii) synthesized a method that\ndoes not pass at least one of the test cases (FAIL); (iii) generated\nan invalid method (i.e., with syntactic errors) (ERROR); (iv)\ndid not generate any method (EMPTY). Syntactic errors have\nbeen identiﬁed as recommendations for which Java Parser [3]\ndid not manage to identify a valid recommended method (i.e.,\ncases in which Java Parser fails to identify a method node in\nthe AST generated for the obtained recommendation). On top\nof the passing/failing methods, we also compute the token-level\nLevenshtein distance and the CodeBLEU [49] between each\nsynthesized method and the target one (i.e., the one originally\nimplemented by the developers). CodeBLEU measures how\nsimilar two methods are. Differently from the BLEU score\n[46], CodeBLEU evaluates the predicted code considering not\nonly the overlapping n-grams but also syntactic and semantic\nmatch of the two pieces of code (predicted and reference) [49].\nD. Replication Package\nThe code and data used in our study are publicly available\n[6]. In particular, we provide (i) the dataset of"
    },
    {
      "rank": 4,
      "distance_l2": 1.4421870708465576,
      "source_id": "NoviceProgramming2024",
      "chunk_id": "NoviceProgramming2024_chunk_012",
      "text": ", we analyzed the level of agree-\nment to the dichotomous (yes, no) statement, “I used AI tools during this assignment”,\non weeks 2, 6, 9 and 11. Those weeks did not require the use of AI tools. The descriptive\nfrequency and percentage of responses to the statement can be found in Table 3.\nTable 3. Frequency and percentage of responses to the statement, “I used AI tools during this\nassignment”, during weeks 2, 6, 9, and 11 (n = 73).\nResponse\nWeek 2\nWeek 6\nWeek 9\nWeek 11\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nyes\n24\n32.9\n30\n41.1\n30\n41.1\n42\n57.5\nno\n49\n67.1\n43\n58.9\n43\n58.9\n31\n42.5\nNon-parametric Cohran’s Q test was used to compare the response to the item, “I used\nAI tools during this assignment”, on weeks 2, 6, 9, and 11 (yes, no). Cohran’s Q = 34.839,\np < 0.001, meaning that there were significant differences on the response between weeks 2,\n6, 9, and 11, indicating that students used more AI tools.\n3.3. Student Satisfaction with AI Tools\nTo address the third research question, “Are students satisfied with the results pro-\nvided by AI tools, and does this satisfaction improve over time?”, we analyzed students’\nlevel of agreement with the statement, “I was happy with the results provided by AI tools”,\nusing a Likert scale of 1 to 5 during weeks 3, 7, and 10. During these specific weeks, the use\nof AI tools was mandatory for completing an assignment. The descriptive frequency and\npercentage of responses can be found in Table 4.\nThe Friedman test for several related samples was applied comparing the responses to\nthe item, “I was happy with the results provided by AI tools”, on weeks 3, 7, and 10. The\ndifferences in medians were significant: χ2 (2) = 11.594, p = 0.003, meaning that the answers\nwere better. However, since students used the same language model, this result suggests\nthat they improved their prompt-engineering skills over time.\nEduc. Sci. 2024, 14, 1089\n9 of 17\nTable 4. Frequency and percentage of responses to the statement, “I was happy with the results\nprovided by AI tools”, during weeks 3, 7, 10 (n = 73).\nResponse\nWeek 3\nWeek 7\nWeek 10\nFrequency\nPercent\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n0\n0\n0\n0\n0\n0\ndisagree (2)\n3\n4.1\n1\n1.4\n0\n0\nneutral (3)\n10\n13.7\n6\n8.2\n3\n4.1\nagree (4)\n31\n42.5\n34\n46.6\n35\n47.9\nstrongly agree ("
    },
    {
      "rank": 5,
      "distance_l2": 1.44637131690979,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_030",
      "text": " years\n3−5 years\n6−10 years\n11−15 years\n16−20 years\n21−25 years\n26−30 years\n31+ years\n0%\n5%\n10%\n15%\nPercentage of respondents\nTenure with IBM\n(b)\nAmericas\nAPAC\nEMEA\nJapan\n0%\n10%\n20%\n30%\n40%\n50%\nPercentage of respondents\nGeography\n(c)\nFig. 1. Survey respondent demographics: (a) years of experience as a professional software engineer, (b) tenure with IBM, and (c)\ngeography.\nManuscript submitted to ACM\n20\nWeisz et al.\nMotivations\nLiang et al. [28]\nOur respondents\nMotivations for use (Liang et al. [28])\nM1\nTo have an autocomplete or reduce the amount of keystrokes I\nmake\n86%\n6.2%\n47%\n30%\nM2\nTo finish my programming tasks faster\n76%\n12%\n59%\n21%\nM3\nTo skip needing to go online to find specific code snippets, pro-\ngramming syntax, or API calls I’m aware of, but can’t remember\n68%\n14%\n64%\n17%\nM4\nTo discover potential ways or starting points to write a solution\nto a problem I’m facing\n50%\n24%\n64%\n17%\nM5\nTo find an edge case for my code I haven’t considered\n36%\n44%\n48%\n28%\nAdditional motivations for use\nA1\nMy colleagues recommended I use it\n–\n30%\n39%\nA2\nMy line management recommended I use it\n–\n60%\n19%\nA3\nI like to explore new tools\n–\n68%\n14%\nA4\nI wanted to know more about how my work might change in\nthe future\n–\n64%\n18%\nA5\nIt is my responsibility to try new IBM products\n–\n64%\n18%\nMotivations for non-use (Liang et al. [28])\nM6\nCode generation tools write code that doesn’t meet functional\nor non-functional (e.g., security, performance) requirements\nthat I need\n54%\n34%\n14.3%\nM7\nIt’s hard to control code generation tools to get code that I want\n48%\n36%\n14.3%\nM8\nI spend too much time debugging or modifying code written\nby code generation tools\n38%\n45%\n14.3%\nM9\nI don’t think code generation tools provide helpful suggestions\n34%\n46%\n32.1%\nM10\nI don’t want to use a tool that has access to my code\n30%\n51%\n–\nM11\nI write and use proprietary code that code generation tools\nhaven’t seen before and don’t generate\n28%\n59%\n–\nM12\nTo prevent potential intellectual property infringement\n26%\n66%\n–\nM13\nI find the tool’s suggestions too distracting\n26%\n51%\n–\nM14\nI don’t understand the code written by code generation tools\n16%\n76%\n3.6%\nM15\nI don’t want to use open-source code\n10%\n89%\n–\nAdditional motivations for non-use\nA6\nCode generated by WCA doesn’t perform well enough for my\nneeds\n–\n17.9%\nA7\nI don’t understand"
    },
    {
      "rank": 6,
      "distance_l2": 1.4551914930343628,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_061",
      "text": "D) \nLog \n(MergedPR\n_10D) \nLog (Merge_time) \n-0.152*** \n(0.002) \n \n-0.144*** \n(0.002) \n \n-0.119*** \n(0.002) \n \nLog (Merged_PR) \n \n1.027*** \n(0.003) \n \n1.058*** \n(0.002) \n \n1.069*** \n(0.002) \nRepository FE \nYes \nYes \nYes \nYes \nYes \nYes \nMonth FE \nYes \nYes \nYes \nYes \nYes \nYes \n# of repositories \n7,637 \n7,637 \n7,637 \n7,637 \n7,637 \n7,637 \nObservations \n139,329 \n139,329 \n139,329 \n139,329 \n139,329 \n139,329 \nNotes: Robust standard errors clustered at repository level in parentheses. *** p<0.01, ** p<0.05, * p<0.1. \n \n \n \n \n18 \n \nAppendix I: Text Analysis of Comment Content \n \nPrior research in team coordination emphasizes that the content of discussions also plays a role in shaping \ncoordination time, as greater diversity in discussion topics can increase the complexity and difficulty of \ncoordination (Hansen et al. 2018). Building on this insight, we argue that the same logic may apply to \nsoftware development, where developer code discussions serve as a primary mechanism for coordination. \nIn this context, a broader range of topics discussed may similarly introduce additional coordination \nchallenges.  \nTo examine the effect of GitHub Copilot on the diversity of code discussion topics within merged \nPRs, we apply a Latent Dirichlet Allocation (LDA) topic modeling approach. Specifically, we measure both \nthe number of topics discussed per merged PR and the entropy of topic distribution. We adopt the LDA \nmodel for three key reasons. First, LDA addresses the data sparsity challenges inherent in traditional \napproaches such as TF-IDF. Second, the topics and associated keywords generated by LDA are human-\ninterpretable, in contrast to the latent dimensions produced by models like doc2vec. Third, LDA has been \nwidely validated and adopted in prior literature across various research contexts, including scientific \npublications (Griffiths and Steyvers 2004, Wang and Blei 2011), social media content (Ramage et al. 2010, \nWeng et al. 2010, Lee et al. 2016, Shin et al. 2020), business descriptions (Shi et al. 2016), and mobile App \ndescription (Lee et al. 2020). The key advantage of using LDA lies in its ability to identify latent themes \nfrom the data by grouping semantically related keywords into coherent topics, rather than treating each \nword as an independent feature. We exclude comments that are not in English or are too short to convey \nmeaningful information. After this step, we estimate the topic model using a corpus of 5,404,735 merged \nPR comments collected from selected repositories between 2021 and 2022. This modeling approach \nprovides a multinomial distribution over the inferred topics for each merged PR, enabling us to assess both \nthe number of distinct topics present and the evenness of their distribution.  \nA"
    },
    {
      "rank": 7,
      "distance_l2": 1.4688316583633423,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_031",
      "text": " without\npenalty. They will still be compensated even if they fail the attention checks, do not fully complete the task,\nor complete the task incorrectly. Quiz responses are stored in Google Drive, and coding keystrokes are stored\nin the coding Platform. All stored information is completely anonymized; only the data platform can use the\nIDs to identify the participants for payment. We further remove data platform identifiers to annotate coding\npatterns between the two groups.\nB\nQualitative Analysis Data and Details\nB.1\nAnnotation Procedure\n51/52 participants uploaded screen recordings of their work in the warm-up task, main coding task, and quiz.\nWe watched the recordings of for all participants (25 AI condition, 25 no AI condition) for the main coding\ntask. We record the time stamps of the following events:\nWe also note general themes in how participants use AI in each condition based on these codes of events in\nthe event.\nB.2\nData Availability\nWe make the annotated transcripts of each participant available at the following URL: https://github.\ncom/safety-research/how-ai-impacts-skill-formation.\n24\nEvent\nDescription\nAdditional Info\nTask Start\nUser opens each task\nAI Interaction\nUser starts typing into AI window\nDescription of interaction\nAI Query\nUser receives answer from AI\nQuery\nWebsearch\nUser queries search engine\nQuery\nPaste (Direct)\nUser pastes output of AI assistant\nCode Copying\nUser types code using AI output\nError\nCode produces error when run\nError Message\nInterface Error\nDevelopment environment or AI assistant error\nTask Completion\nCorrect output is achieved\nTask Submission\nUser submits task\nCode completion\nTable 5: Events annotated manually for each video recording of the main task.\nB.3\nParticipant Feedback Details\nWe include all participant feedback in Table 6 and 7. Since the average completion time was faster for the AI\ngroup, the AI group left more comments since they felt like they had more time at the end of the task.\nC\nEvaluation Details\nC.1\nEvaluation Design\nQuestion Types We discuss the three types of questions we used: Conceptual Understanding, Code Reading,\nand Debugging in Section 4.2.\nKnowledge Categories The evaluation covers 7 core concepts from the Trio library:\n1. Async and await keywords: When to use await keywords within async functions. For example:\n“What happens when the await keyword is used in an async function?”\n2. Starting Trio functions: Basic Trio usage including how to spawn tasks and how spawned tasks with\ndifferent durations behave.\n3. Error handling in Trio: Understanding error propagation patterns and how to catch errors in child\ntasks. For example “ What happens to a parent task when a child task raises an unhandled exception\nin Trio?”\n4. Coroutines: When calling async functions, how to debug co-routine never awaited errors.\n5. Memory channels using Trio: Understanding that start_soon doesn’t return anything and how\nto use dictionaries, lists and other memory channels to collect data when running multiple tasks in\nparallel.\n6. Opening and closing a Trio nursery: Understanding asynchronous context managers and how\nto use them. For example, a debugging question consisting of a snippet of code where the nursery is\nstarted in correctly.\n7. Sequential vs concurrent execution: The expected behavior of concurrent tasks. For example\n“Read the following code and identify when each task starts"
    },
    {
      "rank": 8,
      "distance_l2": 1.4718654155731201,
      "source_id": "NoviceProgramming2024",
      "chunk_id": "NoviceProgramming2024_chunk_010",
      "text": " consistency reliability, were computed. The values\nwere acceptable: 0.935 for feeling comfortable with usage of AI tools in the assignment on\nEduc. Sci. 2024, 14, 1089\n7 of 17\nweeks 3, 7, 10; 0.924 for using AI tools during the assignment on weeks 2, 6, 9; 0.758 for\nbeing happy with the results provided by AI tools on weeks 3, 7, 10.\nFor the qualitative data, participants’ responses to the open-ended questions included\nin the bonus tasks were analyzed using content analysis. Prior to analyzing the data,\ntwo sets of categories based on prior research and literature were designed. The first set\nof categories was designed for responses to the statement, “I used AI tools during this\nassignment for the following tasks”, and the second set was for responses to the direction,\n“Describe the benefits and concerns about using AI tools in your studies, personally”.\nWe initially employed a deductive coding approach to categorize the data based on\nthese predefined categories. However, several new categories emerged during the analysis,\nwhich were subsequently added to the pre-existing codes. Thus, a hybrid coding approach\nwas adopted: the deductive coding provided a structured framework, while the inductive\ncoding allowed for the discovery of additional categories, adding depth and nuance to\nthe findings.\n3. Results\nBased on the responses received, we analyzed the results to address our research\nquestions. We did not specify the use of particular tools. The majority of students utilized\ntools designed to understand and generate human-quality text in response to a variety of\nprompts and questions. Most of these tools are user-friendly, featuring simple interfaces\nthat allow users to input prompts and receive responses easily.\n3.1. Familiarity with AI Tools\nTo answer the first research question about the familiarity of novice programmers with\nAI tools during their programming education, the level of agreement of the students to the\nfollowing statement, “I feel familiar with AI tools usage”, was analyzed at the beginning\nand at the end of the course. The descriptive frequency and percentage of responses can\nbe found in Table 1. The analysis of survey responses indicated that, initially, only 28% of\nthe teams reported feeling familiar with the usage of AI tools. However, by the end of the\ncourse, this familiarity had increased to 100%.\nTable 1. Frequency and percent of responses to the statement, “I feel familiar with AI tools usage”, at\nthe beginning and at the end of the semester (n = 73).\nResponse\nPre-Semester\nPost-Semester\nFrequency\nPercent\nFrequency\nPercent\nstrongly disagree (1)\n33\n45.2\n0\n0\ndisagree (2)\n16\n21.9\n0\n0\nneutral (3)\n4\n5.5\n0\n0\nagree (4)\n13\n17.8\n39\n53.4\nstrongly agree (5)\n7\n9.6\n34\n46.6\nWilcoxon signed ranks test for two related samples was used to compare the responses\nat the beginning and at the end of the semester to the statement, “I feel familiar with AI\ntools usage”, measured on a Likert 1–5 scale. This test revealed significant differences\nbetween the"
    },
    {
      "rank": 9,
      "distance_l2": 1.4946107864379883,
      "source_id": "CopilotRobustness2023",
      "chunk_id": "CopilotRobustness2023_chunk_003",
      "text": "paraphrase for each original description in our dataset. Then,\nwe manually inspected the obtained paraphrases and classiﬁed\nthem as semantically equivalent or not. We obtained positive\nresults for both the approaches, with TP being the best\nperforming one with 77% of valid paraphrases.\nThen, to answer our main research question, we generated\ndifferent paraphrases for each original description.\narXiv:2302.00438v1  [cs.SE]  1 Feb 2023\nWe used the two previously described automated approaches,\ni.e., PEGASUS and TP, and we also manually generated\nparaphrases by distributing the original descriptions among four\nof the authors, each of which was in charge of paraphrasing a\nsubset of them.\nTherefore, for each original description, we obtained a set of\nsemantically equivalent paraphrased descriptions. We provided\nboth the original and the paraphrased descriptions as input to\nCopilot, asking it to generate the corresponding method body.\nWe analyze the percentage of cases in which the paraphrased\ndescriptions result in a different code prediction as compared\nto the original one, with a particular focus on the impact\non the prediction quality, e.g., cases in which the original\ndescription resulted in the recommendation of a method passing\nits associated test cases while switching to a paraphrased\ndescription made Copilot recommending a method failing its\nrelated tests.\nOur results show that paraphrasing a description results\nin a change in the code recommendation in ∼46% of cases.\nThe resulting changes also cause substantial variations in\nthe percentage of correct predictions. Such ﬁndings indicate\nthe central role played by the model’s input in the code\nrecommendation and the need for testing and improving the\nrobustness of DL-based code generators.\nData and code used in our study are publicly available [6].\nII. STUDY DESIGN\nThe goal of our study is to understand how robust is a state-\nof-the-art DL-based code completion approach (i.e., GitHub\nCopilot). We aim at answering the following research questions:\nRQ0: To what extent can automated paraphrasing\ntechniques be used to test the robustness of DL-based\ncode generators? Not always natural language processing\ntechniques can be used out of the box on software-related\ntext [35]. Therefore, with this preliminary RQ, we want\nto understand whether existing automated techniques for\ngenerating natural language paraphrases are suitable for SE\ntask at hand (i.e., paraphrasing a function description).\nRQ1: To what extent is the output of GitHub Copilot\ninﬂuenced by the code description provided as input by the\ndeveloper? This RQ aims at understanding whether Copilot,\nas a representative of DL-based code generators, is likely to\ngenerate different recommendations for different semantically\nequivalent natural language descriptions provided as input.\nIn the following we detail the context for our study (Sec-\ntion II-A) and how we collected (Section II-B) and analyzed\n(Section II-C) the data needed to answer our RQs.\nA. Context Selection\nThe context of our study is represented by 892 Java methods\ncollected through the following process. We selected all GitHub\nJava repositories having at least 300 commits, 50 contributors,\nand 25 stars. These ﬁlters"
    },
    {
      "rank": 10,
      "distance_l2": 1.5142649412155151,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_028",
      "text": "108352, 2024.\nDavid A Kolb. Experiential learning: Experience as the source of learning and development. FT press, 2014.\n21\nHao-Ping Lee, Advait Sarkar, Lev Tankelevitch, Ian Drosos, Sean Rintel, Richard Banks, and Nicholas Wilson.\nThe impact of generative ai on critical thinking: Self-reported reductions in cognitive effort and confidence\neffects from a survey of knowledge workers. In Proceedings of the 2025 CHI Conference on Human Factors\nin Computing Systems, pages 1–22, 2025.\nJack B Longwell, Ian Hirsch, Fernando Binder, Galileo Arturo Gonzalez Conchas, Daniel Mau, Raymond\nJang, Rahul G Krishnan, and Robert C Grant. Performance of large language models on medical oncology\nexamination questions. JAMA Network Open, 7(6):e2417641–e2417641, 2024.\nBrooke N Macnamara, Ibrahim Berber, M Cenk Çavuşoğlu, Elizabeth A Krupinski, Naren Nallapareddy,\nNoelle E Nelson, Philip J Smith, Amy L Wilson-Delfosse, and Soumya Ray. Does using artificial intelligence\nassistance accelerate skill decay and hinder skill development without performers’ awareness? Cognitive\nResearch: Principles and Implications, 9(1):46, 2024.\nNegar Maleki, Balaji Padmanabhan, and Kaushik Dutta. Ai hallucinations: a misnomer worth clarifying. In\n2024 IEEE conference on artificial intelligence (CAI), pages 133–138. IEEE, 2024.\nShakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of generative artificial\nintelligence. Science, 381(6654):187–192, 2023.\nNicholas Otis, Rowan Clarke, Solene Delecourt, David Holtz, and Rembrand Koning. The uneven impact of\ngenerative ai on entrepreneurial performance. 2024.\nZelin Pan, Zhendong Xie, Tingting Liu, and Tiansheng Xia. Exploring the key factors influencing college\nstudents’ willingness to use ai coding assistant tools: An expanded technology acceptance model. Systems,\n12(5):176, 2024.\nSida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer productivity:\nEvidence from github copilot. arXiv preprint arXiv:2302.06590, 2023.\nGustavo Pinto, Cleidson De Souza, Thayssa Rocha, Igor Steinmacher, Alberto Souza, and Edward Monteiro.\nDeveloper experiences with a contextualized ai coding assistant: Usability, expectations, and outcomes. In\nProceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for\nAI, pages 81–91, 2024.\nEric Poitras, Brent Crane, and Angela Siegel. Generative ai in introductory programming instruction:\nExamining the assistance dilemma with llm-based code generators. In Proceedings of the 2024 on ACM\nVirtual Global Computing Education Conference V. 1, pages 186–192, 2024.\nSiddhar"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2-trust-tightened-qrewrite"
}