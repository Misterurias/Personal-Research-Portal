{
  "timestamp": "2026-03-02T03:38:53+00:00",
  "query": "What impact do AI coding assistants have on code quality?",
  "retrieval_query": "Impact of AI coding assistants on code quality.",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding assistants on code quality.",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks. Here are my answers:\n\n1. What is the main goal of the research assistant in this study?\n\nThe main goal is to evaluate the code quality of AI-assisted code generation tools, specifically GitHub Copilot, Amazon CodeWhisperer, and ChatGPT.\n\nSource: [CodeQualityComparison2023_chunk_001 | CodeQualityComparison2023]\n\n2. How do developers use AI assistants like GitHub Copilot in their workflow?\n\nAI assistants can aid sensemaking tasks in code repositories, enabling developers to identify a shared responsibility between people and AI systems in mitigating the risks of generated outputs.\n\nSource: [EnterpriseImpact2024_chunk_003 | EnterpriseImpact2024]\n\n3. What are some subjective and objective factors that impact developer productivity?\n\nSubjective factors include code quality, technical debt, infrastructure tools and support, team communication, and organizational changes and processes. Objective factors include metrics such as lines of code over time, function points, software system complexity, and error rates.\n\nSource: [EnterpriseImpact2024_chunk_002 | EnterpriseImpact2024]",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.7027324438095093,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023",
      "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1–7.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1–24.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402–412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369–391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172–178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I 23. Springer, 171–190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1–47.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585–1608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot’s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62–71.\n[59] Ramaz"
    },
    {
      "rank": 2,
      "distance_l2": 0.7332595586776733,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 3,
      "distance_l2": 0.7352527379989624,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_001",
      "text": "Examining the Use and Impact of an AI Code Assistant on Developer\nProductivity and Experience in the Enterprise\nJUSTIN D. WEISZ, IBM Research, USA\nSHRADDHA KUMAR∗, Cisco Systems, Inc., India\nMICHAEL MULLER, IBM Research, USA\nKAREN-ELLEN BROWNE, IBM Software, Ireland\nARIELLE GOLDBERG, IBM Infrastructure, USA\nELLICE HEINTZE, IBM Software, Germany\nSHAGUN BAJPAI, IBM Software, India\nAI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and\ntesting code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within\nIBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers’ experiences\nwith WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their\nexpectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated\ncode. Our case study characterizes the impact of an LLM-powered assistant on developers’ perceptions of productivity and it shows\nthat although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.\nCCS Concepts: • Human-centered computing →Empirical studies in HCI; Field studies; • Software and its engineering →\nCollaboration in software development; Automatic programming.\nAdditional Key Words and Phrases: Generative AI, LLM, software engineering, productivity, code assistant\nACM Reference Format:\nJustin D. Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. 2025.\nExamining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA ’25), April 26-May 1, 2025, Yokohama, Japan. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3706599.3706670\n1\nIntroduction\nAI assistants powered by large language models (LLMs) are becoming increasingly prevalent in the workplace. A\nnumber of commercial and open-source coding assistants have been released for software engineers, developers, and\n∗Work conducted while an employee of IBM Software, Kochi, India.\nAuthors’ Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen,"
    },
    {
      "rank": 4,
      "distance_l2": 0.7362462878227234,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 5,
      "distance_l2": 0.7374346852302551,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_028",
      "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI ’22 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1–7. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot’s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE ’22), November 07, 2022. ACM, Singapore, Singapore, 62–71. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n’23), August 06, 2023. ACM, Long Beach, CA, USA, 5673–5684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21–29. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
    },
    {
      "rank": 6,
      "distance_l2": 0.7445272207260132,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_003",
      "text": " assistants can aid sensemaking tasks in code repositories.\n• We identify a shared responsibility between people and AI systems in mitigating the risks of generated outputs.\n2\nRelated Work\nWe outline three areas relevant to our study of AI code assistants: code-fluent LLMs and their incorporation into the\nsoftware engineering workflow; the multi-faceted nature of productivity in software engineering; and studies of AI\ncode assistants.\n2.1\nCode-fluent LLMs and software engineering assistants\nLarge language models that have been exposed to source code in their pre-training have demonstrated a high degree of\naptitude in performing a variety of tasks: converting natural language to code (e.g. [2, 16, 19, 56]), converting code\nto natural language documentation (e.g. [16, 31]) or explanations (e.g. [37]), and converting code to code, such as by\ntranslating it from one language to another (e.g. [2, 19, 46, 57]) or by creating unit tests (e.g. [48]). The introduction of\nthe Codex model [10] and its corresponding incorporation into software developers’ IDEs through GitHub Copilot6\ndemonstrated how code-fluent LLMs could revolutionize the software development workflow. New agentic design\npatterns are enabling coding assistants to perform even more complex tasks, such as converting issues into pull\nrequests [25] and new feature descriptions into specifications and implementation plans7.\n1In this paper, we use the term “developer” as a catch-all to cover individuals who perform code-related work, including software engineers, architects,\nand data scientists.\n2GitHub Copilot: https://github.com/features/copilot\n3Amazon Q Developer: https://aws.amazon.com/q/developer/\n4Gemini Code Assist: https://cloud.google.com/gemini/docs/codeassist/overview\n5watsonx Code Assistant: https://www.ibm.com/products/watsonx-code-assistant\n6GitHub Copilot: https://github.com/features/copilot\n7GitHub Copilot Workspace: https://githubnext.com/projects/copilot-workspace\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n3\n2.2\nSoftware engineering productivity\nProductivity in software engineering is a complex, multi-faceted construct [17, 47]. It is often assessed via objective\nmetrics of productivity that capture the ratio of output to effort [47] (e.g. lines of code over time [15], function points [30]),\nthe complexity of the software system [20, 33], or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}