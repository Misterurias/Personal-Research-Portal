{
  "timestamp": "2026-03-02T03:39:40+00:00",
  "query": "What quality metrics are used to evaluate AI-generated code?",
  "retrieval_query": "\"Evaluation metrics for AI-generated code\"",
  "query_rewritten": true,
  "rewritten_query": "\"Evaluation metrics for AI-generated code\"",
  "top_k": 6,
  "answer": "I'm only allowed to answer based on the provided evidence chunks, which are:\n\n* PerformanceAnalysis2026_chunk_005 | PerformanceAnalysis2026\n* PerformanceAnalysis2026_chunk_010 | PerformanceAnalysis2026\n* PerformanceAnalysis2026_chunk_010 | PerformanceAnalysis2026 (this one is repeated)\n\nBased on these evidence chunks, I can only provide answers that are relevant to the performance analysis of AI-generated code.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5656799077987671,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_015",
      "text": " has focused on evaluating the correctness and security \nof AI-generated code, performance regression has received less attention. However, intui­\ntively, AI-generated code models may not fully grasp the developer’s performance goals, \npotentially leading to code that prioritizes functionality over efficiency. On the other hand, \ntraining data for code generation models might not explicitly emphasize performance con­\nsiderations, impacting the models’ ability to generate efficient code. Given these potential \nshortcomings, it’s crucial to investigate the prevalence of performance regressions in AI-\ngenerated code. Understanding the scope of this issue will inform future research directions \nand development efforts for AI-assisted coding tools and code generation models.\nApproach  Our approach involves two strategies to evaluate the performance of AI-gener­\nated code. In particular, for static performance regression analysis, we employ industry-\nstandard tools, i.e., Spotbugs and PMD, to scan the generated Java code in the AixBench \ndataset. These tools are equipped with pre-defined rules that can detect performance regres­\nsions within the code. The detailed configuration of these rules is available in the replication \npackage we provided. For Python code in the HumanEval, MBPP, and EvalPerf datasets, \nwe use Qodana, a cloud-based static analysis platform, to identify potential performance \nregressions specific to Python code. To facilitate efficient analysis, we create new proj­\nects and establish a dedicated scan workflow within Qodana Cloud. This workflow enables \nQodana to automatically identify and highlight potential performance-related code issues \nwithin the generated Python code.\nFor dynamic performance regression analysis, we compare the generated code with \ncanonical solutions from the HumanEval and MBPP datasets. For the EvalPerf dataset, \nno explicit canonical solution is provided. Instead, each problem in EvalPerf is associated \nwith multiple reference implementations, each annotated with an efficiency score. Since a \n1 3\n   62 \n \nPage 12 of 52\nEmpirical Software Engineering           (2026) 31:62 \nhigher efficiency score indicates better performance (Liu et al. 2024a), we select the refer­\nence implementation with the highest efficiency score as the canonical solution for that \nproblem. This enables us to conduct performance regression analysis in a manner consistent \nwith the HumanEval and MBPP datasets. Using the dynamic performance regression detec­\ntion modules, we conduct dynamic performance regression analysis on the generated and \ncanonical code sets of these three datasets. The Python scripts generated for the HumanEval \nand MBPP datasets are typically short and have brief single-run execution times. To gener­\nate more robust performance data, we adopt a technique called repetitive iteration mea­\nsurement (Laaber and Leitner 2018; Ding et al. 2020; Jangali et al. 2023). This technique \nextends the runtime of the generated code by increasing the number of iterations within \nthe test cases, allowing profiling tools to capture more comprehensive performance data. \nWe achieve this extension by adding a for loop at the beginning of the test cases. This loop \ncauses the existing test cases to be executed repeatedly. Figure 5 illustrates this modifica­\ntion for the script shown in Fig. 4. Once the iterations have been increased, we encapsulate \neach"
    },
    {
      "rank": 2,
      "distance_l2": 0.5929257869720459,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_004",
      "text": " al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming languages used [21]. Nguyen and colleagues put forward a new set of metrics for \nmeasuring AI-generated code and reported that, although AI-powered software is very good at producing \nsyntactically correct code, it often strays from the best design patterns [22]. \nAmong the critical issues raised security implications have been the foremost. Static analysis studies \nshowed that AI systems might learn from their training data and thus recreate the same weaknesses as in \nthe case of existing vulnerabilities [23]. Pearce and co-workers pointed out the presence of a vulnerable \npattern that was quite common in GitHub Copilot's code which included SQL injection, cross-site \nscripting, and use of insecure cryptographic methods [24]. \nNevertheless, there are still some limitations that are inherent to such a growing research area. The \nmajority of research works have been limited to the evaluation of just a few tools and or small domains \nmaking the possibility of drawing general conclusions quite difficult. Only a couple of research works \nhave dealt with long-term effects of refactoring, skilled programmers, and maintenance costs. Moreover, \nthe interaction between the experience level of the developer and the effectiveness of the AI tool is still to \nbe fully explored [25]. Through a thorough and multi-language study involving developers of different \nexperience levels working on varied software engineering tasks, we fill these gaps. \n \n3. Methodology \n3.1 Research Design \nThis research utilized a between-subjects experimental design to assess the influences of AI-assisted code \ngeneration tools on software development outcomes. The independent variable was the presence of AI-\nassisted coding tools (experimental group vs. control group), while the dependent variables were the \nquality metrics, security vulnerability counts, and productivity measures of the coded produced. The \nexperiment was set up in such a way that it reduced the possibility of confounding variables while keeping \nthe ecological validity through the use of realistic programming tasks that are typical of professional \nsoftware development scenarios. Figure 1 shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2"
    },
    {
      "rank": 3,
      "distance_l2": 0.598656952381134,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_028",
      "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI ’22 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1–7. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot’s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE ’22), November 07, 2022. ACM, Singapore, Singapore, 62–71. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n’23), August 06, 2023. ACM, Long Beach, CA, USA, 5673–5684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21–29. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
    },
    {
      "rank": 4,
      "distance_l2": 0.6197192668914795,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_003",
      "text": "olve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious programming languages and projects. Finally, the research gives the software organizations that \nwant to use AI tools good insights and practices for risk reduction. Thus, the implications of our results \nare very important for the education of software engineers, the industry's practices and the direction of \nfuture research in the area of AI and software development. \n \n2. Literature Review \nThe areas where artificial intelligence and software engineering meet have become the center of a huge \nnumber of research studies, with code generation and program synthesis being the two main areas. The \nfirst automated code generation attempts relied on template-based methods and rule-based systems \nproducing the so-called boilerplate codes from high-level specifications [10]. The deep learning era totally \nchanged the picture, with the application of sequence-to-sequence models and recurrent neural networks \nto the code synthesis tasks [11]. The introduction of the transformer architectures was the turning point, \nwith models such as CodeBERT and GraphCodeBERT being able to perform on a par with the best \nmethods in the problem categories of code understanding and generation [12, 13]. \nRecently, the development of large language models has further changed the scenery in code generation. \nFor instance, GPT-3 showed outstanding learning ability through a few examples for programming tasks \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n3\n \n[14], while Codex, the engine behind GitHub Copilot, scored highly in competitive programming \nproblems [15]. The following studies have looked into the models' capabilities in various programming \nlanguages and difficult algorithms [16, 17]. \nThe findings of empirical studies of AI-assisted coding tools presented a somewhat mixed picture. Ziegler \net al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming"
    },
    {
      "rank": 5,
      "distance_l2": 0.6295480132102966,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_010",
      "text": " new solutions against a set of reference solutions with varying efficiency \nlevels using a cluster-based scoring mechanism, allowing for finer-grained and stable evalu­\nation of code performance across different tasks and platforms. This makes EvalPerf par­\nticularly suitable for benchmarking AI-generated code in terms of execution efficiency. By \nincluding EvalPerf alongside general-purpose benchmarks, we ensure that our study covers \nboth functional correctness and performance-critical aspects of AI-generated code.\n3.2  Experimental Setup\nIn this subsection, we describe the process of collecting the generated code and performance \ndata for each question in our study datasets. Figure 3 illustrates the overall process of our \napproach. We follow four steps to collect the needed data. In the first step, we prepare \nprompts by parsing the four datasets, i.e., HumanEval, AixBench, MBPP, and Evalperf. In \nthe second step, for each prepared prompt, we feed the prepared prompt to GitHub Copilot, \nCopilot Chat, CodeLlama, and DeepSeek-Coder to generate code. In the third step, we filter \nthe generated code using test cases. Finally, we analyze the performance regressions of the \ngenerated code.\nFig. 3  An overview of our approach to collecting data\n \nDataset\nLanguage\n#Instances\nYear\nReference\nHumanEval\nPython\n164\n2021\nChen et al. (2021)\nAixBench\nJava\n187\n2022\nHao et al. (2022)\nMBPP\nPython\n974\n2021\nAustin et al. (2021)\nEvalPerf\nPython\n118\n2024\nLiu et al. (2024a)\nTable 1  Overview of datasets \nused in our study\n \n1 3\n   62 \n \nPage 8 of 52\nEmpirical Software Engineering           (2026) 31:62 \nStep 1: Preparing prompt. The data from HumanEval, MBPP, Evalperf, and Aixbench \ndatasets are stored in JSON files. We first parse these files to extract relevant code infor­\nmation. New files are created in either .py or .java format for each code snippet requir­\ning completion. As an example in Fig. 4, the 4.py file is extracted from the corresponding \nHumanEval JSON file. The extracted function name, parameters, and comments are used as \nthe prompt for Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder.\nStep 2: Generating code. In this step, we use code generation models to generate code \nfor each prompt of each question. We designed specific parameter configurations for dif­\nferent code generation models and datasets to ensure stability and efficiency during the \ngeneration process.\n\t–\nGitHub Copilot: We leverage Copilot within the VSCode (Microsoft 2025) environ­\nment to generate code for each prompt. For each newly created file, we open the com­\nmand panel using Ctrl+Shift+P and execute GitHub Copilot: Open Completions Panel \nto activate the suggestions panel. Copilot provides 1-10 code completion suggestions. \nWe consistently accept the first suggestion (see Fig. 1) for consistency. GitHub Copi­\nlot was used with its"
    },
    {
      "rank": 6,
      "distance_l2": 0.6368709802627563,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_005",
      "text": " \neffectiveness of prompt engineering is highly model- and dataset-dependent. This high­\nlights the importance of aligning prompt design with concrete performance optimization \nobjectives and provides actionable insights for future work on efficiency-aware code \ngeneration with LLMs.\nThis work systematically extends our previous work Li et al. (2024b). First, we add more \nAI-powered coding assistants, i.e., Copilot Chat, CodeLlama, and DeepSeek-Coder, to more \ncomprehensively evaluate the performance of AI-generated code and investigate the perfor­\nmance discrepancy among different models in code generation tasks. Second, we extend our \nevaluation with a newly introduced performance-oriented dataset, EvalPerf, which provides \na richer set of benchmarks for assessing performance regressions in AI-generated code. \nThird, we investigate prompt engineering strategies, including few-shot and CoT prompt­\ning, to study their impact on performance regressions across different models and datasets. \nIn addition, we incorporated detailed profiling analyses and manual inspections of static \nanalysis results to uncover dominant sources of performance regressions, and included sta­\ntistical testing to ensure the robustness of observed effects. Finally, we provide a discussion \nof model-specific performance differences, architectural implications, and prompt sensi­\ntivity, offering insights into how design choices, model characteristics, and prompt strate­\ngies jointly shape the performance of AI-generated code, providing actionable guidance for \nfuture research on performance-aware AI-assisted code generation.\nThe rest of this paper is organized as follows. Section 2 introduces how to generate code \nusing GitHub Copilot, Copilot Chat, CodeLlama, and DeepSeek-Coder and a motivation \nexample. Section 3 presents our case study setup. Section 4 presents detailed results of \nour research questions and findings. Section 5 discusses the implications of our findings. \nSection 6 discusses the threats to the validity of our study. Section 7 presents related prior \nstudies. Section 8 summarizes the conclusion of our study.\n2  Background and Motivating Example\nIn this section, we first introduce four AI-powered code generation models, GitHub Copilot, \nCopilot Chat, CodeLlama, and DeepSeek-Coder. We then present a motivating example of \nperformance challenges in AI-generated code.\n2.1  AI-Powered Code Generation Models\nGitHub Copilot is an AI-assisted programming tool that enhances developer productivity \nby providing code generation services. GitHub Copilot empowers programmers by offer­\ning various forms of code completion. This functionality can be particularly beneficial in \nscenarios where developers have a clear understanding of the desired outcome but require \nassistance in translating that concept into functional code. Copilot offers two primary meth­\nods for code completion:\n\t\n●\nDevelopers can select a specific section of code and request Copilot to automatically \ncomplete it. This functionality leverages the surrounding code context to generate rel­\nevant suggestions.\n1 3\n   62 \n \nPage 4 of 52\nEmpirical Software Engineering           (2026) 31:62 \n\t\n●\nDevelopers can use natural language comments to describe their desired functionality. \nCopilot then analyzes these comments and suggests code that aligns with the described \nrequirements.\nGitHub Copilot Chat builds upon GitHub Copilot and represents a more advanced evolu­\ntion of AI-powered coding"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}