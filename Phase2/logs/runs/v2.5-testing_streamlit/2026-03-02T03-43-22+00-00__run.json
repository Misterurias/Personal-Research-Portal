{
  "timestamp": "2026-03-02T03:43:22+00:00",
  "query": "How do AI coding tools affect junior versus senior developers?",
  "retrieval_query": "Impact of AI coding tools on developer productivity and collaboration between junior and senior programmers.",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding tools on developer productivity and collaboration between junior and senior programmers.",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks. Here are my answers:\n\nPlease provide the specific question or topic you would like me to address, and I will respond based solely on the provided evidence chunks.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6132874488830566,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_005",
      "text": " underlying mechanism suggests that \nthe observed increase in project-level code contributions is accompanied by a significant increase in both \nindividual code contributions and developer participation. At the same time, the increase in coordination \ntime is driven by a higher volume of discussions surrounding code contributions, a broader set of developers \nparticipating in these discussions, and greater discussion intensity per developer. Importantly, the combined \neffect of these two competing forces still yields an overall positive effect on the project-level productivity, \nmeasured by the total code contributions with timely integration into the codebase. \nFurthermore, we find that compared to core developers, AI pair programmers lead to a smaller \nincrease in project-level code contributions made by peripheral developers; following the adoption of AI \npair programmers, there is also a larger increase in coordination time for integrating code contributed by \nperipheral developers. These results are consistent with our hypothesis that due to the different levels of \nproject familiarity held by core versus peripheral developers and the limitations of generative AI tools, \nperipheral developers may realize less productivity gain from AI pair programmers than core developers. \nOur study provides several contributions to the literature. First, it contributes to the literature on \ngenerative AI in software development (Imai 2022, Barke et al. 2023, Peng et al. 2023, Cui et al. 2024). \nWhile prior research has shown that generative AI improves individual developer productivity (e.g., Peng \net al. 2023; Cui et al. 2024), less is known about its impact on voluntary participation in collaborative \n 5 \nsoftware development. Existing studies suggest that generative AI might reduce voluntary participation in \nQ&A communities by substituting for information exchange (Xu et al. 2023, Burtch et al. 2024). However, \nOSS communities are fundamentally different in that they involve not only information sharing but also \ncomplex problem solving and team collaboration. To our knowledge, we are among the first to show \ngenerative AI encourages more participation in OSS development, including both coding and non-coding \nparticipation (i.e., code discussions). \nSecond, our study contributes to the literature on generative AI in team-based collaboration (Li et \nal. 2024, Dell'Acqua et al. 2025). While prior work has examined the impact of generative AI within \ntraditional teams characterized by fixed size and formal coordination processes for performing a common \ntask, we extend this work to open collaborative environments, where team composition is fluid, \nparticipation is voluntary, and individuals perform distinct tasks that must be integrated. Our study is among \nthe first to uncover some unexpected impacts of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023"
    },
    {
      "rank": 2,
      "distance_l2": 0.6137455701828003,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
      "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
    },
    {
      "rank": 3,
      "distance_l2": 0.6233384013175964,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_004",
      "text": ", because AI pair programmers not only boost individual code contributions but also encourage \ngreater developer participation. However, because AI pair programmers also encourage more participation \nin discussions on OSS projects, they could lead to longer coordination time for code integration. Moreover, \nwe argue that because of peripheral developers’ lower project familiarity and the limited ability of AI pair \nprogrammers to learn the project’s full context, the increase in project-level code contributions from \nperipheral developers may be smaller than that from core developers. At the same time, code contributed \nby peripheral developers may require more coordination time to integrate than code from core developers. \nOverall, the productivity gains from AI pair programmers may be smaller for peripheral developers than \nfor core developers. \nTo empirically test these hypotheses, we examine how the AI pair programmer, GitHub Copilot, \ninfluences project-level code contributions and coordination time of OSS projects on GitHub, as well as \nhow its effect varies among core versus peripheral developers. GitHub is one of the largest code-hosting \nrepositories based on the Git version control system (Dabbish et al. 2012). In this setting, a repository is a \nfundamental unit that typically contains the source code and resource files for a software project, along with \ninformation related to the project’s evolution history, high-level features, and developer details (Zhang et \nal. 2017). Such repositories are often used to investigate collaborative development practices (Dabbish et \nal. 2012). Our unit of analysis is at the repository-month-level, with the sample period from January 2021 \nto December 2022. To examine our research questions, we use a combination of publicly available data on \nGitHub repositories and proprietary data on Copilot use provided by GitHub organization. Our treatment \n 4 \ngroup consists of repositories where Copilot was both supported by local coding environments and used by \ndevelopers to code. Thus, the post-treatment period includes the months during which Copilot was \nsupported and used in a focal repository, and the pre-treatment period includes all other months. The control \ngroup includes repositories where Copilot was not used throughout the sample period. We estimate our \nmodel using the Generalized Synthetic Control Method (GSCM) and validate the results through alternative \nmatching techniques and a comprehensive set of robustness checks. \nOur empirical results show that the adoption of GitHub Copilot is associated with a 5.9% increase \nin the number of project-level code contributions but also an 8% increase in coordination time for code \nintegration. These findings indicate a tradeoff between contribution gains and coordination time in the OSS \ndevelopment following the adoption of Copilot. Further analysis of the underlying mechanism suggests that \nthe observed increase in project-level code contributions is accompanied by a significant increase in both \nindividual code contributions and developer participation. At the same time, the increase in coordination \ntime is driven by a higher volume of discussions surrounding code contributions, a broader set of developers \nparticipating in these discussions, and greater discussion intensity per developer. Importantly, the combined \neffect of these two competing forces still yields an overall positive effect on the project-level productivity, \nmeasured by the total code contributions with timely integration into the codebase. \nFurthermore, we find that compared to core developers, AI pair programmers lead to a smaller \nincrease in project-level code contributions made by peripheral developers; following the adoption of AI \npair programmers"
    },
    {
      "rank": 4,
      "distance_l2": 0.6328662037849426,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_006",
      "text": " of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023, Cui et al. 2024). Our results draw a sharp contrast with \nthese findings, as we demonstrate that peripheral developers obtain less productivity gain from AI pair \nprogrammers than core developers in OSS settings, potentially because the former do not possess important \nand necessary contextual knowledge about an OSS project to effectively use the AI tools. \n2. Literature Review  \n2.1 Generative AI in Software Development \nA growing body of literature has started to examine the impact of generative AI on software development. \nMost studies have focused on the implications of generative AI for individual productivity on specific tasks. \n 6 \nFor example, Imai (2022) finds that GitHub Copilot produces more lines of code than a human pair \nprogrammer when completing a task in Python. Peng et al. (2023) find that GitHub Copilot enables \nindividual developers to implement an HTTP server 55.8% faster than those not using the tool. Hoffmann \net al. (2024) show that GitHub Copilot causes individual developers to shift focus towards coding tasks and \naway from project management.  \nDespite these insights, research on how generative AI influences project-level outcomes for \ncomplex tasks involving multiple developers remains limited. Yeverechyahu et al. (2024) investigate the \ninnovation capabilities of generative AI, particularly its role in extrapolative versus interpolative thinking, \nand compare its influence on innovation in Python versus R. Different from the existing literature and built \nupon the OSS literature, our hypotheses are motivated by the unique characteristics of OSS, namely, the \nsoftware development process in an open and collaborative environment. Because participation is often \nvoluntary and coordination does not follow formal centralized processes in this environment, it remains \nunclear how generative AI influences open participation in both coding and non-coding activities, as well \nas team coordination, all of which could have important implications for project-level software \ndevelopment productivity. \nIn addition, existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al."
    },
    {
      "rank": 5,
      "distance_l2": 0.6660960912704468,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_034",
      "text": "ver effects enabled by AI pair programmers. \n 36 \nSecond, our work represents the first step in examining how generative AI affects core and \nperipheral developers differently. We acknowledge that beyond different levels of project familiarity, \ndevelopers may differ in motivation, ability, and preference for the type of work they undertake. Due to the \nobservational nature of our data, we are not able to measure these metrics. Future work could explore these \nnuanced behavioral dynamics, potentially through surveys, interviews, or mixed-method approaches.  \nThird, although our study emphasizes the quantitative tradeoff between project-level code \ncontributions and coordination time, it does not capture the subjective aspects of working with AI tools. \nFuture research could examine how developers perceive and adapt to AI pair programmers, including their \nsatisfaction, trust, and perceived control over their contributions. \nFinally, the potential risks associated with AI pair programmers warrant further investigation. One \nimportant concern is the possibility of skill erosion. Developers may become overly reliant on AI-generated \nsuggestions, potentially diminishing their coding proficiency over time. In addition, AI systems trained on \nlarge-scale public repositories may perpetuate existing coding biases, potentially reinforcing suboptimal \npractices. Furthermore, AI-generated code may inadvertently introduce security vulnerabilities, especially \nif it lacks the contextual awareness required for sensitive or security-critical components. Future studies \nshould propose frameworks to ensure safe and ethical AI usage in software development. \nReferences \nAbadie A, Diamond A, Hainmueller J (2010) Synthetic control methods for comparative case studies: \nEstimating the effect of California’s tobacco control program. Journal of the American statistical \nAssociation 105(490):493-505. \nAlMarzouq M, AlZaidan A, AlDallal J (2020) Mining GitHub for research and education: challenges and \nopportunities. International Journal of Web Information Systems 16(4):451-473. \nAncona DG, Caldwell DF (1992) Demography and design: Predictors of new product team performance. \nOrganization science 3(3):321-341. \nAtkinson JW (1957) Motivational determinants of risk-taking behavior. Psychological review 64(6p1):359. \nBai J (2009) Panel data models with interactive fixed effects. Econometrica 77(4):1229-1279. \nBakos JY, Brynjolfsson E (1993) Information technology, incentives, and the optimal number of suppliers. \nJournal of Management Information Systems 10(2):37-53. \nBarke S, James MB, Polikarpova N (2023) Grounded copilot: How programmers interact with code-generating \nmodels. Proceedings of the ACM on Programming Languages 7(OOPSLA1):85-111. \nBrynjolfsson E, Li D, Raymond L (2025) Generative AI at work. The Quarterly Journal of Economics:qjae044. \nBurtch G, Lee D, Chen Z (2024) The consequences of generative AI for online knowledge communities. \nScientific Reports 14(1):10413. \n 37 \nCataldo M, Wagstrom PA, Herbsleb JD, Carley KM (2006) Identification of coordination requirements: \nImplications for the design of collaboration and awareness"
    },
    {
      "rank": 6,
      "distance_l2": 0.672929048538208,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}