{
  "timestamp": "2026-03-02T03:40:22+00:00",
  "query": "Is AI-assisted code more error-prone than human-written code?",
  "retrieval_query": "Here is a rewritten search query for retrieval:\n\n\"error rate comparison between AI-generated code and human-written code\"",
  "query_rewritten": true,
  "rewritten_query": "Here is a rewritten search query for retrieval:\n\n\"error rate comparison between AI-generated code and human-written code\"",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.7829129695892334,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_008",
      "text": " record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants’ current version of the code and can produce the full, correct code\nfor both tasks directly when prompted.\n4.2\nEvaluation Design\nBased on a previous meta-analysis of evaluations in computer science education [Cheng et al., 2022], we\nidentify four types of questions used to assess the mastery of coding skills. Returning to our initial motivation\nof developing and retaining the skills required for supervising automation, proficiency in some of these areas\nmay be more important than others for the oversight of AI-generated code. The four types of questions we\nconsider are the following.\n• Debugging The ability to identify and diagnose errors in code. This skill is crucial for detecting when\nAI-generated code is incorrect and understanding why it fails.\n• Code Reading The ability to read and comprehend what code does. This skill enables humans to\nunderstand and verify AI-written code before deployment.\n• Code Writing The ability to write or pick the right way to write code. Low-level code writing, like\nremembering the syntax of functions, will be less important with further integration of AI coding tools\nthan high-level system design.\n• Conceptual The ability to understand the core principles behind tools and libraries. Conceptual\nunderstanding is critical to assess whether AI-generated code uses appropriate design patterns that\nadheres to how the library should be used.\nThe two tasks in our study cover 7 core concepts from the Trio library. We designed a quiz with debugging,\ncode reading, and conceptual questions that cover these 7 concepts. We exclude code writing questions to\nreduce the impact of syntax errors in our evaluation; these errors can be easily corrected with an AI query or\nweb search. We tested 5 versions (Table 2) of the quiz in user testing and preliminary studies based on item\nresponse theory. For example, we ensure that all questions are sufficiently correlated with the overall quiz\nscore, that each question has an appropriate average score, and that the questions are split up such that there\nis no local item dependence between questions (i.e., participants could not infer the answers to a question by\nlooking at other questions). The final evaluation we used contained 14 questions for a total of 27 points. We\nsubmitted the grading rubric for the quiz in our study pre-registration before running the experiment.\n4.3\nStudy Design\nWe use a between-subjects randomized experiment to test for the effects of using AI in the coding skill\nformation process. Each participant first completed a warm-up coding task on a coding platform, where they\nneeded to add a border around a list of strings. This Python coding question takes an average of 4 minutes to\ncomplete among users of this coding platform. There are no asynchronous concepts in this coding question.\n6\nFigure 4: Overview of learning task and"
    },
    {
      "rank": 2,
      "distance_l2": 0.7923619747161865,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_060",
      "text": " pages 925–930. Association for Computational \nLinguistics\nHe P, Wang S, Chen T-H (2025) Codepromptzip: Code-specific prompt compression for retrieval-augmented \ngeneration in coding tasks with lms. arXiv:2502.14925\nHou W, Ji Z (2024) A systematic evaluation of large language models for generating programming code. \narXiv:2403.00894\nHuang D, Zeng G, Dai J, Luo M, Weng H, Qing Y, Cui H, Guo Z, Zhang JM (2024a) Effi-code: Unleashing \ncode efficiency in language models. CoRR, abs/2410.10209\nHuang D, Qing Y, Shang W, Cui H (2024) Zhang J (2024b) Effibench: Benchmarking the efficiency of auto­\nmatically generated code. In: Globersons A, Mackey L, Belgrave D, Fan A, Paquet U, Tomczak JM, \nZhang C (eds) Advances in Neural Information Processing Systems 38: Annual Conference on Neural \nInformation Processing Systems 2024, NeurIPS 2024. BC, Canada, December, Vancouver, pp 10–15\nHuang D, Dai J, Weng H, Wu P, Qing Y, Cui H, Guo Z (2024) Zhang J (2024c) Effilearner: Enhancing \nefficiency of generated code via self-optimization. In: Globersons A, Mackey L, Belgrave D, Fan A, \nPaquet U, Tomczak JM, Zhang C (eds) Advances in Neural Information Processing Systems 38: Annual \nConference on Neural Information Processing Systems 2024, NeurIPS 2024. BC, Canada, December, \nVancouver, pp 10–15\nHuang Y, Li Y, Wu W, Zhang J, Lyu MR (2023) Do not give away my secrets: Uncovering the privacy issue \nof neural code completion tools. CoRR, abs/2309.07639\n1 3\n   62 \n \nPage 48 of 52\nEmpirical Software Engineering           (2026) 31:62 \nIslam MA, Jonnala DV, Rekhi R, Pokharel P, Cilamkoti S, Imran A, Kosar T, Turkkan BO (2025) Evaluating \nthe energy-efficiency of the code generated by llms. CoRR, abs/2505.20324\nIyer S, Konstas I, Cheung A, Zettlemoyer L (2018) Mapping language to code in programmatic context. In: \nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, \nBelgium, October 31 - November 4, 2018, pages 1643–1652. Association for Computational Linguistics\nJangali M, Tang Y, Alexandersson N, Leitner P, Yang J, Shang W (2023) Automated generation and evalua­\ntion of JMH microbenchmark suites from unit tests. IEEE Trans Software Eng 49(4):1704–1725\nJetBrains s.r.o"
    },
    {
      "rank": 3,
      "distance_l2": 0.8265703320503235,
      "source_id": "AIReview2025",
      "chunk_id": "AIReview2025_chunk_001",
      "text": "236 \nA Review of Research on AI-Assisted Code Generation and AI-\nDriven Code Review \nYuzhi Wang \nBeijing University of Technology, Beijing, China \nshenrenaisite@yeah.net \nAbstract. With the significant breakthroughs of deep learning technologies such as large language \nmodels (LLMs) in the field of code analysis, AI has evolved from an auxiliary tool to a key technology \nthat deeply participates in code optimization and resolving performance issues. As modern software \nsystem architectures become increasingly complex, the requirements for their performance have \nalso become more stringent. During the coding stage, developers find it difficult to effectively identify \nand resolve potential performance issues using traditional methods. This review focuses on the \napplication of artificial intelligence in two key areas: AI-assisted intelligent code generation and AI-\npovered code review. The review systematically analyzed the application of LLMs in software \ndevelopment, revealing a situation where efficiency gains coexist with quality challenges. In terms \nof code generation, models such as Code Llama and Copilot have significantly accelerated the \ndevelopment process. In the field of code review, AI can effectively handle code standards and low-\nseverity defects. However, in the future, this field still needs to address the issues of the reliability \nand security of the code generated by LLMs, as well as the insufficient explainability of the results of \nautomated performance analysis. The future research focus in this field lies in addressing issues \nsuch as the lack of interpretability and insufficient domain knowledge of LLMs. It is necessary to \nprioritize enhancing the reliability of AI recommendations and promoting the transformation of AI \nfrom an auxiliary tool to an intelligent Agent with self-repair capabilities, in order to achieve a truly \nefficient and secure human-machine collaboration paradigm. This article systematically reviews the \nrelevant progress, aiming to promote the transformation of software engineering from an artificial-\ndriven model to an AI-enhanced automated paradigm. It provides theoretical references for ensuring \nthe quality of backend code, improving product delivery speed, and enhancing system reliability. \nKeywords: AI; LLM; Code Generation; Code Review. \n1. Introduction \nRecently, the growing complexity of modern software applications has driven an increased \nemphasis on high-quality, maintainable source code, thereby heightening the difficulty for developers \nto write efficient and error-free code [1-3]. Therefore, there is an urgent need for a smarter approach \nto empover developers. The emergence of Artificial Intelligence has fundamentally transformed \nconventional approaches to code optimization and refactoring by introducing a new dimension of \nautomation [2]. A striking example is LLMs, which have exhibited remarkable potential in \nprogramming tasks [4], especially in code review [5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of"
    },
    {
      "rank": 4,
      "distance_l2": 0.8286622762680054,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 5,
      "distance_l2": 0.8372048139572144,
      "source_id": "CopilotQuality2022",
      "chunk_id": "CopilotQuality2022_chunk_022",
      "text": " and\nlanguage comprehension [3]. For better and more insightful results,\nthe number of problems can be increased, and the comprehension\nof the problems could be broader. For instance, in the experimen-\ntal setup proposed by Xu et al. [15] for their code generation and\nretrieval tool, the scope of the problems consists of basic Python,\nfile, OS, web scraping, web server & client, data analysis & ML, and\ndata visualization. Such topics could be included in our dataset to\nboth broaden the comprehension and increase the number of our\nproblems. We consider this task as future work for our study.\n6\nRELATED WORK\nIn the last few years, code generation has attracted attention from\nresearchers such as [5, 8, 12, 16]. In this study, we focus on GitHub\nCopilot which seems to be the first well-established instance of an\nAI pair-programmer.\nThe underlying model of GitHub Copilot, Codex, is externally\ndeveloped by OpenAI and employed by GitHub. Some of the ear-\nlier versions of the current Codex model used by GitHub Copilot\nwere evaluated by Chen et al. [3]. The Codex model relies on GPT\nmodels that OpenAI previously developed for natural language\ngeneration. The public code available on GitHub was used here\nwhile fine-tuning the model to implement the code recognition and\ngeneration capabilities. Furthermore, the model can recognize some\nother elements such as function signatures, code comments, etc.\nThe model can use such elements as inputs and generate related\noutputs. They found that a success rate of 70.2% could be reached\nin terms of code correctness, by generating 100 solutions for each\nproblem and choosing the most successful one among them. The\nsuccess rate was found to be only 28.8% for the case with one solu-\ntion per problem, which is consistent with our results. In this study,\nwe provided a detailed version of this assessment. We evaluated\nthe time and space complexities for generated solutions. Moreover,\nin order to extend the coverage of our study, we adjusted the Hu-\nmanEval dataset by changing meaningful function names with the\ndummy name \"foo\" and regenerated the solutions.\nThere are also experiment-based studies similar to ours, con-\nducted to evaluate GitHub Copilot. However, since GitHub Copilot\nis a considerably new tool, there are not many studies directly\nrelated to it. We list the available studies in the following.\nOne such study is conducted by Sobania et al. [11] in which the\ncode correctness of GitHub Copilot is evaluated, and the tool is\ncontrasted to the automatic program generators having the Genetic\nProgramming (GP) architecture. They found that there is not a\nsignificant difference between the two approaches on the bench-\nmark problems; however, the program synthesis approaches are not\nsufficient in supporting programmers compared to GitHub Copilot.\n70\nPROMISE ’22, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nAn evaluation of GitHub Copilot in terms of the security of the\ngenerated programs was implemented by Pearce et al. [10]. They\nevaluated the vulnerabilities in the generated code by Copilot. It\nwas determined that 40% of generated programs were vulnerable.\nAnother study discusses the"
    },
    {
      "rank": 6,
      "distance_l2": 0.8593418598175049,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_004",
      "text": " explicit reasoning does not directly translate into \nefficiency gains.\nTo facilitate research reproducibility, we make the original datasets, scripts, profiling \nresults, and analysis rules available in our replication package1. Our contributions are sum­\nmarized below.\n\t–\nPerformance assessment of AI-generated code: We systematically evaluated the \nperformance regressions between code generated by GitHub Copilot, Copilot Chat, \nCodeLlama, and DeepSeek-Coder and canonical solutions. Using static analysis tools \nand dynamic profiling, we analyzed the performance regressions of these code genera­\ntion models across different tasks and datasets including HumanEval, MBPP, AixBench, \nand performance-oriented benchmark EvalPerf. The study revealed that performance \nregression is a common phenomenon in AI-generated code. Compared to human-writ­\nten code, the generated code exhibits significant inefficiencies in terms of execution \nefficiency and resource utilization with the proportion of performance regressions being \neven more pronounced on the performance-oriented benchmark EvalPerf.\n\t–\nPerformance regression root causes of AI-generated code: We qualitatively analyzed \nthe root causes of performance regression in the generated code. Four major catego­\nries of inefficiencies were identified: inefficient function calls, inefficient loops, inef­\nficient algorithms, and suboptimal use of language features. These were further refined \ninto eleven specific subcategories. We conducted this root cause analysis for all models \nacross all datasets, highlighting how different models exhibit distinct patterns of perfor­\nmance regressions depending on the dataset. These findings offer valuable insights into \nthe potential shortcomings of current AI-driven coding assistants and establish a solid \nfoundation for devising strategies to optimize the performance of AI-generated code.\n\t–\nPrompt engineering for performance optimization: We explored prompt engineering \nincluding few-shot and CoT prompting as a technique for optimizing the performance \nof AI-generated code and proposed a few-shot prompt engineering optimization strat­\negy. By embedding specific root causes from the identified subcategories into the few-\nshot prompt design, we enhanced the performance of the generated code in most cases, \nalthough the improvement was limited or inconsistent for certain models and datasets. \nIn contrast, CoT prompting, while reasoning-oriented, does not consistently improve \nperformance and can sometimes degrade performance. These findings suggest that the \n1 https:​//gith​ub.​com/he​bena/Perfor​mance​-An​al​ys​is-of-AI-​Generated-Code\n1 3\nPage 3 of 52 \n   62 \nEmpirical Software Engineering           (2026) 31:62 \neffectiveness of prompt engineering is highly model- and dataset-dependent. This high­\nlights the importance of aligning prompt design with concrete performance optimization \nobjectives and provides actionable insights for future work on efficiency-aware code \ngeneration with LLMs.\nThis work systematically extends our previous work Li et al. (2024b). First, we add more \nAI-powered coding assistants, i.e., Copilot Chat, CodeLlama, and DeepSeek-Coder, to more \ncomprehensively evaluate the performance of AI-generated code and investigate the perfor­\nmance discrepancy among different models in code generation tasks. Second, we extend our \nevaluation with a newly introduced performance-oriented dataset, EvalPerf, which provides \na"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}