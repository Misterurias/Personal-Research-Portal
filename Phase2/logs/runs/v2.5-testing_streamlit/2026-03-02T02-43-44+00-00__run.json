{
  "timestamp": "2026-03-02T02:43:44+00:00",
  "query": "What impact do AI coding assistants have on code quality?",
  "retrieval_query": "Impact of AI coding assistants on code quality",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding assistants on code quality",
  "top_k": 6,
  "answer": "I'll answer your questions based only on the provided evidence chunks. What's your first question?",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.7121053338050842,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023",
      "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1–7.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1–24.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402–412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369–391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172–178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I 23. Springer, 171–190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1–47.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585–1608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot’s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62–71.\n[59] Ramaz"
    },
    {
      "rank": 2,
      "distance_l2": 0.7341235876083374,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_028",
      "text": ", Tianyi Zhang, and Elena L. Glassman. 2022. \nExpectation vs. Experience: Evaluating the Usability of Code Generation \nTools Powered by Large Language Models. In CHI Conference on Human \nFactors in Computing Systems Extended Abstracts (CHI ’22 Extended \nAbstracts), \nApril \n27, \n2022. \nACM, \nNew \nYork, \nNY, \nUSA, \n1–7. \nhttps://doi.org/10.1145/3491101.3519665 \n[46] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of \nGitHub copilot’s code generation. In Proceedings of the 18th International \nConference on Predictive Models and Data Analytics in Software Engineering \n(PROMISE ’22), November 07, 2022. ACM, Singapore, Singapore, 62–71. \nhttps://doi.org/10.1145/3558489.3559072 \n[47] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. \nEvaluating the Code Quality of AI-Assisted Code Generation Tools: An \nEmpirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT. \nArXiv (October 2023). https://doi.org/10.48550/arXiv.2304.10778 \n[48] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei \nShen, Zihan Wang, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. \n2023. CodeGeeX: A Pre-Trained Model for Code Generation with \nMultilingual Benchmarking on HumanEval-X. In Proceedings of the 29th \nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD \n’23), August 06, 2023. ACM, Long Beach, CA, USA, 5673–5684. \nhttps://doi.org/10.1145/3580305.3599790 \n[49] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, \nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. \nProductivity assessment of neural code completion. In MAPS 2022: \nProceedings of the 6th ACM SIGPLAN International Symposium on Machine \nProgramming, \nJune \n13, \n2022. \n21–29. \nhttps://doi.org/10.1145/3520312.3534864 \n \n"
    },
    {
      "rank": 3,
      "distance_l2": 0.7388505935668945,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 4,
      "distance_l2": 0.7452186346054077,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 5,
      "distance_l2": 0.7506582736968994,
      "source_id": "AIReview2025",
      "chunk_id": "AIReview2025_chunk_001",
      "text": "236 \nA Review of Research on AI-Assisted Code Generation and AI-\nDriven Code Review \nYuzhi Wang \nBeijing University of Technology, Beijing, China \nshenrenaisite@yeah.net \nAbstract. With the significant breakthroughs of deep learning technologies such as large language \nmodels (LLMs) in the field of code analysis, AI has evolved from an auxiliary tool to a key technology \nthat deeply participates in code optimization and resolving performance issues. As modern software \nsystem architectures become increasingly complex, the requirements for their performance have \nalso become more stringent. During the coding stage, developers find it difficult to effectively identify \nand resolve potential performance issues using traditional methods. This review focuses on the \napplication of artificial intelligence in two key areas: AI-assisted intelligent code generation and AI-\npovered code review. The review systematically analyzed the application of LLMs in software \ndevelopment, revealing a situation where efficiency gains coexist with quality challenges. In terms \nof code generation, models such as Code Llama and Copilot have significantly accelerated the \ndevelopment process. In the field of code review, AI can effectively handle code standards and low-\nseverity defects. However, in the future, this field still needs to address the issues of the reliability \nand security of the code generated by LLMs, as well as the insufficient explainability of the results of \nautomated performance analysis. The future research focus in this field lies in addressing issues \nsuch as the lack of interpretability and insufficient domain knowledge of LLMs. It is necessary to \nprioritize enhancing the reliability of AI recommendations and promoting the transformation of AI \nfrom an auxiliary tool to an intelligent Agent with self-repair capabilities, in order to achieve a truly \nefficient and secure human-machine collaboration paradigm. This article systematically reviews the \nrelevant progress, aiming to promote the transformation of software engineering from an artificial-\ndriven model to an AI-enhanced automated paradigm. It provides theoretical references for ensuring \nthe quality of backend code, improving product delivery speed, and enhancing system reliability. \nKeywords: AI; LLM; Code Generation; Code Review. \n1. Introduction \nRecently, the growing complexity of modern software applications has driven an increased \nemphasis on high-quality, maintainable source code, thereby heightening the difficulty for developers \nto write efficient and error-free code [1-3]. Therefore, there is an urgent need for a smarter approach \nto empover developers. The emergence of Artificial Intelligence has fundamentally transformed \nconventional approaches to code optimization and refactoring by introducing a new dimension of \nautomation [2]. A striking example is LLMs, which have exhibited remarkable potential in \nprogramming tasks [4], especially in code review [5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of"
    },
    {
      "rank": 6,
      "distance_l2": 0.7526141405105591,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_003",
      "text": " assistants can aid sensemaking tasks in code repositories.\n• We identify a shared responsibility between people and AI systems in mitigating the risks of generated outputs.\n2\nRelated Work\nWe outline three areas relevant to our study of AI code assistants: code-fluent LLMs and their incorporation into the\nsoftware engineering workflow; the multi-faceted nature of productivity in software engineering; and studies of AI\ncode assistants.\n2.1\nCode-fluent LLMs and software engineering assistants\nLarge language models that have been exposed to source code in their pre-training have demonstrated a high degree of\naptitude in performing a variety of tasks: converting natural language to code (e.g. [2, 16, 19, 56]), converting code\nto natural language documentation (e.g. [16, 31]) or explanations (e.g. [37]), and converting code to code, such as by\ntranslating it from one language to another (e.g. [2, 19, 46, 57]) or by creating unit tests (e.g. [48]). The introduction of\nthe Codex model [10] and its corresponding incorporation into software developers’ IDEs through GitHub Copilot6\ndemonstrated how code-fluent LLMs could revolutionize the software development workflow. New agentic design\npatterns are enabling coding assistants to perform even more complex tasks, such as converting issues into pull\nrequests [25] and new feature descriptions into specifications and implementation plans7.\n1In this paper, we use the term “developer” as a catch-all to cover individuals who perform code-related work, including software engineers, architects,\nand data scientists.\n2GitHub Copilot: https://github.com/features/copilot\n3Amazon Q Developer: https://aws.amazon.com/q/developer/\n4Gemini Code Assist: https://cloud.google.com/gemini/docs/codeassist/overview\n5watsonx Code Assistant: https://www.ibm.com/products/watsonx-code-assistant\n6GitHub Copilot: https://github.com/features/copilot\n7GitHub Copilot Workspace: https://githubnext.com/projects/copilot-workspace\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n3\n2.2\nSoftware engineering productivity\nProductivity in software engineering is a complex, multi-faceted construct [17, 47]. It is often assessed via objective\nmetrics of productivity that capture the ratio of output to effort [47] (e.g. lines of code over time [15], function points [30]),\nthe complexity of the software system [20, 33], or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2-trust-tightened-qrewrite"
}