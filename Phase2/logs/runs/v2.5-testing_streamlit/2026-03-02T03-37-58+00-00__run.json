{
  "timestamp": "2026-03-02T03:37:58+00:00",
  "query": "How do AI coding assistants affect task completion time?",
  "retrieval_query": "AI coding assistant effect on task completion time",
  "query_rewritten": true,
  "rewritten_query": "AI coding assistant effect on task completion time",
  "top_k": 6,
  "answer": "How do people's learning experiences and skills change when they use an AI assistant compared to not using one?\n\nThe provided evidence suggests that using an AI assistant reduces skill formation, particularly in conceptual understanding, code reading, and debugging skills. Participants who used AI assistance had lower quiz scores (24%-39%) compared to those who didn't use AI (65%-86%). Additionally, participants who manually typed their own code without copying or pasting the generated code from the AI assistant demonstrated high proficiency and were relatively fast.\n\nThe study found that participants in the AI condition spent much less active time on the task, shifting from coding to reading and understanding. This is consistent with previous research [Becker et al., 2025]. The control group achieved higher quiz scores with a higher total active time without the use of AI.\n\nFeedback from participants also suggested a more positive experience among those who didn't use AI assistance, while participants in the AI condition reported feeling \"lazy\" and having gaps in their understanding.\n\nOverall, the evidence suggests that using an AI assistant can lead to reduced skill formation and a decrease in cognitive effort, potentially resulting in lower performance on tasks requiring new skills.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6495128870010376,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_018",
      "text": " AI assistant\nis produced (annotated as an AI Query event).\nSince participants could ask the AI assistant as many questions as time allowed, a handful of participants\nasked more than five questions and spent up to six minutes composing a single query during this 35-minute\nassignment (Figure 12).5 Since the median completion time is only 19 minutes in the AI condition, spending\nup to 6 minutes composing a single query amounts to a significant amount of the total time spent interacting\nwith the AI assistant. Although this effect might be due to the short duration of our task, Becker et al. also\nfound a slowdown effect for expert coders on longer tasks when participants waiting for AI-written code may\nbecome distracted.\nHowever, from the lens of skill formation, the time spent composing queries may aid in better understanding\nthe task and, consequently, better acquisition of skills. Screen recordings show participants contemplating\nwhat to ask the AI assistant (e.g., rereading instructions and rewriting queries). As a result, some participants\ntook several minutes to compose a single query. Thus, while this time cost would be more prominent in\nchat-based assistants than agentic coding assistants, the loss in knowledge is likely even greater in an agentic\nor autocomplete setting where composing queries is not required. A more significant difference in completion\ntime due to shorter interactions with AI assistance would likely translate to an even larger negative impact\non skill formation. When we look at individual queries, not all queries involve significant thinking and time.\nThus, we analyze individual queries to better understand how participants from new skills.\nAI Queries\nWe categorized user inputs into the AI assistant, queries, into 5 broad categories: explanation,\ngeneration, debugging, capabilities questions, and appreciation (Table 3). The most common type of query\nwas explanations (q=79); users requested more information about the trio library, details about asynchronous\noperations, and high-level conceptual introductions. 21 out of 25 participants in the treatment group asked\nan explanation question; this reflects the high level of engagement among our participants. The second most\ncommon were queries asking for code to be generated (q=51); some participants asked for an entire task to\nbe completed, while other participants asked for specific functions to be implemented. Only 16 of 25 or two\nthirds of the participants used AI to generate code. 4 of these participants only asked for code generation\nand no other types of question. In fact, 3 of the 8 lowest-scoring participants asked AI to generate code\nwithout asking for explanations, suggesting that if all participants in the AI group were to use AI for solely\ngenerating code, the skill-formation differences compared to the control group would be even greater.\nA third category of common queries was debugging (q=9). Our tasks were designed to be straightforward,\nbut the participants still encountered various errors (Section 6.2). This is a broader category of queries that\nincludes errors directly pasted as input to the AI assistant as well as asking the AI assistant to confirm the\ncode written is correct. A higher fraction of debugging queries correlates with slower completion times (Figure\n18) and lower quiz scores (Figure 19). This suggests that relying on AI for debugging (e.g. repetatedly asking\nAI to check and fix things without understanding) when learning a new task is correlated with less learning.\n5Participants were instructed to"
    },
    {
      "rank": 2,
      "distance_l2": 0.745134711265564,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_024",
      "text": "18\nscore) vs low-scoring interaction patterns (24%-39% quiz score). The high scorers only asked AI conceptual\nquestions instead of code generation or asked for explanations to accompany generated code; these usage\npatterns demonstrate a high level of cognitive engagement.\nContrary to our initial hypothesis, we did not observe a significant performance boost in task completion\nin our main study. While using AI improved the average completion time of the task, the improvement in\nefficiency was not significant in our study, despite the AI Assistant being able to generate the complete code\nsolution when prompted. Our qualitative analysis reveals that our finding is largely due to the heterogeneity\nin how participants decide to use AI during the task. There is a group of participants who relied on AI to\ngenerate all the code and never asked conceptual questions or for explanations. This group finished much\nfaster than the control group (19.5 minutes vs 23 minutes), but this group only accounted for around 20%\nof the participants in the treatment group. Other participants in the AI group who asked a large number\nof queries (e.g., 15 queries), spent a long time composing queries (e.g., 10 minutes), or asked for follow-up\nexplanations, raised the average task completion time. These contrasting patterns of AI usage suggest that\naccomplishing a task with new knowledge or skills does not necessarily lead to the same productive gains as\ntasks that require only existing knowledge.\nTogether, our results suggest that the aggressive incorporation of AI into the workplace can have negative\nimpacts on the professional development workers if they do not remain cognitatively engaged. Given time\nconstraints and organizational pressures, junior developers or other professionals may rely on AI to complete\ntasks as fast as possible at the cost of real skill development. Furthermore, we found that the biggest difference\nin test scores is between the debugging questions. This suggests that as companies transition to more AI\ncode writing with human supervision, humans may not possess the necessary skills to validate and debug\nAI-written code if their skill formation was inhibited by using AI in the first place.\n7.1\nFuture Work\nOur work is a first step to understanding the impact of AI assistance on humans in the human-AI collaboration\nprocess. We hope that this work will motivate future work that addresses the following limitations:\n• Task Selection: This study focuses on a single task using a chat-based interface. This should be\na lower bound for cognitive offloading since agentic AI coding tools would require even less human\nparticipation. In our work, users who relied on AI without thinking performed the worst on the\nevaluation; a completely agentic tool would create a similar effect. Future work should investigate the\nimpacts of agentic coding tools on learning outcomes and skill development.\n• Task Length: Ideally, skill formation takes place over months to years. We measured skill formation for\na specific Python library over a one-hour period. Future work should study real-world skill development\nthrough longitudinal measurement of the impacts of AI adoption.\n• Participant Realism: While participants in our study were professional or freelance programmers,\nthere was not the same incentive to learn the library as if it were required for their actual job. Future\nstudies should aim at studying the skill acquisition fro novice workers within a real company.\n• Prompting Skills: We collect self-reported familiarity with AI coding tools, but we do not actually\nmeasure differences in prompting techniques."
    },
    {
      "rank": 3,
      "distance_l2": 0.7479408383369446,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_014",
      "text": "50\n60\n70\nScore %\nQuiz Score % by Subarea and Condition\nCondition\nAI\nNo AI\nFigure 8: Score breakdown by questions type relating to each task and skill area. Debugging questions\nrevealed the largest differences in average quiz score between the treatment and control groups.\nPrior works have presented mixed results on whether AI helps or hinders coding productivity [Peng et al.,\n2023, Becker et al., 2025]; our study differs from prior results in that it is designed to study how AI affects\nskill formation while performing a task requiring new knowledge. While we do observe a slightly lower average\ncompletion time in the AI group among novice programmers, due to the small group size of the 1-3 year\nparticipant group (n=4), the difference in task time was not significant. 4 of the 26 participants in the control\n(No AI) group did not complete the second task within the 35-minute limit, while every participant in the AI\ncondition completed the second task. Our results do not conclusively find a speed up or slow down using AI\nin this task.\nAcross all levels of prior coding experience, users scored higher on average in the control (no AI) than in\nthe treatment (AI assistance) group (Figure 7). This shows that our choice of tasks and task design did not\ncritically hinge on the participants’ experience level of the but presented new skills to be acquired for every\nexperience group.\nConcept Group Analysis In exploratory data analysis (not pre-registered), the quiz score was decomposed\ninto subareas and question types (Figure 8). Each question in the quiz belonged to exactly one task (e.g.,\nTask 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). For both\ntasks, there is a gap between the quiz scores between the treatment and control groups. Among the different\ntypes of questions, the largest score gap occurs in the debugging questions and the smallest score gap in\nthe code reading questions. This outcome is expected since treatment and control groups may have similar\nexposure to reading code through the task, but the control group with no access to AI assistance encountered\nmore errors during the task and became more capable at debugging.\nTask Experience In further exploratory data analysis, we also find differences in the way participants’\nexperience of completing the study. The control group (No AI) reported higher self-reported learning (on a\n7-point scale), while both groups reported high levels of enjoyment in completing the task (Figure 9). In terms\nof difficulty of the task, Figure 10 shows that although participants in the treatment group (AI Assistance)\nfound the task easier than the control group, both groups found the post-task quiz similarly challenging.\n6\nQualitative Analysis\nAlthough overall statistics on productivity and quiz score shed light on a high-level trend of how AI assistance\naffects a new learning task, a deeper analysis of how each participant completed the learning task allows us\n11\nLearning\nEnjoyment\nMetric\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nRating (Self-Reported)\nTask Learning and Enjoyment by Condition (1 to 7 Scale)\nCondition\nAI\nNo AI\nFigure 9: Self-reported enjoyment and learning\nby condition during our study.\n"
    },
    {
      "rank": 4,
      "distance_l2": 0.7795270681381226,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_020",
      "text": "\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\ndirectly pasted AI output. This suggests that spending more time manually typing may not yield better\nconceptual understanding. Cognitive effort may be more important than the raw time spent on completing\nthe task.\n6.2\nEncountering Errors\nThe way participants encountered and resolved errors was notably different between the treatment and control\nconditions. In the platform, participants could use the run button or the terminal to run their code as often\nas they wanted. In general, most of the participants ran the code for the first time after trying to complete\nmost of the question and ran the code again only after the changes were made. We recorded every error\nencountered by each participant as we watched the screen recordings of the task progress.\n15\nQuery Type\nExample Query\nExplanation (q=79)\n“can trio.sleep use partial seconds?”\n“Can you remind me what the different trio async operations are?”\n“Looks good, can you give me a really brief overview of the general idea behind\nall of this?”\nGeneration (q=51)\n“given this instruction to trio, can you implement the missing bits of main.py?”\n“complete get_user_data”\n“implement delayed_hello(). It should simply sleep for 2.1 seconds upon which\nit prints ’Hello World!’ ”\nDebugging (q=9)\n“Does that look right? If so let’s move on to delayed_hello()”\n“I’m having issues getting my code to work. I’m getting a notimplementederror\nfor delayed_hello”\nPasted\nError\n(e.g.,\n“Traceback\n(most\nrecent\ncall\nlast):\nFile\n\"/user-\ncode/FILESYSTEM/main.py3\", line 81, in... ”)\nCapabilities\nQues-\ntion (q=4)\n“Can you see the current question?”\n“So what can you do for me here? Can you write code directly into the file?”\n“Are you aware of how trio works? Are there parallels in its execution model to\nanother library I’d be more familiar with like asyncio”\nAppreciation (q=4)\n“Great job, we got the expected output on the first try.”\n“Looks like it worked, thanks!”\n“Trueeee!”\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI ("
    },
    {
      "rank": 5,
      "distance_l2": 0.7897502779960632,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_034",
      "text": " to my time management.\nEven if I hadn’t spent too much time on the first part, though, it still\nwould have been a tight finish for me in the 30 minute window I think.\nTable 7: Feedback from Participants in the No AI (control) Condition\nSample Question Types\nCode Reading\nDebugging\nConceptual Question\nFigure 20: Example question types from our evaluation. We designed the evaluation to test three different\nsoftware skills: conceputal understanding, code reading, and code writing.\n28\nFigure 21: Pledge taken by control group partic-\nipants: participants agree to not using AI assis-\ntance.\nFigure 22: Pledge taken by treatment group par-\nticipants.\nFigure 23: Instructions given to control group\nparticipants. We heavily emphasize not using AI\ntools.\nFigure 24: Instructions given to treatment group\nparticipants. This group was encouraged to use\nthe AI assistant to complete the task as quickly\nas possible.\n29\nFigure 25: Screenshot of the task platform in the control condition. The instructions are on the left and the\ncoding editor is on the right.\nFigure 26: Screenshot of the task platform in the AI condition. The instructions are on the left and the\ncoding editor is on the right. There is a nudge to use the AI assistant on the left tool plane.\n30\nFigure 27: Screenshot of the task platform when interacting with AI Assistant.\n31\n"
    },
    {
      "rank": 6,
      "distance_l2": 0.8067098259925842,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_023",
      "text": "\nNo AI\nAI\nexperience\n4-6 years\n1-3 years\n7+ years\nFigure 16: Active coding time vs. quiz score: active coding time represents the amount of time actually spent\ncoding and is often a very small fraction of total task time. The No AI condition participants spent more\nactive time coding and achieved higher quiz scores.\n6.3\nShifts in Active Coding Time\nAlthough the outcome we measure in our main analysis is productivity through total task time, the actual\namount to time spent actively coding illustrates a clearer pattern. Figure 16 shows that participants in the\nAI condition spent much less active time on the task. This shift from coding to reading and understanding\nhas also been found in previous work [Becker et al., 2025]. When we look at quiz score, the control group\nachieves high quiz scores with a higher total active time without the use of AI. Within each condition, higher\nactive time correlates with lower quiz score, this is because the more experienced programmers spend less\ntime actively coding while having better base knowledge compared to novice programmers.\n6.4\nParticipant Feedback\nA quarter of the participants left feedback after the task and quiz were completed. In the control group (No\nAI), participants remarked that they found the task fun and that the tasks instructions were good at helping\ndevelop an understanding of Trio. In the treatment group (AI Assistance), participants remarked that they\nwished they had paid more attention to the details of the Trio library during the task, either by reading\nthe generated code or by generating explanations in more depth. Specifically, participants reported feeling\n‘lazy’ and that ‘there are still a lot of gaps in (their) understanding’. The sentiment of participants’ feedback\nsuggested a more positive experience among the control group even though the task instructions and quiz\nquestions were identical across groups (Table 6 and Table 7 provide all of the participant feedback from all\nparticipants).\n7\nDiscussion\nOur main finding is that using AI to complete tasks that require a new skill (i.e., knowledge of a new Python\nlibrary) reduces skill formation. In a randomized controlled trial, participants were assigned to the treatment\ncondition (using an AI assistant, web search, and instructions) or the control condition (completing tasks with\nweb search and instructions alone). The erosion of conceptual understanding, code reading, and debugging\nskills that we measured among participants using AI assistance suggests that workers acquiring new skills\nshould be mindful of their reliance on AI during the learning process. Among participants who use AI, we\nfind a stark divide in skill formation outcomes between high-scoring interaction patterns (65%-86% quiz\n18\nscore) vs low-scoring interaction patterns (24%-39% quiz score). The high scorers only asked AI conceptual\nquestions instead of code generation or asked for explanations to accompany generated code; these usage\npatterns demonstrate a high level of cognitive engagement.\nContrary to our initial hypothesis, we did not observe a significant performance boost in task completion\nin our main study. While using AI improved the average completion time of the task, the improvement in\nefficiency was not significant in our study, despite the AI Assistant being able to generate the complete code\nsolution when prompted. Our qualitative analysis reveals that our finding is largely due to the heterogeneity\nin how participants decide to use AI during the task. There is a group of participants"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}