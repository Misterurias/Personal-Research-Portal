{
  "timestamp": "2026-03-02T02:58:33+00:00",
  "query": "How do AI-assisted coding tools impact developer productivity, code quality, and long-term technical debt across experience levels?",
  "retrieval_query": "Impact of AI-assisted coding tools on developer productivity, code quality, and technical debt by experience level.",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI-assisted coding tools on developer productivity, code quality, and technical debt by experience level.",
  "top_k": 7,
  "answer": "I can only answer based on the provided evidence chunks. Here are my answers:\n\n**Your first question**: How will AI shape the future of knowledge-intensive industries?\n\nAnswer: The introduction of GenAI technologies, such as GitHub Copilot, may fundamentally reshape how knowledge-intensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\n\n**Another consideration for AI code assistants**: Their impact on the work process.\n\nAnswer: AI code assistants like GitHub Copilot may have complementary types of usage, including \"acceleration mode\" and \"exploration mode\", which can aid developers in different ways.\n\nLet me know if you'd like me to answer any more questions!",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5955991148948669,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_009",
      "text": " Ramasubbu and Kemerer 2021). Both approaches have value: the former\nmetric-based techniques are scalable and actionable for tooling, while the later process indicators capture\nthe economic and human costs that managers and maintainers ultimately face.\nEmpirical studies across industry and OSS ecosystems consistently find that technical debt impairs long-\nrun productivity and increases maintenance burdens (Paramitha and Massacci 2023). Work in empirical\nsoftware engineering shows that higher measured debt correlates with increased bug rates, longer defect\nresolution times, and reduced velocity for feature delivery (Ramasubbu and Kemerer 2016). Rinta-Kahila\net al. (2023) further demonstrate that organizations can become “trapped” in technical debt due to coor-\ndination failures, organizational inertia, and escalating switching costs. The introduction of GenAI tools\nsuch as GitHub Copilot represents a departure from these assumptions (Yeverechyahu et al. 2024). Unlike\nprior productivity-enhancing tools, GenAI dramatically lowers the marginal cost of producing code and\nreduces skill barriers to contribution (Peng et al. 2023). While prior studies document productivity gains\nfrom AI-assisted coding, they provide limited insight into how these gains translate into technical debt and\nmaintenance related challenges.\nFrom a technical debt perspective, GenAI may accelerate debt accumulation by increasing code volume\nwithout proportionate improvements in quality, the integration with the software, or architectural coher-\nence (Pimenova et al. 2025). By lowering the cost of producing code, AI-assisted programming encourages\nrapid iteration and experimentation, but can shift attention away from longer-term concerns such as main-\ntainability, readability, and alignment with existing system design (Barrett et al. 2023, Schreiber and Tippe\n2025). As a result, defects, inconsistencies, and design shortcuts may be introduced more quickly than\nthey can be identified and resolved. Over time, this imbalance can compound, transforming short-term pro-\nductivity gains into persistent maintenance obligations that must be absorbed by experienced developers\n(Eghbal 2020). While GenAI promises to enhance development speed and broaden participation, it may\nsimultaneously intensify the very technical debt that constrains system reliability, scalability, and long-run\nperformance (Ramasubbu and Kemerer 2021). If this logic is valid, we expect that GenAI may lead to a\ngreater accumulation of technical debt in OSS projects.\n8\nTable 1\nSelected Literature on Technical Debt and Software Maintenance\nStudy\nContext\nMethod\nMeasurement\nKey Findings\nBanker et al.\n(2021)\nCustomer\nrelationship\nmanagement\n(CRM)\nsystems in 26 firms\nEconometric analysis\nPercentage of customized codes in the\nCRM system that do not adhere to vendor-\nprescribed standard\nHigher technical debt is associated\nwith\nlower\nfirm\nperformance\nand\nreduced operational efficiency over\ntime.\nRamasubbu\n&\nKemerer\n(2021)\nOutsourced Commercial\nOff-The-Shelf\n(COT)\nenterprise systems\nEconometric analysis\nViolations of the design and programming\nstandards established by the vendor of the\nCOTS enterprise system\nActive remediation policies reduce\nlong-term\nmaintenance\ncosts,\nbut\nexcessive deferral leads to escalating\ntechnical debt.\nRamas"
    },
    {
      "rank": 2,
      "distance_l2": 0.5976265668869019,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_002",
      "text": " code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n2\n \ncode completion capabilities [3].The acceptance of AI-assisted coding tools is getting faster, and it is \nrevealed by the latest industry surveys, which show that more than 65% of professional developers use AI \nsupport in some form as part of their daily routine [4]. These tools have been incorporated into the \ndevelopment processes of large tech companies that account for 30-50% productivity gains and have also \nreported significant time-to-market reductions for software products [5]. Nevertheless, the fast adoption \nof these tools has been so extensive that even empirical studies have not been able to catch up with their \nimplications on critical software engineering outcomes, like code quality, security, and long-term \nmaintainability, through rigorous research [6]. \nThe research gap is even more pronounced when the risk factors of AI-generated code are taken into \naccount. Initial experiments have pointed out issues such as security flaws, licensing confusion, and the \noccurrence of hidden bugs that will not be discovered during code reviewing process [7, 8]. Moreover, the \nimpact on the development of programmers' skills, particularly that of junior developers, who would \notherwise depend on AI-generated suggestions, is still unclear [9]. There being no empirical studies taking \na comprehensive approach to the assessment of these issues, the bottleneck of knowledge in software \nengineering research is in fact the multifaceted impacts of these issues. \nThis research paper fills this void by performing a large-scale controlled experiment whose main objective \nis to evaluate in a systematic way the impact of AI-assisted code generator tools on the three main \ndimensions: code quality, security vulnerability introduction, and developer productivity. Previous studies \nhave limited themselves to specific scenarios or synthetic benchmarks. On the contrary, our study will \ninvolve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious"
    },
    {
      "rank": 3,
      "distance_l2": 0.6359161138534546,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 4,
      "distance_l2": 0.6389945149421692,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_027",
      "text": " the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced contrib-\nutors face higher review workloads and a concomitant decline in their own development activity. These\nresults highlight a redistribution of effort within OSS communities that has received limited attention in\nprior work.\nFrom a technical debt perspective, our findings suggest that AI-assisted programming alters the intertem-\nporal trade-off between short-term development speed and long-term maintainability. The shortcuts enabled\nby AI tools may accelerate the output while introducing code that is difficult to integrate, extend, or refactor.\nThe widespread use of AI-assisted pair programming - and, in extreme cases, “vibe coding” - can inject a\nlarger volume of difficult-to-maintain code (Pimenova et al. 2025, Fawzy et al. 2025) into software projects,\naccelerating the accumulation of technical debt. Such contributions create latent liabilities for projects, as\nmaintainers must later invest substantial effort to review, revise, or rewrite code to meet repository stan-\ndards. In this sense, AI does not merely increase the volume of contributions; it changes the composition of\nincoming code in ways that intensify technical debt accumulation.\nA key contribution of our study is to operationalize technical debt at its point of entry. We conceptual-\nize extensive PR rework as realized technical debt: the additional modification, coordination, and revision\n16 https://github.blog/news-insights/octoverse/octoverse-2024/\n24\neffort required to bring submitted code up to acceptable standards. This measure complements prior work\nthat captures technical debt through architectural metrics, defect accumulation, or long-run performance\noutcomes. By focusing on PR-level dynamics, we provide a micro-level view of how technical debt emerges\nin real time and how it is managed through ongoing maintenance effort.\nOur findings also raise concerns about the learning implications of AI-assisted development. With AI\nproviding rapid solutions, peripheral contributors may engage less deeply with underlying programming\nprinciples and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation"
    },
    {
      "rank": 5,
      "distance_l2": 0.6521773338317871,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_004",
      "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60–62]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: “acceleration mode” in which the tool aided\ndevelopers when they knew what to do next, and “exploration mode” to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM’s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as “WCA@IBM,” was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
    },
    {
      "rank": 6,
      "distance_l2": 0.6722249388694763,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_001",
      "text": "AI-Assisted Programming Decreases the Productivity of\nExperienced Developers by Increasing the Technical Debt\nand Maintenance Burden\nFeiyang (Amber) Xu, Poonacha K. Medappa, Murat M. Tunc\nMartijn Vroegindeweij, Jan C. Fransoo\nTilburg University, the Netherlands\nf.xu_1@tilburguniversity.edu, p.k.medappa@tilburguniversity.edu, m.m.tunc@tilburguniversity.edu\nw.m.vroegindeweij@tilburguniversity.edu, jan.fransoo@tilburguniversity.edu\nGenAI solutions like GitHub Copilot have been shown to increase the productivity of software developers. Yet prior\nwork remains unclear on the quality of code produced and the challenges of maintaining it in software projects. If quality\ndeclines as volume grows, technical debt accumulates as experienced developers face increased workloads reviewing\nand reworking code from less-experienced contributors. We analyze developer activity in Open Source Software (OSS)\nprojects following the introduction of GitHub Copilot. We find that productivity indeed increases. However, the increase\nin productivity is primarily driven by less-experienced (peripheral) developers. We also find that code written after the\nadoption of AI requires more rework to satisfy repository standards, indicating a potential increase in technical debt.\nImportantly, the added rework burden falls on the more experienced (core) developers, who review 6.5% more code after\nCopilot’s introduction, but show a 19% drop in their original code productivity. More broadly, this finding raises caution\nthat productivity gains of AI may mask the growing burden of maintenance on a shrinking pool of experts, together with\nincreased technical debt for the projects. The results highlight a fundamental tension in AI-assisted software development\nbetween short-term productivity gains and long-term system sustainability.\nKey words: GenAI, GitHub Copilot, Open Source Software, Software Maintenance, Technical Debt,\nDifference-in-Differences\n1.\nIntroduction\nHow will AI shape the future of knowledge-intensive industries? This question has taken on renewed\nsignificance with the recent rise of Genarative AI (GenAI) technologies, which are becoming an integral part\nof daily operations of software development, scientific research, healthcare and other expert-driven fields.\nA prominent example is GitHub Copilot, an AI-powered coding assistant designed to support developers by\ngenerating code suggestions and accelerating routine programming tasks (Peng et al. 2023). When GitHub\nlaunched Copilot, it was introduced as “your AI pair programmer,\" emphasizing not only its role as an\nautomation tool but also as a team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming"
    },
    {
      "rank": 7,
      "distance_l2": 0.6752727031707764,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_001",
      "text": "Examining the Use and Impact of an AI Code Assistant on Developer\nProductivity and Experience in the Enterprise\nJUSTIN D. WEISZ, IBM Research, USA\nSHRADDHA KUMAR∗, Cisco Systems, Inc., India\nMICHAEL MULLER, IBM Research, USA\nKAREN-ELLEN BROWNE, IBM Software, Ireland\nARIELLE GOLDBERG, IBM Infrastructure, USA\nELLICE HEINTZE, IBM Software, Germany\nSHAGUN BAJPAI, IBM Software, India\nAI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and\ntesting code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within\nIBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers’ experiences\nwith WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their\nexpectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated\ncode. Our case study characterizes the impact of an LLM-powered assistant on developers’ perceptions of productivity and it shows\nthat although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.\nCCS Concepts: • Human-centered computing →Empirical studies in HCI; Field studies; • Software and its engineering →\nCollaboration in software development; Automatic programming.\nAdditional Key Words and Phrases: Generative AI, LLM, software engineering, productivity, code assistant\nACM Reference Format:\nJustin D. Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. 2025.\nExamining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA ’25), April 26-May 1, 2025, Yokohama, Japan. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3706599.3706670\n1\nIntroduction\nAI assistants powered by large language models (LLMs) are becoming increasingly prevalent in the workplace. A\nnumber of commercial and open-source coding assistants have been released for software engineers, developers, and\n∗Work conducted while an employee of IBM Software, Kochi, India.\nAuthors’ Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen,"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2-trust-tightened-qrewrite"
}