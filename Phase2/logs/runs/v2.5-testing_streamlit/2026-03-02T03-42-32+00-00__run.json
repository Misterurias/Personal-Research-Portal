{
  "timestamp": "2026-03-02T03:42:32+00:00",
  "query": "Do AI coding assistants increase long-term maintenance burden?",
  "retrieval_query": "Are AI coding assistants a net positive or negative factor in reducing long-term software maintenance workload?",
  "query_rewritten": true,
  "rewritten_query": "Are AI coding assistants a net positive or negative factor in reducing long-term software maintenance workload?",
  "top_k": 6,
  "answer": "Based on the provided evidence chunks, here are my answers:\n\n1. How does AI-assisted programming impact the development and maintenance activities and the technical debt of OSS projects?\n\nAccording to the study, while AI adoption leads to productivity gains, it also increases maintenance-related activities, resulting in a buildup of technical debt.\n\n2. What is the scale of Copilot's impact on OSS communities?\n\nThe study found that Microsoft core contributors conduct an average of 976 commits, 160 PRs, and 166 PR reviews annually before Copilot's introduction. After Copilot's release, this increased to an additional workload for each core contributor, resulting in a reduction of 164 commits and 9 PR contributions per year.\n\n3. How do less-experienced contributors impact the development and maintenance activities?\n\nAccording to the study, while less-experienced contributors increase their output, experienced contributors' productive output declines as maintenance demands rise.\n\n4. What are some future research directions?\n\nThe study suggests that longitudinal studies, cross-comparative studies of different AI tool implementations, interventions to reduce vulnerabilities, and investigations into AI tooling's impact on developers' skills should be explored.\n\nPlease note that these answers are based solely on the provided evidence chunks and may not reflect the full scope or findings of the research.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.792670726776123,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_027",
      "text": " the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced contrib-\nutors face higher review workloads and a concomitant decline in their own development activity. These\nresults highlight a redistribution of effort within OSS communities that has received limited attention in\nprior work.\nFrom a technical debt perspective, our findings suggest that AI-assisted programming alters the intertem-\nporal trade-off between short-term development speed and long-term maintainability. The shortcuts enabled\nby AI tools may accelerate the output while introducing code that is difficult to integrate, extend, or refactor.\nThe widespread use of AI-assisted pair programming - and, in extreme cases, “vibe coding” - can inject a\nlarger volume of difficult-to-maintain code (Pimenova et al. 2025, Fawzy et al. 2025) into software projects,\naccelerating the accumulation of technical debt. Such contributions create latent liabilities for projects, as\nmaintainers must later invest substantial effort to review, revise, or rewrite code to meet repository stan-\ndards. In this sense, AI does not merely increase the volume of contributions; it changes the composition of\nincoming code in ways that intensify technical debt accumulation.\nA key contribution of our study is to operationalize technical debt at its point of entry. We conceptual-\nize extensive PR rework as realized technical debt: the additional modification, coordination, and revision\n16 https://github.blog/news-insights/octoverse/octoverse-2024/\n24\neffort required to bring submitted code up to acceptable standards. This measure complements prior work\nthat captures technical debt through architectural metrics, defect accumulation, or long-run performance\noutcomes. By focusing on PR-level dynamics, we provide a micro-level view of how technical debt emerges\nin real time and how it is managed through ongoing maintenance effort.\nOur findings also raise concerns about the learning implications of AI-assisted development. With AI\nproviding rapid solutions, peripheral contributors may engage less deeply with underlying programming\nprinciples and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation"
    },
    {
      "rank": 2,
      "distance_l2": 0.8600189089775085,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_001",
      "text": "AI-Assisted Programming Decreases the Productivity of\nExperienced Developers by Increasing the Technical Debt\nand Maintenance Burden\nFeiyang (Amber) Xu, Poonacha K. Medappa, Murat M. Tunc\nMartijn Vroegindeweij, Jan C. Fransoo\nTilburg University, the Netherlands\nf.xu_1@tilburguniversity.edu, p.k.medappa@tilburguniversity.edu, m.m.tunc@tilburguniversity.edu\nw.m.vroegindeweij@tilburguniversity.edu, jan.fransoo@tilburguniversity.edu\nGenAI solutions like GitHub Copilot have been shown to increase the productivity of software developers. Yet prior\nwork remains unclear on the quality of code produced and the challenges of maintaining it in software projects. If quality\ndeclines as volume grows, technical debt accumulates as experienced developers face increased workloads reviewing\nand reworking code from less-experienced contributors. We analyze developer activity in Open Source Software (OSS)\nprojects following the introduction of GitHub Copilot. We find that productivity indeed increases. However, the increase\nin productivity is primarily driven by less-experienced (peripheral) developers. We also find that code written after the\nadoption of AI requires more rework to satisfy repository standards, indicating a potential increase in technical debt.\nImportantly, the added rework burden falls on the more experienced (core) developers, who review 6.5% more code after\nCopilot’s introduction, but show a 19% drop in their original code productivity. More broadly, this finding raises caution\nthat productivity gains of AI may mask the growing burden of maintenance on a shrinking pool of experts, together with\nincreased technical debt for the projects. The results highlight a fundamental tension in AI-assisted software development\nbetween short-term productivity gains and long-term system sustainability.\nKey words: GenAI, GitHub Copilot, Open Source Software, Software Maintenance, Technical Debt,\nDifference-in-Differences\n1.\nIntroduction\nHow will AI shape the future of knowledge-intensive industries? This question has taken on renewed\nsignificance with the recent rise of Genarative AI (GenAI) technologies, which are becoming an integral part\nof daily operations of software development, scientific research, healthcare and other expert-driven fields.\nA prominent example is GitHub Copilot, an AI-powered coding assistant designed to support developers by\ngenerating code suggestions and accelerating routine programming tasks (Peng et al. 2023). When GitHub\nlaunched Copilot, it was introduced as “your AI pair programmer,\" emphasizing not only its role as an\nautomation tool but also as a team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming"
    },
    {
      "rank": 3,
      "distance_l2": 0.8642374277114868,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_005",
      "text": "its\n4\nmeasured technical debt at the project level using PR rework, which reflects the extent to which initially\nsubmitted contributions require modification before being integrated into the codebase (Ramasubbu and\nKemerer 2016, 2021). Higher levels of rework indicate greater reliance on expedient or insufficiently inte-\ngrated solutions. Second, we measured maintenance effort at the individual level using PR reviews, which\ncapture the time and effort required from contributors to evaluate, correct, and integrate submitted code\n(Medappa et al. 2023).\nBased on our analysis of a large-scale panel dataset from GitHub, we find that while AI adoption leads to\nproductivity gains, they also increase maintenance-related activities due to a higher volume of review and\nrework needed per PR. Specifically, our analysis reveals a double-edged effect of GitHub Copilot on OSS\ndevelopment. At the project level, Copilot adoption is associated with a significant boost in productivity:\nprojects that supported Copilot saw increases in lines of code added, commits, and PRs. However, this surge\nin contributions came also with an increase in PR rework (2.4% more code revisions), indicating a possible\ndecline in the quality of code initially submitted resulting in an accumulation of technical debt. At the\ncontributor level, we observe an important redistribution of effort: the peripheral contributors (the less active\ncontributors to the projects) increased their development activity, taking advantage of Copilot’s ability to\nlower coding barriers. Specifically, peripheral contributors, particularly those in the bottom percentiles in\nterms of their previous contributions, increased their commit activity by 43.5% and submitted 17.7% more\nPRs. In contrast, the core contributors reduced their commit activity by 19%, shifting their focus toward\nreviewing and maintaining code (a 6.5% increase), and shouldering a heavier quality assurance burden.\nTogether, these findings highlight how AI can enable broader participation in OSS, but also raise concerns\nabout the sustainability of these gains and the strain placed on a shrinking pool of experienced contributors\nwho maintain quality in OSS projects.\n2.\nRelated Literature and Conceptual Development\n2.1.\nAI Assisted Code Development in OSS Projects\nThe rapid advancement of GenAI technologies, particularly LLMs like ChatGPT and GitHub Copi-\nlot, is transforming how software is developed and how online knowledge communities operate. Current\nresearch on AI-assisted code development has shown the substantial impact of the technology on pro-\nductivity. A study to assess the productivity benefits of using Copilot revealed that developers who use\nCopilot completed their programming task 55.8% faster than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant"
    },
    {
      "rank": 4,
      "distance_l2": 0.864338755607605,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_026",
      "text": ". First, the introduction of GitHub Copilot leads to higher\ndevelopment activity at both the repository and individual levels, measured by commits and PR, consistent\nwith industry evidence on AI-driven productivity gains (Peng et al. 2023). Second, these gains are accompa-\nnied by a significant increase in maintenance-related activities, as AI-generated contributions require more\nrevisions before integration - an early indicator of technical debt accumulation. Third, the effects are highly\nheterogeneous: core contributors review more PRs, contribute fewer commits, and extend their mainte-\nnance responsibilities across a wider range of repositories, suggesting that AI-assisted contributions from\nperipheral developers increase coordination and review burdens.\nBy conceptualizing GenAI as an endogenous shock to software production, this study advances the tech-\nnical debt literature in several important ways. First, consistent with prior work linking technical debt to firm\nperformance (Banker et al. 2021) and remediation costs (Ramasubbu and Kemerer 2016, 2021), we demon-\nstrate that AI-assisted code increases realized remediation effort, measured through PR rework. Second, we\nshow that technical debt is increasingly an outdated workload distribution phenomenon: maintenance costs\nare concentrated among a shrinking pool of core contributors, whose own productive output declines as\nmaintenance demands rise. Third, our findings complement research on organizational design and auton-\nomy (Paramitha and Massacci 2023, Yoo et al. 2025) by revealing how technological change can exacerbate\nasymmetries in effort and responsibility, even when the OSS repository workflow remains unchanged.\n15 https://mashable.com/archive/heartbleed-bug-websites-affected\n23\nTo illustrate the scale of Copilot’s impact on OSS communities, Microsoft core contributors in our dataset\nconduct on average, 976 commits, 160 PRs, and 166 PR reviews annually before its introduction. The\nincreased volume of code associated with Copilot adoption results in an additional workload – each core\ncontributor is expected to review approximately 10 more PRs annually. This added maintenance burden\ncorresponds to a reduction of 164 commits and 9 PR contributions per year per core contributor. More\ncritically, GitHub’s 2024 surveys reveal that more than one-third of contributors to the 10 most popular OSS\nprojects made their first contribution after signing up for GitHub Copilot, highlighting a significant influx\nof new and often less experienced developers16.\nWith annual contributions to OSS projects approaching 1 billion, this surge in participation significantly\nincreases the burden on core contributors, who take on the maintenance related tasks in the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced"
    },
    {
      "rank": 5,
      "distance_l2": 0.8785360455513,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_004",
      "text": "1, Ramasubbu and Kemerer 2021). These maintenance tasks are generally less intrinsically driven and\ntypically assigned to contributors who possess both technical expertise and trust of the community to ensure\nthe quality and reliability of the codebase (Medappa et al. 2023, Eghbal 2020). Thus, we seek to answer the\nquestion: How does AI-assisted programming impact the development and maintenance activities and the\ntechnical debt of the OSS projects?\nIn this study, we examine whether technical debt and maintenance efforts of OSS projects changed after\nthe introduction of GitHub Copilot through increased code review and rework effort on PRs. To empirically\ntest this, we exploit the release of GitHub Copilot as a technical preview in June 2021, which included lim-\nited programming language endorsement. We focus on OSS projects owned by Microsoft, as the company\nhad exclusive access to OpenAI’s GPT-3, the model powering GitHub Copilot during its technical preview,\ndue to its investment in OpenAI and its prior acquisition of GitHub.4,5 The individual users in our dataset\nare contributors to Microsoft-owned OSS projects. We estimate the effect of Copilot at both the project\nand contributor levels using a Difference-in-Differences (DiD) design. Treatment and control groups were\ndefined based on the primary programming language: those using Copilot-endorsed languages formed the\ntreatment group, while non-endorsed language users served as the control (Yeverechyahu et al. 2024). For\nboth project and contributor levels, we collected data on programming activities and aggregated them at the\nmonthly level.\nWe examine the changes in code productivity after Copilot by three measures: lines of code added,\ncommits 6 and PRs submitted to the project. To capture the secondary effects of AI adoption on technical\ndebt and maintenance effortsof OSS communities, we focused on two complementary outcomes. First, we\n4 https://www.technologyreview.com/2020/09/23/1008729/openai-is-giving-microsoft-exclusive-access-to-its-gpt-3-language-\nmodel/\n5 https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/thomas-dohmke-on-improving-\nengineering-experience-using-generative-ai\n6 A commit is the fundamental unit of change on GitHub. Similar to saving a file that’s been edited, a commit records changes to\none or more files on GitHub - https://docs.github.com/en/pull-requests/committing-changes-to-your-project/creating-and-editing-\ncommits/about-commits\n4\nmeasured technical debt at the project level using PR rework, which reflects the extent to which initially\nsubmitted contributions require modification before being integrated into the codebase (Ramasubbu and\nKemerer 2016, 2021). Higher levels of rework indicate greater reliance on expedient or insufficiently inte-\ngrated solutions. Second, we measured maintenance effort at the individual level using PR reviews, which\ncapture the time and effort required from contributors to evaluate, correct, and integrate submitted code\n(Medappa et al. 2023).\nBased on our analysis of a large-scale panel dataset from GitHub, we find that while AI adoption leads to\nproductivity gains, they also increase maintenance-related activities"
    },
    {
      "rank": 6,
      "distance_l2": 0.9023784399032593,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_013",
      "text": " we cannot conclude validity of other tools leveraged AI in different forms or \narchitectures. The 60-minute tasks did not allow us to explore the long-term ramifications of maintenance. \nThe sample of practitioners was drawn from Western technology companies and there are implications for \ngeneralizability among practice globally based on our sample. Additionally, the static analysis tools used \nas part of the scanning for a security vulnerability had important limitations, as discussed in several works, \nof being too discriminatory (false positives) and failing to positively identify classes of vulnerabilities that \ncan only be found using a dynamic analysis. Therefore, even with some of the limitations addressed by \nsome manual validation, we evaluated only the immediate introduction of a vulnerability and not the \nprocesses by which a vulnerability is subsequently identified and remediated once a code sample is placed \ninto production software, when investigating a security vulnerability. \n5.5 Future Research Directions \nNumerous important questions springing from our observations will require investigation. Longitudinal \nstudies depicting the development of AI-assisted codebases over extended periods of time will provide \nimportant insights into maintenance costs and the buildup of technical debt. Cross-comparative studies of \ndifferent AI tool implementations will also help clarify whether observed effects are associated with a \nspecific tool, or are more generalizable. Equally pressing, interventions also merit investigation, which \ncould include making AI models security-aware, humans-in-the-loop generation that performs automated \nsecurity-checks, enhanced developer training, or techniques of prompt engineering that pro-actively \nreduce the actual introduction of vulnerabilities. Interactions between AI tooling and developers' own \nskills should also be in steady inquiry, especially whether junior developers using AI tools are developing \nproblem-solving skills equivalent to developers going through training in the traditional sense. The \nanswers to such questions would be noteworthy for educational practice and hiring. Finally, we could \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n10\n \nconduct research that examines adaptive AI assistance that provides support at each stage based on \ndevelopers' competencies in order to move optimal learning trajectories forward, and that also assesses \nproductivity gains in relationship to skills development. \n \n6. Conclusion \nIn this study, we provide an in-depth data-driven investigation of AI-assisted code generation technologies \nthat demonstrate considerable productivity increases (i.e., an average of 31.4%) and, concerning, a total \nincrease in vulnerabilities of 23.7% and a total increase in critical severity of 89%. We also demonstrate \nthat some dimensions of code quality are improved with AI-assisted code generation tools (i.e., \nmaintainability, cyclomatic complexity) but caution is warranted with operational risks, to code itself (i.e. \nextra code duplication) and security vulnerabilities. We also examined differences across programming \nlanguages, and in particular, we found that while using AI-assisted code generation technologies is \nconstructive in Python, it warrants heightened caution around codex in C++ (to name only one). Finally, \nwhile we examined experience differences, we found that junior developers require support to prevent \nexcessive dependency on AI and senior developers could receive the maximum"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_streamlit"
}