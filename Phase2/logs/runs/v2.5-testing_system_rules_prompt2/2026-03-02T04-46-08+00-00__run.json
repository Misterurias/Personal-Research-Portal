{
  "timestamp": "2026-03-02T04:46:08+00:00",
  "query": "Are experienced developers more effective when using AI assistants?",
  "retrieval_query": "Effectiveness of experienced developers in software development processes with AI assistant usage.",
  "query_rewritten": true,
  "rewritten_query": "Effectiveness of experienced developers in software development processes with AI assistant usage.",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks. Please provide the specific question you'd like me to address, and I'll do my best to respond using the information from these chunks.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6972228288650513,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_006",
      "text": " code, fix\nerrors, and explain algorithmic concepts; students with less coding proficiency were more likely to seek AI\nassistance. Other works use surveys to find that students may be hesitant to use AI coding assistant tools\ndue to “dependence worry” (i.e., overreliance on coding tools) [Pan et al., 2024]. For formal methods, Prasad\net al. coded the different ways in which students used LLMs for course work and found that upper-year\nstudents taking the class did not rely on LLM assistance and only asked a few questions at the beginning.\nUser studies have also been conducted in the professional development environments. Wang et al. study\ndifferent patterns in usage between users with and without chat access to AI models in completing coding\npuzzles and development tasks. They found rich interaction patterns including interactive debugging, code\ndiscussions, and asking specific questions. Participants ranged from asking ChatGPT to do then the entire\nproblem (lowest quality code output) to only asking minimal questions (highest efficiency). Other studies\nhave reported that AI tools help the software development process through easier access to documentation\nand accurate generation code for specific APIs [Pinto et al., 2024].\n3\nFramework\nProfessional Skill Acquisition\nThe “learning by doing” philosophy has been suggested by many learning\nframeworks such as the Kolb’s experiential learning cycle, and the Problem-Based Learning (PBL) [Kolb,\n2014, Schmidt, 1994]. The frameworks connect the completion of real-world tasks with the learning of\nnew concepts and the development of new skills. Experiential learning has also been explored specifically\nin software engineering courses in higher education in order to mimic solving problems in a professional\nsetting [Gonzalez-Huerta et al., 2020]. In its simplest form, we model AI tool assistance as taking a different\nlearning path than without AI. We hypothesize that using AI tools to generate code in the development\nprocess effectively amounts to taking a shortcut to task completion without a pronounced learning stage.\nAI for Coding Usage Patterns\nPrior works have found that humans use AI in many different ways\nfor coding: from question answering to writing code, to debugging [Poitras et al., 2024, Wang et al., 2020,\nPinto et al., 2024]. In our framework, different ways of using AI assistance represent different learning paths\ntaken to reach the goals of completing the task. We analyze these different usage patterns in the qualitative\nanalysis of this work (Section 6).\nResearch Questions\nBased on this background, we focus on on-the-job learning: settings where workers\nmust acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n• RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on"
    },
    {
      "rank": 2,
      "distance_l2": 0.713977575302124,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    },
    {
      "rank": 3,
      "distance_l2": 0.7259669303894043,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_001",
      "text": "Examining the Use and Impact of an AI Code Assistant on Developer\nProductivity and Experience in the Enterprise\nJUSTIN D. WEISZ, IBM Research, USA\nSHRADDHA KUMAR∗, Cisco Systems, Inc., India\nMICHAEL MULLER, IBM Research, USA\nKAREN-ELLEN BROWNE, IBM Software, Ireland\nARIELLE GOLDBERG, IBM Infrastructure, USA\nELLICE HEINTZE, IBM Software, Germany\nSHAGUN BAJPAI, IBM Software, India\nAI assistants are being created to help software engineers conduct a variety of coding-related tasks, such as writing, documenting, and\ntesting code. We describe the use of the watsonx Code Assistant (WCA), an LLM-powered coding assistant deployed internally within\nIBM. Through surveys of two user cohorts (N=669) and unmoderated usability testing (N=15), we examined developers’ experiences\nwith WCA and its impact on their productivity. We learned about their motivations for using (or not using) WCA, we examined their\nexpectations of its speed and quality, and we identified new considerations regarding ownership of and responsibility for generated\ncode. Our case study characterizes the impact of an LLM-powered assistant on developers’ perceptions of productivity and it shows\nthat although such tools do often provide net productivity increases, these benefits may not always be experienced by all users.\nCCS Concepts: • Human-centered computing →Empirical studies in HCI; Field studies; • Software and its engineering →\nCollaboration in software development; Automatic programming.\nAdditional Key Words and Phrases: Generative AI, LLM, software engineering, productivity, code assistant\nACM Reference Format:\nJustin D. Weisz, Shraddha Kumar, Michael Muller, Karen-Ellen Browne, Arielle Goldberg, Ellice Heintze, and Shagun Bajpai. 2025.\nExamining the Use and Impact of an AI Code Assistant on Developer Productivity and Experience in the Enterprise. In Extended\nAbstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA ’25), April 26-May 1, 2025, Yokohama, Japan. ACM,\nNew York, NY, USA, 21 pages. https://doi.org/10.1145/3706599.3706670\n1\nIntroduction\nAI assistants powered by large language models (LLMs) are becoming increasingly prevalent in the workplace. A\nnumber of commercial and open-source coding assistants have been released for software engineers, developers, and\n∗Work conducted while an employee of IBM Software, Kochi, India.\nAuthors’ Contact Information: Justin D. Weisz, jweisz@us.ibm.com, IBM Research, Yorktown Heights, NY, USA; Shraddha Kumar, shraddku@cisco.com,\nCisco Systems, Inc., Bangalore, India; Michael Muller, michael_muller@us.ibm.com, IBM Research, Cambridge, MA, USA; Karen-Ellen Browne, karen-\nellen@ibm.com, IBM Software, Dublin, Ireland; Arielle Goldberg, arielle.goldberg1@ibm.com, IBM Infrastructure, Poughkeepsie, NY, USA; Ellice Heintze,\nke.heintze@de.ibm.com, IBM Software, Boeblingen,"
    },
    {
      "rank": 4,
      "distance_l2": 0.7283358573913574,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
      "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
    },
    {
      "rank": 5,
      "distance_l2": 0.7423298954963684,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_010",
      "text": " tool is so new and very few people on my team use it. There is an inherent suspicion against\nAI-generated code.” Although only two respondents raised this issue, it may be important for organizational leaders to\nfoster a culture in which AI assistance is viewed as acceptable to garner wide-spread adoption.\nFinally, we discovered a small group of technical writers who found utility in using WCA to understand technical\nconcepts without having to disturb their developer counterparts. R1.57 remarked, “My favorite feature is to understand\nthe technical terms and code provided by the Dev. This reduces the time in understanding the API terms and code rather\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n7\nthan discussing with the Dev.” Multiple respondents desired using WCA to “create customer facing documentation”\n(R1.27) and “help me writ[e] drafts following IBM content guidelines.” (R1.68). These “off-label” use cases by people in\ndeveloper-adjacent roles surprised us and indicate the importance of taking a holistic view on the potential beneficiaries\nof AI code assistants.\n4.2\nUse of generated content\nRespondents reported using generated outputs – code, documentation, unit tests, and explanations – in different ways.\nOverall, the use of generated outputs without modification was not common (2-4% of respondents reported doing\nthis, depending on output type); rather, respondents often modified outputs before using them (9-19%) or used them\nfor another purpose, such as learning something new (23-35%) or getting new ideas (24-37%). Users described how\n“the results give me new ideas” (R2.180) and “It is very helpful to get started writing code in a new language” (R2.626)\nby “recommend[ing] an approach I haven’t thought of or I wasn’t even aware of” (R2.405). P1 described how “creating\ndiagrams in markdown works from the code.” Users also talked about how WCA helped them recall “concepts which may\nbe I have forgotten during [the] course of time” (R2.296) and aiding them when “[I] know what to do, but don’t know how\nto do it or forgot about that.” (R2.292). These uses reinforce the value that generative AI provides in helping people\nlearn [1, 6, 49, 59].\n4.3\nImpact on productivity\nWe evaluated the impact of WCA on respondents’ perceptions of productivity using multiple measures: 7-point semantic\ndifferential scales14 that assessed effort, quality, and speed [53] and a 4-item scale of self-efficacy15 (derived from [44]).\nOverall, respondents felt that WCA made their work easier (M (SD) = .78 (1.45), t(609) = 13.35, p < .001, 95% CI =\n[.67, .90]), of a better quality (M (SD) = .66 (1.25), t(603) = 13.02, p < .001, 95% CI = [.56, .76]), and faster (M (SD) = .57\n(1.48), t(606) = 9.57, p"
    },
    {
      "rank": 6,
      "distance_l2": 0.7526505589485168,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_002",
      "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n∗Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n†Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}