{
  "timestamp": "2026-03-02T03:55:52+00:00",
  "query": "Does GitHub Copilot increase developer productivity?",
  "retrieval_query": "Effectiveness of GitHub Copilot in boosting developer productivity.",
  "query_rewritten": true,
  "rewritten_query": "Effectiveness of GitHub Copilot in boosting developer productivity.",
  "top_k": 6,
  "answer": "According to the provided evidence chunks, you are a research assistant and I will only answer based on those chunks.\n\nWhat's your question?",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.4836835265159607,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_006",
      "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers’ effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation’s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that “general housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors’ time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there’s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and “vibe coding” practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed–quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
    },
    {
      "rank": 2,
      "distance_l2": 0.4927663207054138,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 3,
      "distance_l2": 0.564727783203125,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_001",
      "text": "AI-Assisted Programming Decreases the Productivity of\nExperienced Developers by Increasing the Technical Debt\nand Maintenance Burden\nFeiyang (Amber) Xu, Poonacha K. Medappa, Murat M. Tunc\nMartijn Vroegindeweij, Jan C. Fransoo\nTilburg University, the Netherlands\nf.xu_1@tilburguniversity.edu, p.k.medappa@tilburguniversity.edu, m.m.tunc@tilburguniversity.edu\nw.m.vroegindeweij@tilburguniversity.edu, jan.fransoo@tilburguniversity.edu\nGenAI solutions like GitHub Copilot have been shown to increase the productivity of software developers. Yet prior\nwork remains unclear on the quality of code produced and the challenges of maintaining it in software projects. If quality\ndeclines as volume grows, technical debt accumulates as experienced developers face increased workloads reviewing\nand reworking code from less-experienced contributors. We analyze developer activity in Open Source Software (OSS)\nprojects following the introduction of GitHub Copilot. We find that productivity indeed increases. However, the increase\nin productivity is primarily driven by less-experienced (peripheral) developers. We also find that code written after the\nadoption of AI requires more rework to satisfy repository standards, indicating a potential increase in technical debt.\nImportantly, the added rework burden falls on the more experienced (core) developers, who review 6.5% more code after\nCopilot’s introduction, but show a 19% drop in their original code productivity. More broadly, this finding raises caution\nthat productivity gains of AI may mask the growing burden of maintenance on a shrinking pool of experts, together with\nincreased technical debt for the projects. The results highlight a fundamental tension in AI-assisted software development\nbetween short-term productivity gains and long-term system sustainability.\nKey words: GenAI, GitHub Copilot, Open Source Software, Software Maintenance, Technical Debt,\nDifference-in-Differences\n1.\nIntroduction\nHow will AI shape the future of knowledge-intensive industries? This question has taken on renewed\nsignificance with the recent rise of Genarative AI (GenAI) technologies, which are becoming an integral part\nof daily operations of software development, scientific research, healthcare and other expert-driven fields.\nA prominent example is GitHub Copilot, an AI-powered coding assistant designed to support developers by\ngenerating code suggestions and accelerating routine programming tasks (Peng et al. 2023). When GitHub\nlaunched Copilot, it was introduced as “your AI pair programmer,\" emphasizing not only its role as an\nautomation tool but also as a team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming"
    },
    {
      "rank": 4,
      "distance_l2": 0.5794616341590881,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_004",
      "text": " from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators… \n(1) … efficiency \n(2) … accuracy \n(3) … the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is one of several GenAIs that can assist developers by \ngenerating, completing and modifying programming code \nbased on the context of the codebase and natural language \nprompts. \nIt \nis \nintegrated \nin \nsoftware \ndevelopment \nenvironments (IDEs) like VS Code and can function as an “AI \npair programmer” for developers.  \nIn recent years, multiple studies have been conducted to \nevaluate Copilot’s output quality and developers’ productivity \ngains through AI usage. Based on benchmark tasks, Dakhel et \nal. [32] found that Copilot can generate solutions for nearly all \ngiven tasks, but its outputs are less often correct than those of \nhumans, making it a potential asset for experienced \ndevelopers but a liability for novices who may not detect non-\noptimal suggestions. So far, there is no consensus on how to \nbest measure software developers’ productivity, but time \nsavings, acceptance rate of Copilot suggestions, and successful \ncompletion of predefined tasks are frequently considered \nmetrics to evaluate developer productivity in the context of \nAI-assisted programming. Empirical studies using these \nmetrics report mixed results: Bakal et al. [1]  found that \ndevelopers using Copilot accepted 33% of suggestions and \nhad a 20% increase in lines of code; Peng et al. [37] observed \nthat programmers completed an HTTP server task 55.8% \nfaster with Copilot, with the largest productivity gains among \nless experienced programmers; while Vaithilingam et al. [45] \nfound no significant improvement in task time or success, \nthough participants still valued Copilot as a useful starting \npoint for daily programming tasks. In this study, we consider \ntask duration, the number of Copilot’s suggestions, and \nsuccessful \ntask \ncompletion \nas \nproductivity \nmetrics. \nAdditionally, we examine the impact of Copilot interaction \ntypes, \ndevelopers’ \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers’ Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers’ \nexperiences during work [6, 17, 36]. However, only a few \n"
    },
    {
      "rank": 5,
      "distance_l2": 0.6216663122177124,
      "source_id": "CopilotQuality2022",
      "chunk_id": "CopilotQuality2022_chunk_003",
      "text": ". GitHub Copilot generates the code\nand presents the results of the OpenAI Codex Model by adjusting\nthe generated code to the current workspace of the programmer\n[4]. The Codex model relies on Generative Pre-trained Transformer\n(GPT) models that the company previously invented for text gen-\neration. The public code available on GitHub was used during the\nfine-tuning of the model to implement the code recognition and\ngeneration capabilities.\nThere are mixed reviews about the prospect of the GitHub Copi-\nlot. On the one hand, reducing development time, easing the devel-\nopment process by suggesting code for small utilities, and suggest-\ning better alternatives for code snippets are some of the positive\nfeedback developers provided [2, 7, 13]. On the other hand, it is\nargued that the current state of technology is not promising enough\nto match human ingenuity. Considering the previous studies, the\nservice requires a vast amount of human interaction, making the\ncoding routine still heavily reliant on the programmer [1].\nThe reviews about GitHub Copilot we touched upon only in-\nclude brief and heuristic feedback in terms of the evaluation of the\nservice. We agree with the general consensus of the opinions about\nGitHub Copilot and find it worthwhile to evaluate the possible en-\nhancements a service like GitHub Copilot can offer. Clearly, GitHub\nCopilot is capable of generating code, but its value is undetermined.\nTo systematically evaluate GitHub Copilot, we propose to construct\nan experimental setup to assess the generated code in terms of\nvalidity, correctness, and efficiency. In this context, we defined the\nfollowing research questions:\nRQ1 What is the quality of the code generated by GitHub Copilot?\nRQ1.1 How valid are GitHub Copilot’s code suggestions?\nRQ1.2 How correct are GitHub Copilot’s code suggestions?\nRQ1.3 How efficient are GitHub Copilot’s code suggestions?\nRQ2 What is the effect of using the docstrings on the generated\ncode quality?\nRQ3 What is the effect of using appropriate function names on the\ngenerated code quality?\nIn the following sections, we first elaborate on our experimental\nsetup in Section 2. In Section 3, we present the results we gathered\nfrom our setup. In Section 4, we share and evaluate our results. In\n1copilot.github.com\n2openai.com/blog/openai-codex/\n3github.com/features/codespaces\n62\nPROMISE ’22, November 17, 2022, Singapore, Singapore\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun\nSection 5, we discuss factors that might influence the validity of our\nresults. We provide an overview of other works that study GitHub\nCopilot in Section 6. Lastly, we conclude our study in Section 7.\n2\nMETHODOLOGY\nIn our experiment, we used HumanEval dataset [3], which is de-\nscribed in Section 2.1. To address the research questions, we created\nan experimental setup, which systematically evaluates the effective-\nness of GitHub Copilot that is described in Section 2.2. The details\nof our assessment are presented in Sections 2.3–2.5. In Sections\n2.6 and 2.7, we elaborate on the two additional experiments we\ncon"
    },
    {
      "rank": 6,
      "distance_l2": 0.623735785484314,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_002",
      "text": " team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming tasks 55.8% faster (Peng\net al. 2023). Such productivity benefits lead to promises of faster time-to-market and increased revenue\nfor organizations developing software applications. Considering these shifts, major tech organizations have\nstarted to increasingly rely on AI in their projects - “more than a quarter of all new code at Google is gen-\nerated by AI, then reviewed and accepted by engineers,\" reported Google CEO Sundar Pichai in January,\n2025.1 Moreover, Microsoft CTO Kevin Scott expects that 95% of all code will be AI-generated by 2030.2\nWhile these productivity gains are promising, they also raise important questions about the quality and\nmaintainability of AI-generated code. Because AI tools can lower the skill barrier for writing code (Dakhel\net al. 2023), AI tools enable broader participation but may also encourage developers to rely on gener-\nated solutions without fully understanding the underlying design rationale and potential integration issues\n(Barrett et al. 2023). Such reliance increases the likelihood of quick fixes that favor short-term function-\nality over long-term maintainability (Barrett et al. 2023). Extant literature characterizes “quick and dirty”\nsoftware customizations made without a complete understanding of their future implications as technical\ndebt, as they undermine system reliability and impose long-term maintenance obligations (Kruchten et al.\n2012, Brown et al. 2010, Banker et al. 2021). As a result, project maintainers must devote additional effort\nto understanding, reviewing, and reworking AI-generated code before it can be safely integrated. In our\ncontext, we contend that the growing reliance on AI-assisted development may accelerate the accumulation\nof technical debt, as design shortcuts taken to expedite system deployment become embedded in software\nsystems (Ramasubbu and Kemerer 2016, 2021).\nThe technical debt and maintenance challenges that AI poses are expected to be especially pronounced\nin distributed software development teams, such as in Open Source Software (OSS) communities. In these\ncommunities, contributors from around the world collaborate, often voluntarily, to develop and maintain\nsoftware that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making\nit freely or cheaply available to the public (Eghbal 2020, Nagle 2019). Despite the voluntary nature of\nwork in these communities, OSS constitutes critical digital infrastructure for modern society, with estimates\nsuggesting that the total cost of reproducing this software would amount to $8.8 trillion (Hoffmann et al.\n2024).3 Given"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}