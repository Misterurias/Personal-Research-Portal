{
  "timestamp": "2026-03-02T04:41:40+00:00",
  "query": "Is AI-assisted code more error-prone than human-written code?",
  "retrieval_query": "Code written with artificial intelligence assistance compared to human-written code: is it more prone to errors?",
  "query_rewritten": true,
  "rewritten_query": "Code written with artificial intelligence assistance compared to human-written code: is it more prone to errors?",
  "top_k": 6,
  "answer": "I'll only answer based on the provided evidence chunks. Please note that these answers are limited to the information presented in the evidence chunks.\n\nPlease provide a specific question or topic you'd like me to address, and I'll respond accordingly.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.7186123132705688,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_002",
      "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n∗Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n†Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
    },
    {
      "rank": 2,
      "distance_l2": 0.7955730557441711,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_017",
      "text": " \nfor Computing Machinery, Article 332 (2022), 7; \n10.1145/3491101.3519665\n24.\t Wagner, S. and Ruhe, M. A systematic review of \nproductivity factors in software development. arXiv \npreprint arXiv:1801.06475 (2018).\n25.\t Wang, D. et al. From human-human collaboration to \nhuman-AI collaboration: Designing AI systems that \ncan work together with people. In Proceedings of \nthe 2020 CHI Conf. on Human Factors in Computing \nSystems (2020), 1–6.\n26.\t Weisz, J.D. et al. Perfection not required? Human-AI \npartnerships in code translation. In Proceedings of \nthe 26th Intern. Conf. on Intelligent User Interfaces, T. \nHammond et al (eds). ACM, (April 2021), 402–412; \n10.1145/3397481.3450656\n27.\t Winters, T., Manshreck, T., and Wright, H. Software \nEngineering at Google: Lessons Learned from \nProgramming Over Time. O’Reilly Media (2020).\n28.\t Wold, S., Sjöström, M., and Eriksson, L. PLS-regression: \nA basic tool of chemometrics. Chemometrics and \nIntelligent Laboratory Systems 58, 2 (2001), 109–130; \n10.1016/S0169-7439(01)00155-1.\n29.\t Zhou, W., Kim, S., Murali, V., and Ari Aye, G. Improving \ncode autocompletion with transfer learning. \nCoRR abs/2105.05991 (2021); https://arxiv.org/\nabs/2105.05991\nAlbert Ziegler (wunderalbert@github.com ) is a principal \nresearcher at GitHub, Inc., San Francisco, CA, USA.\nEirini Kalliamvakou is a staff researcher at GitHub, Inc., \nSan Francisco, CA, USA.\nX. Alice Li is a staff researcher for Machine Learning at \nGitHub, San Francisco, CA, USA.\nAndrew Rice is a principal researcher at GitHub, Inc., San \nFrancisco, CA, USA.\nDevon Rifkin is a principal research engineer at GitHub, \nInc., San Francisco, CA, USA.\nShawn Simister is a staff software engineer at GitHub, \nInc., San Francisco, CA, USA.\nGanesh Sittampalam is a principal software engineer at \nGitHub, Inc., San Francisco, CA, USA.\nEdward Aftandilian is a principal researcher at GitHub, \nInc., San Francisco, CA, USA.\nFigure 6. Average acceptance rate during the week. Each point represents the average \nfor a one-hour period, whereas the shaded ribbon shows the min-max variation during \nthe observed four-week period.\nSaturday\n12:00\n26%\noff hours\nDaily and weekly patterns in acceptance rate in the US\n(all users between 2022-01-15 and 2022-02-12)\nweekend\nworking hours\n24%\n22%\n20%\nSunday\n12:00\nMonday\n12:00\nTuesday"
    },
    {
      "rank": 3,
      "distance_l2": 0.8058082461357117,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_020",
      "text": "\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\ndirectly pasted AI output. This suggests that spending more time manually typing may not yield better\nconceptual understanding. Cognitive effort may be more important than the raw time spent on completing\nthe task.\n6.2\nEncountering Errors\nThe way participants encountered and resolved errors was notably different between the treatment and control\nconditions. In the platform, participants could use the run button or the terminal to run their code as often\nas they wanted. In general, most of the participants ran the code for the first time after trying to complete\nmost of the question and ran the code again only after the changes were made. We recorded every error\nencountered by each participant as we watched the screen recordings of the task progress.\n15\nQuery Type\nExample Query\nExplanation (q=79)\n“can trio.sleep use partial seconds?”\n“Can you remind me what the different trio async operations are?”\n“Looks good, can you give me a really brief overview of the general idea behind\nall of this?”\nGeneration (q=51)\n“given this instruction to trio, can you implement the missing bits of main.py?”\n“complete get_user_data”\n“implement delayed_hello(). It should simply sleep for 2.1 seconds upon which\nit prints ’Hello World!’ ”\nDebugging (q=9)\n“Does that look right? If so let’s move on to delayed_hello()”\n“I’m having issues getting my code to work. I’m getting a notimplementederror\nfor delayed_hello”\nPasted\nError\n(e.g.,\n“Traceback\n(most\nrecent\ncall\nlast):\nFile\n\"/user-\ncode/FILESYSTEM/main.py3\", line 81, in... ”)\nCapabilities\nQues-\ntion (q=4)\n“Can you see the current question?”\n“So what can you do for me here? Can you write code directly into the file?”\n“Are you aware of how trio works? Are there parallels in its execution model to\nanother library I’d be more familiar with like asyncio”\nAppreciation (q=4)\n“Great job, we got the expected output on the first try.”\n“Looks like it worked, thanks!”\n“Trueeee!”\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI ("
    },
    {
      "rank": 4,
      "distance_l2": 0.8400654792785645,
      "source_id": "ExpectationVsExperience2022",
      "chunk_id": "ExpectationVsExperience2022_chunk_005",
      "text": " increased unless an explanation was provided. However,\nthese explanations may also lead to over-reliance on the system\neven when unwarranted, signaling the importance and difficulty\nof providing explanations that help people to calibrate trust appro-\npriately. Kocielnik et al. [23] examined the effect of giving people\ncontrol over the types of errors made by a scheduling assistant,\neither by avoiding false positives or false negatives. They found\nthat even when the system was only 50% accurate, users who ex-\npected a reduction in the false positive rate had a lower perception\nof accuracy and lower acceptance of the system than the users who\nexpected a reduction in the false negative rate. [3, 52] showed that\nconfidence scores helped calibrate users’ trust, form a good mental\nmodel of the AI, and understand the error boundaries better.\nSimilar to other AI techniques, AI-based code generation tools\nalso suffer from inherent uncertainty and imperfection. They may\ninevitably generate code with errors or even code that wildly differs\nfrom users’ expectations. However, unlike other domains, code\ngeneration demands a much higher level of correctness: code either\ncompiles or not, and it is either correct or contains bugs such as\nlogic errors and security vulnerabilities. Therefore, existing findings\nof other types of AI techniques may not generalize to the domain\nof code generation.\nCurrently, there are only a few studies on how programmers\nuse such imperfect code generation tools [44, 47]. Xu et al. [47] did\na user study with 31 participants to evaluate the usefulness of a\nNL-to-code plugin [46]. They found that there was no statistically\nsignificant difference in task completion time or task correctness\nscores when using or not using the NL-to-code plugin. Furthermore,\nmost participants stayed neural or somewhat positive to the NL-\nto-code plugin. The main reason for these negative results was the\ncorrectness and quality of generated code as pointed out by many\nparticipants in the post-study survey. However, these findings may\nnot hold as more recent large language models have significantly\nboosted the correctness and quality of generated code. This further\nmotivates us to conduct the user study with Copilot.\nWeisz et al. [44] interviewed 11 software engineers at IBM and\nsolicited their feedback on a neural machine translation (NMT)\nmodel for an adjacent domain—translating code from one program-\nming language to another. They found that the user’s acceptance\nof the NMT model was contingent on the number of errors in the\ntranslated code. They also identified several common themes in\nparticipants’ feedback such as acceptance via verification and the\ndesire to provide guidance to the NMT model. Our study was de-\nsigned to complement this knowledge but for daily programming\ntasks.\n3\nSTUDY DESIGN\nTo understand how programmers use an LLM-based code genera-\ntion tool, we designed and carried out a within-subjects comparative\nstudy with 24 participants. For the control condition, each partici-\npant was asked to complete a Python programming task in Visual\nStudio Code (VSCode) IDE with the default code completion tool\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI ’22 Extended Abstracts, April 29-May 5, 2022, New Orleans"
    },
    {
      "rank": 5,
      "distance_l2": 0.8466852903366089,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_008",
      "text": " record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants’ current version of the code and can produce the full, correct code\nfor both tasks directly when prompted.\n4.2\nEvaluation Design\nBased on a previous meta-analysis of evaluations in computer science education [Cheng et al., 2022], we\nidentify four types of questions used to assess the mastery of coding skills. Returning to our initial motivation\nof developing and retaining the skills required for supervising automation, proficiency in some of these areas\nmay be more important than others for the oversight of AI-generated code. The four types of questions we\nconsider are the following.\n• Debugging The ability to identify and diagnose errors in code. This skill is crucial for detecting when\nAI-generated code is incorrect and understanding why it fails.\n• Code Reading The ability to read and comprehend what code does. This skill enables humans to\nunderstand and verify AI-written code before deployment.\n• Code Writing The ability to write or pick the right way to write code. Low-level code writing, like\nremembering the syntax of functions, will be less important with further integration of AI coding tools\nthan high-level system design.\n• Conceptual The ability to understand the core principles behind tools and libraries. Conceptual\nunderstanding is critical to assess whether AI-generated code uses appropriate design patterns that\nadheres to how the library should be used.\nThe two tasks in our study cover 7 core concepts from the Trio library. We designed a quiz with debugging,\ncode reading, and conceptual questions that cover these 7 concepts. We exclude code writing questions to\nreduce the impact of syntax errors in our evaluation; these errors can be easily corrected with an AI query or\nweb search. We tested 5 versions (Table 2) of the quiz in user testing and preliminary studies based on item\nresponse theory. For example, we ensure that all questions are sufficiently correlated with the overall quiz\nscore, that each question has an appropriate average score, and that the questions are split up such that there\nis no local item dependence between questions (i.e., participants could not infer the answers to a question by\nlooking at other questions). The final evaluation we used contained 14 questions for a total of 27 points. We\nsubmitted the grading rubric for the quiz in our study pre-registration before running the experiment.\n4.3\nStudy Design\nWe use a between-subjects randomized experiment to test for the effects of using AI in the coding skill\nformation process. Each participant first completed a warm-up coding task on a coding platform, where they\nneeded to add a border around a list of strings. This Python coding question takes an average of 4 minutes to\ncomplete among users of this coding platform. There are no asynchronous concepts in this coding question.\n6\nFigure 4: Overview of learning task and"
    },
    {
      "rank": 6,
      "distance_l2": 0.8502850532531738,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023",
      "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1–7.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1–24.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402–412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369–391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172–178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I 23. Springer, 171–190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1–47.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585–1608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot’s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62–71.\n[59] Ramaz"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}