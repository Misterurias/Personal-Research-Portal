{
  "timestamp": "2026-03-02T04:39:55+00:00",
  "query": "What quality metrics are used to evaluate AI-generated code?",
  "retrieval_query": "Evaluate AI-generated code using \"quality metrics\" for software engineering.",
  "query_rewritten": true,
  "rewritten_query": "Evaluate AI-generated code using \"quality metrics\" for software engineering.",
  "top_k": 6,
  "answer": "I'll only answer based on the provided evidence chunks.\n\nThere is no specific question asked. If you'd like me to provide an answer based on the provided evidence chunks, please let me know what type of information you're looking for (e.g., code quality metrics, productivity results, etc.).",
  "citations": [
    {
      "source_id": "e.g.",
      "chunk_id": "code quality metrics, productivity results, etc."
    }
  ],
  "citations_valid": false,
  "invalid_citations": [
    [
      "e.g.",
      "code quality metrics, productivity results, etc."
    ]
  ],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5058046579360962,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_004",
      "text": " al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming languages used [21]. Nguyen and colleagues put forward a new set of metrics for \nmeasuring AI-generated code and reported that, although AI-powered software is very good at producing \nsyntactically correct code, it often strays from the best design patterns [22]. \nAmong the critical issues raised security implications have been the foremost. Static analysis studies \nshowed that AI systems might learn from their training data and thus recreate the same weaknesses as in \nthe case of existing vulnerabilities [23]. Pearce and co-workers pointed out the presence of a vulnerable \npattern that was quite common in GitHub Copilot's code which included SQL injection, cross-site \nscripting, and use of insecure cryptographic methods [24]. \nNevertheless, there are still some limitations that are inherent to such a growing research area. The \nmajority of research works have been limited to the evaluation of just a few tools and or small domains \nmaking the possibility of drawing general conclusions quite difficult. Only a couple of research works \nhave dealt with long-term effects of refactoring, skilled programmers, and maintenance costs. Moreover, \nthe interaction between the experience level of the developer and the effectiveness of the AI tool is still to \nbe fully explored [25]. Through a thorough and multi-language study involving developers of different \nexperience levels working on varied software engineering tasks, we fill these gaps. \n \n3. Methodology \n3.1 Research Design \nThis research utilized a between-subjects experimental design to assess the influences of AI-assisted code \ngeneration tools on software development outcomes. The independent variable was the presence of AI-\nassisted coding tools (experimental group vs. control group), while the dependent variables were the \nquality metrics, security vulnerability counts, and productivity measures of the coded produced. The \nexperiment was set up in such a way that it reduced the possibility of confounding variables while keeping \nthe ecological validity through the use of realistic programming tasks that are typical of professional \nsoftware development scenarios. Figure 1 shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2"
    },
    {
      "rank": 2,
      "distance_l2": 0.5457220077514648,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_003",
      "text": "olve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious programming languages and projects. Finally, the research gives the software organizations that \nwant to use AI tools good insights and practices for risk reduction. Thus, the implications of our results \nare very important for the education of software engineers, the industry's practices and the direction of \nfuture research in the area of AI and software development. \n \n2. Literature Review \nThe areas where artificial intelligence and software engineering meet have become the center of a huge \nnumber of research studies, with code generation and program synthesis being the two main areas. The \nfirst automated code generation attempts relied on template-based methods and rule-based systems \nproducing the so-called boilerplate codes from high-level specifications [10]. The deep learning era totally \nchanged the picture, with the application of sequence-to-sequence models and recurrent neural networks \nto the code synthesis tasks [11]. The introduction of the transformer architectures was the turning point, \nwith models such as CodeBERT and GraphCodeBERT being able to perform on a par with the best \nmethods in the problem categories of code understanding and generation [12, 13]. \nRecently, the development of large language models has further changed the scenery in code generation. \nFor instance, GPT-3 showed outstanding learning ability through a few examples for programming tasks \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n3\n \n[14], while Codex, the engine behind GitHub Copilot, scored highly in competitive programming \nproblems [15]. The following studies have looked into the models' capabilities in various programming \nlanguages and difficult algorithms [16, 17]. \nThe findings of empirical studies of AI-assisted coding tools presented a somewhat mixed picture. Ziegler \net al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming"
    },
    {
      "rank": 3,
      "distance_l2": 0.5707787275314331,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 4,
      "distance_l2": 0.585599958896637,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 5,
      "distance_l2": 0.6081095933914185,
      "source_id": "AIReview2025",
      "chunk_id": "AIReview2025_chunk_002",
      "text": "5] and code generation [6]. This review aims to \nsystematically review and analyze the current application status and research progress of AI in \nsoftware program development and code generation and review. The analysis focuses on two aspects: \none is AI-assisted intelligent code generation, and the other is AI-powered code review. Through in-\ndepth exploration of these cutting-edge studies, the value of this review lies in clarifying how AI can \nassist developers in performing more convenient programming tasks and enabling the early detection \nof program defects as well as the control of the root causes of performance issues. This not only \nsignificantly enhances the efficiency and code delivery speed of the development team, but more \nimportantly, it greatly improves the reliability and quality of the final software program [7], thereby \nreducing operational costs and driving the entire software engineering field towards a higher-level \nintelligent-assisted development paradigm. \n237 \n2. Literature Review \nIn recent years, the field of software engineering has developed rapidly. Software engineering \nencompasses the systematic and controlled design, development, maintenance, implementation and \nevolution of software systems [1]. As artificial intelligence has become a highly popular field at \npresent, it has also become a new component of software engineering [8]. To assist in software \ndevelopment, AI is now widely used in scenarios such as code generation and code review [9]. In \nparticular, large language models, as the latest breakthrough in the field of natural language \nprocessing in AI, have made particularly significant contributions to software development. LLM \nmainly employs the Transformer model as its core architecture. The LLM, which is built on large \ndatasets and advanced neural network architectures, demonstrates extremely high comprehension \ncapabilities, bringing about significant breakthroughs and possibilities to the field of software \ndevelopment [10]. For instance, models such as Codex [11], StarCode [12], Incoder [13], and Code \nLlama [14], Copilot[15]can all generate code with efficiency and accuracy comparable to that of \ndevelopers. These LLMs not only have extremely high efficiency but also excellent quality in code \nreview [10].  \n2.1 AI-Assisted Intelligent Code Generation \nCode generation is an automated process that converts structured or unstructured input information \n(such as natural language requirements descriptions, design documents, code snippets, etc.) into \nsource code. Its essence is to reflect the abstract intentions and task goals of the developers into \nspecific programming projects [16]. And based on LLM (Large Language Model) for code generation, \nby breaking down the tasks, having data storage with long-term and short-term memories, as well as \nthe invocation of external tools, these are currently important technical supports in the field of code \ngeneration [16]. This section will summarize the application effects and code generation quality of \nCodex and Copilot in the field of code generation. \n2.1.1 Code Llama \nThe Code Llama model series released by Meta AI focuses on code generation through long \nsequence contexts and instruction fine-tuning, and its mechanism is of great significance for the \ngeneration of complex logic at the back end [14]. \n2.1.1.1 Strong Context-Dependent and Long Sequence Processing Capabilities \nModern backend development typically involves complex distributed systems and microservice \narchitectures, and code generation must take into account global dependencies and architectural \nspecifications. Code L"
    },
    {
      "rank": 6,
      "distance_l2": 0.6452646255493164,
      "source_id": "CopilotQuality2022",
      "chunk_id": "CopilotQuality2022_chunk_002",
      "text": " validity, correctness, and efficiency. Our results suggest that\nGitHub Copilot was able to generate valid code with a 91.5% success\nrate. In terms of code correctness, out of 164 problems, 47 (28.7%)\nwere correctly, while 84 (51.2%) were partially correctly, and 33\n(20.1%) were incorrectly generated. Our empirical analysis shows\nthat GitHub Copilot is a promising tool based on the results we\nobtained, however further and more comprehensive assessment is\nneeded in the future.\nCCS CONCEPTS\n• Software and its engineering →Source code generation.\nKEYWORDS\nGitHub Copilot, code generation, code completion, AI pair program-\nmer, empirical study\nACM Reference Format:\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the Quality of\nGitHub Copilot’s Code Generation. In Proceedings of the 18th International\nConference on Predictive Models and Data Analytics in Software Engineering\n(PROMISE ’22), November 17, 2022, Singapore, Singapore. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3558489.3559072\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nPROMISE ’22, November 17, 2022, Singapore, Singapore\n© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9860-2/22/11...$15.00\nhttps://doi.org/10.1145/3558489.3559072\n1\nINTRODUCTION\nGitHub Copilot1 is a code generation tool that utilizes a variety of\ntechnologies, including a compatible IDE, and the OpenAI Codex\nModel2. GitHub Copilot can be installed and used as an extension to\nVisual Studio Code, Neovim, IDEs developed by JetBrains [6], and\nGitHub Codespaces3. The underlying service continuously takes\ncode samples from the users and sends the snippets to the under-\nlying OpenAI Codex Model. GitHub Copilot generates the code\nand presents the results of the OpenAI Codex Model by adjusting\nthe generated code to the current workspace of the programmer\n[4]. The Codex model relies on Generative Pre-trained Transformer\n(GPT) models that the company previously invented for text gen-\neration. The public code available on GitHub was used during the\nfine-tuning of the model to implement the code recognition and\ngeneration capabilities.\nThere are mixed reviews about the prospect of the GitHub Copi-\nlot. On the one hand, reducing development time, easing the devel-\nopment process by suggesting code for small utilities, and suggest-\ning better alternatives for code snippets are some of the positive\nfeedback developers provided [2, "
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}