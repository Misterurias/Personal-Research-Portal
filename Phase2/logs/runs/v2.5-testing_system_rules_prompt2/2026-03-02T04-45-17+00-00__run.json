{
  "timestamp": "2026-03-02T04:45:17+00:00",
  "query": "How do AI coding tools affect junior versus senior developers?",
  "retrieval_query": "Impact of AI coding tools on developer productivity by junior and senior developers.",
  "query_rewritten": true,
  "rewritten_query": "Impact of AI coding tools on developer productivity by junior and senior developers.",
  "top_k": 6,
  "answer": "According to the evidence chunks provided, here are some answers:\n\n1. How does GenAI interaction impact the productivity indicators...?\n\nFrom [DevExperienceGenAI2025_chunk_004 | DevExperienceGenAI2025], it is mentioned that several studies in software engineering have demonstrated benefits of using mixed-methods approaches that combine objective telemetry, physiological measures and subjective data to provide deeper insights into developers’ experiences during work.\n\n2. What are the productivity metrics used in this study?\n\nAccording to [CopilotExperiment2023_chunk_001 | CopilotExperiment2023], the productivity metrics used in this study include task duration, the number of Copilot’s suggestions, and successful task completion.\n\n3. What is the main finding of this study?\n\nThe main finding of this study is that the treated group (with access to GitHub Copilot) completed the task 55.8% faster than the control group (without access to GitHub Copilot), with a statistically significant difference (95% confidence interval: 21-89%).\n\n4. What are the implications of this study?\n\nThis study has major implications for the labor market and firms, including changes in employment, skills, and firm organization [Raj and Seamans, 2018; Agrawal et al., 2019].",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5682651400566101,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
      "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
    },
    {
      "rank": 2,
      "distance_l2": 0.6079787015914917,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_004",
      "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60–62]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: “acceleration mode” in which the tool aided\ndevelopers when they knew what to do next, and “exploration mode” to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM’s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as “WCA@IBM,” was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
    },
    {
      "rank": 3,
      "distance_l2": 0.6341111660003662,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_005",
      "text": " underlying mechanism suggests that \nthe observed increase in project-level code contributions is accompanied by a significant increase in both \nindividual code contributions and developer participation. At the same time, the increase in coordination \ntime is driven by a higher volume of discussions surrounding code contributions, a broader set of developers \nparticipating in these discussions, and greater discussion intensity per developer. Importantly, the combined \neffect of these two competing forces still yields an overall positive effect on the project-level productivity, \nmeasured by the total code contributions with timely integration into the codebase. \nFurthermore, we find that compared to core developers, AI pair programmers lead to a smaller \nincrease in project-level code contributions made by peripheral developers; following the adoption of AI \npair programmers, there is also a larger increase in coordination time for integrating code contributed by \nperipheral developers. These results are consistent with our hypothesis that due to the different levels of \nproject familiarity held by core versus peripheral developers and the limitations of generative AI tools, \nperipheral developers may realize less productivity gain from AI pair programmers than core developers. \nOur study provides several contributions to the literature. First, it contributes to the literature on \ngenerative AI in software development (Imai 2022, Barke et al. 2023, Peng et al. 2023, Cui et al. 2024). \nWhile prior research has shown that generative AI improves individual developer productivity (e.g., Peng \net al. 2023; Cui et al. 2024), less is known about its impact on voluntary participation in collaborative \n 5 \nsoftware development. Existing studies suggest that generative AI might reduce voluntary participation in \nQ&A communities by substituting for information exchange (Xu et al. 2023, Burtch et al. 2024). However, \nOSS communities are fundamentally different in that they involve not only information sharing but also \ncomplex problem solving and team collaboration. To our knowledge, we are among the first to show \ngenerative AI encourages more participation in OSS development, including both coding and non-coding \nparticipation (i.e., code discussions). \nSecond, our study contributes to the literature on generative AI in team-based collaboration (Li et \nal. 2024, Dell'Acqua et al. 2025). While prior work has examined the impact of generative AI within \ntraditional teams characterized by fixed size and formal coordination processes for performing a common \ntask, we extend this work to open collaborative environments, where team composition is fluid, \nparticipation is voluntary, and individuals perform distinct tasks that must be integrated. Our study is among \nthe first to uncover some unexpected impacts of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023"
    },
    {
      "rank": 4,
      "distance_l2": 0.6425879001617432,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_006",
      "text": " of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023, Cui et al. 2024). Our results draw a sharp contrast with \nthese findings, as we demonstrate that peripheral developers obtain less productivity gain from AI pair \nprogrammers than core developers in OSS settings, potentially because the former do not possess important \nand necessary contextual knowledge about an OSS project to effectively use the AI tools. \n2. Literature Review  \n2.1 Generative AI in Software Development \nA growing body of literature has started to examine the impact of generative AI on software development. \nMost studies have focused on the implications of generative AI for individual productivity on specific tasks. \n 6 \nFor example, Imai (2022) finds that GitHub Copilot produces more lines of code than a human pair \nprogrammer when completing a task in Python. Peng et al. (2023) find that GitHub Copilot enables \nindividual developers to implement an HTTP server 55.8% faster than those not using the tool. Hoffmann \net al. (2024) show that GitHub Copilot causes individual developers to shift focus towards coding tasks and \naway from project management.  \nDespite these insights, research on how generative AI influences project-level outcomes for \ncomplex tasks involving multiple developers remains limited. Yeverechyahu et al. (2024) investigate the \ninnovation capabilities of generative AI, particularly its role in extrapolative versus interpolative thinking, \nand compare its influence on innovation in Python versus R. Different from the existing literature and built \nupon the OSS literature, our hypotheses are motivated by the unique characteristics of OSS, namely, the \nsoftware development process in an open and collaborative environment. Because participation is often \nvoluntary and coordination does not follow formal centralized processes in this environment, it remains \nunclear how generative AI influences open participation in both coding and non-coding activities, as well \nas team coordination, all of which could have important implications for project-level software \ndevelopment productivity. \nIn addition, existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al."
    },
    {
      "rank": 5,
      "distance_l2": 0.6644496917724609,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_004",
      "text": " from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators… \n(1) … efficiency \n(2) … accuracy \n(3) … the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is one of several GenAIs that can assist developers by \ngenerating, completing and modifying programming code \nbased on the context of the codebase and natural language \nprompts. \nIt \nis \nintegrated \nin \nsoftware \ndevelopment \nenvironments (IDEs) like VS Code and can function as an “AI \npair programmer” for developers.  \nIn recent years, multiple studies have been conducted to \nevaluate Copilot’s output quality and developers’ productivity \ngains through AI usage. Based on benchmark tasks, Dakhel et \nal. [32] found that Copilot can generate solutions for nearly all \ngiven tasks, but its outputs are less often correct than those of \nhumans, making it a potential asset for experienced \ndevelopers but a liability for novices who may not detect non-\noptimal suggestions. So far, there is no consensus on how to \nbest measure software developers’ productivity, but time \nsavings, acceptance rate of Copilot suggestions, and successful \ncompletion of predefined tasks are frequently considered \nmetrics to evaluate developer productivity in the context of \nAI-assisted programming. Empirical studies using these \nmetrics report mixed results: Bakal et al. [1]  found that \ndevelopers using Copilot accepted 33% of suggestions and \nhad a 20% increase in lines of code; Peng et al. [37] observed \nthat programmers completed an HTTP server task 55.8% \nfaster with Copilot, with the largest productivity gains among \nless experienced programmers; while Vaithilingam et al. [45] \nfound no significant improvement in task time or success, \nthough participants still valued Copilot as a useful starting \npoint for daily programming tasks. In this study, we consider \ntask duration, the number of Copilot’s suggestions, and \nsuccessful \ntask \ncompletion \nas \nproductivity \nmetrics. \nAdditionally, we examine the impact of Copilot interaction \ntypes, \ndevelopers’ \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers’ Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers’ \nexperiences during work [6, 17, 36]. However, only a few \n"
    },
    {
      "rank": 6,
      "distance_l2": 0.671136200428009,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}