{
  "timestamp": "2026-03-02T04:38:16+00:00",
  "query": "How do AI coding assistants affect task completion time?",
  "retrieval_query": "Effect of AI coding assistants on task completion time.",
  "query_rewritten": true,
  "rewritten_query": "Effect of AI coding assistants on task completion time.",
  "top_k": 6,
  "answer": "According to the provided evidence chunks, here are some answers:\n\n**RQ1: Does AI assistance improve task completion productivity when new skills are required?**\n\n* There is no notable difference between groups that typed vs directly pasted AI output in terms of skill formation (measured by quiz score). This suggests that spending more time manually typing may not yield better conceptual understanding. (Evidence Chunk 6.1)\n* The way participants encountered and resolved errors was notably different between the treatment and control conditions. In general, most participants ran the code for the first time after trying to complete most of the question and ran the code again only after the changes were made. (Evidence Chunk 6.2)\n\n**RQ2: How does using AI assistance affect the development of these new skills?**\n\n* Only a subset of AI-assisted interactions yielded productivity improvements. For example, some participants only asked AI assistant clarification questions and demonstrated high proficiency. (Evidence Chunk 11)\n* There was no notable difference in skill formation between groups that used the AI assistant and those that did not. This suggests that AI assistance may not provide a shortcut to enhance both immediate productivity and longer-term skill development. (Evidence Chunk 6.1)",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5909684896469116,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_018",
      "text": " AI assistant\nis produced (annotated as an AI Query event).\nSince participants could ask the AI assistant as many questions as time allowed, a handful of participants\nasked more than five questions and spent up to six minutes composing a single query during this 35-minute\nassignment (Figure 12).5 Since the median completion time is only 19 minutes in the AI condition, spending\nup to 6 minutes composing a single query amounts to a significant amount of the total time spent interacting\nwith the AI assistant. Although this effect might be due to the short duration of our task, Becker et al. also\nfound a slowdown effect for expert coders on longer tasks when participants waiting for AI-written code may\nbecome distracted.\nHowever, from the lens of skill formation, the time spent composing queries may aid in better understanding\nthe task and, consequently, better acquisition of skills. Screen recordings show participants contemplating\nwhat to ask the AI assistant (e.g., rereading instructions and rewriting queries). As a result, some participants\ntook several minutes to compose a single query. Thus, while this time cost would be more prominent in\nchat-based assistants than agentic coding assistants, the loss in knowledge is likely even greater in an agentic\nor autocomplete setting where composing queries is not required. A more significant difference in completion\ntime due to shorter interactions with AI assistance would likely translate to an even larger negative impact\non skill formation. When we look at individual queries, not all queries involve significant thinking and time.\nThus, we analyze individual queries to better understand how participants from new skills.\nAI Queries\nWe categorized user inputs into the AI assistant, queries, into 5 broad categories: explanation,\ngeneration, debugging, capabilities questions, and appreciation (Table 3). The most common type of query\nwas explanations (q=79); users requested more information about the trio library, details about asynchronous\noperations, and high-level conceptual introductions. 21 out of 25 participants in the treatment group asked\nan explanation question; this reflects the high level of engagement among our participants. The second most\ncommon were queries asking for code to be generated (q=51); some participants asked for an entire task to\nbe completed, while other participants asked for specific functions to be implemented. Only 16 of 25 or two\nthirds of the participants used AI to generate code. 4 of these participants only asked for code generation\nand no other types of question. In fact, 3 of the 8 lowest-scoring participants asked AI to generate code\nwithout asking for explanations, suggesting that if all participants in the AI group were to use AI for solely\ngenerating code, the skill-formation differences compared to the control group would be even greater.\nA third category of common queries was debugging (q=9). Our tasks were designed to be straightforward,\nbut the participants still encountered various errors (Section 6.2). This is a broader category of queries that\nincludes errors directly pasted as input to the AI assistant as well as asking the AI assistant to confirm the\ncode written is correct. A higher fraction of debugging queries correlates with slower completion times (Figure\n18) and lower quiz scores (Figure 19). This suggests that relying on AI for debugging (e.g. repetatedly asking\nAI to check and fix things without understanding) when learning a new task is correlated with less learning.\n5Participants were instructed to"
    },
    {
      "rank": 2,
      "distance_l2": 0.7034982442855835,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_024",
      "text": "18\nscore) vs low-scoring interaction patterns (24%-39% quiz score). The high scorers only asked AI conceptual\nquestions instead of code generation or asked for explanations to accompany generated code; these usage\npatterns demonstrate a high level of cognitive engagement.\nContrary to our initial hypothesis, we did not observe a significant performance boost in task completion\nin our main study. While using AI improved the average completion time of the task, the improvement in\nefficiency was not significant in our study, despite the AI Assistant being able to generate the complete code\nsolution when prompted. Our qualitative analysis reveals that our finding is largely due to the heterogeneity\nin how participants decide to use AI during the task. There is a group of participants who relied on AI to\ngenerate all the code and never asked conceptual questions or for explanations. This group finished much\nfaster than the control group (19.5 minutes vs 23 minutes), but this group only accounted for around 20%\nof the participants in the treatment group. Other participants in the AI group who asked a large number\nof queries (e.g., 15 queries), spent a long time composing queries (e.g., 10 minutes), or asked for follow-up\nexplanations, raised the average task completion time. These contrasting patterns of AI usage suggest that\naccomplishing a task with new knowledge or skills does not necessarily lead to the same productive gains as\ntasks that require only existing knowledge.\nTogether, our results suggest that the aggressive incorporation of AI into the workplace can have negative\nimpacts on the professional development workers if they do not remain cognitatively engaged. Given time\nconstraints and organizational pressures, junior developers or other professionals may rely on AI to complete\ntasks as fast as possible at the cost of real skill development. Furthermore, we found that the biggest difference\nin test scores is between the debugging questions. This suggests that as companies transition to more AI\ncode writing with human supervision, humans may not possess the necessary skills to validate and debug\nAI-written code if their skill formation was inhibited by using AI in the first place.\n7.1\nFuture Work\nOur work is a first step to understanding the impact of AI assistance on humans in the human-AI collaboration\nprocess. We hope that this work will motivate future work that addresses the following limitations:\n• Task Selection: This study focuses on a single task using a chat-based interface. This should be\na lower bound for cognitive offloading since agentic AI coding tools would require even less human\nparticipation. In our work, users who relied on AI without thinking performed the worst on the\nevaluation; a completely agentic tool would create a similar effect. Future work should investigate the\nimpacts of agentic coding tools on learning outcomes and skill development.\n• Task Length: Ideally, skill formation takes place over months to years. We measured skill formation for\na specific Python library over a one-hour period. Future work should study real-world skill development\nthrough longitudinal measurement of the impacts of AI adoption.\n• Participant Realism: While participants in our study were professional or freelance programmers,\nthere was not the same incentive to learn the library as if it were required for their actual job. Future\nstudies should aim at studying the skill acquisition fro novice workers within a real company.\n• Prompting Skills: We collect self-reported familiarity with AI coding tools, but we do not actually\nmeasure differences in prompting techniques."
    },
    {
      "rank": 3,
      "distance_l2": 0.7101227045059204,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_014",
      "text": "50\n60\n70\nScore %\nQuiz Score % by Subarea and Condition\nCondition\nAI\nNo AI\nFigure 8: Score breakdown by questions type relating to each task and skill area. Debugging questions\nrevealed the largest differences in average quiz score between the treatment and control groups.\nPrior works have presented mixed results on whether AI helps or hinders coding productivity [Peng et al.,\n2023, Becker et al., 2025]; our study differs from prior results in that it is designed to study how AI affects\nskill formation while performing a task requiring new knowledge. While we do observe a slightly lower average\ncompletion time in the AI group among novice programmers, due to the small group size of the 1-3 year\nparticipant group (n=4), the difference in task time was not significant. 4 of the 26 participants in the control\n(No AI) group did not complete the second task within the 35-minute limit, while every participant in the AI\ncondition completed the second task. Our results do not conclusively find a speed up or slow down using AI\nin this task.\nAcross all levels of prior coding experience, users scored higher on average in the control (no AI) than in\nthe treatment (AI assistance) group (Figure 7). This shows that our choice of tasks and task design did not\ncritically hinge on the participants’ experience level of the but presented new skills to be acquired for every\nexperience group.\nConcept Group Analysis In exploratory data analysis (not pre-registered), the quiz score was decomposed\ninto subareas and question types (Figure 8). Each question in the quiz belonged to exactly one task (e.g.,\nTask 1 or Task 2) and exactly one question type (e.g., Conceptual, Debugging, or Code Reading). For both\ntasks, there is a gap between the quiz scores between the treatment and control groups. Among the different\ntypes of questions, the largest score gap occurs in the debugging questions and the smallest score gap in\nthe code reading questions. This outcome is expected since treatment and control groups may have similar\nexposure to reading code through the task, but the control group with no access to AI assistance encountered\nmore errors during the task and became more capable at debugging.\nTask Experience In further exploratory data analysis, we also find differences in the way participants’\nexperience of completing the study. The control group (No AI) reported higher self-reported learning (on a\n7-point scale), while both groups reported high levels of enjoyment in completing the task (Figure 9). In terms\nof difficulty of the task, Figure 10 shows that although participants in the treatment group (AI Assistance)\nfound the task easier than the control group, both groups found the post-task quiz similarly challenging.\n6\nQualitative Analysis\nAlthough overall statistics on productivity and quiz score shed light on a high-level trend of how AI assistance\naffects a new learning task, a deeper analysis of how each participant completed the learning task allows us\n11\nLearning\nEnjoyment\nMetric\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nRating (Self-Reported)\nTask Learning and Enjoyment by Condition (1 to 7 Scale)\nCondition\nAI\nNo AI\nFigure 9: Self-reported enjoyment and learning\nby condition during our study.\n"
    },
    {
      "rank": 4,
      "distance_l2": 0.7287855744361877,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_020",
      "text": "\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\ndirectly pasted AI output. This suggests that spending more time manually typing may not yield better\nconceptual understanding. Cognitive effort may be more important than the raw time spent on completing\nthe task.\n6.2\nEncountering Errors\nThe way participants encountered and resolved errors was notably different between the treatment and control\nconditions. In the platform, participants could use the run button or the terminal to run their code as often\nas they wanted. In general, most of the participants ran the code for the first time after trying to complete\nmost of the question and ran the code again only after the changes were made. We recorded every error\nencountered by each participant as we watched the screen recordings of the task progress.\n15\nQuery Type\nExample Query\nExplanation (q=79)\n“can trio.sleep use partial seconds?”\n“Can you remind me what the different trio async operations are?”\n“Looks good, can you give me a really brief overview of the general idea behind\nall of this?”\nGeneration (q=51)\n“given this instruction to trio, can you implement the missing bits of main.py?”\n“complete get_user_data”\n“implement delayed_hello(). It should simply sleep for 2.1 seconds upon which\nit prints ’Hello World!’ ”\nDebugging (q=9)\n“Does that look right? If so let’s move on to delayed_hello()”\n“I’m having issues getting my code to work. I’m getting a notimplementederror\nfor delayed_hello”\nPasted\nError\n(e.g.,\n“Traceback\n(most\nrecent\ncall\nlast):\nFile\n\"/user-\ncode/FILESYSTEM/main.py3\", line 81, in... ”)\nCapabilities\nQues-\ntion (q=4)\n“Can you see the current question?”\n“So what can you do for me here? Can you write code directly into the file?”\n“Are you aware of how trio works? Are there parallels in its execution model to\nanother library I’d be more familiar with like asyncio”\nAppreciation (q=4)\n“Great job, we got the expected output on the first try.”\n“Looks like it worked, thanks!”\n“Trueeee!”\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI ("
    },
    {
      "rank": 5,
      "distance_l2": 0.7296009063720703,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 6,
      "distance_l2": 0.774232029914856,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_007",
      "text": " acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n• RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on coding skill formation.\nFigure 3: Experiment interface: We used a online interview platform to run our experiment. The treatment\ncondition participants are prompted to use the AI assistant.\n• RQ2: How does using AI assistance affect the development of these new skills?\n4\nMethods\n4.1\nTask Selection: Learning Asynchronous Programming with Trio\nWe prototyped tasks for several different skills that junior software engineers may encounter on the job:\nfrom data analysis to plotting. We designed an experiment around the Python Trio library,1 which is\ndesigned for asynchronous concurrency and input-output processing (I/O). This library is less well known\nthan asyncio (according to the number of StackOverflow questions) and involves new concepts (e.g.,\nstructured concurrency) beyond just Python fluency. It is also explicitly designed to be easy to use – making\nit particularly suitable for a learning experiment.\nWe designed and tested five tasks that use the Trio library for asynchronous programming, a skill often\n1See documentation at: https://trio.readthedocs.io/en/stable/\n5\nlearned in a professional setting when working with large-scale data or software systems. The tasks we created\ninclude problem descriptions, starter code, and brief descriptions of the Trio concepts required to complete\nthe task. These tasks are designed to parallel the process of learning to use a new library or new software\ntool through a brief self-guided tutorial. For example, in software engineers’ on-boarding materials, there is\noften a description of how to use an internal library and small tasks to build skills with the new library.\nAfter several pilot studies, we used the first two tasks in our main study; each task took 10 - 20 minutes\nduring initial testing. The first task is to write a timer that prints every passing second while other functions\nrun. This task introduces the core concepts of nurseries, starting tasks, and running functions concurrently\nin Trio. The second task involves implementing a record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants’ current version of the code and can produce the full, correct code\nfor both tasks directly when"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}