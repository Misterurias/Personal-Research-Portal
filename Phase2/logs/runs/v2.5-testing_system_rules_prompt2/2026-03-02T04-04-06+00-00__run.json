{
  "timestamp": "2026-03-02T04:04:06+00:00",
  "query": "Are experienced developers more effective when using AI assistants?",
  "retrieval_query": "Effective developer performance compared to novice developers utilizing AI-assisted tools.",
  "query_rewritten": true,
  "rewritten_query": "Effective developer performance compared to novice developers utilizing AI-assisted tools.",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks. Here are my answers:\n\nRQ1: Does AI assistance improve task completion productivity when new skills are required?\n\n* According to \"The impact of AI on developer productivity: Evidence from GitHub Copilot\" (Kalliamvakou et al., 2024), the treated group, with access to GitHub Copilot, completed the task 55.8% faster than the control group.\n* Another study, \"CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation\" (Wang et al., 2021), found that students with less coding proficiency were more likely to seek AI assistance.\n\nSo, based on these evidence chunks, it appears that AI assistance can improve task completion productivity when new skills are required.",
  "citations": [
    {
      "source_id": "Kalliamvakou et al.",
      "chunk_id": "2024"
    },
    {
      "source_id": "Wang et al.",
      "chunk_id": "2021"
    }
  ],
  "citations_valid": false,
  "invalid_citations": [
    [
      "Kalliamvakou et al.",
      "2024"
    ],
    [
      "Wang et al.",
      "2021"
    ]
  ],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6140271425247192,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_006",
      "text": " of generative AI tools—because these AI tools encourage \ndevelopers’ participation in non-coding activities such as discussions, they could lead to longer \ncoordination time in order to reconcile different ideas and perspectives among developers. \nThird, our study adds to the literature that explores the heterogeneity in the roles of generative AI \namong individuals (Dell'Acqua et al. 2023, Demirci et al. 2025). Prior studies have focused on how \nindividual skills play a role in shaping the effect of generative AI tools on completing discrete tasks and \nthey found that individuals with lower skills usually obtain greater productivity gain from these tools than \nhighly skilled individuals (e.g., Peng et al. 2023, Cui et al. 2024). Our results draw a sharp contrast with \nthese findings, as we demonstrate that peripheral developers obtain less productivity gain from AI pair \nprogrammers than core developers in OSS settings, potentially because the former do not possess important \nand necessary contextual knowledge about an OSS project to effectively use the AI tools. \n2. Literature Review  \n2.1 Generative AI in Software Development \nA growing body of literature has started to examine the impact of generative AI on software development. \nMost studies have focused on the implications of generative AI for individual productivity on specific tasks. \n 6 \nFor example, Imai (2022) finds that GitHub Copilot produces more lines of code than a human pair \nprogrammer when completing a task in Python. Peng et al. (2023) find that GitHub Copilot enables \nindividual developers to implement an HTTP server 55.8% faster than those not using the tool. Hoffmann \net al. (2024) show that GitHub Copilot causes individual developers to shift focus towards coding tasks and \naway from project management.  \nDespite these insights, research on how generative AI influences project-level outcomes for \ncomplex tasks involving multiple developers remains limited. Yeverechyahu et al. (2024) investigate the \ninnovation capabilities of generative AI, particularly its role in extrapolative versus interpolative thinking, \nand compare its influence on innovation in Python versus R. Different from the existing literature and built \nupon the OSS literature, our hypotheses are motivated by the unique characteristics of OSS, namely, the \nsoftware development process in an open and collaborative environment. Because participation is often \nvoluntary and coordination does not follow formal centralized processes in this environment, it remains \nunclear how generative AI influences open participation in both coding and non-coding activities, as well \nas team coordination, all of which could have important implications for project-level software \ndevelopment productivity. \nIn addition, existing research that explored heterogeneity in the roles of generative AI among \nindividuals has mostly focused on understanding the differential effects between workers with high skills \nagainst those with low skills for discrete tasks (e.g., Cui et al. 2024, Brynjolfsson et al. 2025). However, it \nremains unclear whether the results hold in settings with complex tasks that require not only skills but also \ncontextual knowledge and team collaboration. In the context of OSS development, the distinction between \ncore and peripheral developers lies not in their programming skills, but in their roles, responsibilities, and \nthe resulting level of contextual knowledge about a focal project (Crowston et al. 2006, Setia et al."
    },
    {
      "rank": 2,
      "distance_l2": 0.6282560229301453,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_010",
      "text": " went undetected in code reviews by participants compared to just 52% in human-written \ncode (p < 0.001). This suggests developers may be placing too much trust in AI-generated code, using less \nsecurity judgment in reviewing the AI-generated code compared to their own code, and devolving to a \nless strict and acute heuristic during their security review. \n4.4 Developer Experience and Learning Curve \nThe analysis of patterns of aid tool uptake revealed a learning curve. The experimental group reported \nincreasing productivity improvements across tasks: for Task 1, productivity improved by 18.2%; for Task \n2, it improved by 28.7%; for Task 3, it improved by 36.4%; and for Task 4, it improved by 41.8%, which \nsuggested that the developers were becoming better at using AI in helping them later in the tasks. This \nphenomenon was not as pronounced for the senior developers who showed consistent productivity, \nregardless of task. Experience had a significant impact on the relationship between AI tool use and code \nquality results (F (2,114) =7.43, p=0.001). The junior developers showed larger qualitative improvements \nyet produced a significantly larger proportion of bugs compared to the seniors. While junior developers \naccepted virtually all assistance provided by AI tools (89% of recommendations were accepted, p < .001), \nthe senior developers appeared to the center to weigh their engaging with the AI (62% acceptance of AI \nrecommendations) and could more easily articulate and locate fixes if bugs were identified by the AI tools. \nBoth think-aloud protocol and post-task interviews illuminated three different AI tool use strategies. The \nfirst strategy was the \"prompt-and-accept\" approach (38% of participants), in which participants took the \nAI-generated code suggestion fairly literally and made little modifications. The second strategy, \"iterative \nrefinement\", (47% of participants) used the AI suggestion as a starting point and modified and revised it \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n8\n \nsignificantly. The third strategy was \"validation-oriented\" (15% of participants) to seek reliability and use \nthe AI-generated output as a source of reference when writing code independently. The validation-focused \nstrategy produced mostly high-quality code, but with few productivity gains. \n \n5. Discussion \n5.1 Interpretation of Findings \nThe findings from this research reveal a complicated trade-off landscape in AI-assisted code generation, \nwhich challenges simplistic narrative accounts focused explicitly on productivity. The 31.4% productivity \nincrease is consistent with claims made by practitioners in the field, yet we observe that our results \nhighlight previously under-documented security concerns; further, this demonstrates an inherently basic \ntension associated with our fountain of AI-assisted software engineering: while speed may be enhanced, \nthere are potential security and maintainability trade-offs for future versions of AI-assisted software. The \nparticular performance of particular programming languages might also be providing data interpretations \nthat leave something to be desired: the improved results for Python programming likely derived from the \n"
    },
    {
      "rank": 3,
      "distance_l2": 0.6541744470596313,
      "source_id": "DevExperienceGenAI2025",
      "chunk_id": "DevExperienceGenAI2025_chunk_004",
      "text": " from the uncontrolled study period. \nThe following research questions are going to be addressed: \nHow does GenAI interaction impact the productivity \nindicators… \n(1) … efficiency \n(2) … accuracy \n(3) … the developer experience indicator perceived \nworkload \nduring simulated software engineering tasks? \nThis study contributes to emerging research that takes a \ndeveloper-centered view on the developer-GenAI interaction \nin a real-world firm setting.  \n2 Related Work \nGenerative AI (GenAI) refers to AI technology that learns from \ndata to autonomously generate new, meaningful, and \ncontextually appropriate content across various applications \n[10]. GitHub Copilot - from here on referred to only as Copilot \n- is one of several GenAIs that can assist developers by \ngenerating, completing and modifying programming code \nbased on the context of the codebase and natural language \nprompts. \nIt \nis \nintegrated \nin \nsoftware \ndevelopment \nenvironments (IDEs) like VS Code and can function as an “AI \npair programmer” for developers.  \nIn recent years, multiple studies have been conducted to \nevaluate Copilot’s output quality and developers’ productivity \ngains through AI usage. Based on benchmark tasks, Dakhel et \nal. [32] found that Copilot can generate solutions for nearly all \ngiven tasks, but its outputs are less often correct than those of \nhumans, making it a potential asset for experienced \ndevelopers but a liability for novices who may not detect non-\noptimal suggestions. So far, there is no consensus on how to \nbest measure software developers’ productivity, but time \nsavings, acceptance rate of Copilot suggestions, and successful \ncompletion of predefined tasks are frequently considered \nmetrics to evaluate developer productivity in the context of \nAI-assisted programming. Empirical studies using these \nmetrics report mixed results: Bakal et al. [1]  found that \ndevelopers using Copilot accepted 33% of suggestions and \nhad a 20% increase in lines of code; Peng et al. [37] observed \nthat programmers completed an HTTP server task 55.8% \nfaster with Copilot, with the largest productivity gains among \nless experienced programmers; while Vaithilingam et al. [45] \nfound no significant improvement in task time or success, \nthough participants still valued Copilot as a useful starting \npoint for daily programming tasks. In this study, we consider \ntask duration, the number of Copilot’s suggestions, and \nsuccessful \ntask \ncompletion \nas \nproductivity \nmetrics. \nAdditionally, we examine the impact of Copilot interaction \ntypes, \ndevelopers’ \ninteraction \nintensity, \nand \nsoftware \ndevelopment task categories on these metrics. This data is \nalso combined with subjective workload ratings from the \ndevelopers to gain a more nuanced analysis of not solely the \noutput quality and productivity gains of Copilot, but also the \ninteraction behavior and experience with Copilot.  \nSeveral studies in the software engineering context have \ndemonstrated benefits of using mixed-methods approaches \nthat combine objective telemetry, physiological measures and \nDevelopers’ Experience with Generative AI \nICSE SEIP 2026, Rio de Janeiro, Brazil \n \n \nsubjective data to provide deeper insights into developers’ \nexperiences during work [6, 17, 36]. However, only a few \n"
    },
    {
      "rank": 4,
      "distance_l2": 0.6558747887611389,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    },
    {
      "rank": 5,
      "distance_l2": 0.70453280210495,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_015",
      "text": " Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n11\n \n6. Sarkar, A., Ross, N. A., Anantharaman, V., et al. (2024). What is it like to program with artificial \nintelligence? Proceedings of the ACM Conference on Computer-Supported Cooperative Work, 156-\n189. \n7. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI \nassistants? ACM Conference on Computer and Communications Security, 2785-2799. \n8. Asare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub Copilot a substitute for human pair-\nprogramming? An empirical study. ACM Transactions on Software Engineering, 49(2), 1-34. \n9. Dakhel, A. M., Majdinasab, V., Nikanjam, A., et al. (2023). GitHub Copilot AI pair programmer: \nAsset or liability? Journal of Systems and Software, 203, 111734. \n10. Allamanis, M., Brockschmidt, M., & Khademi, M. (2018). Learning to represent programs with \ngraphs. International Conference on Learning Representations, 1-16. \n11. Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and \nnatural languages. Findings of the Association for Computational Linguistics: EMNLP 2020, 1536-\n1547. \n12. Guo, D., Ren, S., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems,"
    },
    {
      "rank": 6,
      "distance_l2": 0.7238243818283081,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_006",
      "text": " code, fix\nerrors, and explain algorithmic concepts; students with less coding proficiency were more likely to seek AI\nassistance. Other works use surveys to find that students may be hesitant to use AI coding assistant tools\ndue to “dependence worry” (i.e., overreliance on coding tools) [Pan et al., 2024]. For formal methods, Prasad\net al. coded the different ways in which students used LLMs for course work and found that upper-year\nstudents taking the class did not rely on LLM assistance and only asked a few questions at the beginning.\nUser studies have also been conducted in the professional development environments. Wang et al. study\ndifferent patterns in usage between users with and without chat access to AI models in completing coding\npuzzles and development tasks. They found rich interaction patterns including interactive debugging, code\ndiscussions, and asking specific questions. Participants ranged from asking ChatGPT to do then the entire\nproblem (lowest quality code output) to only asking minimal questions (highest efficiency). Other studies\nhave reported that AI tools help the software development process through easier access to documentation\nand accurate generation code for specific APIs [Pinto et al., 2024].\n3\nFramework\nProfessional Skill Acquisition\nThe “learning by doing” philosophy has been suggested by many learning\nframeworks such as the Kolb’s experiential learning cycle, and the Problem-Based Learning (PBL) [Kolb,\n2014, Schmidt, 1994]. The frameworks connect the completion of real-world tasks with the learning of\nnew concepts and the development of new skills. Experiential learning has also been explored specifically\nin software engineering courses in higher education in order to mimic solving problems in a professional\nsetting [Gonzalez-Huerta et al., 2020]. In its simplest form, we model AI tool assistance as taking a different\nlearning path than without AI. We hypothesize that using AI tools to generate code in the development\nprocess effectively amounts to taking a shortcut to task completion without a pronounced learning stage.\nAI for Coding Usage Patterns\nPrior works have found that humans use AI in many different ways\nfor coding: from question answering to writing code, to debugging [Poitras et al., 2024, Wang et al., 2020,\nPinto et al., 2024]. In our framework, different ways of using AI assistance represent different learning paths\ntaken to reach the goals of completing the task. We analyze these different usage patterns in the qualitative\nanalysis of this work (Section 6).\nResearch Questions\nBased on this background, we focus on on-the-job learning: settings where workers\nmust acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n• RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt2"
}