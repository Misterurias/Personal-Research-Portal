{
  "timestamp": "2026-03-02T04:06:09+00:00",
  "query": "Does GitHub Copilot increase developer productivity?",
  "retrieval_query": "\"Effectiveness of GitHub Copilot in improving developer efficiency and productivity metrics.\"",
  "query_rewritten": true,
  "rewritten_query": "\"Effectiveness of GitHub Copilot in improving developer efficiency and productivity metrics.\"",
  "top_k": 6,
  "answer": "I'll only answer based on the provided evidence chunks.\n\nPlease provide the question you'd like me to answer from the evidence chunks. I'll do my best to provide a response using only the information presented in the chunks.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5116249918937683,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_006",
      "text": " than the control group (Peng et al. 2023). Another\nstudy by GitHub reports that the use of Copilot Chat increases programmers’ confidence, with participants\nself-reporting improvements in code readability, reusability, conciseness, maintainability, and resilience\n(Rodriguez 2023). These productivity gains also translate into labor-market outcomes: developers exposed\nto AI-assisted coding experience faster career progression in the short- to medium-term (Li et al. 2025).\nWork also finds that AI coding assistants reshape the allocation of work. For instance, Yeverechyahu et al.\n5\n(2024) investigate the impact of GitHub Copilot on innovation in OSS projects. They find a significant\nincrease in overall code contributions, accompanied by a shift in the nature of innovation toward more\nroutine and incremental changes. Song et al. (2024) find that Copilot adoption increases project-level code\ncontributions, though this comes at the cost of an increase in coordination time for code integration. Relat-\nedly, Hoffmann et al. (2025) show that access to GitHub Copilot reallocates developers’ effort toward core\ncoding tasks and away from project management and coordination activities.\nWhile AI-assisted code development promises substantial productivity gains, its implications for software\nmaintenance remain less well understood. Prior research on software development has long recognized that\ndevelopment costs are often small relative to maintenance costs, which include sustaining activities associ-\nated with ensuring software quality and security (Nagle 2019). In the case of OSS, while users can benefit\nfrom reduced up-front costs, collective intelligence of the crowd, and flexibility to implement changes, the\nchallenges of maintenance get magnified as contributors are not contractually obligated to maintain the\nsoftware (von Hippel and von Krogh 2003, Nagle 2019). The Linux Foundation’s OSS Contributor Sur-\nvey provides insightful perspectives on the complexities involved in maintaining OSS (Nagle et al. 2020).\nFirstly, it highlights that “general housekeeping\" tasks, such as project maintenance, bug reporting and\ndocumentation, and organizational or administrative duties, often consume a more significant portion of\ncontributors’ time than desired. Secondly, despite a preference among contributors to spend less time on\nmaintenance tasks, there’s a broad acknowledgment of the importance of these activities, especially those\nrelated to software security, for the success and integrity of their projects (Nagle et al. 2020).\nFurthermore, AI code assistants, including prompt-based and “vibe coding” practices, promise to increase\nproductivity while easing access for contributors to submit code, even in complex and mature OSS projects.\nRecent work has begun to examine vibe coding as an emerging and controversial paradigm in AI-assisted\nsoftware development, in which programmers rely on natural language interaction with generative models\nto maintain flow and rapidly explore solutions, often with minimal upfront specification (Pimenova et al.\n2025, Fawzy et al. 2025). While this approach can substantially accelerate development and foster exper-\nimentation, the literature consistently highlights associated risks, including underspecified requirements,\nreduced reliability, difficulties in debugging, increased latency, and heavier burdens on code review and col-\nlaboration (He et al. 2025). A recurring theme is a speed–quality paradox (Fawzy et al. 2025): although vibe\ncoding enables rapid"
    },
    {
      "rank": 2,
      "distance_l2": 0.5966917276382446,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 3,
      "distance_l2": 0.6064621806144714,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_015",
      "text": " during our panel \nperiod. We select repositories with non-zero size, at least one specified programming language and license, \na description, and no mirror or personal store designation. To exclude ghost or abandoned projects, we \nrequire at least one code submission every six months from 2021 to 2022 and at least one additional activity, \nsuch as a release or creation, each year. As we are interested in evaluating project-level code contributions \nand coordination time involving multiple developers, we focus on repositories with at least three developers \ncontributing each month. Additionally, as discussed in greater detail below, we use IDE4 information to \n \n2 GitHub launched the technical preview of Copilot in June 2021: https://github.blog/2021-06-29-introducing-github-\ncopilot-ai-pair-programmer/. It then announced the formal launch and public availability of Copilot in June 2022: \nhttps://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/. \n3 We choose this endpoint to ensure our results are not influenced by the rise in popularity of Chat GPT, which began \nin early 2023.  \n4 IDE stands for integrated development environment, which is a software application that provides local environments \nfor coding, testing, and debugging. \n 16 \nidentify repositories in the treatment group versus those in the control groups, so we further restrict our \nsample to those that disclose IDE information. These criteria result in a sample of 9,244 repositories. \nTo identify repositories where Copilot was used by developers (i.e., the treatment group), we \ncollaborated with GitHub organization, which provided proprietary aggregated Copilot usage data at the \nrepository level. This dataset indicates the monthly proportion of developers contributing code to a given \nrepository who also used Copilot.5 Since Copilot requires a compatible IDE, we also consider IDE usage. \nDuring our analysis period, only a limited number of IDEs supported Copilot: Visual Studio Code, the \nJetBrains suite of IDEs, Neovim, and Visual Studio.6 To determine IDE usage, we examine the webpages \nof the repositories in our sample to gather information on the IDE usage by contributing developers. We \ncategorize repositories with developers who used Copilot and one of the supported IDEs as our treatment \ngroup, and those using unsupported IDEs and not using Copilot as our control group. In total, our sample \nincludes 5,687 repositories in the treatment group and 3,557 repositories in the control group. We designate \nthe first month when Copilot was supported by IDEs and used by a non-zero proportion of developers as \nthe treatment start time for each repository. The months before this time are defined as the pre-treatment \nperiod and the months after this time as the post-treatment period for each repository, covering the two-\nyear span from 2021 to 2022. \nTo further improve comparability, we restrict the sample to repository-month observations with at \nleast one code contribution. This focus allows us to estimate Copilot’s effect on collaborative development \nin actively maintained projects. Moreover, because the analysis of coordination time requires at least one \ncode contribution to compute acceptance time, this criterion allows us to use the same sample to investigate \nH1 and H2. The final sample includes 7,637 repositories, with 4"
    },
    {
      "rank": 4,
      "distance_l2": 0.6209291219711304,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_002",
      "text": " team member who partners with the developer to create knowledge (Friedman\n2021). Unlike earlier coding automation tools that primarily targeted productivity, GitHub Copilot’s framing\nas a pair programmer signals a deeper shift. It implies that AI may fundamentally reshape how knowledge-\nintensive work is performed, coordinated, and organized, rather than merely accelerating existing tasks.\nFor organizations and communities involved in software development, the addition of AI pair program-\nmers in teams offers the potential for significant productivity gains. Right after the launch of GitHub Copi-\n1\narXiv:2510.10165v3  [econ.GN]  28 Jan 2026\n2\nlot, research shows that developers who use Copilot completed their programming tasks 55.8% faster (Peng\net al. 2023). Such productivity benefits lead to promises of faster time-to-market and increased revenue\nfor organizations developing software applications. Considering these shifts, major tech organizations have\nstarted to increasingly rely on AI in their projects - “more than a quarter of all new code at Google is gen-\nerated by AI, then reviewed and accepted by engineers,\" reported Google CEO Sundar Pichai in January,\n2025.1 Moreover, Microsoft CTO Kevin Scott expects that 95% of all code will be AI-generated by 2030.2\nWhile these productivity gains are promising, they also raise important questions about the quality and\nmaintainability of AI-generated code. Because AI tools can lower the skill barrier for writing code (Dakhel\net al. 2023), AI tools enable broader participation but may also encourage developers to rely on gener-\nated solutions without fully understanding the underlying design rationale and potential integration issues\n(Barrett et al. 2023). Such reliance increases the likelihood of quick fixes that favor short-term function-\nality over long-term maintainability (Barrett et al. 2023). Extant literature characterizes “quick and dirty”\nsoftware customizations made without a complete understanding of their future implications as technical\ndebt, as they undermine system reliability and impose long-term maintenance obligations (Kruchten et al.\n2012, Brown et al. 2010, Banker et al. 2021). As a result, project maintainers must devote additional effort\nto understanding, reviewing, and reworking AI-generated code before it can be safely integrated. In our\ncontext, we contend that the growing reliance on AI-assisted development may accelerate the accumulation\nof technical debt, as design shortcuts taken to expedite system deployment become embedded in software\nsystems (Ramasubbu and Kemerer 2016, 2021).\nThe technical debt and maintenance challenges that AI poses are expected to be especially pronounced\nin distributed software development teams, such as in Open Source Software (OSS) communities. In these\ncommunities, contributors from around the world collaborate, often voluntarily, to develop and maintain\nsoftware that form the digital infrastructure of our society (e.g., Linux, Apache, LaTeX, Python), making\nit freely or cheaply available to the public (Eghbal 2020, Nagle 2019). Despite the voluntary nature of\nwork in these communities, OSS constitutes critical digital infrastructure for modern society, with estimates\nsuggesting that the total cost of reproducing this software would amount to $8.8 trillion (Hoffmann et al.\n2024).3 Given"
    },
    {
      "rank": 5,
      "distance_l2": 0.6209385991096497,
      "source_id": "NoviceProgramming2024",
      "chunk_id": "NoviceProgramming2024_chunk_024",
      "text": " E.;\net al. On the opportunities and risks of foundation models. arXiv 2021, arXiv:2108.07258.\n7.\nZawacki-Richter, O.; Marín, V.I.; Bond, M.; Gouverneur, F. Systematic review of research on artificial intelligence applications in\nhigher education–where are the educators? Int. J. Educ. Technol. High. Educ. 2019, 16, 1–27. [CrossRef]\n8.\nKalliamvakou, E. Research: Quantifying GitHub Copilot’s Impact on Developer Productivity and Happiness. GitHub Blog\n2022. Available online: https://github.blog/news-insights/research/research-quantifying-github-copilots-impact-on-developer-\nproductivity-and-happiness/ (accessed on 1 May 2024).\n9.\nPeng, S.; Kalliamvakou, E.; Cihon, P.; Demirer, M. The impact of ai on developer productivity: Evidence from github copilot.\narXiv 2023, arXiv:2302.06590.\n10.\nFinnie-Ansley, J.; Denny, P.; Becker, B.A.; Luxton-Reilly, A.; Prather, J. The robots are coming: Exploring the implications of\nOpenAI Codex on introductory programming. In Proceedings of the 24th Australasian Computing Education Conference, Virtual\nEvent, 14–18 February 2022; pp. 10–19.\nEduc. Sci. 2024, 14, 1089\n16 of 17\n11.\nYilmaz, R.; Yilmaz, F.G.K. The effect of generative artificial intelligence (AI)-based tool use on students’ computational thinking\nskills, programming self-efficacy and motivation. Comput. Educ. Artif. Intell. 2023, 4, 100147. [CrossRef]\n12.\nBird, C.; Ford, D.; Zimmermann, T.; Forsgren, N.; Kalliamvakou, E.; Lowdermilk, T.; Gazit, I. Taking Flight with Copilot: Early\ninsights and opportunities of AI-powered pair-programming tools. Queue 2022, 20, 35–57. [CrossRef]\n13.\nLau, S.; Guo, P. From “Ban it till we understand it” to “Resistance is futile”: How university programming instructors plan to\nadapt as more students use AI code generation and explanation tools such as ChatGPT and GitHub Copilot. In Proceedings of the\n2023 ACM Conference on International Computing Education Research-Volume 1; Association for Computing Machinery: New York,\nNY, USA, 2023; pp. 106–121.\n14.\nRay, P.P. ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future\nscope. Internet Things Cyber-Phys. Syst. 2023, 3, 121–154. [CrossRef]\n15.\nYin, J.; Goh, T.T.; Yang, B.; Xiaobin, Y. Conversation technology with micro-learning: The impact of chatbot-based learning on\nstudents’ learning motivation and performance. J. Educ. Comput"
    },
    {
      "rank": 6,
      "distance_l2": 0.6428431272506714,
      "source_id": "OpenSourceImpact2024",
      "chunk_id": "OpenSourceImpact2024_chunk_016",
      "text": " of developers as \nthe treatment start time for each repository. The months before this time are defined as the pre-treatment \nperiod and the months after this time as the post-treatment period for each repository, covering the two-\nyear span from 2021 to 2022. \nTo further improve comparability, we restrict the sample to repository-month observations with at \nleast one code contribution. This focus allows us to estimate Copilot’s effect on collaborative development \nin actively maintained projects. Moreover, because the analysis of coordination time requires at least one \ncode contribution to compute acceptance time, this criterion allows us to use the same sample to investigate \nH1 and H2. The final sample includes 7,637 repositories, with 4,491 in the treatment group and 3,146 in \nthe control group. Table 1 provides descriptions and summary statistics for repository-month-level variables \nused in the main analysis. We check the robustness of our results by removing the restriction on the number \n \n5 We do not know the identity of individual Copilot users because of privacy concerns. Although such Copilot usage \nis measured at the GitHub platform level, developers are likely to integrate Copilot into their workflows extensively \nand thus would use it for all repositories where it is feasible (Marangunić and Granić 2015).  \n6 GitHub lists the supported IDEs for Copilot: https://github.com/features/copilot. \n 17 \nof code contribution. Additionally, we test the robustness using a larger sample that includes repositories \nwith as few as two developers.  \n[insert Table 1 here] \n5. Empirical Analyses \n5.1 Measures \nTo test H1 and H2, the objective in our empirical analyses is to determine how Copilot influences project-\nlevel code contributions and coordination time for code integration. To measure project-level code \ncontributions, we use the total number of pull requests (PRs) submitted to each repository that were \neventually merged7. PRs represent code changes submitted by developers and need further evaluation by \ncore developers. This evaluation results in either approval, leading to merged PRs, or rejection, leading to \nclosed but unmerged PRs. Thus, merged PRs reflect successful code changes that have been eventually \nincorporated into the development of repositories and are commonly used in the literature to assess \nmeaningful contributions by developers (Gousios et al. 2014, Tsay et al. 2014, Kononenko et al. 2018).  \nTo evaluate coordination time for code integration, we follow prior work in both economics \n(Simcoe 2012) and software engineering (Espinosa et al. 2007, Yu et al. 2015, El Mezouar et al. 2019) by \nmeasuring the duration between the submission and acceptance of each PR. Because a repository may \nreceive many PRs in a month, we then compute the average duration (i.e., average time) to merge a PR for \na repository in a month.  \n5.2 Model and Estimation \nIn our setup, repositories adopt Copilot at different times after it is available. The variation in treatment \ntiming and the larger number of treated repositories compared to untreated ones complicate the application \nof traditional matching techniques in this context. Given these complexities, we adopt the Generalized \nSynthetic Control Method (G"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}