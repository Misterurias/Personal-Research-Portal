{
  "timestamp": "2026-03-02T04:11:19+00:00",
  "query": "Is AI-assisted code more error-prone than human-written code?",
  "retrieval_query": "\"error rate of AI-assisted code compared to human-written code\"",
  "query_rewritten": true,
  "rewritten_query": "\"error rate of AI-assisted code compared to human-written code\"",
  "top_k": 6,
  "answer": "I'll only answer based on the provided evidence chunks.\n\nPlease provide the specific chunk you'd like me to respond to, and I'll do my best to address your question or prompt.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6947503089904785,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_017",
      "text": " \nfor Computing Machinery, Article 332 (2022), 7; \n10.1145/3491101.3519665\n24.\t Wagner, S. and Ruhe, M. A systematic review of \nproductivity factors in software development. arXiv \npreprint arXiv:1801.06475 (2018).\n25.\t Wang, D. et al. From human-human collaboration to \nhuman-AI collaboration: Designing AI systems that \ncan work together with people. In Proceedings of \nthe 2020 CHI Conf. on Human Factors in Computing \nSystems (2020), 1–6.\n26.\t Weisz, J.D. et al. Perfection not required? Human-AI \npartnerships in code translation. In Proceedings of \nthe 26th Intern. Conf. on Intelligent User Interfaces, T. \nHammond et al (eds). ACM, (April 2021), 402–412; \n10.1145/3397481.3450656\n27.\t Winters, T., Manshreck, T., and Wright, H. Software \nEngineering at Google: Lessons Learned from \nProgramming Over Time. O’Reilly Media (2020).\n28.\t Wold, S., Sjöström, M., and Eriksson, L. PLS-regression: \nA basic tool of chemometrics. Chemometrics and \nIntelligent Laboratory Systems 58, 2 (2001), 109–130; \n10.1016/S0169-7439(01)00155-1.\n29.\t Zhou, W., Kim, S., Murali, V., and Ari Aye, G. Improving \ncode autocompletion with transfer learning. \nCoRR abs/2105.05991 (2021); https://arxiv.org/\nabs/2105.05991\nAlbert Ziegler (wunderalbert@github.com ) is a principal \nresearcher at GitHub, Inc., San Francisco, CA, USA.\nEirini Kalliamvakou is a staff researcher at GitHub, Inc., \nSan Francisco, CA, USA.\nX. Alice Li is a staff researcher for Machine Learning at \nGitHub, San Francisco, CA, USA.\nAndrew Rice is a principal researcher at GitHub, Inc., San \nFrancisco, CA, USA.\nDevon Rifkin is a principal research engineer at GitHub, \nInc., San Francisco, CA, USA.\nShawn Simister is a staff software engineer at GitHub, \nInc., San Francisco, CA, USA.\nGanesh Sittampalam is a principal software engineer at \nGitHub, Inc., San Francisco, CA, USA.\nEdward Aftandilian is a principal researcher at GitHub, \nInc., San Francisco, CA, USA.\nFigure 6. Average acceptance rate during the week. Each point represents the average \nfor a one-hour period, whereas the shaded ribbon shows the min-max variation during \nthe observed four-week period.\nSaturday\n12:00\n26%\noff hours\nDaily and weekly patterns in acceptance rate in the US\n(all users between 2022-01-15 and 2022-02-12)\nweekend\nworking hours\n24%\n22%\n20%\nSunday\n12:00\nMonday\n12:00\nTuesday"
    },
    {
      "rank": 2,
      "distance_l2": 0.7391567230224609,
      "source_id": "ExpectationVsExperience2022",
      "chunk_id": "ExpectationVsExperience2022_chunk_005",
      "text": " increased unless an explanation was provided. However,\nthese explanations may also lead to over-reliance on the system\neven when unwarranted, signaling the importance and difficulty\nof providing explanations that help people to calibrate trust appro-\npriately. Kocielnik et al. [23] examined the effect of giving people\ncontrol over the types of errors made by a scheduling assistant,\neither by avoiding false positives or false negatives. They found\nthat even when the system was only 50% accurate, users who ex-\npected a reduction in the false positive rate had a lower perception\nof accuracy and lower acceptance of the system than the users who\nexpected a reduction in the false negative rate. [3, 52] showed that\nconfidence scores helped calibrate users’ trust, form a good mental\nmodel of the AI, and understand the error boundaries better.\nSimilar to other AI techniques, AI-based code generation tools\nalso suffer from inherent uncertainty and imperfection. They may\ninevitably generate code with errors or even code that wildly differs\nfrom users’ expectations. However, unlike other domains, code\ngeneration demands a much higher level of correctness: code either\ncompiles or not, and it is either correct or contains bugs such as\nlogic errors and security vulnerabilities. Therefore, existing findings\nof other types of AI techniques may not generalize to the domain\nof code generation.\nCurrently, there are only a few studies on how programmers\nuse such imperfect code generation tools [44, 47]. Xu et al. [47] did\na user study with 31 participants to evaluate the usefulness of a\nNL-to-code plugin [46]. They found that there was no statistically\nsignificant difference in task completion time or task correctness\nscores when using or not using the NL-to-code plugin. Furthermore,\nmost participants stayed neural or somewhat positive to the NL-\nto-code plugin. The main reason for these negative results was the\ncorrectness and quality of generated code as pointed out by many\nparticipants in the post-study survey. However, these findings may\nnot hold as more recent large language models have significantly\nboosted the correctness and quality of generated code. This further\nmotivates us to conduct the user study with Copilot.\nWeisz et al. [44] interviewed 11 software engineers at IBM and\nsolicited their feedback on a neural machine translation (NMT)\nmodel for an adjacent domain—translating code from one program-\nming language to another. They found that the user’s acceptance\nof the NMT model was contingent on the number of errors in the\ntranslated code. They also identified several common themes in\nparticipants’ feedback such as acceptance via verification and the\ndesire to provide guidance to the NMT model. Our study was de-\nsigned to complement this knowledge but for daily programming\ntasks.\n3\nSTUDY DESIGN\nTo understand how programmers use an LLM-based code genera-\ntion tool, we designed and carried out a within-subjects comparative\nstudy with 24 participants. For the control condition, each partici-\npant was asked to complete a Python programming task in Visual\nStudio Code (VSCode) IDE with the default code completion tool\nExpectation vs. Experience: Evaluating the Usability of Code\nGeneration Tools Powered by Large Language Models\nCHI ’22 Extended Abstracts, April 29-May 5, 2022, New Orleans"
    },
    {
      "rank": 3,
      "distance_l2": 0.7605158090591431,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_020",
      "text": "\npasted (n = 9) AI code finished the tasks the fastest while participants who manually copied (n = 9) AI\ngenerated code or used a hybrid of both methods (n = 4) finished the task at a speed similar to the control\ncondition (No AI). There was a smaller group of participants in the AI condition who mostly wrote their\nown code without copying or pasting the generated code (n = 4); these participants were relatively fast and\ndemonstrated high proficiency by only asking AI assistant clarification questions. These results demonstrate\nthat only a subset of AI-assisted interactions yielded productivity improvements.\nFor skill formation, measured by quiz score, there was no notable difference between groups that typed vs\ndirectly pasted AI output. This suggests that spending more time manually typing may not yield better\nconceptual understanding. Cognitive effort may be more important than the raw time spent on completing\nthe task.\n6.2\nEncountering Errors\nThe way participants encountered and resolved errors was notably different between the treatment and control\nconditions. In the platform, participants could use the run button or the terminal to run their code as often\nas they wanted. In general, most of the participants ran the code for the first time after trying to complete\nmost of the question and ran the code again only after the changes were made. We recorded every error\nencountered by each participant as we watched the screen recordings of the task progress.\n15\nQuery Type\nExample Query\nExplanation (q=79)\n“can trio.sleep use partial seconds?”\n“Can you remind me what the different trio async operations are?”\n“Looks good, can you give me a really brief overview of the general idea behind\nall of this?”\nGeneration (q=51)\n“given this instruction to trio, can you implement the missing bits of main.py?”\n“complete get_user_data”\n“implement delayed_hello(). It should simply sleep for 2.1 seconds upon which\nit prints ’Hello World!’ ”\nDebugging (q=9)\n“Does that look right? If so let’s move on to delayed_hello()”\n“I’m having issues getting my code to work. I’m getting a notimplementederror\nfor delayed_hello”\nPasted\nError\n(e.g.,\n“Traceback\n(most\nrecent\ncall\nlast):\nFile\n\"/user-\ncode/FILESYSTEM/main.py3\", line 81, in... ”)\nCapabilities\nQues-\ntion (q=4)\n“Can you see the current question?”\n“So what can you do for me here? Can you write code directly into the file?”\n“Are you aware of how trio works? Are there parallels in its execution model to\nanother library I’d be more familiar with like asyncio”\nAppreciation (q=4)\n“Great job, we got the expected output on the first try.”\n“Looks like it worked, thanks!”\n“Trueeee!”\nTable 3: Examples of different types of queries received by AI assistant and counts of each type of query. 11\nqueries have multiple (two) labels.\nNo AI\nAI (Manual Coding)\nAI (Code Pasting)\nAI (Hybrid: Pasting and Copying)\nAI (Manual Code Copying)\nPaste Behavior\n0\n5\n10\n15\n20\n25\nTask Time (minutes)\nTotal Time\nNo AI\nAI ("
    },
    {
      "rank": 4,
      "distance_l2": 0.7767086029052734,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_051",
      "text": "uyen and Nadi (2022) and Yetistiren \net al. (2023) evaluate code correctness, efficiency, and overall quality, with Yetistiren et al. \n(2023) observing improvements in generated code over time. However, recent research has \nbegun to emphasize user experience and the broader impact of these tools on developer \nproductivity.   Barke et al. (2023) showed that while Copilot might not directly shorten \ndevelopment time, it often serves as a valuable starting point, though challenges remain \nin understanding, editing, and debugging generated code snippets. Lertbanjongngam et al. \n(2022) compared human-written code with AlphaCode-generated code, emphasizing the \nneed for developer review to identify performance bottlenecks. Coignion et al. (2024) found \nthat although LLM-generated code performs well in some cases, it is slower than 27% of \nhuman-written code on the LeetCode dataset. Liu et al. (2024b) found three instances of \ninefficient implementations within the HumanEval ground truth, which caused slow per­\nformance on inputs of reasonable size. The prior studies underscore the need for a more \ncomprehensive evaluation methodology that considers not only functional correctness but \nalso potential performance implications. Hou and Ji (2024) found performance limitations \nin GPT-4-generated code, which can be partially addressed through optimization.  Garg \net al. (2023) proposed leveraging LLMs with prompt engineering to optimize code perfor­\nmance, showing effective improvements in addressing performance regressions. Moreover, \na growing number of performance-centric evaluation benchmarks, e.g., EffiBench (Huang \net al. 2024b), Mercury (Du et al. 2024), and ECCO (Waghjale et al. 2024), have emerged. \nConcurrently, certain studies have explored the optimization of code performance using \nclassification and reinforcement learning approaches (Seo et al. 2024; He et al. 2025). In \naddition, several recent works have proposed model-side improvements for efficiency. \nEffiLearner (Huang et al. 2024c) introduces a self-optimization framework that iteratively \nimproves code efficiency through execution overhead profiling, demonstrating significant \n1 3\n   62 \n \nPage 44 of 52\nEmpirical Software Engineering           (2026) 31:62 \nreductions in execution time across multiple models and benchmarks.  Shypula et al. (2024) \npresent Performance-Improving Edits, a method for learning performance-improving code \nedits from a curated dataset of over 77,000 human-made optimizations in competitive \nprogramming, enabling LLMs to suggest high-level algorithmic and API optimizations. \nACECode (Yang et al. 2024) employs reinforcement learning with a dual-objective reward \nsystem to simultaneously optimize code efficiency and correctness in code LLMs, achieving \nnotable improvements in both pass rates and runtime performance. PerfCodeGen (Peng et al. \n2025) leverages execution feedback to guide LLMs toward more performant generations, \n"
    },
    {
      "rank": 5,
      "distance_l2": 0.7820405960083008,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_023",
      "text": " vs. experience: Evaluating the usability of code generation tools\npowered by large language models. In Chi conference on human factors in computing systems extended abstracts. 1–7.\n[51] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray.\n2019. Human-AI collaboration in data science: Exploring data scientists’ perceptions of automated AI. Proceedings of the ACM on human-computer\ninteraction 3, CSCW (2019), 1–24.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n13\n[52] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula.\n2021. Perfection not required? Human-AI partnerships in code translation. In Proceedings of the 26th International Conference on Intelligent User\nInterfaces. 402–412.\n[53] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards.\n2022. Better together? an evaluation of ai-supported code translation. In Proceedings of the 27th International Conference on Intelligent User Interfaces.\n369–391.\n[54] Michel Wermelinger. 2023. Using github copilot to solve simple programming problems. In Proceedings of the 54th ACM Technical Symposium on\nComputer Science Education V. 1. 172–178.\n[55] Zhuohao Wu, Danwen Ji, Kaiwen Yu, Xianxu Zeng, Dingming Wu, and Mohammad Shidujaman. 2021. AI creativity and the human-AI co-creation\nmodel. In Human-Computer Interaction. Theory, Methods and Tools: Thematic Area, HCI 2021, Held as Part of the 23rd HCI International Conference,\nHCII 2021, Virtual Event, July 24–29, 2021, Proceedings, Part I 23. Springer, 171–190.\n[56] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation from natural language: Promise and challenges. ACM Transactions\non Software Engineering and Methodology (TOSEM) 31, 2 (2022), 1–47.\n[57] Zhen Yang, Fang Liu, Zhongxing Yu, Jacky Wai Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. 2024. Exploring and unleashing\nthe power of large language models in automated code translation. Proceedings of the ACM on Software Engineering 1, FSE (2024), 1585–1608.\n[58] Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the quality of GitHub copilot’s code generation. In Proceedings of the 18th international\nconference on predictive models and data analytics in software engineering. 62–71.\n[59] Ramaz"
    },
    {
      "rank": 6,
      "distance_l2": 0.8096528053283691,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_002",
      "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n∗Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n†Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}