{
  "timestamp": "2026-03-02T04:09:37+00:00",
  "query": "What quality metrics are used to evaluate AI-generated code?",
  "retrieval_query": "Evaluate AI-generated code using quality metrics such as.",
  "query_rewritten": true,
  "rewritten_query": "Evaluate AI-generated code using quality metrics such as.",
  "top_k": 6,
  "answer": "I can only answer based on the provided evidence chunks. Please provide the specific evidence chunk you would like me to respond to.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.5205108523368835,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_004",
      "text": " al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming languages used [21]. Nguyen and colleagues put forward a new set of metrics for \nmeasuring AI-generated code and reported that, although AI-powered software is very good at producing \nsyntactically correct code, it often strays from the best design patterns [22]. \nAmong the critical issues raised security implications have been the foremost. Static analysis studies \nshowed that AI systems might learn from their training data and thus recreate the same weaknesses as in \nthe case of existing vulnerabilities [23]. Pearce and co-workers pointed out the presence of a vulnerable \npattern that was quite common in GitHub Copilot's code which included SQL injection, cross-site \nscripting, and use of insecure cryptographic methods [24]. \nNevertheless, there are still some limitations that are inherent to such a growing research area. The \nmajority of research works have been limited to the evaluation of just a few tools and or small domains \nmaking the possibility of drawing general conclusions quite difficult. Only a couple of research works \nhave dealt with long-term effects of refactoring, skilled programmers, and maintenance costs. Moreover, \nthe interaction between the experience level of the developer and the effectiveness of the AI tool is still to \nbe fully explored [25]. Through a thorough and multi-language study involving developers of different \nexperience levels working on varied software engineering tasks, we fill these gaps. \n \n3. Methodology \n3.1 Research Design \nThis research utilized a between-subjects experimental design to assess the influences of AI-assisted code \ngeneration tools on software development outcomes. The independent variable was the presence of AI-\nassisted coding tools (experimental group vs. control group), while the dependent variables were the \nquality metrics, security vulnerability counts, and productivity measures of the coded produced. The \nexperiment was set up in such a way that it reduced the possibility of confounding variables while keeping \nthe ecological validity through the use of realistic programming tasks that are typical of professional \nsoftware development scenarios. Figure 1 shows Research Design Framework \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n4\n \n \nFigure 1: Research Design Framework \n \n3.2 Experimental Tasks \nDuring this study four programming tasks are designed per language (Python, Java, JavaScript, C++), \ntotalling 16 tasks across the study. Tasks were categorized by complexity: (1) simple algorithmic \nimplementations (e.g., sorting algorithms, string manipulations), (2"
    },
    {
      "rank": 2,
      "distance_l2": 0.5543187856674194,
      "source_id": "CodeQualityComparison2023",
      "chunk_id": "CodeQualityComparison2023_chunk_001",
      "text": "Noname manuscript No.\n(will be inserted by the editor)\nEvaluating the Code Quality of AI-Assisted Code\nGeneration Tools: An Empirical Study on GitHub Copilot,\nAmazon CodeWhisperer, and ChatGPT\nBurak Yetiştiren · Işık Özsoy · Miray\nAyerdem · Eray Tüzün\nthe date of receipt and acceptance should be inserted later\nAbstract\nContext AI-assisted code generation tools have become increasingly prevalent in soft-\nware engineering, offering the ability to generate code from natural language prompts or\npartial code inputs. Notable examples of these tools include GitHub Copilot, Amazon\nCodeWhisperer, and OpenAI’s ChatGPT.\nObjective This study aims to compare the performance of these prominent code gen-\neration tools in terms of code quality metrics, such as Code Validity, Code Correctness,\nCode Security, Code Reliability, and Code Maintainability, to identify their strengths\nand shortcomings.\nMethod We assess the code generation capabilities of GitHub Copilot, Amazon Code-\nWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\nResults Our analysis reveals that the latest versions of ChatGPT, GitHub Copilot,\nand Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1% of the\ntime, respectively. In comparison, the newer versions of GitHub CoPilot and Amazon\nCodeWhisperer showed improvement rates of 18% for GitHub Copilot and 7% for\nBurak Yetiştiren\nBilkent University,\nE-mail: burakyetistiren@hotmail.com\nIşık Özsoy\nBilkent University,\nE-mail: ozsoyisik@gmail.com\nMiray Ayerdem\nBilkent University,\nE-mail: miray.ayerdem@ug.bilkent.edu.tr\nEray Tüzün\nBilkent University,\nE-mail: eraytuzun@cs.bilkent.edu.tr\narXiv:2304.10778v2  [cs.SE]  22 Oct 2023\n2\nBurak Yetiştiren et al.\nAmazon CodeWhisperer. The average technical debt, considering code smells, was\nfound to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub Copilot, and 5.6 minutes\nfor Amazon CodeWhisperer.\nConclusions This study highlights the strengths and weaknesses of some of the\nmost popular code generation tools, providing valuable insights for practitioners. By\ncomparing these generators, our results may assist practitioners in selecting the optimal\ntool for specific tasks, enhancing their decision-making process.\nKeywords ChatGPT, OpenAI, Amazon CodeWhisperer, GitHub Copilot, code\ngeneration, code completion, AI pair programmer, empirical study\n1 Introduction\nCode completion and generation tools are essential for enhancing programmers’ per-\nformance and output quality in software development. Omar et al. (2012) define code\ncompletion tools as tools that are offered in most editors, which list contextually-relevant\nvariables, fields, methods, types, and other code snippets in the form of a floating menu.\nBy exploring and making choices from this menu, developers can avoid frequent gram-\nmatical and logical errors, reduce redundant"
    },
    {
      "rank": 3,
      "distance_l2": 0.5895600318908691,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_003",
      "text": "olve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious programming languages and projects. Finally, the research gives the software organizations that \nwant to use AI tools good insights and practices for risk reduction. Thus, the implications of our results \nare very important for the education of software engineers, the industry's practices and the direction of \nfuture research in the area of AI and software development. \n \n2. Literature Review \nThe areas where artificial intelligence and software engineering meet have become the center of a huge \nnumber of research studies, with code generation and program synthesis being the two main areas. The \nfirst automated code generation attempts relied on template-based methods and rule-based systems \nproducing the so-called boilerplate codes from high-level specifications [10]. The deep learning era totally \nchanged the picture, with the application of sequence-to-sequence models and recurrent neural networks \nto the code synthesis tasks [11]. The introduction of the transformer architectures was the turning point, \nwith models such as CodeBERT and GraphCodeBERT being able to perform on a par with the best \nmethods in the problem categories of code understanding and generation [12, 13]. \nRecently, the development of large language models has further changed the scenery in code generation. \nFor instance, GPT-3 showed outstanding learning ability through a few examples for programming tasks \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n3\n \n[14], while Codex, the engine behind GitHub Copilot, scored highly in competitive programming \nproblems [15]. The following studies have looked into the models' capabilities in various programming \nlanguages and difficult algorithms [16, 17]. \nThe findings of empirical studies of AI-assisted coding tools presented a somewhat mixed picture. Ziegler \net al. indicated that developers using GitHub Copilot completed assignments on average 55.8% more \nquickly and reported higher satisfaction [18]. In contrast, Sandoval et al. indicated that code assemblages \nby AI showed significantly higher vulnerabilities, especially to the integrity of verification of input and \ncryptographic processes [19]. Perry et al. raised concerns about issues of misunderstanding in cloning \nobfuscated code in libraries; licensing problems in open-source projects; and the potential for unnoticed \nsubtle logical problems in open-source projects [20]. \nThe issue of how to assess and ensure the quality of AI-generated code remains an open question in \nresearch. Old-fashioned software measurement techniques have yielded different results depending on the \ntools and programming"
    },
    {
      "rank": 4,
      "distance_l2": 0.5911482572555542,
      "source_id": "PerformanceAnalysis2026",
      "chunk_id": "PerformanceAnalysis2026_chunk_015",
      "text": " has focused on evaluating the correctness and security \nof AI-generated code, performance regression has received less attention. However, intui­\ntively, AI-generated code models may not fully grasp the developer’s performance goals, \npotentially leading to code that prioritizes functionality over efficiency. On the other hand, \ntraining data for code generation models might not explicitly emphasize performance con­\nsiderations, impacting the models’ ability to generate efficient code. Given these potential \nshortcomings, it’s crucial to investigate the prevalence of performance regressions in AI-\ngenerated code. Understanding the scope of this issue will inform future research directions \nand development efforts for AI-assisted coding tools and code generation models.\nApproach  Our approach involves two strategies to evaluate the performance of AI-gener­\nated code. In particular, for static performance regression analysis, we employ industry-\nstandard tools, i.e., Spotbugs and PMD, to scan the generated Java code in the AixBench \ndataset. These tools are equipped with pre-defined rules that can detect performance regres­\nsions within the code. The detailed configuration of these rules is available in the replication \npackage we provided. For Python code in the HumanEval, MBPP, and EvalPerf datasets, \nwe use Qodana, a cloud-based static analysis platform, to identify potential performance \nregressions specific to Python code. To facilitate efficient analysis, we create new proj­\nects and establish a dedicated scan workflow within Qodana Cloud. This workflow enables \nQodana to automatically identify and highlight potential performance-related code issues \nwithin the generated Python code.\nFor dynamic performance regression analysis, we compare the generated code with \ncanonical solutions from the HumanEval and MBPP datasets. For the EvalPerf dataset, \nno explicit canonical solution is provided. Instead, each problem in EvalPerf is associated \nwith multiple reference implementations, each annotated with an efficiency score. Since a \n1 3\n   62 \n \nPage 12 of 52\nEmpirical Software Engineering           (2026) 31:62 \nhigher efficiency score indicates better performance (Liu et al. 2024a), we select the refer­\nence implementation with the highest efficiency score as the canonical solution for that \nproblem. This enables us to conduct performance regression analysis in a manner consistent \nwith the HumanEval and MBPP datasets. Using the dynamic performance regression detec­\ntion modules, we conduct dynamic performance regression analysis on the generated and \ncanonical code sets of these three datasets. The Python scripts generated for the HumanEval \nand MBPP datasets are typically short and have brief single-run execution times. To gener­\nate more robust performance data, we adopt a technique called repetitive iteration mea­\nsurement (Laaber and Leitner 2018; Ding et al. 2020; Jangali et al. 2023). This technique \nextends the runtime of the generated code by increasing the number of iterations within \nthe test cases, allowing profiling tools to capture more comprehensive performance data. \nWe achieve this extension by adding a for loop at the beginning of the test cases. This loop \ncauses the existing test cases to be executed repeatedly. Figure 5 illustrates this modifica­\ntion for the script shown in Fig. 4. Once the iterations have been increased, we encapsulate \neach"
    },
    {
      "rank": 5,
      "distance_l2": 0.6002597808837891,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 6,
      "distance_l2": 0.6441450119018555,
      "source_id": "CopilotQuality2022",
      "chunk_id": "CopilotQuality2022_chunk_002",
      "text": " validity, correctness, and efficiency. Our results suggest that\nGitHub Copilot was able to generate valid code with a 91.5% success\nrate. In terms of code correctness, out of 164 problems, 47 (28.7%)\nwere correctly, while 84 (51.2%) were partially correctly, and 33\n(20.1%) were incorrectly generated. Our empirical analysis shows\nthat GitHub Copilot is a promising tool based on the results we\nobtained, however further and more comprehensive assessment is\nneeded in the future.\nCCS CONCEPTS\n• Software and its engineering →Source code generation.\nKEYWORDS\nGitHub Copilot, code generation, code completion, AI pair program-\nmer, empirical study\nACM Reference Format:\nBurak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022. Assessing the Quality of\nGitHub Copilot’s Code Generation. In Proceedings of the 18th International\nConference on Predictive Models and Data Analytics in Software Engineering\n(PROMISE ’22), November 17, 2022, Singapore, Singapore. ACM, New York,\nNY, USA, 10 pages. https://doi.org/10.1145/3558489.3559072\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nPROMISE ’22, November 17, 2022, Singapore, Singapore\n© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9860-2/22/11...$15.00\nhttps://doi.org/10.1145/3558489.3559072\n1\nINTRODUCTION\nGitHub Copilot1 is a code generation tool that utilizes a variety of\ntechnologies, including a compatible IDE, and the OpenAI Codex\nModel2. GitHub Copilot can be installed and used as an extension to\nVisual Studio Code, Neovim, IDEs developed by JetBrains [6], and\nGitHub Codespaces3. The underlying service continuously takes\ncode samples from the users and sends the snippets to the under-\nlying OpenAI Codex Model. GitHub Copilot generates the code\nand presents the results of the OpenAI Codex Model by adjusting\nthe generated code to the current workspace of the programmer\n[4]. The Codex model relies on Generative Pre-trained Transformer\n(GPT) models that the company previously invented for text gen-\neration. The public code available on GitHub was used during the\nfine-tuning of the model to implement the code recognition and\ngeneration capabilities.\nThere are mixed reviews about the prospect of the GitHub Copi-\nlot. On the one hand, reducing development time, easing the devel-\nopment process by suggesting code for small utilities, and suggest-\ning better alternatives for code snippets are some of the positive\nfeedback developers provided [2, "
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}