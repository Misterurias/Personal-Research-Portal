{
  "timestamp": "2026-03-02T04:13:41+00:00",
  "query": "Do AI coding assistants increase long-term maintenance burden?",
  "retrieval_query": "AI coding assistants and long-term maintenance burden.",
  "query_rewritten": true,
  "rewritten_query": "AI coding assistants and long-term maintenance burden.",
  "top_k": 6,
  "answer": "I'll only answer based on the provided evidence chunks.\n\nPlease provide the chunk number or a specific question you'd like me to address. I'm ready when you are!",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.8379751443862915,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_002",
      "text": " code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n2\n \ncode completion capabilities [3].The acceptance of AI-assisted coding tools is getting faster, and it is \nrevealed by the latest industry surveys, which show that more than 65% of professional developers use AI \nsupport in some form as part of their daily routine [4]. These tools have been incorporated into the \ndevelopment processes of large tech companies that account for 30-50% productivity gains and have also \nreported significant time-to-market reductions for software products [5]. Nevertheless, the fast adoption \nof these tools has been so extensive that even empirical studies have not been able to catch up with their \nimplications on critical software engineering outcomes, like code quality, security, and long-term \nmaintainability, through rigorous research [6]. \nThe research gap is even more pronounced when the risk factors of AI-generated code are taken into \naccount. Initial experiments have pointed out issues such as security flaws, licensing confusion, and the \noccurrence of hidden bugs that will not be discovered during code reviewing process [7, 8]. Moreover, the \nimpact on the development of programmers' skills, particularly that of junior developers, who would \notherwise depend on AI-generated suggestions, is still unclear [9]. There being no empirical studies taking \na comprehensive approach to the assessment of these issues, the bottleneck of knowledge in software \nengineering research is in fact the multifaceted impacts of these issues. \nThis research paper fills this void by performing a large-scale controlled experiment whose main objective \nis to evaluate in a systematic way the impact of AI-assisted code generator tools on the three main \ndimensions: code quality, security vulnerability introduction, and developer productivity. Previous studies \nhave limited themselves to specific scenarios or synthetic benchmarks. On the contrary, our study will \ninvolve actual programming tasks in different languages and varying degrees of complexity, professional \ndevelopers of different skill and experience levels. Our assumption is that although the use of AI-assisted \ntools will increase productivity, at the same time, they might lead to the poor quality and insecure software \ndevelopment, which will need to be dealt with through proper industrial adoption strategies. \nResearch has contributed in three ways. To start with, the paper provided empirical evidence that through \nthe use of assistance from AI in code production, there was a significant impact on the software quality \nmetrics namely, cyclomatic complexity, maintainability index and code smell density. Next, the authors \nperformed a comprehensive examination of the security vulnerabilities related to AI-generated code in the \nvarious"
    },
    {
      "rank": 2,
      "distance_l2": 0.8445990085601807,
      "source_id": "AIProdDecrease2024",
      "chunk_id": "AIProdDecrease2024_chunk_027",
      "text": " the project. As a\nresult, maintainers are compelled to reallocate their time toward reviewing and managing code submissions\ninstead of writing new code.\n7.2.\nContributions and Future Research\nExtant research on AI pair programming has primarily emphasized productivity and efficiency gains, sug-\ngesting that tools such as GitHub Copilot can substantially accelerate software development (Peng et al.\n2023). While these benefits are evident in our data, our findings reveal a more nuanced set of consequences.\nIn particular, we show that AI-assisted programming also amplifies software maintenance challenges, espe-\ncially for core contributors who bear responsibility for code review and integration. Our individual-level\nanalysis demonstrates that while less-experienced contributors increase their output, experienced contrib-\nutors face higher review workloads and a concomitant decline in their own development activity. These\nresults highlight a redistribution of effort within OSS communities that has received limited attention in\nprior work.\nFrom a technical debt perspective, our findings suggest that AI-assisted programming alters the intertem-\nporal trade-off between short-term development speed and long-term maintainability. The shortcuts enabled\nby AI tools may accelerate the output while introducing code that is difficult to integrate, extend, or refactor.\nThe widespread use of AI-assisted pair programming - and, in extreme cases, “vibe coding” - can inject a\nlarger volume of difficult-to-maintain code (Pimenova et al. 2025, Fawzy et al. 2025) into software projects,\naccelerating the accumulation of technical debt. Such contributions create latent liabilities for projects, as\nmaintainers must later invest substantial effort to review, revise, or rewrite code to meet repository stan-\ndards. In this sense, AI does not merely increase the volume of contributions; it changes the composition of\nincoming code in ways that intensify technical debt accumulation.\nA key contribution of our study is to operationalize technical debt at its point of entry. We conceptual-\nize extensive PR rework as realized technical debt: the additional modification, coordination, and revision\n16 https://github.blog/news-insights/octoverse/octoverse-2024/\n24\neffort required to bring submitted code up to acceptable standards. This measure complements prior work\nthat captures technical debt through architectural metrics, defect accumulation, or long-run performance\noutcomes. By focusing on PR-level dynamics, we provide a micro-level view of how technical debt emerges\nin real time and how it is managed through ongoing maintenance effort.\nOur findings also raise concerns about the learning implications of AI-assisted development. With AI\nproviding rapid solutions, peripheral contributors may engage less deeply with underlying programming\nprinciples and best practices, resulting in code that is functional but brittle. This concern echoes evidence\nfrom other AI-augmented work settings, where less-experienced workers experience large productivity\ngains while more skilled workers see modest improvements and increased coordination burdens (Brynjolf-\nsson et al. 2025). In OSS settings, these dynamics can further worsen technical debt by weakening the\nfeedback loop between contribution and learning.\nThese insights point to several directions for future research. Scholars could examine how different\nproject governance mechanisms moderate AI-induced technical debt, such as automated testing, mod-\nular architectures, or formalized review protocols. Future work may also explore heterogeneity across\nproject types, identifying which OSS projects are most vulnerable to debt accumulation"
    },
    {
      "rank": 3,
      "distance_l2": 0.8633724451065063,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_013",
      "text": " we cannot conclude validity of other tools leveraged AI in different forms or \narchitectures. The 60-minute tasks did not allow us to explore the long-term ramifications of maintenance. \nThe sample of practitioners was drawn from Western technology companies and there are implications for \ngeneralizability among practice globally based on our sample. Additionally, the static analysis tools used \nas part of the scanning for a security vulnerability had important limitations, as discussed in several works, \nof being too discriminatory (false positives) and failing to positively identify classes of vulnerabilities that \ncan only be found using a dynamic analysis. Therefore, even with some of the limitations addressed by \nsome manual validation, we evaluated only the immediate introduction of a vulnerability and not the \nprocesses by which a vulnerability is subsequently identified and remediated once a code sample is placed \ninto production software, when investigating a security vulnerability. \n5.5 Future Research Directions \nNumerous important questions springing from our observations will require investigation. Longitudinal \nstudies depicting the development of AI-assisted codebases over extended periods of time will provide \nimportant insights into maintenance costs and the buildup of technical debt. Cross-comparative studies of \ndifferent AI tool implementations will also help clarify whether observed effects are associated with a \nspecific tool, or are more generalizable. Equally pressing, interventions also merit investigation, which \ncould include making AI models security-aware, humans-in-the-loop generation that performs automated \nsecurity-checks, enhanced developer training, or techniques of prompt engineering that pro-actively \nreduce the actual introduction of vulnerabilities. Interactions between AI tooling and developers' own \nskills should also be in steady inquiry, especially whether junior developers using AI tools are developing \nproblem-solving skills equivalent to developers going through training in the traditional sense. The \nanswers to such questions would be noteworthy for educational practice and hiring. Finally, we could \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n10\n \nconduct research that examines adaptive AI assistance that provides support at each stage based on \ndevelopers' competencies in order to move optimal learning trajectories forward, and that also assesses \nproductivity gains in relationship to skills development. \n \n6. Conclusion \nIn this study, we provide an in-depth data-driven investigation of AI-assisted code generation technologies \nthat demonstrate considerable productivity increases (i.e., an average of 31.4%) and, concerning, a total \nincrease in vulnerabilities of 23.7% and a total increase in critical severity of 89%. We also demonstrate \nthat some dimensions of code quality are improved with AI-assisted code generation tools (i.e., \nmaintainability, cyclomatic complexity) but caution is warranted with operational risks, to code itself (i.e. \nextra code duplication) and security vulnerabilities. We also examined differences across programming \nlanguages, and in particular, we found that while using AI-assisted code generation technologies is \nconstructive in Python, it warrants heightened caution around codex in C++ (to name only one). Finally, \nwhile we examined experience differences, we found that junior developers require support to prevent \nexcessive dependency on AI and senior developers could receive the maximum"
    },
    {
      "rank": 4,
      "distance_l2": 0.8716268539428711,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_008",
      "text": " record retrieval function that can handle missing record\nerrors in the Trio library. This task introduces concepts such as error handling and memory channels to\nstore results. These two tasks are standalone; we provide sufficient instructions and usage examples so that\nparticipants can complete one task without the other.\nWe used an online interview platform with an AI assistant chat interface (Figure 3) for our experiments.\nParticipants in the AI condition are prompted to use the AI assistant to help them complete the task. The\nbase model used for this assistant is GPT-4o, and the model is prompted to be an intelligent coding assistant.\nThe AI assistant has access to participants’ current version of the code and can produce the full, correct code\nfor both tasks directly when prompted.\n4.2\nEvaluation Design\nBased on a previous meta-analysis of evaluations in computer science education [Cheng et al., 2022], we\nidentify four types of questions used to assess the mastery of coding skills. Returning to our initial motivation\nof developing and retaining the skills required for supervising automation, proficiency in some of these areas\nmay be more important than others for the oversight of AI-generated code. The four types of questions we\nconsider are the following.\n• Debugging The ability to identify and diagnose errors in code. This skill is crucial for detecting when\nAI-generated code is incorrect and understanding why it fails.\n• Code Reading The ability to read and comprehend what code does. This skill enables humans to\nunderstand and verify AI-written code before deployment.\n• Code Writing The ability to write or pick the right way to write code. Low-level code writing, like\nremembering the syntax of functions, will be less important with further integration of AI coding tools\nthan high-level system design.\n• Conceptual The ability to understand the core principles behind tools and libraries. Conceptual\nunderstanding is critical to assess whether AI-generated code uses appropriate design patterns that\nadheres to how the library should be used.\nThe two tasks in our study cover 7 core concepts from the Trio library. We designed a quiz with debugging,\ncode reading, and conceptual questions that cover these 7 concepts. We exclude code writing questions to\nreduce the impact of syntax errors in our evaluation; these errors can be easily corrected with an AI query or\nweb search. We tested 5 versions (Table 2) of the quiz in user testing and preliminary studies based on item\nresponse theory. For example, we ensure that all questions are sufficiently correlated with the overall quiz\nscore, that each question has an appropriate average score, and that the questions are split up such that there\nis no local item dependence between questions (i.e., participants could not infer the answers to a question by\nlooking at other questions). The final evaluation we used contained 14 questions for a total of 27 points. We\nsubmitted the grading rubric for the quiz in our study pre-registration before running the experiment.\n4.3\nStudy Design\nWe use a between-subjects randomized experiment to test for the effects of using AI in the coding skill\nformation process. Each participant first completed a warm-up coding task on a coding platform, where they\nneeded to add a border around a list of strings. This Python coding question takes an average of 4 minutes to\ncomplete among users of this coding platform. There are no asynchronous concepts in this coding question.\n6\nFigure 4: Overview of learning task and"
    },
    {
      "rank": 5,
      "distance_l2": 0.8916847109794617,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_002",
      "text": " workers. As humans rely on AI for skills such as brainstorming,\nwriting, and general critical thinking, the development of these skills may be significantly altered depending\non how AI assistance is used.\nSoftware engineering, in particular, has been identified as a profession in which AI tools can be readily applied\nand AI assistance significantly improves productivity in daily tasks [Peng et al., 2023, Cui et al., 2024].\nJunior or novice workers, in particicular, benefit most from AI assistance when writing code. In high-stakes\napplications, AI written code may be debugged and tested by humans before a piece of software is ready\nfor deployment. This additional verification that enhances safety is only possible when human engineers\nthemselves have the skills to understand code and identify errors. As AI development progresses, the problem\nof supervising more and more capable AI systems becomes more difficult if humans have weaker abilities\n∗Work done as a part of the Anthropic Fellows Program, judy@anthropic.com\n†Anthropic, atamkin@anthropic.com\n1\narXiv:2601.20245v2  [cs.CY]  1 Feb 2026\nAI \nDelegation\nConceptual \nInquiry\nIterative AI \nDebugging\nHybrid \nCode-\nExplanation\nProgressive \nAI Reliance\nQuiz Score\nCompletion Time\n24min\n68%\nGeneration-\nThen-\nComprehension\n24min\n86%\n22min\n65%\n31min\n24%\n22min\n35%\n19.5min\n39%\nThe Impact of AI Assistance on Coding Speed and Knowledge Quiz\nAI Usage Patterns \nFigure 1: Overview of results: (Left) We find a significant decrease in library-specific skills (conceptual\nunderstanding, code reading, and debugging) among workers using AI assistance for completing tasks with a\nnew python library. (Right) We categorize AI usage patterns and found three high skill development patterns\nwhere participants stay cognitively engaged when using AI assistance.\nto understand code [Bowman et al., 2022]. When complex software tasks require human-AI collaboration,\nhumans still need to understand the basic concepts of code development even if their software skills are\ncomplementary to the strengths of AI [Wang et al., 2020]. The combination of persistent competency\nrequirements in high-stakes settings and demonstrated productivity gains from AI assistance makes software\nengineering an ideal testbed for studying how AI affects skill formation.\nWe investigate whether using and relying on AI affects the development of software engineering skills [Handa\net al., 2025]. Based on the rapid adoption of AI for software engineering, we are motivated by the scenario of\nengineers acquiring new skills on the job. Although the use of AI tools may improve productivity for these\nengineers, would they also inhibit skill formation? More specifically, does an AI-assisted task completion\nworkflow prevent engineers from gaining in-depth knowledge about the tools used to complete these tasks?\nWe run randomized experiments that measure skill formation by asking participants to complete coding\ntasks with a new library that they have not used before. This represents one way in which engineers acquire\nand learn new skills, since new libraries are frequently introduced in languages such as Python. We then\nevaluate their competency with the new library. Our main research questions are (1) whether AI improves\nproductivity for a coding task requiring"
    },
    {
      "rank": 6,
      "distance_l2": 0.899367094039917,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_017",
      "text": "2024).\nWith the rapid pace of advancement in AI, we anticipate some issues experienced by our users, especially regarding\nquality and speed, will naturally diminish as model performance increases. What won’t change is the need for human\ningenuity and insight to determine what software systems to build, even if the mechanics of how those systems are\nbuilt are increasingly aided by AI.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n11\nAcknowledgments\nWe thank Katharina Schippert and Robin Auer for their support in defining the WCA user research program and\nrecruiting participants for our studies. We also thank Keri Olson and Melissa Modjeski whose support made this research\npossible. Finally, we thank all of our users who provided us with valuable feedback.\nReferences\n[1] Ibrahim Adeshola and Adeola Praise Adepoju. 2023. The opportunities and challenges of ChatGPT in education. Interactive Learning Environments\n(2023), 1–14.\n[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation.\narXiv preprint arXiv:2103.06333 (2021).\n[3] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Narendra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin,\nQian Pan, Christine T Wolf, et al. 2021. Ai-assisted human labeling: Batching for efficiency without overreliance. Proceedings of the ACM on\nHuman-Computer Interaction 5, CSCW1 (2021), 1–27.\n[4] Zahra Ashktorab, Qian Pan, Werner Geyer, Michael Desmond, Marina Danilevsky, James M Johnson, Casey Dugan, and Michelle Bachman. 2024.\nEmerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions. arXiv\npreprint arXiv:2409.08937 (2024).\n[5] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with code-generating models.\nProceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111.\n[6] Brett A Becker, Michelle Craig, Paul Denny, Hieke Keuning, Natalie Kiesler, Juho Leinonen, Andrew Luxton-Reilly, James Prather, and Keith Quille.\n2023. Generative ai in introductory programming. Name of Journal (2023).\n[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language\nmodels be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610–623.\n[8] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 202"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}