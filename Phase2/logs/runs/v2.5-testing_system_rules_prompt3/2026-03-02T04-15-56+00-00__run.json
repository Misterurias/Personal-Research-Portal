{
  "timestamp": "2026-03-02T04:15:56+00:00",
  "query": "Are experienced developers more effective when using AI assistants?",
  "retrieval_query": "Here is a rewritten search query:\n\n\"effectiveness of developers using AI assistants by experience level\"",
  "query_rewritten": true,
  "rewritten_query": "Here is a rewritten search query:\n\n\"effectiveness of developers using AI assistants by experience level\"",
  "top_k": 6,
  "answer": "According to the provided evidence chunks, here are my answers:\n\n**Grounding constraints:**\n\n1. Does AI assistance improve task completion productivity when new skills are required?\n\nRQ1: Yes, AI assistance improves task completion productivity when new skills are required.\n\n2. What is the impact of AI code assistant on dev. productivity and experience in the enterprise?\n\nAccording to [EnterpriseImpact2024_chunk_010], there was an inherent suspicion against AI-generated code among respondents, but others found utility in using WCA to understand technical concepts without having to disturb their developer counterparts.\n\n3. How do developers use generated content from AI code assistants?\n\nAccording to [Use of generated content], users often modified outputs before using them or used them for another purpose, such as learning something new or getting new ideas. The generated results helped users recall forgotten concepts and aided them when they knew what to do but didn't know how.\n\n4. What is the impact of AI code assistant on productivity?\n\nAccording to [Impact on productivity], respondents felt that WCA made their work easier, of a better quality, and faster.\n\n**Additional insights:**\n\n1. How do developers perceive the usefulness of AI-generated code in their workflows?\n\nAccording to [R1.57], \"My favorite feature is to understand the technical terms and code provided by the Dev. This reduces the time in understanding the API terms and code rather than discussing with the Dev.\"",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.8815375566482544,
      "source_id": "NoviceProgramming2024",
      "chunk_id": "NoviceProgramming2024_chunk_016",
      "text": " levels.\nThe significant findings from the Friedman test suggest that not only did students’\nsatisfaction improve over time, but the quality of their interactions with AI tools also likely\nimproved. Since the same AI model was used throughout the course, this trend indicates\nthat students became more adept at prompt engineering, enabling them to extract better\nresults from the AI tools for any topic, not only programming. Additionally, a majority\nof students reported switching to English as their prompt language, which could also\nprovide better results. These findings open up several areas for further discussion. First,\nthe improvement in satisfaction over time underscores the importance of practice and\nEduc. Sci. 2024, 14, 1089\n11 of 17\nexperience when using AI tools. As students became more proficient with the capabilities\nand limitations of these tools, they were able to craft better prompts, leading to more\nsatisfactory outcomes. This suggests that prompt engineering is a critical skill that can be\ndeveloped over time and should be an integral part of AI education. Therefore, teaching\nstudents how to effectively interact with AI tools can enhance their learning experiences, a\npoint that has not been extensively addressed in the prior literature.\nThe analysis of the fourth research question reveals a diverse range of applications\nfor AI tools in an introductory programming course. These findings suggest that novice\nprogramming teams are leveraging AI tools primarily to optimize and enhance their\ncoding processes, particularly in areas that require repetitive or detailed work, such as\ncommenting and debugging. The varied use of AI tools across different tasks also highlights\ntheir versatility and the teams’ growing confidence in incorporating these tools into various\naspects of their assignments as the course progresses.\nThe data indicates that, as the course demands increased, so did the reliance on\nAI tools for more complex tasks, reflecting a deeper integration of these tools into the\nprogramming workflow. This underscores the importance of AI tools as a valuable resource\nin programming education, particularly for tasks that can support learning and improve\ncode quality. However, it also raises questions about the potential for over-reliance and the\nneed to balance AI tool usage with the development of fundamental programming skills.\nThe analysis of the fifth research question, which explored the common benefits and\nconcerns students have regarding the use of AI tools in their studies, revealed a complex\ninterplay of advantages and challenges experienced by novice programmers. The most\nfrequently cited benefit was the provision of instant help and quick answers or explana-\ntions, mentioned by 79.4% of teams (58 teams). This was closely followed by assistance\nwith repetitive tasks, such as writing comments, reported by 75.3% of teams (55 teams).\nThis means that AI tools can minimize extraneous cognitive load by automating routine\ntasks, thereby enabling students to focus more effectively on mastering core programming\nprinciples. Immediate response and feedback from AI tools were valued by 57.5% of\nteams (42 teams), while help with identifying and correcting errors was acknowledged\nby 54.8% of teams (40 teams). Other notable benefits included access to diverse resources\nand additional examples (45.21%), spelling correction (23.3%), and translation between\nlanguages (19.2%). These findings are in line with Malinka et al. [22], who suggested\nthat ChatGPT could serve as a valuable aid in discussing challenges encountered during\nassignments"
    },
    {
      "rank": 2,
      "distance_l2": 0.8900366425514221,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    },
    {
      "rank": 3,
      "distance_l2": 0.9138274192810059,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_004",
      "text": " et al., 2024, Shen and\nGuestrin, 2025]. For example, AI tools are being used in professional domains such as software development,\neducation, design, and the sciences [Handa et al., 2025].\nProductivity Gains\nMany studies have found improvements in productivity using these AI assistants.\nFor example, Brynjolfsson et al. found that AI-based conversational assistants increased the number of\nissues call center workers were able to resolve on average by 15%.\nDell’Acqua et al. find similar results\nin which consultants completed 12.2% more tasks on average with the help of AI than without it. While\nthe skill-based effects differ across studies, a consistent pattern emerges in call center work, consulting, legal\nquestion-answering, and writing: less experienced and lower-skilled workers tend to benefit most [Brynjolfsson\net al., 2025, Dell’Acqua et al., 2023, Choi and Schwarcz, 2023, Noy and Zhang, 2023]. One exception was\nwhen GPT-4 was given to Kenyan small business owners, AI business advice helped high performers (by\nrevenue) improve business results while worsening the results for lower performers [Otis et al., 2024].\nFor software engineering in particular, Peng et al. found that crowd-sourced software developers using copilot\ncompleted a task 55.5% faster than the control group and novice programmers benefited more from AI coding\nassistance. Follow-up studies of developers in major software companies and found that AI-generated code\ncompletions provide a 26. 8% boost in productivity as measured by pull requests, commits, and software\nproduct builds [Cui et al., 2024]. This study also found that less experienced coders experienced greater\nboosts in productivity. While studies find that junior or less experienced developers experience greater\nproductivity uplift from using AI, these very same workers should be quickly developing new skills in the\nworkplace. Yet the effect of these tools on the skill formation of this subgroup remains unknown. Will the\nskill development of novice workers be affected significantly since they are still in the process of learning their\ntrade? We are motivated by whether this productivity comes from free or at a cost.\nCognitive Offloading\nConcerns around the impact of AI assistance and skill depletion have been highlighted\nby recent works. For example, medical professionals trained with AI assistance might not develop keen visual\nskills to identify certain conditions [Macnamara et al., 2024]. In surveys given to knowledge workers, frequent\nuse of AI has been associated with worse critical thinking abilities and increased cognitive offloading [Gerlich,\n2025]. Furthermore, knowledge workers reported a lower cognitive effort and confidence when using generative\nAI tools [Lee et al., 2025]. However, these surveys are observational and may not capture the causal effects of\nAI usage.\nSkill Retention\nAn adjacent line of inquiry to our research is how well humans retain knowledge and\nskills after AI assistance. Wu et al. find that even when generative AI improved immediate performance on\ncontent creation tasks (e.g., writing a Facebook post, writing a performance review, drafting a welcoming\nemail), the performance increase did not persist in subsequent tasks performed independently by humans\nafterward. For data science tasks"
    },
    {
      "rank": 4,
      "distance_l2": 0.9144132137298584,
      "source_id": "SkillFormation2026",
      "chunk_id": "SkillFormation2026_chunk_006",
      "text": " code, fix\nerrors, and explain algorithmic concepts; students with less coding proficiency were more likely to seek AI\nassistance. Other works use surveys to find that students may be hesitant to use AI coding assistant tools\ndue to “dependence worry” (i.e., overreliance on coding tools) [Pan et al., 2024]. For formal methods, Prasad\net al. coded the different ways in which students used LLMs for course work and found that upper-year\nstudents taking the class did not rely on LLM assistance and only asked a few questions at the beginning.\nUser studies have also been conducted in the professional development environments. Wang et al. study\ndifferent patterns in usage between users with and without chat access to AI models in completing coding\npuzzles and development tasks. They found rich interaction patterns including interactive debugging, code\ndiscussions, and asking specific questions. Participants ranged from asking ChatGPT to do then the entire\nproblem (lowest quality code output) to only asking minimal questions (highest efficiency). Other studies\nhave reported that AI tools help the software development process through easier access to documentation\nand accurate generation code for specific APIs [Pinto et al., 2024].\n3\nFramework\nProfessional Skill Acquisition\nThe “learning by doing” philosophy has been suggested by many learning\nframeworks such as the Kolb’s experiential learning cycle, and the Problem-Based Learning (PBL) [Kolb,\n2014, Schmidt, 1994]. The frameworks connect the completion of real-world tasks with the learning of\nnew concepts and the development of new skills. Experiential learning has also been explored specifically\nin software engineering courses in higher education in order to mimic solving problems in a professional\nsetting [Gonzalez-Huerta et al., 2020]. In its simplest form, we model AI tool assistance as taking a different\nlearning path than without AI. We hypothesize that using AI tools to generate code in the development\nprocess effectively amounts to taking a shortcut to task completion without a pronounced learning stage.\nAI for Coding Usage Patterns\nPrior works have found that humans use AI in many different ways\nfor coding: from question answering to writing code, to debugging [Poitras et al., 2024, Wang et al., 2020,\nPinto et al., 2024]. In our framework, different ways of using AI assistance represent different learning paths\ntaken to reach the goals of completing the task. We analyze these different usage patterns in the qualitative\nanalysis of this work (Section 6).\nResearch Questions\nBased on this background, we focus on on-the-job learning: settings where workers\nmust acquire new skills to complete tasks. We seek to understand both the impact of AI on productivity\nand skill formation. We ask whether AI assistance presents a tradeoff between immediate productivity and\nlonger-term skill development or if AI assistance presents a shortcut to enhance both. Our research questions\nare as follows:\n• RQ1: Does AI assistance improve task completion productivity when new skills are required?\n4\nWith AI Assistance\nNovice \nWorker\nLearning\nTask \nCompletion\nWithout AI Assistance\nFigure 2: With AI assistance becoming more ubiquitous in the workplace, novice workers may complete tasks\nwithout the same learning outcomes. Our experiments aim to investigate the process of task completion\nrequiring a new skill to understand the impact of AI assistance on"
    },
    {
      "rank": 5,
      "distance_l2": 0.9356217384338379,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_017",
      "text": "2024).\nWith the rapid pace of advancement in AI, we anticipate some issues experienced by our users, especially regarding\nquality and speed, will naturally diminish as model performance increases. What won’t change is the need for human\ningenuity and insight to determine what software systems to build, even if the mechanics of how those systems are\nbuilt are increasingly aided by AI.\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n11\nAcknowledgments\nWe thank Katharina Schippert and Robin Auer for their support in defining the WCA user research program and\nrecruiting participants for our studies. We also thank Keri Olson and Melissa Modjeski whose support made this research\npossible. Finally, we thank all of our users who provided us with valuable feedback.\nReferences\n[1] Ibrahim Adeshola and Adeola Praise Adepoju. 2023. The opportunities and challenges of ChatGPT in education. Interactive Learning Environments\n(2023), 1–14.\n[2] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021. Unified pre-training for program understanding and generation.\narXiv preprint arXiv:2103.06333 (2021).\n[3] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Narendra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin,\nQian Pan, Christine T Wolf, et al. 2021. Ai-assisted human labeling: Batching for efficiency without overreliance. Proceedings of the ACM on\nHuman-Computer Interaction 5, CSCW1 (2021), 1–27.\n[4] Zahra Ashktorab, Qian Pan, Werner Geyer, Michael Desmond, Marina Danilevsky, James M Johnson, Casey Dugan, and Michelle Bachman. 2024.\nEmerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions. arXiv\npreprint arXiv:2409.08937 (2024).\n[5] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023. Grounded copilot: How programmers interact with code-generating models.\nProceedings of the ACM on Programming Languages 7, OOPSLA1 (2023), 85–111.\n[6] Brett A Becker, Michelle Craig, Paul Denny, Hieke Keuning, Natalie Kiesler, Juho Leinonen, Andrew Luxton-Reilly, James Prather, and Keith Quille.\n2023. Generative ai in introductory programming. Name of Journal (2023).\n[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language\nmodels be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 610–623.\n[8] Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z Gajos. 202"
    },
    {
      "rank": 6,
      "distance_l2": 0.9411985278129578,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_010",
      "text": " tool is so new and very few people on my team use it. There is an inherent suspicion against\nAI-generated code.” Although only two respondents raised this issue, it may be important for organizational leaders to\nfoster a culture in which AI assistance is viewed as acceptable to garner wide-spread adoption.\nFinally, we discovered a small group of technical writers who found utility in using WCA to understand technical\nconcepts without having to disturb their developer counterparts. R1.57 remarked, “My favorite feature is to understand\nthe technical terms and code provided by the Dev. This reduces the time in understanding the API terms and code rather\nManuscript submitted to ACM\nExamining the Use and Impact of an AI Code Assistant on Dev. Productivity and Experience in the Enterprise\n7\nthan discussing with the Dev.” Multiple respondents desired using WCA to “create customer facing documentation”\n(R1.27) and “help me writ[e] drafts following IBM content guidelines.” (R1.68). These “off-label” use cases by people in\ndeveloper-adjacent roles surprised us and indicate the importance of taking a holistic view on the potential beneficiaries\nof AI code assistants.\n4.2\nUse of generated content\nRespondents reported using generated outputs – code, documentation, unit tests, and explanations – in different ways.\nOverall, the use of generated outputs without modification was not common (2-4% of respondents reported doing\nthis, depending on output type); rather, respondents often modified outputs before using them (9-19%) or used them\nfor another purpose, such as learning something new (23-35%) or getting new ideas (24-37%). Users described how\n“the results give me new ideas” (R2.180) and “It is very helpful to get started writing code in a new language” (R2.626)\nby “recommend[ing] an approach I haven’t thought of or I wasn’t even aware of” (R2.405). P1 described how “creating\ndiagrams in markdown works from the code.” Users also talked about how WCA helped them recall “concepts which may\nbe I have forgotten during [the] course of time” (R2.296) and aiding them when “[I] know what to do, but don’t know how\nto do it or forgot about that.” (R2.292). These uses reinforce the value that generative AI provides in helping people\nlearn [1, 6, 49, 59].\n4.3\nImpact on productivity\nWe evaluated the impact of WCA on respondents’ perceptions of productivity using multiple measures: 7-point semantic\ndifferential scales14 that assessed effort, quality, and speed [53] and a 4-item scale of self-efficacy15 (derived from [44]).\nOverall, respondents felt that WCA made their work easier (M (SD) = .78 (1.45), t(609) = 13.35, p < .001, 95% CI =\n[.67, .90]), of a better quality (M (SD) = .66 (1.25), t(603) = 13.02, p < .001, 95% CI = [.56, .76]), and faster (M (SD) = .57\n(1.48), t(606) = 9.57, p"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}