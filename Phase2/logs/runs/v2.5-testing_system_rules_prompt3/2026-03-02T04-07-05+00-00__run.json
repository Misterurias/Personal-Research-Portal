{
  "timestamp": "2026-03-02T04:07:05+00:00",
  "query": "What empirical evidence measures productivity gains from AI coding tools?",
  "retrieval_query": "empirical studies measuring productivity gains from AI coding tools",
  "query_rewritten": true,
  "rewritten_query": "empirical studies measuring productivity gains from AI coding tools",
  "top_k": 6,
  "answer": "According to the evidence chunks provided, the answers to your question would be:\n\n1. The use of AI-assisted code generation tools lead to a 31.4% increase in average developer productivity.\n\n2. However, 23.7% more security vulnerabilities were introduced in the codes generated.\n\n3. Code maintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%.\n\n4. The variations in programming languages were significant, with Python being the one that realized the highest quality improvement (26.3%) and C++ the one that faced the most security risk increase (34.8%).\n\n5. In a controlled experiment with GitHub Copilot, an AI pair programmer, recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group.\n\nNote: These answers are based on the provided evidence chunks only and may not reflect a comprehensive understanding of the topic.",
  "citations": [],
  "citations_valid": true,
  "invalid_citations": [],
  "references_consistent": false,
  "retrieved": [
    {
      "rank": 1,
      "distance_l2": 0.6680762767791748,
      "source_id": "EnterpriseImpact2024",
      "chunk_id": "EnterpriseImpact2024_chunk_004",
      "text": " or the presence of errors or defects [26]. However, Meyer et al. [34]\nconsiders “when software developers perceive themselves to be productive and... unproductive” [34, p.1] as an important\naspect of productivity.\nCheng et al. [11] outline a number of subjective and objective factors that impact developer productivity, including\ncode quality, technical debt, infrastructure tools and support, team communication, and organizational changes and\nprocesses. In addition, researchers have found correlations between subjective and objective productivity metrics, such\nas the acceptance rate of suggested code [62] and the number of source code files owned by a developer [40] being\ncorrelated with perceived productivity.\nThe comprehensive landscape of software engineering productivity is captured by the SPACE framework [17], which\noutlines both objective and subjective metrics across individuals, teams, and organizations. In this paper, we focus on\nattitudinal and human-centered measures of productivity such as self-efficacy [44] and the impact of AI on the work\nprocess [53].\n2.3\nImpact of LLM-based assistants on developer productivity\nMany studies have been conducted to examine the impact of LLM-based coding assistants on various aspects of\nproductivity, albeit with mixed results [13, 23, 27, 38, 41, 44, 50, 53, 54, 58, 60–62]. One early study by Weisz et al. [53]\nexamined AI-assisted code translation and found a net benefit to working with AI, though that benefit was not equally\nexperienced by all participants. Kuttal et al. [27] examined human-human and human-AI pair-programming teams but\ndid not find strong differences in outcomes such as productivity, code quality, or self-efficacy.\nZiegler et al. examined the impact of GitHub Copilot on developer productivity [61, 62] and found that developers\nwho used the tool self-reported higher levels of productivity. Contrarily, studies by Imai [23] and GitClear [18] both\nsuggest that the quality of the code produced by GitHub Copilot may be harming productivity due to the number of\nlines that must be changed or deleted.\nAnother consideration for AI code assistants is their impact on the work process. Both Barke et al. [5] and Liang\net al. [28] identified two complementary types of usage of GitHub Copilot: “acceleration mode” in which the tool aided\ndevelopers when they knew what to do next, and “exploration mode” to help developers brainstorm potential solutions\nto coding problems when they were unsure of how to proceed.\n3\nCase Study of an AI Code Assistant\nIBM’s watsonx Code Assistant (WCA) is family of software engineering assistants that supports enterprise-specific\nuse cases including IT automation8 and mainframe application modernization9. In mid-2024, a new variant of WCA,\nknown as “WCA@IBM,” was released internally within IBM and was rapidly adopted by over 12,000 IBM developers.\nThis variant provided general programming assistance in languages including Python, Java, JavaScript, C++, and more.\nIt was implemented as plugins to VSCode and Eclipse, and it supported code generation from natural language, code\nautocompletion, code explanation and documentation, unit test generation, and conversational Q&A.\n8IBM watson"
    },
    {
      "rank": 2,
      "distance_l2": 0.6941633224487305,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_001",
      "text": "CODE-COMPLETION SYSTEMS OFFERING suggestions \nto a developer in their integrated development \nenvironment (IDE) have become the most frequently \nused kind of programmer assistance.1 When \ngenerating whole snippets of code, they typically use \na large language model (LLM) to predict what the user \nmight type next (the completion) from the context of \nwhat they are working on at the moment (the prompt).2 \nThis system allows for completions at any position in \nMeasuring \nGitHub \nCopilot’s \nImpact on \nProductivity\nDOI:10.1145/3633453\nCase study asks Copilot users about its impact \non their productivity, and seeks to find their \nperceptions mirrored in user data.\nBY ALBERT ZIEGLER, EIRINI KALLIAMVAKOU, X. ALICE LI, \nANDREW RICE, DEVON RIFKIN, SHAWN SIMISTER, \nGANESH SITTAMPALAM, AND EDWARD AFTANDILIAN\n key insights\n\t\n˽ AI pair-programming tools such as GitHub \nCopilot have a big impact on developer \nproductivity. This holds for developers \nof all skill levels, with junior developers \nseeing the largest gains.\n\t\n˽ The reported benefits of receiving AI \nsuggestions while coding span the full \nrange of typically investigated aspects of \nproductivity, such as task time, product \nquality, cognitive load, enjoyment, and \nlearning.\n\t\n˽ Perceived productivity gains are reflected \nin objective measurements of developer \nactivity.\n\t\n˽ While suggestion correctness is \nimportant, the driving factor for these \nimprovements appears to be not \ncorrectness as such, but whether the \nsuggestions are useful as a starting point \nfor further development.\n54    COMMUNICATIONS OF THE ACM  |  MARCH 2024  |  VOL. 67  |  NO. 3\nresearch\nthe code, often spanning multiple \nlines at once.\nPotential benefits of generating \nlarge sections of code automatically \nare huge, but evaluating these sys­\ntems is challenging. Offline evalua­\ntion, where the system is shown a par­\ntial snippet of code and then asked \nto complete it, is difficult not least \nbecause for longer completions there \nare many acceptable alternatives and \nno straightforward mechanism for \nlabeling them automatically.5 An ad­\nditional step taken by some research­\ners3,21,29 is to use online evaluation \nand track the frequency of real us­\ners accepting suggestions, assuming \nthat the more contributions a system \nmakes to the developer’s code, the \nhigher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways"
    },
    {
      "rank": 3,
      "distance_l2": 0.7046757936477661,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_015",
      "text": " Human \nFactors in Computing Systems, 1-23. \n4. Kalliamvakou, E., Bird, C., Zimmermann, T., et al. (2024). The impact of AI on developer \nproductivity: Evidence from GitHub Copilot. IEEE Software, 41(3), 34-42. \n5. Peng, S., Kalliamvakou, E., Cihon, P., & Demirer, M. (2023). The impact of AI on developer \nproductivity: Findings from a study of GitHub Copilot. arXiv preprint arXiv:2302.06590. \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n11\n \n6. Sarkar, A., Ross, N. A., Anantharaman, V., et al. (2024). What is it like to program with artificial \nintelligence? Proceedings of the ACM Conference on Computer-Supported Cooperative Work, 156-\n189. \n7. Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023). Do users write more insecure code with AI \nassistants? ACM Conference on Computer and Communications Security, 2785-2799. \n8. Asare, O., Nagappan, M., & Asokan, N. (2023). Is GitHub Copilot a substitute for human pair-\nprogramming? An empirical study. ACM Transactions on Software Engineering, 49(2), 1-34. \n9. Dakhel, A. M., Majdinasab, V., Nikanjam, A., et al. (2023). GitHub Copilot AI pair programmer: \nAsset or liability? Journal of Systems and Software, 203, 111734. \n10. Allamanis, M., Brockschmidt, M., & Khademi, M. (2018). Learning to represent programs with \ngraphs. International Conference on Learning Representations, 1-16. \n11. Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and \nnatural languages. Findings of the Association for Computational Linguistics: EMNLP 2020, 1536-\n1547. \n12. Guo, D., Ren, S., Lu, S., et al. (2021). GraphCodeBERT: Pre-training code representations with data \nflow. International Conference on Learning Representations, 1-18. \n13. Wang, Y., Wang, W., Joty, S., & Hoi, S. C. H. (2021). CodeT5: Identifier-aware unified pre-trained \nencoder-decoder models for code understanding and generation. Proceedings of the 2021 Conference \non Empirical Methods in Natural Language Processing, 8696-8708. \n14. Brown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. Advances in \nNeural Information Processing Systems,"
    },
    {
      "rank": 4,
      "distance_l2": 0.7124300599098206,
      "source_id": "CopilotCACM2022",
      "chunk_id": "CopilotCACM2022_chunk_002",
      "text": "higher its benefit. The validity of this \nassumption is not obvious when con­\nsidering issues such as whether two \nshort completions are more valuable \nthan one long one, or whether review­\ning suggestions can be detrimental to \nprogramming flow.\nCode completion in IDEs using lan­\nguage models was first proposed in \nHindle et al.,9 and today neural syn­\nthesis tools such as GitHub Copilot, \nCodeWhisperer, and TabNine suggest \ncode snippets within an IDE with the \nexplicitly stated intention to increase \na user’s productivity. Developer pro­\nductivity has many aspects, and a re­\ncent study has shown that tools like \nthese are helpful in ways that are only \npartially reflected by measures such \nas completion times for standardized \ntasks.23,a Alternatively, we can leverage \nthe developers themselves as expert \nassessors of their own productivity. \nThis meshes well with current think­\ning in software engineering research \nsuggesting \nmeasuring \nproductiv­\nity on multiple dimensions and using \nself-reported data.6 Thus, we focus on \nstudying perceived productivity.\nHere, we investigate whether usage \nmeasurements of developer interac­\ntions with GitHub Copilot can predict \nperceived productivity as reported \nby developers. We analyze ​2,631​ sur­\na\t Nevertheless, such completion times are \ngreatly reduced in many settings, often by \nmore than half.16\nMARCH 2024  |  VOL. 67  |  NO. 3  |  COMMUNICATIONS OF THE ACM    55\nILLUSTRATION BY JUSTIN METZ\nresearch\nFigure 1. GitHub Copilot’s code completion funnel.\nFigure 2. Demographic composition of survey respondents.\nThink of \nthe language \nyou have used \nthe most with \nOurTool. \nHow proﬁcient\nare you in \nthat language?\nBeginner\nIntermediate\nAdvanced\nStudent/Learning\n0–2 Years Prof. Experience\n3–5 Years Prof. Experience\n6–10 Years Prof. Experience\n11–15 Years Prof. Experience\n16+ Years Prof. Experience\nStudent\nProfessional\nHobbyist\nConsultant/Freelancer\nResearcher\nOther\nPython\nJavaScript\nTypeScript\nJava\nRuby\nGo\nC#\nRust\nHTML\nOther\nWhich best\ndescribes your\nprogramming\nexperience?\nWhich of \nthe following \nbest describes \nwhat you do?\nWhat\nprogramming\nlanguages\ndo you usually \nuse? Choose\nup to three\nfrom the list.\n0%\n25%\n50%\n75%\n100%\nAverage number of events per survey user active hour\n50\n170\nmostly unchanged\nCompletion\nunchanged\n40\n30\n20\n24\n6.6\ncompletion\nopportunity\ncompletion\nshown\ncompletion\naccepted\nafter 30\nseconds\nafter 2\nminutes\nafter 5\nminutes\nafter 10\nminutes\n5.6\n4.3\n5.3\n5.1\n5\n3.1\n3.4\n3.8\n10\n0\nvey responses from developers using \nGitHub Copilot and match their re­\nsponses to measurements collected \nfrom the IDE. We consider acceptance \ncounts and more detailed measures \nof contribution, such as the amount"
    },
    {
      "rank": 5,
      "distance_l2": 0.7203953862190247,
      "source_id": "EmpiricalToolAnalysis2025",
      "chunk_id": "EmpiricalToolAnalysis2025_chunk_001",
      "text": "International Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website: www.ijfmr.com       ●   Email: editor@ijfmr.com \n \nIJFMR250661350 \nVolume 7, Issue 6, November-December 2025 \n1\n \nEmpirical Analysis of AI-Assisted Code \nGeneration Tools: Impact on Code Quality, \nSecurity and Developer Productivity \n \nMrs. Purvi Sankhe1, Dr. Neeta Patil2, Mrs. Minakshi Ghorpade 3,  \nMrs. Pratibha Prasad4, Mrs. Monisha Linkesh5 \n \n2Associate Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n1,3,4,5Assistant Professor, IT Department, Thakur College of Engineering and Technology, Mumbai India \n \nAbstract \nAI-assisted code generation tools have been the main cause of the increase in practices like code \ncompletion, bug fixing, and documentation among developers. However, the main concern regarding their \neffects on code quality, security vulnerabilities, and developer productivity still lacks empirical evidence. \nObjective: This study conducts an empirical assessment of the AI-assisted code generation tools' \neffectiveness in terms of software quality metrics, security vulnerability introduction, and developer \nproductivity, depending on the programming languages and project complexities. Methodology: A \ncontrolled experiment was performed with 120 professional developers where they were divided into \nexperimental and control groups and 480 code modules were analyzed among Python, Java, JavaScript, \nand C++ projects. Cyclomatic complexity, maintainability index, and code smell density were the three \nparameters for measuring code quality. Static analysis tools were employed in the evaluation of security \nvulnerabilities, while productivity was gauged through measuring task completion time and conducting \ncognitive load surveys. Results: The use of AI-assistive tools lead to a 31.4% increase in average developer \nproductivity; however, 23.7% more security vulnerabilities were introduced in the codes generated. Code \nmaintainability went up 18.2%, while cyclomatic complexity decreased by 14.6%. The variations in \nprogramming languages were significant, with Python being the one that realized the highest quality \nimprovement (26.3%) and C++ the one that faced the most security risk increase (34.8%). \n \nKeywords: Large language models, Software security, Static code analysis, Cyclomatic complexity. \n \n1. Introduction \nThe software engineering landscape has been drastically changed by the integration of artificial \nintelligence and machine learning technologies into development environments. AI-assisted code \ngeneration tools, which are based on huge language models that have been trained with billions of lines \nof code, have been identified as the most powerful of the innovative technologies that will significantly \ncontribute to the developer's productivity, lessening of cognitive burden, and speeding up of software \ndelivery cycles [1, 2]. In this manner interaction with such tools as GitHub Copilot, Amazon \nCodeWhisperer, and ChatGPT-based coding assistants radically changes the way developers write and \nmaintain software since they all provide real-time code suggestions, automated bug fixes, and intelligent \nInternational Journal for Multidisciplinary Research (IJFMR) \n \nE-ISSN: 2582-2160   ●   Website:"
    },
    {
      "rank": 6,
      "distance_l2": 0.7255871295928955,
      "source_id": "CopilotExperiment2023",
      "chunk_id": "CopilotExperiment2023_chunk_001",
      "text": "The Impact of AI on Developer Productivity:\nEvidence from GitHub Copilot\nSida Peng,1∗Eirini Kalliamvakou,2 Peter Cihon,2 Mert Demirer3\n1Microsoft Research, 14820 NE 36th St, Redmond, USA\n2GitHub Inc., 88 Colin P Kelly Jr St, San Francisco, USA\n3MIT Sloan School of Management, 100 Main Street Cambridge, USA\n∗To whom correspondence should be addressed; E-mail: sidpeng@microsoft.com.\nAbstract\nGenerative AI tools hold promise to increase human productivity. This paper presents re-\nsults from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited\nsoftware developers were asked to implement an HTTP server in JavaScript as quickly as\npossible. The treatment group, with access to the AI pair programmer, completed the task\n55.8% faster than the control group. Observed heterogenous effects show promise for AI\npair programmers to help people transition into software development careers.\nIntroduction\nArtiﬁcial intelligence (AI) applications hold promise to increase human productivity. A va-\nriety of AI models have demonstrated human-level capabilities in ﬁelds ranging from natural\nlanguage understanding to image recognition [Zhang et al., 2022]. As these systems are de-\nployed in the real-world, how do they change labor productivity? While there is a growing\nliterature studying perceptions of AI tools, how people use them, and their implications for\nsecurity and education [Nguyen and Nadi, 2022, Barke et al., 2022, Finnie-Ansley et al., 2022,\nSandoval et al., 2022] there has been little research on productivity impacts of AI-powered tools\n1\narXiv:2302.06590v1  [cs.SE]  13 Feb 2023\nin professional contexts, cf. [Mozannar et al., 2022, Vaithilingam et al., 2022, Ziegler et al., 2022].\nThe potential productivity impacts of AI have major implications for the labor market and\nﬁrms, including changes in employment, skills, and ﬁrm organization [Raj and Seamans, 2018,\nAgrawal et al., 2019].\nThis paper studies the productivity effects of AI tools on software development. We present\na controlled trial of GitHub Copilot, an AI pair programmer that suggests code and entire func-\ntions in real time based on context. GitHub Copilot is powered by OpenAI’s generative AI\nmodel, Codex [Chen et al., 2021]. In the trial, programmers were tasked and incentivized to\nimplement an HTTP server in JavaScript as quickly as possible. The treated group had access\nto GitHub Copilot and watched a brief video explaining how to use the tool. The control group\ndid not have access to GitHub Copilot but was otherwise unconstrained, i.e., they were free to\nuse internet search and Stack Overﬂow to complete the task.\nThe performance difference between treated and control groups are statistically and practi-\ncally signiﬁcant: the treated group completed the task 55.8% faster (95% conﬁdence interval:\n21-89%). Developers with less programming"
    }
  ],
  "embed_model": "sentence-transformers/all-MiniLM-L6-v2",
  "ollama_model": "llama3",
  "prompt_version": "v2.5-testing_system_rules_prompt3"
}